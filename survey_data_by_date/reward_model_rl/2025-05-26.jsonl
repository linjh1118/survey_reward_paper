{"id": "2505.17352", "pdf": "https://arxiv.org/pdf/2505.17352", "abs": "https://arxiv.org/abs/2505.17352", "authors": ["Preeti Lamba", "Kiran Ravish", "Ankita Kushwaha", "Pawan Kumar"], "title": "Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have emerged as leading generative models for images and\nother modalities, but aligning their outputs with human preferences and safety\nconstraints remains a critical challenge. This thesis proposal investigates\nmethods to align diffusion models using reinforcement learning (RL) and reward\nmodeling. We survey recent advances in fine-tuning text-to-image diffusion\nmodels with human feedback, including reinforcement learning from human and AI\nfeedback, direct preference optimization, and differentiable reward approaches.\nWe classify these methods based on the type of feedback (human, automated,\nbinary or ranked preferences), the fine-tuning technique (policy gradient,\nreward-weighted likelihood, direct backpropagation, etc.), and their efficiency\nand safety outcomes. We compare key algorithms and frameworks, highlighting how\nthey improve alignment with user intent or safety standards, and discuss\ninter-relationships such as how newer methods build on or diverge from earlier\nones. Based on the survey, we identify five promising research directions for\nthe next two years: (1) multi-objective alignment with combined rewards, (2)\nefficient human feedback usage and active learning, (3) robust safety alignment\nagainst adversarial inputs, (4) continual and online alignment of diffusion\nmodels, and (5) interpretable and trustworthy reward modeling for generative\nimages. Each direction is elaborated with its problem statement, challenges,\nrelated work, and a proposed research plan. The proposal is organized as a\ncomprehensive document with literature review, comparative tables of methods,\nand detailed research plans, aiming to contribute new insights and techniques\nfor safer and value-aligned diffusion-based generative AI.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "human feedback", "policy gradient", "reinforcement learning", "preference", "alignment", "direct preference optimization"], "score": 7}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17122", "pdf": "https://arxiv.org/pdf/2505.17122", "abs": "https://arxiv.org/abs/2505.17122", "authors": ["Xuan Qi", "Jiahao Qiu", "Xinzhe Juan", "Yue Wu", "Mengdi Wang"], "title": "Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?", "categories": ["cs.CL"], "comment": "17 pages, 7 figures", "summary": "Aligning large language models (LLMs) with human preferences remains a key\nchallenge in AI. Preference-based optimization methods, such as Reinforcement\nLearning with Human Feedback (RLHF) and Direct Preference Optimization (DPO),\nrely on human-annotated datasets to improve alignment. In this work, we\nidentify a crucial property of the existing learning method: the distinguishing\nsignal obtained in preferred responses is often concentrated in the early\ntokens. We refer to this as shallow preference signals.\n  To explore this property, we systematically truncate preference datasets at\nvarious points and train both reward models and DPO models on the truncated\ndata. Surprisingly, models trained on truncated datasets, retaining only the\nfirst half or fewer tokens, achieve comparable or even superior performance to\nthose trained on full datasets. For example, a reward model trained on the\nSkywork-Reward-Preference-80K-v0.2 dataset outperforms the full dataset when\ntrained on a 40\\% truncated dataset. This pattern is consistent across multiple\ndatasets, suggesting the widespread presence of shallow preference signals.\n  We further investigate the distribution of the reward signal through decoding\nstrategies. We consider two simple decoding strategies motivated by the shallow\nreward signal observation, namely Length Control Decoding and KL Threshold\nControl Decoding, which leverage shallow preference signals to optimize the\ntrade-off between alignment and computational efficiency. The performance is\neven better, which again validates our hypothesis.\n  The phenomenon of shallow preference signals highlights potential issues in\nLLM alignment: existing alignment methods often focus on aligning only the\ninitial tokens of responses, rather than considering the full response. This\ncould lead to discrepancies with real-world human preferences, resulting in\nsuboptimal alignment performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "human feedback", "preference", "alignment", "DPO", "direct preference optimization"], "score": 7}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.18126", "pdf": "https://arxiv.org/pdf/2505.18126", "abs": "https://arxiv.org/abs/2505.18126", "authors": ["Lorenz Wolf", "Robert Kirk", "Mirco Musolesi"], "title": "Reward Model Overoptimisation in Iterated RLHF", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "20 pages, 17 figures, 5 tables", "summary": "Reinforcement learning from human feedback (RLHF) is a widely used method for\naligning large language models with human preferences. However, RLHF often\nsuffers from reward model overoptimisation, in which models overfit to the\nreward function, resulting in non-generalisable policies that exploit the\nidiosyncrasies and peculiarities of the reward function. A common mitigation is\niterated RLHF, in which reward models are repeatedly retrained with updated\nhuman feedback and policies are re-optimised. Despite its increasing adoption,\nthe dynamics of overoptimisation in this setting remain poorly understood. In\nthis work, we present the first comprehensive study of overoptimisation in\niterated RLHF. We systematically analyse key design choices - how reward model\ntraining data is transferred across iterations, which reward function is used\nfor optimisation, and how policies are initialised. Using the controlled\nAlpacaFarm benchmark, we observe that overoptimisation tends to decrease over\nsuccessive iterations, as reward models increasingly approximate ground-truth\npreferences. However, performance gains diminish over time, and while\nreinitialising from the base policy is robust, it limits optimisation\nflexibility. Other initialisation strategies often fail to recover from early\noveroptimisation. These findings offer actionable insights for building more\nstable and generalisable RLHF pipelines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward function", "RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.18071", "pdf": "https://arxiv.org/pdf/2505.18071", "abs": "https://arxiv.org/abs/2505.18071", "authors": ["Jia-Nan Li", "Jian Guan", "Wei Wu", "Rui Yan"], "title": "Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated significant success in complex\nreasoning tasks such as math and coding. In contrast to these tasks where\ndeductive reasoning predominates, inductive reasoning\\textemdash the ability to\nderive general rules from incomplete evidence, remains underexplored. This\npaper investigates extended inductive reasoning in LLMs through the lens of\npersonalized preference inference, a critical challenge in LLM alignment where\ncurrent approaches struggle to capture diverse user preferences. The task\ndemands strong inductive reasoning capabilities as user preferences are\ntypically embedded implicitly across various interaction forms, requiring\nmodels to synthesize consistent preference patterns from scattered signals. We\npropose \\textsc{AlignXplore}, a model that leverages extended reasoning chains\nto enable systematic preference inference from behavioral signals in users'\ninteraction histories. We develop \\textsc{AlignXplore} by combining cold-start\ntraining based on synthetic data with subsequent online reinforcement learning.\nThrough extensive experiments, we demonstrate that \\textsc{AlignXplore}\nachieves substantial improvements over the backbone model by an average of\n11.05\\% on in-domain and out-of-domain benchmarks, while maintaining strong\ngeneralization ability across different input formats and downstream models.\nFurther analyses establish best practices for preference inference learning\nthrough systematic comparison of reward modeling strategies, while revealing\nthe emergence of human-like inductive reasoning patterns during training.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning", "preference", "comparison", "alignment"], "score": 5}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17714", "pdf": "https://arxiv.org/pdf/2505.17714", "abs": "https://arxiv.org/abs/2505.17714", "authors": ["Ben Rahman"], "title": "PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "This manuscript builds upon an earlier version posted to TechRxiv.\n  This arXiv version includes an updated comparison with GRPO (Group Relative\n  Policy Optimization)", "summary": "Despite Proximal Policy Optimization (PPO) dominating policy gradient methods\n-- from robotic control to game AI -- its static trust region forces a brittle\ntrade-off: aggressive clipping stifles early exploration, while late-stage\nupdates destabilize convergence. PPO-BR establishes a new paradigm in adaptive\nRL by fusing exploration and convergence signals into a single bounded trust\nregion -- a theoretically grounded innovation that outperforms five SOTA\nbaselines with less than 2% overhead. This work bridges a critical gap in\nphase-aware learning, enabling real-world deployment in safety-critical systems\nlike robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1%\nfaster convergence by combining: (1) entropy-driven expansion (epsilon up) for\nexploration in high-uncertainty states, and (2) reward-guided contraction\n(epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo,\nAtari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001),\n2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with\nonly five lines of code change. PPO-BR's simplicity and theoretical guarantees\nmake it ready-to-deploy in safety-critical domains -- from surgical robotics to\nautonomous drones. In contrast to recent methods such as Group Relative Policy\nOptimization (GRPO), PPO-BR offers a unified entropy-reward mechanism\napplicable to both language models and general reinforcement learning\nenvironments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "proximal policy optimization", "policy gradient", "reinforcement learning", "policy optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17540", "pdf": "https://arxiv.org/pdf/2505.17540", "abs": "https://arxiv.org/abs/2505.17540", "authors": ["Mingrui Wu", "Lu Wang", "Pu Zhao", "Fangkai Yang", "Jianjin Zhang", "Jianfeng Liu", "Yuefeng Zhan", "Weihao Han", "Hao Sun", "Jiayi Ji", "Xiaoshuai Sun", "Qingwei Lin", "Weiwei Deng", "Dongmei Zhang", "Feng Sun", "Qi Zhang", "Rongrong Ji"], "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Code is available at:\n  https://github.com/microsoft/DKI_LLM/tree/main/RePrompt", "summary": "Despite recent progress in text-to-image (T2I) generation, existing models\noften struggle to faithfully capture user intentions from short and\nunder-specified prompts. While prior work has attempted to enhance prompts\nusing large language models (LLMs), these methods frequently generate stylistic\nor unrealistic content due to insufficient grounding in visual semantics and\nreal-world composition. Inspired by recent advances in reasoning for language\nmodel, we propose RePrompt, a novel reprompting framework that introduces\nexplicit reasoning into the prompt enhancement process via reinforcement\nlearning. Instead of relying on handcrafted rules or stylistic rewrites, our\nmethod trains a language model to generate structured, self-reflective prompts\nby optimizing for image-level outcomes. The tailored reward models assesse the\ngenerated images in terms of human preference, semantic alignment, and visual\ncomposition, providing indirect supervision to refine prompt generation. Our\napproach enables end-to-end training without human-annotated data. Experiments\non GenEval and T2I-Compbench show that RePrompt significantly boosts spatial\nlayout fidelity and compositional generalization across diverse T2I backbones,\nestablishing new state-of-the-art results.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17574", "pdf": "https://arxiv.org/pdf/2505.17574", "abs": "https://arxiv.org/abs/2505.17574", "authors": ["Xueji Fang", "Liyuan Ma", "Zhiyang Chen", "Mingyuan Zhou", "Guo-jun Qi"], "title": "InfLVG: Reinforce Inference-Time Consistent Long Video Generation with GRPO", "categories": ["cs.CV"], "comment": "Preprint. Under review", "summary": "Recent advances in text-to-video generation, particularly with autoregressive\nmodels, have enabled the synthesis of high-quality videos depicting individual\nscenes. However, extending these models to generate long, cross-scene videos\nremains a significant challenge. As the context length grows during\nautoregressive decoding, computational costs rise sharply, and the model's\nability to maintain consistency and adhere to evolving textual prompts\ndeteriorates. We introduce InfLVG, an inference-time framework that enables\ncoherent long video generation without requiring additional long-form video\ndata. InfLVG leverages a learnable context selection policy, optimized via\nGroup Relative Policy Optimization (GRPO), to dynamically identify and retain\nthe most semantically relevant context throughout the generation process.\nInstead of accumulating the entire generation history, the policy ranks and\nselects the top-$K$ most contextually relevant tokens, allowing the model to\nmaintain a fixed computational budget while preserving content consistency and\nprompt alignment. To optimize the policy, we design a hybrid reward function\nthat jointly captures semantic alignment, cross-scene consistency, and artifact\nreduction. To benchmark performance, we introduce the Cross-scene Video\nBenchmark (CsVBench) along with an Event Prompt Set (EPS) that simulates\ncomplex multi-scene transitions involving shared subjects and varied\nactions/backgrounds. Experimental results show that InfLVG can extend video\nlength by up to 9$\\times$, achieving strong consistency and semantic fidelity\nacross scenes. Our code is available at https://github.com/MAPLE-AIGC/InfLVG.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "policy optimization", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17119", "pdf": "https://arxiv.org/pdf/2505.17119", "abs": "https://arxiv.org/abs/2505.17119", "authors": ["Zongru Shao", "Xin Wang", "Zhanyang Liu", "Chenhan Wang", "K. P. Subbalakshmi"], "title": "Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages without references", "summary": "Recent research leverages large language models (LLMs) for early mental\nhealth detection, such as depression, often optimized with machine-generated\ndata. However, their detection may be subject to unknown weaknesses. Meanwhile,\nquality control has not been applied to these generated corpora besides limited\nhuman verifications. Our goal is to systematically evaluate LLM reasoning and\nreveal potential weaknesses. To this end, we first provide a systematic\nevaluation of the reasoning over machine-generated detection and\ninterpretation. Then we use the models' reasoning abilities to explore\nmitigation strategies for enhanced performance. Specifically, we do the\nfollowing: A. Design an LLM instruction strategy that allows for systematic\nanalysis of the detection by breaking down the task into several subtasks. B.\nDesign contrastive few-shot and chain-of-thought prompts by selecting typical\npositive and negative examples of detection reasoning. C. Perform human\nannotation for the subtasks identified in the first step and evaluate the\nperformance. D. Identify human-preferred detection with desired logical\nreasoning from the few-shot generation and use them to explore different\noptimization strategies. We conducted extensive comparisons on the DepTweet\ndataset across the following subtasks: 1. identifying whether the speaker is\ndescribing their own depression; 2. accurately detecting the presence of PHQ-9\nsymptoms, and 3. finally, detecting depression. Human verification of\nstatistical outliers shows that LLMs demonstrate greater accuracy in analyzing\nand detecting explicit language of depression as opposed to implicit\nexpressions of depression. Two optimization methods are used for performance\nenhancement and reduction of the statistic bias: supervised fine-tuning (SFT)\nand direct preference optimization (DPO). Notably, the DPO approach achieves\nsignificant performance improvement.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation", "accuracy"], "score": 4}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17217", "pdf": "https://arxiv.org/pdf/2505.17217", "abs": "https://arxiv.org/abs/2505.17217", "authors": ["Kangda Wei", "Hasnat Md Abdullah", "Ruihong Huang"], "title": "Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit gender bias, resulting in unequal\ntreatment of male and female subjects across different contexts. To address\nthis issue, we propose a novel data generation framework that fosters\nexploratory thinking in LLMs. Our approach prompts models to generate story\npairs featuring male and female protagonists in structurally identical, morally\nambiguous scenarios, then elicits and compares their moral judgments. When\ninconsistencies arise, the model is guided to produce balanced, gender-neutral\njudgments. These story-judgment pairs are used to fine-tune or optimize the\nmodels via Direct Preference Optimization (DPO). Experimental results show that\nour method significantly reduces gender bias while preserving or even enhancing\ngeneral model capabilities. We will release the code and generated data.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO", "direct preference optimization"], "score": 3}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17391", "pdf": "https://arxiv.org/pdf/2505.17391", "abs": "https://arxiv.org/abs/2505.17391", "authors": ["Yuelyu Ji", "Rui Meng", "Zhuochun Li", "Daqing He"], "title": "Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) grounds large language models (LLMs) in\nup-to-date external evidence, yet existing multi-hop RAG pipelines still issue\nredundant subqueries, explore too shallowly, or wander through overly long\nsearch chains. We introduce EVO-RAG, a curriculum-guided reinforcement learning\nframework that evolves a query-rewriting agent from broad early-stage\nexploration to concise late-stage refinement. EVO-RAG couples a seven-factor,\nstep-level reward vector (covering relevance, redundancy, efficiency, and\nanswer correctness) with a time-varying scheduler that reweights these signals\nas the episode unfolds. The agent is trained with Direct Preference\nOptimization over a multi-head reward model, enabling it to learn when to\nsearch, backtrack, answer, or refuse. Across four multi-hop QA benchmarks\n(HotpotQA, 2WikiMultiHopQA, MuSiQue, and Bamboogle), EVO-RAG boosts Exact Match\nby up to 4.6 points over strong RAG baselines while trimming average retrieval\ndepth by 15 %. Ablation studies confirm the complementary roles of curriculum\nstaging and dynamic reward scheduling. EVO-RAG thus offers a general recipe for\nbuilding reliable, cost-effective multi-hop RAG systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning", "preference"], "score": 3}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17510", "pdf": "https://arxiv.org/pdf/2505.17510", "abs": "https://arxiv.org/abs/2505.17510", "authors": ["Marcus Ma", "Georgios Chochlakis", "Niyantha Maruthu Pandiyan", "Jesse Thomason", "Shrikanth Narayanan"], "title": "Large Language Models Do Multi-Label Classification Differently", "categories": ["cs.CL"], "comment": "18 pages, 11 figures, 6 tables", "summary": "Multi-label classification is prevalent in real-world settings, but the\nbehavior of Large Language Models (LLMs) in this setting is understudied. We\ninvestigate how autoregressive LLMs perform multi-label classification, with a\nfocus on subjective tasks, by analyzing the output distributions of the models\nin each generation step. We find that their predictive behavior reflects the\nmultiple steps in the underlying language modeling required to generate all\nrelevant labels as they tend to suppress all but one label at each step. We\nfurther observe that as model scale increases, their token distributions\nexhibit lower entropy, yet the internal ranking of the labels improves.\nFinetuning methods such as supervised finetuning and reinforcement learning\namplify this phenomenon. To further study this issue, we introduce the task of\ndistribution alignment for multi-label settings: aligning LLM-derived label\ndistributions with empirical distributions estimated from annotator responses\nin subjective tasks. We propose both zero-shot and supervised methods which\nimprove both alignment and predictive performance over existing approaches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "ranking", "alignment"], "score": 3}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17897", "pdf": "https://arxiv.org/pdf/2505.17897", "abs": "https://arxiv.org/abs/2505.17897", "authors": ["Zi-Ao Ma", "Tian Lan", "Rong-Cheng Tu", "Shu-Hang Liu", "Heyan Huang", "Zhijing Wu", "Chen Xu", "Xian-Ling Mao"], "title": "T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The rapid progress in diffusion-based text-to-image (T2I) generation has\ncreated an urgent need for interpretable automatic evaluation methods that can\nassess the quality of generated images, therefore reducing the human annotation\nburden. To reduce the prohibitive cost of relying on commercial models for\nlarge-scale evaluation, and to improve the reasoning capabilities of\nopen-source models, recent research has explored supervised fine-tuning (SFT)\nof multimodal large language models (MLLMs) as dedicated T2I evaluators.\nHowever, SFT approaches typically rely on high-quality critique datasets, which\nare either generated by proprietary LLMs-with potential issues of bias and\ninconsistency-or annotated by humans at high cost, limiting their scalability\nand generalization. To address these limitations, we propose T2I-Eval-R1, a\nnovel reinforcement learning framework that trains open-source MLLMs using only\ncoarse-grained quality scores, thereby avoiding the need for annotating\nhigh-quality interpretable evaluation rationale. Our approach integrates Group\nRelative Policy Optimization (GRPO) into the instruction-tuning process,\nenabling models to generate both scalar scores and interpretable reasoning\nchains with only easy accessible annotated judgment scores or preferences.\nFurthermore, we introduce a continuous reward formulation that encourages score\ndiversity and provides stable optimization signals, leading to more robust and\ndiscriminative evaluation behavior. Experimental results on three established\nT2I meta-evaluation benchmarks demonstrate that T2I-Eval-R1 achieves\nsignificantly higher alignment with human assessments and offers more accurate\ninterpretable score rationales compared to strong baseline methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17082", "pdf": "https://arxiv.org/pdf/2505.17082", "abs": "https://arxiv.org/abs/2505.17082", "authors": ["Abderrahman Skiredj", "Ferdaous Azhari", "Houdaifa Atou", "Nouamane Tazi", "Ismail Berrada"], "title": "GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Open-source large language models (LLMs) still marginalise Moroccan Arabic\n(Darija), forcing practitioners either to bolt on heavyweight Arabic adapters\nor to sacrifice the very reasoning skills that make LLMs useful. We show that a\nrigorously quality-over-quantity alignment strategy can surface fluent Darija\nwhile safeguarding the backbone s cross-lingual reasoning at a sliver of the\nusual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6\nK and TULU 50 K into Darija, preserve 20 of the English originals, and add\nmathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on\n5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the\nreasoning-dense TULU portion pushes it to 47.5 with no English regression.\nScaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which\nmatches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense,\nscoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc\nretains Gemma-27B s strong maths and general-reasoning ability, showing only\nminimal movement on GSM8K and English benchmarks. The entire model is trained\nin just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable\nlanguage technology. We release code, data and checkpoints to spur\nDarija-centric applications in education, public services and everyday digital\ninteraction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "LIMA"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17086", "pdf": "https://arxiv.org/pdf/2505.17086", "abs": "https://arxiv.org/abs/2505.17086", "authors": ["Yihong Wu", "Liheng Ma", "Muzhi Li", "Jiaming Zhou", "Jianye Hao", "Ho-fung Leung", "Irwin King", "Yingxue Zhang", "Jian-Yun Nie"], "title": "Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable versatility, due to\nthe lack of factual knowledge, their application to Question Answering (QA)\ntasks remains hindered by hallucination.\n  While Retrieval-Augmented Generation mitigates these issues by integrating\nexternal knowledge, existing approaches rely heavily on in-context learning,\nwhose performance is constrained by the fundamental reasoning capabilities of\nLLMs.\n  In this paper, we propose Mujica, a Multi-hop Joint Intelligence for Complex\nQuestion Answering, comprising a planner that decomposes questions into a\ndirected acyclic graph of subquestions and a worker that resolves questions via\nretrieval and reasoning. Additionally, we introduce MyGO (Minimalist policy\nGradient Optimization), a novel reinforcement learning method that replaces\ntraditional policy gradient updates with Maximum Likelihood Estimation (MLE) by\nsampling trajectories from an asymptotically optimal policy. MyGO eliminates\nthe need for gradient rescaling and reference models, ensuring stable and\nefficient training.\n  Empirical results across multiple datasets demonstrate the effectiveness of\nMujica-MyGO in enhancing multi-hop QA performance for various LLMs, offering a\nscalable and resource-efficient solution for complex QA tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy gradient", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17534", "pdf": "https://arxiv.org/pdf/2505.17534", "abs": "https://arxiv.org/abs/2505.17534", "authors": ["Jingjing Jiang", "Chongjie Si", "Jun Luo", "Hanwang Zhang", "Chao Ma"], "title": "Co-Reinforcement Learning for Unified Multimodal Understanding and Generation", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "This paper presents a pioneering exploration of reinforcement learning (RL)\nvia group relative policy optimization for unified multimodal large language\nmodels (ULMs), aimed at simultaneously reinforcing generation and understanding\ncapabilities. Through systematic pilot studies, we uncover the significant\npotential of ULMs to enable the synergistic co-evolution of dual capabilities\nwithin a shared policy optimization framework. Building on this insight, we\nintroduce \\textbf{CoRL}, a co-reinforcement learning framework comprising a\nunified RL stage for joint optimization and a refined RL stage for\ntask-specific enhancement. With the proposed CoRL, our resulting model,\n\\textbf{ULM-R1}, achieves average improvements of \\textbf{7%} on three\ntext-to-image generation datasets and \\textbf{23%} on nine multimodal\nunderstanding benchmarks. These results demonstrate the effectiveness of CoRL\nand highlight the substantial benefit of reinforcement learning in facilitating\ncross-task synergy and optimization for ULMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17112", "pdf": "https://arxiv.org/pdf/2505.17112", "abs": "https://arxiv.org/abs/2505.17112", "authors": ["Robin Segerer"], "title": "Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek", "categories": ["cs.CL"], "comment": "15 pages, 1 table, 1 figure", "summary": "This study examines cultural value alignment in large language models (LLMs)\nby analyzing how Gemini, ChatGPT, and DeepSeek prioritize values from\nSchwartz's value framework. Using the 40-item Portrait Values Questionnaire, we\nassessed whether DeepSeek, trained on Chinese-language data, exhibits distinct\nvalue preferences compared to Western models. Results of a Bayesian ordinal\nregression model show that self-transcendence values (e.g., benevolence,\nuniversalism) were highly prioritized across all models, reflecting a general\nLLM tendency to emphasize prosocial values. However, DeepSeek uniquely\ndownplayed self-enhancement values (e.g., power, achievement) compared to\nChatGPT and Gemini, aligning with collectivist cultural tendencies. These\nfindings suggest that LLMs reflect culturally situated biases rather than a\nuniversal ethical framework. To address value asymmetries in LLMs, we propose\nmulti-perspective reasoning, self-reflective feedback, and dynamic\ncontextualization. This study contributes to discussions on AI fairness,\ncultural neutrality, and the need for pluralistic AI alignment frameworks that\nintegrate diverse moral perspectives.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17266", "pdf": "https://arxiv.org/pdf/2505.17266", "abs": "https://arxiv.org/abs/2505.17266", "authors": ["Cehao Yang", "Xueyuan Lin", "Chengjin Xu", "Xuhui Jiang", "Xiaojun Wu", "Honghao Liu", "Hui Xiong", "Jian Guo"], "title": "Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A practical approach to activate long chain-of-thoughts reasoning ability in\npre-trained large language models is to perform supervised fine-tuning on\ninstruction datasets synthesized by strong Large Reasoning Models such as\nDeepSeek-R1, offering a cost-effective alternative to reinforcement learning.\nHowever, large-scale instruction sets with more than 100k samples incur\nsignificant training overhead, while effective strategies for automatic\nlong-CoT instruction selection still remain unexplored. In this work, we\npropose Select2Reason, a novel and efficient instruction-tuning data selection\nframework for long-CoT reasoning. From the perspective of emergence of\nrethinking behaviors like self-correction and backtracking, we investigate\ncommon metrics that may determine the quality of long-CoT reasoning\ninstructions. Select2Reason leverages a quantifier to estimate difficulty of\nquestion and jointly incorporates a reasoning trace length-based heuristic\nthrough a weighted scheme for ranking to prioritize high-utility examples.\nEmpirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only\n10% of the data selected by Select2Reason achieves performance competitive with\nor superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across\nthree competition-level and six comprehensive mathematical benchmarks. Further\nexperiments highlight the scalability in varying data size, efficiency during\ninference, and its adaptability to other instruction pools with minimal cost.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "self-correction"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "ranking"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.18025", "pdf": "https://arxiv.org/pdf/2505.18025", "abs": "https://arxiv.org/abs/2505.18025", "authors": ["Evangelos Sariyanidi", "Claudio Ferrari", "Federico Nocentini", "Stefano Berretti", "Andrea Cavallaro", "Birkan Tunc"], "title": "3D Face Reconstruction Error Decomposed: A Modular Benchmark for Fair and Fast Method Evaluation", "categories": ["cs.CV"], "comment": "To be published in IEEE International Conference on Automatic Face\n  and Gesture Recognition, 2025", "summary": "Computing the standard benchmark metric for 3D face reconstruction, namely\ngeometric error, requires a number of steps, such as mesh cropping, rigid\nalignment, or point correspondence. Current benchmark tools are monolithic\n(they implement a specific combination of these steps), even though there is no\nconsensus on the best way to measure error. We present a toolkit for a\nModularized 3D Face reconstruction Benchmark (M3DFB), where the fundamental\ncomponents of error computation are segregated and interchangeable, allowing\none to quantify the effect of each. Furthermore, we propose a new component,\nnamely correction, and present a computationally efficient approach that\npenalizes for mesh topology inconsistency. Using this toolkit, we test 16 error\nestimators with 10 reconstruction methods on two real and two synthetic\ndatasets. Critically, the widely used ICP-based estimator provides the worst\nbenchmarking performance, as it significantly alters the true ranking of the\ntop-5 reconstruction methods. Notably, the correlation of ICP with the true\nerror can be as low as 0.41. Moreover, non-rigid alignment leads to significant\nimprovement (correlation larger than 0.90), highlighting the importance of\nannotating 3D landmarks on datasets. Finally, the proposed correction scheme,\ntogether with non-rigid warping, leads to an accuracy on a par with the best\nnon-rigid ICP-based estimators, but runs an order of magnitude faster. Our\nopen-source codebase is designed for researchers to easily compare alternatives\nfor each component, thus helping accelerating progress in benchmarking for 3D\nface reconstruction and, furthermore, supporting the improvement of learned\nreconstruction methods, which depend on accurate error estimation for effective\ntraining.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "correlation", "accuracy"], "score": 4}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17558", "pdf": "https://arxiv.org/pdf/2505.17558", "abs": "https://arxiv.org/abs/2505.17558", "authors": ["Shrey Pandit", "Ashwin Vinod", "Liu Leqi", "Ying Ding"], "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection", "categories": ["cs.CL", "cs.AI"], "comment": "Code and dataset are available at https://teachingwithlies.github.io/", "summary": "Aligning large language models (LLMs) to accurately detect hallucinations\nremains a significant challenge due to the sophisticated nature of hallucinated\ntext. Recognizing that hallucinated samples typically exhibit higher deceptive\nquality than traditional negative samples, we use these carefully engineered\nhallucinations as negative examples in the DPO alignment procedure. Our method\nincorporates a curriculum learning strategy, gradually transitioning the\ntraining from easier samples, identified based on the greatest reduction in\nprobability scores from independent fact checking models, to progressively\nharder ones. This structured difficulty scaling ensures stable and incremental\nlearning. Experimental evaluation demonstrates that our HaluCheck models,\ntrained with curriculum DPO approach and high quality negative samples,\nsignificantly improves model performance across various metrics, achieving\nimprovements of upto 24% on difficult benchmarks like MedHallu and HaluEval.\nAdditionally, HaluCheck models demonstrate robustness in zero-shot settings,\nsignificantly outperforming larger state-of-the-art models across various\nbenchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "DPO"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17565", "pdf": "https://arxiv.org/pdf/2505.17565", "abs": "https://arxiv.org/abs/2505.17565", "authors": ["Wei Zhou", "Mohsen Mesgar", "Heike Adel", "Annemarie Friedrich"], "title": "PPT: A Process-based Preference Learning Framework for Self Improving Table Question Answering Models", "categories": ["cs.CL"], "comment": null, "summary": "Improving large language models (LLMs) with self-generated data has\ndemonstrated success in tasks such as mathematical reasoning and code\ngeneration. Yet, no exploration has been made on table question answering\n(TQA), where a system answers questions based on tabular data. Addressing this\ngap is crucial for TQA, as effective self-improvement can boost performance\nwithout requiring costly or manually annotated data. In this work, we propose\nPPT, a Process-based Preference learning framework for TQA. It decomposes\nreasoning chains into discrete states, assigns scores to each state, and\nsamples contrastive steps for preference learning. Experimental results show\nthat PPT effectively improves TQA models by up to 5% on in-domain datasets and\n2.4% on out-of-domain datasets, with only 8,000 preference pairs. Furthermore,\nthe resulting models achieve competitive results compared to more complex and\nlarger state-of-the-art TQA systems, while being five times more efficient\nduring inference.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering", "mathematical reasoning"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17691", "pdf": "https://arxiv.org/pdf/2505.17691", "abs": "https://arxiv.org/abs/2505.17691", "authors": ["Yan Yu", "Yilun Liu", "Minggui He", "Shimin Tao", "Weibin Meng", "Xinhua Yang", "Li Zhang", "Hongxia Ma", "Chang Su", "Hao Yang", "Fuliang Li"], "title": "ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are widely used as evaluators for open-ended\ntasks, while previous research has emphasized biases in LLM evaluations, the\nissue of non-transitivity in pairwise comparisons remains unresolved:\nnon-transitive preferences for pairwise comparisons, where evaluators prefer A\nover B, B over C, but C over A. Our results suggest that low-quality training\ndata may reduce the transitivity of preferences generated by the Evaluator LLM.\nTo address this, We propose a graph-theoretic framework to analyze and mitigate\nthis problem by modeling pairwise preferences as tournament graphs. We quantify\nnon-transitivity and introduce directed graph structural entropy to measure the\noverall clarity of preferences. Our analysis reveals significant\nnon-transitivity in advanced Evaluator LLMs (with Qwen2.5-Max exhibiting\n67.96%), as well as high entropy values (0.8095 for Qwen2.5-Max), reflecting\nlow overall clarity of preferences. To address this issue, we designed a\nfiltering strategy, ELSPR, to eliminate preference data that induces\nnon-transitivity, retaining only consistent and transitive preference data for\nmodel fine-tuning. Experiments demonstrate that models fine-tuned with filtered\ndata reduce non-transitivity by 13.78% (from 64.28% to 50.50%), decrease\nstructural entropy by 0.0879 (from 0.8113 to 0.7234), and align more closely\nwith human evaluators (human agreement rate improves by 0.6% and Spearman\ncorrelation increases by 0.01).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "agreement"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17712", "pdf": "https://arxiv.org/pdf/2505.17712", "abs": "https://arxiv.org/abs/2505.17712", "authors": ["Yi Su", "Jiayi Zhang", "Shu Yang", "Xinhai Wang", "Lijie Hu", "Di Wang"], "title": "Understanding How Value Neurons Shape the Generation of Specified Values in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Rapid integration of large language models (LLMs) into societal applications\nhas intensified concerns about their alignment with universal ethical\nprinciples, as their internal value representations remain opaque despite\nbehavioral alignment advancements. Current approaches struggle to\nsystematically interpret how values are encoded in neural architectures,\nlimited by datasets that prioritize superficial judgments over mechanistic\nanalysis. We introduce ValueLocate, a mechanistic interpretability framework\ngrounded in the Schwartz Values Survey, to address this gap. Our method first\nconstructs ValueInsight, a dataset that operationalizes four dimensions of\nuniversal value through behavioral contexts in the real world. Leveraging this\ndataset, we develop a neuron identification method that calculates activation\ndifferences between opposing value aspects, enabling precise localization of\nvalue-critical neurons without relying on computationally intensive attribution\nmethods. Our proposed validation method demonstrates that targeted manipulation\nof these neurons effectively alters model value orientations, establishing\ncausal relationships between neurons and value representations. This work\nadvances the foundation for value alignment by bridging psychological value\nframeworks with neuron analysis in LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17659", "pdf": "https://arxiv.org/pdf/2505.17659", "abs": "https://arxiv.org/abs/2505.17659", "authors": ["Xiaolong Tang", "Meina Kan", "Shiguang Shan", "Xilin Chen"], "title": "Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Safe and feasible trajectory planning is essential for real-world autonomous\ndriving systems. However, existing learning-based planning methods often rely\non expert demonstrations, which not only lack explicit safety awareness but\nalso risk inheriting unsafe behaviors such as speeding from suboptimal human\ndriving data. Inspired by the success of large language models, we propose\nPlan-R1, a novel two-stage trajectory planning framework that formulates\ntrajectory planning as a sequential prediction task, guided by explicit\nplanning principles such as safety, comfort, and traffic rule compliance. In\nthe first stage, we train an autoregressive trajectory predictor via next\nmotion token prediction on expert data. In the second stage, we design\nrule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the\nmodel using Group Relative Policy Optimization (GRPO), a reinforcement learning\nstrategy, to align its predictions with these planning principles. Experiments\non the nuPlan benchmark demonstrate that our Plan-R1 significantly improves\nplanning safety and feasibility, achieving state-of-the-art performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17374", "pdf": "https://arxiv.org/pdf/2505.17374", "abs": "https://arxiv.org/abs/2505.17374", "authors": ["Seon Gyeom Kim", "Jae Young Choi", "Ryan Rossi", "Eunyee Koh", "Tak Yeon Lee"], "title": "Chart-to-Experience: Benchmarking Multimodal LLMs for Predicting Experiential Impact of Charts", "categories": ["cs.HC", "cs.CL"], "comment": "This paper has been accepted to IEEE PacificVis 2025", "summary": "The field of Multimodal Large Language Models (MLLMs) has made remarkable\nprogress in visual understanding tasks, presenting a vast opportunity to\npredict the perceptual and emotional impact of charts. However, it also raises\nconcerns, as many applications of LLMs are based on overgeneralized assumptions\nfrom a few examples, lacking sufficient validation of their performance and\neffectiveness. We introduce Chart-to-Experience, a benchmark dataset comprising\n36 charts, evaluated by crowdsourced workers for their impact on seven\nexperiential factors. Using the dataset as ground truth, we evaluated\ncapabilities of state-of-the-art MLLMs on two tasks: direct prediction and\npairwise comparison of charts. Our findings imply that MLLMs are not as\nsensitive as human evaluators when assessing individual charts, but are\naccurate and reliable in pairwise comparisons.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17508", "pdf": "https://arxiv.org/pdf/2505.17508", "abs": "https://arxiv.org/abs/2505.17508", "authors": ["Yifan Zhang", "Yifeng Liu", "Huizhuo Yuan", "Yang Yuan", "Quanquan Gu", "Andrew C Yao"], "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "53 pages, 17 figures", "summary": "Policy gradient algorithms have been successfully applied to enhance the\nreasoning capabilities of large language models (LLMs). Despite the widespread\nuse of Kullback-Leibler (KL) regularization in policy gradient algorithms to\nstabilize training, the systematic exploration of how different KL divergence\nformulations can be estimated and integrated into surrogate loss functions for\nonline reinforcement learning (RL) presents a nuanced and systematically\nexplorable design space. In this paper, we propose regularized policy gradient\n(RPG), a systematic framework for deriving and analyzing KL-regularized policy\ngradient methods in the online RL setting. We derive policy gradients and\ncorresponding surrogate loss functions for objectives regularized by both\nforward and reverse KL divergences, considering both normalized and\nunnormalized policy distributions. Furthermore, we present derivations for\nfully differentiable loss functions as well as REINFORCE-style gradient\nestimators, accommodating diverse algorithmic needs. We conduct extensive\nexperiments on RL for LLM reasoning using these methods, showing improved or\ncompetitive results in terms of training stability and performance compared to\nstrong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at\nhttps://github.com/complex-reasoning/RPG.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy gradient", "reinforcement learning"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17534", "pdf": "https://arxiv.org/pdf/2505.17534", "abs": "https://arxiv.org/abs/2505.17534", "authors": ["Jingjing Jiang", "Chongjie Si", "Jun Luo", "Hanwang Zhang", "Chao Ma"], "title": "Co-Reinforcement Learning for Unified Multimodal Understanding and Generation", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "This paper presents a pioneering exploration of reinforcement learning (RL)\nvia group relative policy optimization for unified multimodal large language\nmodels (ULMs), aimed at simultaneously reinforcing generation and understanding\ncapabilities. Through systematic pilot studies, we uncover the significant\npotential of ULMs to enable the synergistic co-evolution of dual capabilities\nwithin a shared policy optimization framework. Building on this insight, we\nintroduce \\textbf{CoRL}, a co-reinforcement learning framework comprising a\nunified RL stage for joint optimization and a refined RL stage for\ntask-specific enhancement. With the proposed CoRL, our resulting model,\n\\textbf{ULM-R1}, achieves average improvements of \\textbf{7%} on three\ntext-to-image generation datasets and \\textbf{23%} on nine multimodal\nunderstanding benchmarks. These results demonstrate the effectiveness of CoRL\nand highlight the substantial benefit of reinforcement learning in facilitating\ncross-task synergy and optimization for ULMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.18116", "pdf": "https://arxiv.org/pdf/2505.18116", "abs": "https://arxiv.org/abs/2505.18116", "authors": ["Huayu Chen", "Kaiwen Zheng", "Qinsheng Zhang", "Ganqu Cui", "Yin Cui", "Haotian Ye", "Tsung-Yi Lin", "Ming-Yu Liu", "Jun Zhu", "Haoxiang Wang"], "title": "Bridging Supervised Learning and Reinforcement Learning in Math Reasoning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL) has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast, Supervised Learning (SL) is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and propose\nNegative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generated negative answers,\nNFT constructs an implicit negative policy to model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling direct policy optimization on all LLMs' generations. We conduct\nexperiments on 7B and 32B models in math reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines like Rejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms like GRPO and DAPO.\nFurthermore, we demonstrate that NFT and GRPO are actually equivalent in\nstrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL methods in binary-feedback learning systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17049", "pdf": "https://arxiv.org/pdf/2505.17049", "abs": "https://arxiv.org/abs/2505.17049", "authors": ["David Rozado"], "title": "Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This study examines the behavior of Large Language Models (LLMs) when\nevaluating professional candidates based on their resumes or curricula vitae\n(CVs). In an experiment involving 22 leading LLMs, each model was\nsystematically given one job description along with a pair of\nprofession-matched CVs, one bearing a male first name, the other a female first\nname, and asked to select the more suitable candidate for the job. Each CV pair\nwas presented twice, with names swapped to ensure that any observed preferences\nin candidate selection stemmed from gendered names cues. Despite identical\nprofessional qualifications across genders, all LLMs consistently favored\nfemale-named candidates across 70 different professions. Adding an explicit\ngender field (male/female) to the CVs further increased the preference for\nfemale applicants. When gendered names were replaced with gender-neutral\nidentifiers \"Candidate A\" and \"Candidate B\", several models displayed a\npreference to select \"Candidate A\". Counterbalancing gender assignment between\nthese gender-neutral identifiers resulted in gender parity in candidate\nselection. When asked to rate CVs in isolation rather than compare pairs, LLMs\nassigned slightly higher average scores to female CVs overall, but the effect\nsize was negligible. Including preferred pronouns (he/him or she/her) next to a\ncandidate's name slightly increased the odds of the candidate being selected\nregardless of gender. Finally, most models exhibited a substantial positional\nbias to select the candidate listed first in the prompt. These findings\nunderscore the need for caution when deploying LLMs in high-stakes autonomous\ndecision-making contexts and raise doubts about whether LLMs consistently apply\nprincipled reasoning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17316", "pdf": "https://arxiv.org/pdf/2505.17316", "abs": "https://arxiv.org/abs/2505.17316", "authors": ["Jiachen Jiang", "Jinxin Zhou", "Bo Peng", "Xia Ning", "Zhihui Zhu"], "title": "Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Achieving better alignment between vision embeddings and Large Language\nModels (LLMs) is crucial for enhancing the abilities of Multimodal LLMs\n(MLLMs), particularly for recent models that rely on powerful pretrained vision\nencoders and LLMs. A common approach to connect the pretrained vision encoder\nand LLM is through a projector applied after the vision encoder. However, the\nprojector is often trained to enable the LLM to generate captions, and hence\nthe mechanism by which LLMs understand each vision token remains unclear. In\nthis work, we first investigate the role of the projector in compressing vision\nembeddings and aligning them with word embeddings. We show that the projector\nsignificantly compresses visual information, removing redundant details while\npreserving essential elements necessary for the LLM to understand visual\ncontent. We then examine patch-level alignment -- the alignment between each\nvision patch and its corresponding semantic words -- and propose a\n*multi-semantic alignment hypothesis*. Our analysis indicates that the\nprojector trained by caption loss improves patch-level alignment but only to a\nlimited extent, resulting in weak and coarse alignment. To address this issue,\nwe propose *patch-aligned training* to efficiently enhance patch-level\nalignment. Our experiments show that patch-aligned training (1) achieves\nstronger compression capability and improved patch-level alignment, enabling\nthe MLLM to generate higher-quality captions, (2) improves the MLLM's\nperformance by 16% on referring expression grounding tasks, 4% on\nquestion-answering tasks, and 3% on modern instruction-following benchmarks\nwhen using the same supervised fine-tuning (SFT) setting. The proposed method\ncan be easily extended to other multimodal models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17050", "pdf": "https://arxiv.org/pdf/2505.17050", "abs": "https://arxiv.org/abs/2505.17050", "authors": ["Yanhao Jia", "Xinyi Wu", "Qinglin Zhang", "Yiran Qin", "Luwei Xiao", "Shuai Zhao"], "title": "Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.CY", "cs.MM"], "comment": null, "summary": "Project-Based Learning (PBL) involves a variety of highly correlated\nmultimodal data, making it a vital educational approach within STEM\ndisciplines. With the rapid development of multimodal large language models\n(MLLMs), researchers have begun exploring their potential to enhance tasks such\nas information retrieval, knowledge comprehension, and data generation in\neducational settings. However, existing benchmarks fall short in providing both\na free-form output structure and a rigorous human expert validation process,\nlimiting their effectiveness in evaluating real-world educational tasks.\nAdditionally, few methods have developed automated pipelines to assist with the\ncomplex responsibilities of teachers leveraging MLLMs, largely due to model\nhallucination and instability, which lead to unreliable implementation. To\naddress this gap, we introduce PBLBench, a novel benchmark designed to evaluate\ncomplex reasoning grounded in domain-specific knowledge and long-context\nunderstanding, thereby challenging models with tasks that closely resemble\nthose handled by human experts. To establish reliable ground truth, we adopt\nthe Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise\ncomparisons to derive structured and weighted evaluation criteria. We assess\nthe performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that\neven the most advanced models achieve only 59% rank accuracy, underscoring the\nsignificant challenges presented by this benchmark. We believe PBLBench will\nserve as a catalyst for the development of more capable AI agents, ultimately\naiming to alleviate teacher workload and enhance educational productivity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "criteria"], "score": 4}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17060", "pdf": "https://arxiv.org/pdf/2505.17060", "abs": "https://arxiv.org/abs/2505.17060", "authors": ["Wenyi Yu", "Siyin Wang", "Xiaoyu Yang", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Guangzhi Sun", "Lu Lu", "Yuxuan Wang", "Chao Zhang"], "title": "SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In order to enable fluid and natural human-machine speech interaction,\nexisting full-duplex conversational systems often adopt modular architectures\nwith auxiliary components such as voice activity detectors, interrupters,\nconversation state predictors, or multiple LLMs. These systems, however, suffer\nfrom error accumulation across modules and struggle with key challenges such as\ncontext-dependent barge-in and echo cancellation. Recent approaches, most\nnotably Moshi, simplify the pipeline by injecting audio codecs into the token\nspace of a single LLM. However, such methods still incur significant\nperformance degradation when operating on the speech rather than text modality.\nIn this paper, we introduce SALMONN-omni, the first single, standalone\nfull-duplex speech LLM that operates without audio codecs in its token space.\nIt features a novel dynamic thinking mechanism within the LLM backbone,\nenabling the model to learn when to transition between speaking and listening\nstates. Experiments on widely used benchmarks for spoken question answering and\nopen-domain dialogue show that SALMONN-omni achieves at least 30\\% relative\nperformance improvement over existing open-source full-duplex models and\nperforms highly competitively to half-duplex and turn-based systems, despite\nusing substantially less training data. Moreover, SALMONN-omni demonstrates\nstrong performance in complex conversational scenarios, including turn-taking,\nbackchanneling, echo cancellation and context-dependent barge-in, with further\nimprovements achieved through reinforcement learning. Some demo conversations\nbetween user and SALMONN-omni are provided in the following repository\nhttps://github.com/bytedance/SALMONN.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue", "question answering"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17063", "pdf": "https://arxiv.org/pdf/2505.17063", "abs": "https://arxiv.org/abs/2505.17063", "authors": ["Yiduo Guo", "Zhen Guo", "Chuanwei Huang", "Zi-Ang Wang", "Zekai Zhang", "Haofei Yu", "Huishuai Zhang", "Yikang Shen"], "title": "Synthetic Data RL: Task Definition Is All You Need", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) is a powerful way to adapt foundation models to\nspecialized tasks, but its reliance on large-scale human-labeled data limits\nbroad adoption. We introduce Synthetic Data RL, a simple and general framework\nthat reinforcement fine-tunes models using only synthetic data generated from a\ntask definition. Our method first generates question and answer pairs from the\ntask definition and retrieved documents, then adapts the difficulty of the\nquestion based on model solvability, and selects questions using the average\npass rate of the model across samples for RL training. On Qwen-2.5-7B, our\nmethod achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9\npp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on\nGPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA\n(finance). It surpasses supervised fine-tuning under the same data budget and\nnearly matches RL with full human data across datasets (e.g., +17.2 pp on\nGSM8K). Adding 100 human demonstrations improves the performance of GSM8K only\nby 0.4 pp, showing a limited added value. By reducing human data annotation,\nSynthetic Data RL enables scalable and efficient RL-based model adaptation.\nCode and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17423", "pdf": "https://arxiv.org/pdf/2505.17423", "abs": "https://arxiv.org/abs/2505.17423", "authors": ["Shenghui Chen", "Po-han Li", "Sandeep Chichali", "Ufuk Topcu"], "title": "VIBE: Video-to-Text Information Bottleneck Evaluation for TL;DR", "categories": ["cs.CV", "cs.HC", "cs.IT", "math.IT"], "comment": null, "summary": "Many decision-making tasks, where both accuracy and efficiency matter, still\nrequire human supervision. For example, tasks like traffic officers reviewing\nhour-long dashcam footage or researchers screening conference videos can\nbenefit from concise summaries that reduce cognitive load and save time. Yet\ncurrent vision-language models (VLMs) often produce verbose, redundant outputs\nthat hinder task performance. Existing video caption evaluation depends on\ncostly human annotations and overlooks the summaries' utility in downstream\ntasks. We address these gaps with Video-to-text Information Bottleneck\nEvaluation (VIBE), an annotation-free method that scores VLM outputs using two\nmetrics: grounding (how well the summary aligns with visual content) and\nutility (how informative it is for the task). VIBE selects from randomly\nsampled VLM outputs by ranking them according to the two scores to support\neffective human decision-making. Human studies on LearningPaper24,\nSUTD-TrafficQA, and LongVideoBench show that summaries selected by VIBE\nconsistently improve performance-boosting task accuracy by up to 61.23% and\nreducing response time by 75.77% compared to naive VLM summaries or raw video.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17449", "pdf": "https://arxiv.org/pdf/2505.17449", "abs": "https://arxiv.org/abs/2505.17449", "authors": ["Inpyo Song", "Jangwon Lee"], "title": "Real-time Traffic Accident Anticipation with Feature Reuse", "categories": ["cs.CV"], "comment": "Accepted to ICIP 2025", "summary": "This paper addresses the problem of anticipating traffic accidents, which\naims to forecast potential accidents before they happen. Real-time anticipation\nis crucial for safe autonomous driving, yet most methods rely on\ncomputationally heavy modules like optical flow and intermediate feature\nextractors, making real-world deployment challenging. In this paper, we thus\nintroduce RARE (Real-time Accident anticipation with Reused Embeddings), a\nlightweight framework that capitalizes on intermediate features from a single\npre-trained object detector. By eliminating additional feature-extraction\npipelines, RARE significantly reduces latency. Furthermore, we introduce a\nnovel Attention Score Ranking Loss, which prioritizes higher attention on\naccident-related objects over non-relevant ones. This loss enhances both\naccuracy and interpretability. RARE demonstrates a 4-8 times speedup over\nexisting approaches on the DAD and CCD benchmarks, achieving a latency of\n13.6ms per frame (73.3 FPS) on an RTX 6000. Moreover, despite its reduced\ncomplexity, it attains state-of-the-art Average Precision and reliably\nanticipates imminent collisions in real time. These results highlight RARE's\npotential for safety-critical applications where timely and explainable\nanticipation is essential.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17561", "pdf": "https://arxiv.org/pdf/2505.17561", "abs": "https://arxiv.org/abs/2505.17561", "authors": ["Kwanyoung Kim", "Sanghyun Kim"], "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 10 figures", "summary": "The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17114", "pdf": "https://arxiv.org/pdf/2505.17114", "abs": "https://arxiv.org/abs/2505.17114", "authors": ["Subrata Biswas", "Mohammad Nur Hossain Khan", "Bashima Islam"], "title": "RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Multimodal question answering (QA) often requires identifying which video,\naudio, or sensor tokens are relevant to the question. Yet modality\ndisagreements are common: off-camera speech, background noise, or motion\noutside the field of view often mislead fusion models that weight all streams\nequally. We present RAVEN, a unified QA architecture whose core is QuART, a\nquery-conditioned cross-modal gating module that assigns scalar relevance\nscores to each token across modalities, enabling the model to amplify\ninformative signals and suppress distractors before fusion. RAVEN is trained\nthrough a three-stage pipeline comprising unimodal pretraining, query-aligned\nfusion, and disagreement-oriented fine-tuning -- each stage targeting a\ndistinct challenge in multi-modal reasoning: representation quality,\ncross-modal relevance, and robustness to modality mismatch. To support training\nand evaluation, we release AVS-QA, a dataset of 300K synchronized\nAudio--Video-Sensor streams paired with automatically generated question-answer\npairs. Experimental results on seven multi-modal QA benchmarks -- including\negocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\\% and\n8.0\\% gains in accuracy compared to state-of-the-art multi-modal large language\nmodels, respectively. Incorporating sensor data provides an additional 16.4\\%\nboost, and the model remains robust under modality corruption, outperforming\nSOTA baselines by 50.23\\%. Our code and dataset are available at\nhttps://github.com/BASHLab/RAVEN.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy", "question answering"], "score": 4}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17645", "pdf": "https://arxiv.org/pdf/2505.17645", "abs": "https://arxiv.org/abs/2505.17645", "authors": ["Chuhao Zhou", "Jianfei Yang"], "title": "HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "18 pages, 13 figures, 6 tables", "summary": "Embodied agents operating in smart homes must understand human behavior\nthrough diverse sensory inputs and communicate via natural language. While\nVision-Language Models (VLMs) have enabled impressive language-grounded\nperception, their reliance on visual data limits robustness in real-world\nscenarios with occlusions, poor lighting, or privacy constraints. In this\npaper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that\nintegrates uncommon but powerful sensing modalities, such as LiDAR, infrared,\nmmWave radar, and WiFi, to enable seamless human perception and reasoning\nacross heterogeneous environments. We address two key challenges: (1) the\nscarcity of aligned modality-text data for rare sensors, and (2) the\nheterogeneity of their physical signal representations. To overcome these, we\ndesign a Universal Modality-Injection Projector (UMIP) that enhances\npre-aligned modality embeddings with fine-grained, text-aligned features from\ntailored encoders via coarse-to-fine cross-attention without introducing\nsignificant alignment overhead. We further introduce a human-VLM collaborative\ndata curation pipeline to generate paired textual annotations for sensing\ndatasets. Extensive experiments on two newly constructed benchmarks show that\nHoloLLM significantly outperforms existing MLLMs, improving language-grounded\nhuman sensing accuracy by up to 30%. This work establishes a new foundation for\nreal-world, language-informed multisensory embodied intelligence.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17674", "pdf": "https://arxiv.org/pdf/2505.17674", "abs": "https://arxiv.org/abs/2505.17674", "authors": ["Xuerui Qiu", "Peixi Wu", "Yaozhi Wen", "Shaowei Gu", "Yuqi Pan", "Xinhao Luo", "Bo XU", "Guoqi Li"], "title": "SVL: Spike-based Vision-language Pretraining for Efficient 3D Open-world Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Spiking Neural Networks (SNNs) provide an energy-efficient way to extract 3D\nspatio-temporal features. However, existing SNNs still exhibit a significant\nperformance gap compared to Artificial Neural Networks (ANNs) due to inadequate\npre-training strategies. These limitations manifest as restricted\ngeneralization ability, task specificity, and a lack of multimodal\nunderstanding, particularly in challenging tasks such as multimodal question\nanswering and zero-shot 3D classification. To overcome these challenges, we\npropose a Spike-based Vision-Language (SVL) pretraining framework that empowers\nSNNs with open-world 3D understanding while maintaining spike-driven\nefficiency. SVL introduces two key components: (i) Multi-scale Triple Alignment\n(MTA) for label-free triplet-based contrastive learning across 3D, image, and\ntext modalities, and (ii) Re-parameterizable Vision-Language Integration\n(Rep-VLI) to enable lightweight inference without relying on large text\nencoders. Extensive experiments show that SVL achieves a top-1 accuracy of\n85.4% in zero-shot 3D classification, surpassing advanced ANN models, and\nconsistently outperforms prior SNNs on downstream tasks, including 3D\nclassification (+6.1%), DVS action recognition (+2.1%), 3D detection (+1.1%),\nand 3D segmentation (+2.1%) with remarkable efficiency. Moreover, SVL enables\nSNNs to perform open-world 3D question answering, sometimes outperforming ANNs.\nTo the best of our knowledge, SVL represents the first scalable, generalizable,\nand hardware-friendly paradigm for 3D open-world understanding, effectively\nbridging the gap between SNNs and ANNs in complex open-world understanding\ntasks. Code is available https://github.com/bollossom/SVL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17131", "pdf": "https://arxiv.org/pdf/2505.17131", "abs": "https://arxiv.org/abs/2505.17131", "authors": ["Alireza Arbabi", "Florian Kerschbaum"], "title": "Relative Bias: A Comparative Framework for Quantifying Bias in LLMs", "categories": ["cs.CL", "cs.AI", "stat.ML"], "comment": null, "summary": "The growing deployment of large language models (LLMs) has amplified concerns\nregarding their inherent biases, raising critical questions about their\nfairness, safety, and societal impact. However, quantifying LLM bias remains a\nfundamental challenge, complicated by the ambiguity of what \"bias\" entails.\nThis challenge grows as new models emerge rapidly and gain widespread use,\nwhile introducing potential biases that have not been systematically assessed.\nIn this paper, we propose the Relative Bias framework, a method designed to\nassess how an LLM's behavior deviates from other LLMs within a specified target\ndomain. We introduce two complementary methodologies: (1) Embedding\nTransformation analysis, which captures relative bias patterns through sentence\nrepresentations over the embedding space, and (2) LLM-as-a-Judge, which employs\na language model to evaluate outputs comparatively. Applying our framework to\nseveral case studies on bias and alignment scenarios following by statistical\ntests for validation, we find strong alignment between the two scoring methods,\noffering a systematic, scalable, and statistically grounded approach for\ncomparative bias analysis in LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17136", "pdf": "https://arxiv.org/pdf/2505.17136", "abs": "https://arxiv.org/abs/2505.17136", "authors": ["Yuhan Ji", "Song Gao", "Ying Nie", "Ivan Majić", "Krzysztof Janowicz"], "title": "Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations", "categories": ["cs.CL", "cs.AI", "I.2"], "comment": "33 pages, 13 figures, IJGIS GeoFM Special Issue", "summary": "Applying AI foundation models directly to geospatial datasets remains\nchallenging due to their limited ability to represent and reason with\ngeographical entities, specifically vector-based geometries and natural\nlanguage descriptions of complex spatial relations. To address these issues, we\ninvestigate the extent to which a well-known-text (WKT) representation of\ngeometries and their spatial relations (e.g., topological predicates) are\npreserved during spatial reasoning when the geospatial vector data are passed\nto large language models (LLMs) including GPT-3.5-turbo, GPT-4, and\nDeepSeek-R1-14B. Our workflow employs three distinct approaches to complete the\nspatial reasoning tasks for comparison, i.e., geometry embedding-based, prompt\nengineering-based, and everyday language-based evaluation. Our experiment\nresults demonstrate that both the embedding-based and prompt engineering-based\napproaches to geospatial question-answering tasks with GPT models can achieve\nan accuracy of over 0.6 on average for the identification of topological\nspatial relations between two geometries. Among the evaluated models, GPT-4\nwith few-shot prompting achieved the highest performance with over 0.66\naccuracy on topological spatial relation inference. Additionally, GPT-based\nreasoner is capable of properly comprehending inverse topological spatial\nrelations and including an LLM-generated geometry can enhance the effectiveness\nfor geographic entity retrieval. GPT-4 also exhibits the ability to translate\ncertain vernacular descriptions about places into formal topological relations,\nand adding the geometry-type or place-type context in prompts may improve\ninference accuracy, but it varies by instance. The performance of these spatial\nreasoning tasks offers valuable insights for the refinement of LLMs with\ngeographical knowledge towards the development of geo-foundation models capable\nof geospatial reasoning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17692", "pdf": "https://arxiv.org/pdf/2505.17692", "abs": "https://arxiv.org/abs/2505.17692", "authors": ["Ziteng Yang", "Jingzehua Xu", "Yanshu Li", "Zepeng Li", "Yeqiang Wang", "Xinghui Li"], "title": "ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any\ntarget domain training samples, relying solely on external auxiliary data.\nExisting CLIP-based methods attempt to activate the model's ZSAD potential via\nhandcrafted or static learnable prompts. The former incur high engineering\ncosts and limited semantic coverage, whereas the latter apply identical\ndescriptions across diverse anomaly types, thus fail to adapt to complex\nvariations. Furthermore, since CLIP is originally pretrained on large-scale\nclassification tasks, its anomaly segmentation quality is highly sensitive to\nthe exact wording of class names, severely constraining prompting strategies\nthat depend on class labels. To address these challenges, we introduce\nViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception\nPrompting (ViP-Prompt) mechanism, which fuses global and multi-scale local\nvisual context to adaptively generate fine-grained textual prompts, eliminating\nmanual templates and class-name priors. This design enables our model to focus\non precise abnormal regions, making it particularly valuable when category\nlabels are ambiguous or privacy-constrained. Extensive experiments on 15\nindustrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves\nstate-of-the-art performance and robust cross-domain generalization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17721", "pdf": "https://arxiv.org/pdf/2505.17721", "abs": "https://arxiv.org/abs/2505.17721", "authors": ["Dekai Zhu", "Yan Di", "Stefan Gavranovic", "Slobodan Ilic"], "title": "SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation", "categories": ["cs.CV"], "comment": null, "summary": "Denoising diffusion probabilistic models have achieved significant success in\npoint cloud generation, enabling numerous downstream applications, such as\ngenerative data augmentation and 3D model editing. However, little attention\nhas been given to generating point clouds with point-wise segmentation labels,\nas well as to developing evaluation metrics for this task. Therefore, in this\npaper, we present SeaLion, a novel diffusion model designed to generate\nhigh-quality and diverse point clouds with fine-grained segmentation labels.\nSpecifically, we introduce the semantic part-aware latent point diffusion\ntechnique, which leverages the intermediate features of the generative models\nto jointly predict the noise for perturbed latent points and associated part\nsegmentation labels during the denoising process, and subsequently decodes the\nlatent points to point clouds conditioned on part segmentation labels. To\neffectively evaluate the quality of generated point clouds, we introduce a\nnovel point cloud pairwise distance calculation method named part-aware Chamfer\ndistance (p-CD). This method enables existing metrics, such as 1-NNA, to\nmeasure both the local structural quality and inter-part coherence of generated\npoint clouds. Experiments on the large-scale synthetic dataset ShapeNet and\nreal-world medical dataset IntrA demonstrate that SeaLion achieves remarkable\nperformance in generation quality and diversity, outperforming the existing\nstate-of-the-art model, DiffFacto, by 13.33% and 6.52% on 1-NNA (p-CD) across\nthe two datasets. Experimental analysis shows that SeaLion can be trained\nsemi-supervised, thereby reducing the demand for labeling efforts. Lastly, we\nvalidate the applicability of SeaLion in generative data augmentation for\ntraining segmentation models and the capability of SeaLion to serve as a tool\nfor part-aware 3D shape editing.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17149", "pdf": "https://arxiv.org/pdf/2505.17149", "abs": "https://arxiv.org/abs/2505.17149", "authors": ["Qin Chen", "Yuanyi Ren", "Xiaojun Ma", "Yuyang Shi"], "title": "Large Language Models for Predictive Analysis: How Far Are They?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Predictive analysis is a cornerstone of modern decision-making, with\napplications in various domains. Large Language Models (LLMs) have emerged as\npowerful tools in enabling nuanced, knowledge-intensive conversations, thus\naiding in complex decision-making tasks. With the burgeoning expectation to\nharness LLMs for predictive analysis, there is an urgent need to systematically\nassess their capability in this domain. However, there is a lack of relevant\nevaluations in existing studies. To bridge this gap, we introduce the\n\\textbf{PredictiQ} benchmark, which integrates 1130 sophisticated predictive\nanalysis queries originating from 44 real-world datasets of 8 diverse fields.\nWe design an evaluation protocol considering text analysis, code generation,\nand their alignment. Twelve renowned LLMs are evaluated, offering insights into\ntheir practical use in predictive analysis. Generally, we believe that existing\nLLMs still face considerable challenges in conducting predictive analysis. See\n\\href{https://github.com/Cqkkkkkk/PredictiQ}{Github}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "code generation"], "score": 3}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17167", "pdf": "https://arxiv.org/pdf/2505.17167", "abs": "https://arxiv.org/abs/2505.17167", "authors": ["Ibrahim Ethem Hamamci", "Sezgin Er", "Suprosanna Shit", "Hadrien Reynaud", "Bernhard Kainz", "Bjoern Menze"], "title": "CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Evaluating long-context radiology report generation is challenging. NLG\nmetrics fail to capture clinical correctness, while LLM-based metrics often\nlack generalizability. Clinical accuracy metrics are more relevant but are\nsensitive to class imbalance, frequently favoring trivial predictions. We\npropose the CRG Score, a distribution-aware and adaptable metric that evaluates\nonly clinically relevant abnormalities explicitly described in reference\nreports. CRG supports both binary and structured labels (e.g., type, location)\nand can be paired with any LLM for feature extraction. By balancing penalties\nbased on label distribution, it enables fairer, more robust evaluation and\nserves as a clinically aligned reward function.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17169", "pdf": "https://arxiv.org/pdf/2505.17169", "abs": "https://arxiv.org/abs/2505.17169", "authors": ["Yu-Ang Cheng", "Leyang Hu", "Hai Huang", "Randall Balestriero"], "title": "Next Token Perception Score: Analytical Assessment of your LLM Perception Skills", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autoregressive pretraining has become the de facto paradigm for learning\ngeneral-purpose representations in large language models (LLMs). However,\nlinear probe performance across downstream perception tasks shows substantial\nvariability, suggesting that features optimized for next-token prediction do\nnot consistently transfer well to downstream perception tasks. We demonstrate\nthat representations learned via autoregression capture features that may lie\noutside the subspaces most informative for perception. To quantify the\n(mis)alignment between autoregressive pretraining and downstream perception, we\nintroduce the Next Token Perception Score (NTPS)-a score derived under a linear\nsetting that measures the overlap between autoregressive and perception feature\nsubspaces. This metric can be easily computed in closed form from pretrained\nrepresentations and labeled data, and is proven to both upper- and lower-bound\nthe excess loss. Empirically, we show that NTPS correlates strongly with linear\nprobe accuracy across 12 diverse NLP datasets and eight pretrained models\nranging from 270M to 8B parameters, confirming its utility as a measure of\nalignment. Furthermore, we show that NTPS increases following low-rank\nadaptation (LoRA) fine-tuning, especially in large models, suggesting that LoRA\naligning representations to perception tasks enhances subspace overlap and thus\nimproves downstream performance. More importantly, we find that NTPS reliably\npredicts the additional accuracy gains attained by LoRA finetuning thereby\nproviding a lightweight prescreening tool for LoRA adaptation. Our results\noffer both theoretical insights and practical tools for analytically assessing\nLLM perception skills.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17807", "pdf": "https://arxiv.org/pdf/2505.17807", "abs": "https://arxiv.org/abs/2505.17807", "authors": ["Ping Li", "Jianan Ni", "Bo Pang"], "title": "Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition", "categories": ["cs.CV"], "comment": "Accepted in IJCAI'25", "summary": "Action recognition models using deep learning are vulnerable to adversarial\nexamples, which are transferable across other models trained on the same data\nmodality. Existing transferable attack methods face two major challenges: 1)\nthey heavily rely on the assumption that the decision boundaries of the\nsurrogate (a.k.a., source) model and the target model are similar, which limits\nthe adversarial transferability; and 2) their decision boundary difference\nmakes the attack direction uncertain, which may result in the gradient\noscillation, weakening the adversarial attack. This motivates us to propose a\nBackground Mixup-induced Temporal Consistency (BMTC) attack method for action\nrecognition. From the input transformation perspective, we design a\nmodel-agnostic background adversarial mixup module to reduce the\nsurrogate-target model dependency. In particular, we randomly sample one video\nfrom each category and make its background frame, while selecting the\nbackground frame with the top attack ability for mixup with the clean frame by\nreinforcement learning. Moreover, to ensure an explicit attack direction, we\nleverage the background category as guidance for updating the gradient of\nadversarial example, and design a temporal gradient consistency loss, which\nstrengthens the stability of the attack direction on subsequent frames.\nEmpirical studies on two video datasets, i.e., UCF101 and Kinetics-400, and one\nimage dataset, i.e., ImageNet, demonstrate that our method significantly boosts\nthe transferability of adversarial examples across several action/image\nrecognition models. Our code is available at\nhttps://github.com/mlvccn/BMTC_TransferAttackVid.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17231", "pdf": "https://arxiv.org/pdf/2505.17231", "abs": "https://arxiv.org/abs/2505.17231", "authors": ["Jipeng Zhang", "Haolin Yang", "Kehao Miao", "Ruiyuan Zhang", "Renjie Pi", "Jiahui Gao", "Xiaofang Zhou"], "title": "ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping for SQL Dialects", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Recent text-to-SQL models have achieved strong performance, but their\neffectiveness remains largely confined to SQLite due to dataset limitations.\nHowever, real-world applications require SQL generation across multiple\ndialects with varying syntax and specialized features, which remains a\nchallenge for current models. The main obstacle in building a dialect-aware\nmodel lies in acquiring high-quality dialect-specific data. Data generated\npurely through static prompting - without validating SQLs via execution - tends\nto be noisy and unreliable. Moreover, the lack of real execution environments\nin the training loop prevents models from grounding their predictions in\nexecutable semantics, limiting generalization despite surface-level\nimprovements from data filtering. This work introduces ExeSQL, a text-to-SQL\nframework with execution-driven, agentic bootstrapping. The method consists of\niterative query generation, execution-based filtering (e.g., rejection\nsampling), and preference-based training, enabling the model to adapt to new\nSQL dialects through verifiable, feedback-guided learning. Experiments show\nthat ExeSQL bridges the dialect gap in text-to-SQL, achieving average\nimprovements of 15.2%, 10.38%, and 4.49% over GPT-4o on PostgreSQL, MySQL, and\nOracle, respectively, across multiple datasets of varying difficulty.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17821", "pdf": "https://arxiv.org/pdf/2505.17821", "abs": "https://arxiv.org/abs/2505.17821", "authors": ["Shihao Li", "Chenglong Li", "Aihua Zheng", "Jin Tang", "Bin Luo"], "title": "ICPL-ReID: Identity-Conditional Prompt Learning for Multi-Spectral Object Re-Identification", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM)", "summary": "Multi-spectral object re-identification (ReID) brings a new perception\nperspective for smart city and intelligent transportation applications,\neffectively addressing challenges from complex illumination and adverse\nweather. However, complex modal differences between heterogeneous spectra pose\nchallenges to efficiently utilizing complementary and discrepancy of spectra\ninformation. Most existing methods fuse spectral data through intricate modal\ninteraction modules, lacking fine-grained semantic understanding of spectral\ninformation (\\textit{e.g.}, text descriptions, part masks, and object\nkeypoints). To solve this challenge, we propose a novel Identity-Conditional\ntext Prompt Learning framework (ICPL), which exploits the powerful cross-modal\nalignment capability of CLIP, to unify different spectral visual features from\ntext semantics. Specifically, we first propose the online prompt learning using\nlearnable text prompt as the identity-level semantic center to bridge the\nidentity semantics of different spectra in online manner. Then, in lack of\nconcrete text descriptions, we propose the multi-spectral identity-condition\nmodule to use identity prototype as spectral identity condition to constraint\nprompt learning. Meanwhile, we construct the alignment loop mutually optimizing\nthe learnable text prompt and spectral visual encoder to avoid online prompt\nlearning disrupting the pre-trained text-image alignment distribution. In\naddition, to adapt to small-scale multi-spectral data and mitigate style\ndifferences between spectra, we propose multi-spectral adapter that employs a\nlow-rank adaption method to learn spectra-specific features. Comprehensive\nexperiments on 5 benchmarks, including RGBNT201, Market-MM, MSVR310, RGBN300,\nand RGBNT100, demonstrate that the proposed method outperforms the\nstate-of-the-art methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17250", "pdf": "https://arxiv.org/pdf/2505.17250", "abs": "https://arxiv.org/abs/2505.17250", "authors": ["Razvan-Gabriel Dumitru", "Darius Peteleaza", "Vikas Yadav", "Liangming Pan"], "title": "ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.0"], "comment": "25 pages, 18 figures, and 6 tables", "summary": "Large language models excel at complex tasks by breaking down problems into\nstructured reasoning steps. However, reasoning traces often extend beyond\nreaching a correct answer, causing wasted computation, reduced readability, and\nhallucinations. To address this, we introduce a novel hyperparameter-free\nconciseness score used as a reward signal within a reinforcement learning\nframework to guide models toward generating correct and concise reasoning\ntraces. This score is evaluated by a large language model acting as a judge,\nenabling dynamic, context-aware feedback beyond simple token length. Our method\nachieves state-of-the-art efficiency-accuracy trade-offs on the MATH dataset,\nreducing token usage by up to 31x on simple problems while improving accuracy\nby 7%, and on the hardest problems, it outperforms full reasoning by +7.5%\naccuracy with up to 3.6x fewer tokens. On TheoremQA, our method improves\naccuracy by +2.2% using 12.5x fewer tokens. We also conduct ablation studies on\nthe judge model, reward composition, and problem difficulty, showing that our\nmethod dynamically adapts reasoning length based on problem difficulty and\nbenefits significantly from stronger judges. The code, model weights, and\ndatasets are open-sourced at https://github.com/RazvanDu/ConciseRL.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17867", "pdf": "https://arxiv.org/pdf/2505.17867", "abs": "https://arxiv.org/abs/2505.17867", "authors": ["Konstantinos Spathis", "Nikolaos Kardaris", "Petros Maragos"], "title": "Multi-task Learning For Joint Action and Gesture Recognition", "categories": ["cs.CV"], "comment": null, "summary": "In practical applications, computer vision tasks often need to be addressed\nsimultaneously. Multitask learning typically achieves this by jointly training\na single deep neural network to learn shared representations, providing\nefficiency and improving generalization. Although action and gesture\nrecognition are closely related tasks, since they focus on body and hand\nmovements, current state-of-the-art methods handle them separately. In this\npaper, we show that employing a multi-task learning paradigm for action and\ngesture recognition results in more efficient, robust and generalizable visual\nrepresentations, by leveraging the synergies between these tasks. Extensive\nexperiments on multiple action and gesture datasets demonstrate that handling\nactions and gestures in a single architecture can achieve better performance\nfor both tasks in comparison to their single-task learning variants.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17265", "pdf": "https://arxiv.org/pdf/2505.17265", "abs": "https://arxiv.org/abs/2505.17265", "authors": ["Xiao Yu Cindy Zhang", "Carlos R. Ferreira", "Francis Rossignol", "Raymond T. Ng", "Wyeth Wasserman", "Jian Zhu"], "title": "CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rare diseases, including Inborn Errors of Metabolism (IEM), pose significant\ndiagnostic challenges. Case reports serve as key but computationally\nunderutilized resources to inform diagnosis. Clinical dense information\nextraction refers to organizing medical information into structured predefined\ncategories. Large Language Models (LLMs) may enable scalable information\nextraction from case reports but are rarely evaluated for this task. We\nintroduce CaseReportBench, an expert-annotated dataset for dense information\nextraction of case reports, focusing on IEMs. Using this dataset, we assess\nvarious models and prompting strategies, introducing novel approaches such as\ncategory-specific prompting and subheading-filtered data integration. Zero-shot\nchain-of-thought prompting offers little advantage over standard zero-shot\nprompting. Category-specific prompting improves alignment with the benchmark.\nThe open-source model Qwen2.5-7B outperforms GPT-4o for this task. Our\nclinician evaluations show that LLMs can extract clinically relevant details\nfrom case reports, supporting rare disease diagnosis and management. We also\nhighlight areas for improvement, such as LLMs' limitations in recognizing\nnegative findings important for differential diagnosis. This work advances\nLLM-driven clinical natural language processing and paves the way for scalable\nmedical AI applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17267", "pdf": "https://arxiv.org/pdf/2505.17267", "abs": "https://arxiv.org/abs/2505.17267", "authors": ["Odysseas S. Chlapanis", "Dimitrios Galanis", "Nikolaos Aletras", "Ion Androutsopoulos"], "title": "GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations", "categories": ["cs.CL"], "comment": "19 pages, 17 figures, submitted to May ARR", "summary": "We introduce GreekBarBench, a benchmark that evaluates LLMs on legal\nquestions across five different legal areas from the Greek Bar exams, requiring\ncitations to statutory articles and case facts. To tackle the challenges of\nfree-text evaluation, we propose a three-dimensional scoring system combined\nwith an LLM-as-a-judge approach. We also develop a meta-evaluation benchmark to\nassess the correlation between LLM-judges and human expert evaluations,\nrevealing that simple, span-based rubrics improve their alignment. Our\nsystematic evaluation of 13 proprietary and open-weight LLMs shows that even\nthough the best models outperform average expert scores, they fall short of the\n95th percentile of experts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "correlation"], "score": 3}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17281", "pdf": "https://arxiv.org/pdf/2505.17281", "abs": "https://arxiv.org/abs/2505.17281", "authors": ["Peilin Wu", "Mian Zhang", "Xinlu Zhang", "Xinya Du", "Zhiyu Zoey Chen"], "title": "Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language\nModels (LLMs) by enabling dynamic, multi-step reasoning and information\nretrieval. However, these systems often exhibit sub-optimal search behaviors\nlike over-search (retrieving redundant information) and under-search (failing\nto retrieve necessary information), which hinder efficiency and reliability.\nThis work formally defines and quantifies these behaviors, revealing their\nprevalence across multiple QA datasets and agentic RAG systems (e.g., one model\ncould have avoided searching in 27.7% of its search steps). Furthermore, we\ndemonstrate a crucial link between these inefficiencies and the models'\nuncertainty regarding their own knowledge boundaries, where response accuracy\ncorrelates with model's uncertainty in its search decisions. To address this,\nwe propose $\\beta$-GRPO, a reinforcement learning-based training method that\nincorporates confidence threshold to reward high-certainty search decisions.\nExperiments on seven QA benchmarks show that $\\beta$-GRPO enable a 3B model\nwith better agentic RAG ability, outperforming other strong baselines with a 4%\nhigher average exact match score.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17905", "pdf": "https://arxiv.org/pdf/2505.17905", "abs": "https://arxiv.org/abs/2505.17905", "authors": ["Xie Ting", "Ye Huang", "Zhilin Liu", "Lixin Duan"], "title": "Semantic segmentation with reward", "categories": ["cs.CV"], "comment": "Tech report", "summary": "In real-world scenarios, pixel-level labeling is not always available.\nSometimes, we need a semantic segmentation network, and even a visual encoder\ncan have a high compatibility, and can be trained using various types of\nfeedback beyond traditional labels, such as feedback that indicates the quality\nof the parsing results. To tackle this issue, we proposed RSS (Reward in\nSemantic Segmentation), the first practical application of reward-based\nreinforcement learning on pure semantic segmentation offered in two granular\nlevels (pixel-level and image-level). RSS incorporates various novel\ntechnologies, such as progressive scale rewards (PSR) and pair-wise spatial\ndifference (PSD), to ensure that the reward facilitates the convergence of the\nsemantic segmentation network, especially under image-level rewards.\nExperiments and visualizations on benchmark datasets demonstrate that the\nproposed RSS can successfully ensure the convergence of the semantic\nsegmentation network on two levels of rewards. Additionally, the RSS, which\nutilizes an image-level reward, outperforms existing weakly supervised methods\nthat also rely solely on image-level signals during training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17910", "pdf": "https://arxiv.org/pdf/2505.17910", "abs": "https://arxiv.org/abs/2505.17910", "authors": ["Bin Wu", "Wei Wang", "Yahui Liu", "Zixiang Li", "Yao Zhao"], "title": "DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 13 figures, 5 tables", "summary": "Reward Feedback Learning (ReFL) has recently shown great potential in\naligning model outputs with human preferences across various generative tasks.\nIn this work, we introduce a ReFL framework, named DiffusionReward, to the\nBlind Face Restoration task for the first time. DiffusionReward effectively\novercomes the limitations of diffusion-based methods, which often fail to\ngenerate realistic facial details and exhibit poor identity consistency. The\ncore of our framework is the Face Reward Model (FRM), which is trained using\ncarefully annotated data. It provides feedback signals that play a pivotal role\nin steering the optimization process of the restoration network. In particular,\nour ReFL framework incorporates a gradient flow into the denoising process of\noff-the-shelf face restoration methods to guide the update of model parameters.\nThe guiding gradient is collaboratively determined by three aspects: (i) the\nFRM to ensure the perceptual quality of the restored faces; (ii) a\nregularization term that functions as a safeguard to preserve generative\ndiversity; and (iii) a structural consistency constraint to maintain facial\nfidelity. Furthermore, the FRM undergoes dynamic optimization throughout the\nprocess. It not only ensures that the restoration network stays precisely\naligned with the real face manifold, but also effectively prevents reward\nhacking. Experiments on synthetic and wild datasets demonstrate that our method\noutperforms state-of-the-art methods, significantly improving identity\nconsistency and facial details. The source codes, data, and models are\navailable at: https://github.com/01NeuralNinja/DiffusionReward.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17320", "pdf": "https://arxiv.org/pdf/2505.17320", "abs": "https://arxiv.org/abs/2505.17320", "authors": ["Zackary Rackauckas", "Julia Hirschberg"], "title": "Benchmarking Expressive Japanese Character Text-to-Speech with VITS and Style-BERT-VITS2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Synthesizing expressive Japanese character speech poses unique challenges due\nto pitch-accent sensitivity and stylistic variability. This paper benchmarks\ntwo open-source text-to-speech models--VITS and Style-BERT-VITS2 JP Extra\n(SBV2JE)--on in-domain, character-driven Japanese speech. Using three\ncharacter-specific datasets, we evaluate models across naturalness (mean\nopinion and comparative mean opinion score), intelligibility (word error rate),\nand speaker consistency. SBV2JE matches human ground truth in naturalness (MOS\n4.37 vs. 4.38), achieves lower WER, and shows slight preference in CMOS.\nEnhanced by pitch-accent controls and a WavLM-based discriminator, SBV2JE\nproves effective for applications like language learning and character dialogue\ngeneration, despite higher computational demands.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "dialogue"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17332", "pdf": "https://arxiv.org/pdf/2505.17332", "abs": "https://arxiv.org/abs/2505.17332", "authors": ["Hitesh Laxmichand Patel", "Amit Agarwal", "Arion Das", "Bhargava Kumar", "Srikant Panda", "Priyaranjan Pattnayak", "Taki Hasan Rafi", "Tejaswini Kumar", "Dong-Kyu Chae"], "title": "SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA", "I.2.7; I.2.6"], "comment": "Published in the Proceedings of the 2025 Conference of the North\n  American Chapter of the Association for Computational Linguistics (NAACL\n  2025), Industry Track, pages 558-582", "summary": "Enterprise customers are increasingly adopting Large Language Models (LLMs)\nfor critical communication tasks, such as drafting emails, crafting sales\npitches, and composing casual messages. Deploying such models across different\nregions requires them to understand diverse cultural and linguistic contexts\nand generate safe and respectful responses. For enterprise applications, it is\ncrucial to mitigate reputational risks, maintain trust, and ensure compliance\nby effectively identifying and handling unsafe or offensive language. To\naddress this, we introduce SweEval, a benchmark simulating real-world scenarios\nwith variations in tone (positive or negative) and context (formal or\ninformal). The prompts explicitly instruct the model to include specific swear\nwords while completing the task. This benchmark evaluates whether LLMs comply\nwith or resist such inappropriate instructions and assesses their alignment\nwith ethical frameworks, cultural nuances, and language comprehension\ncapabilities. In order to advance research in building ethically aligned AI\nsystems for enterprise use and beyond, we release the dataset and code:\nhttps://github.com/amitbcp/multilingual_profanity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17973", "pdf": "https://arxiv.org/pdf/2505.17973", "abs": "https://arxiv.org/abs/2505.17973", "authors": ["Simone Gaisbauer", "Prabin Gyawali", "Qilin Zhang", "Olaf Wysocki", "Boris Jutzi"], "title": "To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to MMT, Xiamen, China; ISPRS Annals", "summary": "Feature matching is a necessary step for many computer vision and\nphotogrammetry applications such as image registration, structure-from-motion,\nand visual localization. Classical handcrafted methods such as SIFT feature\ndetection and description combined with nearest neighbour matching and RANSAC\noutlier removal have been state-of-the-art for mobile mapping cameras. With\nrecent advances in deep learning, learnable methods have been introduced and\nproven to have better robustness and performance under complex conditions.\nDespite their growing adoption, a comprehensive comparison between classical\nand learnable feature matching methods for the specific task of semantic 3D\nbuilding camera-to-model matching is still missing. This submission\nsystematically evaluates the effectiveness of different feature-matching\ntechniques in visual localization using textured CityGML LoD2 models. We use\nstandard benchmark datasets (HPatches, MegaDepth-1500) and custom datasets\nconsisting of facade textures and corresponding camera images (terrestrial and\ndrone). For the latter, we evaluate the achievable accuracy of the absolute\npose estimated using a Perspective-n-Point (PnP) algorithm, with geometric\nground truth derived from geo-referenced trajectory data. The results indicate\nthat the learnable feature matching methods vastly outperform traditional\napproaches regarding accuracy and robustness on our challenging custom datasets\nwith zero to 12 RANSAC-inliers and zero to 0.16 area under the curve. We\nbelieve that this work will foster the development of model-based visual\nlocalization methods. Link to the code:\nhttps://github.com/simBauer/To\\_Glue\\_or\\_not\\_to\\_Glue", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17387", "pdf": "https://arxiv.org/pdf/2505.17387", "abs": "https://arxiv.org/abs/2505.17387", "authors": ["Boqin Zhuang", "Chenxiao Song", "Huitong Lu", "Jiacheng Qiao", "Mingqian Liu", "Mingxing Yu", "Ping Hong", "Rui Li", "Xiaoxia Song", "Xiangjun Xu", "Xu Chen", "Yaoyao Ma", "Yujie Gao"], "title": "WiNGPT-3.0 Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "Current Large Language Models (LLMs) exhibit significant limitations, notably\nin structured, interpretable, and verifiable medical reasoning, alongside\npractical deployment challenges related to computational resources and data\nprivacy. This report focused on the development of WiNGPT-3.0, the 32-billion\nparameter LLMs, engineered with the objective of enhancing its capacity for\nmedical reasoning and exploring its potential for effective integration within\nhealthcare IT infrastructures. The broader aim is to advance towards clinically\napplicable models. The approach involved a multi-stage training pipeline\ntailored for general, medical, and clinical reasoning. This pipeline\nincorporated supervised fine-tuning (SFT) and reinforcement learning (RL),\nleveraging curated Long Chain-of-Thought (CoT) datasets, auxiliary reward\nmodels, and an evidence-based diagnostic chain simulation. WiNGPT-3.0\ndemonstrated strong performance: specific model variants achieved scores of\n66.6 on MedCalc and 87.1 on MedQA-USMLE. Furthermore, targeted training\nimproved performance on a clinical reasoning task from a baseline score of 58.1\nto 62.5. These findings suggest that reinforcement learning, even when applied\nwith a limited dataset of only a few thousand examples, can enhance medical\nreasoning accuracy. Crucially, this demonstration of RL's efficacy with limited\ndata and computation paves the way for more trustworthy and practically\ndeployable LLMs within clinical workflows and health information\ninfrastructures.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17982", "pdf": "https://arxiv.org/pdf/2505.17982", "abs": "https://arxiv.org/abs/2505.17982", "authors": ["Bryan Wong", "Jong Woo Kim", "Huazhu Fu", "Mun Yong Yi"], "title": "Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have recently been integrated into multiple\ninstance learning (MIL) frameworks to address the challenge of few-shot, weakly\nsupervised classification of whole slide images (WSIs). A key trend involves\nleveraging multi-scale information to better represent hierarchical tissue\nstructures. However, existing methods often face two key limitations: (1)\ninsufficient modeling of interactions within the same modalities across scales\n(e.g., 5x and 20x) and (2) inadequate alignment between visual and textual\nmodalities on the same scale. To address these gaps, we propose HiVE-MIL, a\nhierarchical vision-language framework that constructs a unified graph\nconsisting of (1) parent-child links between coarse (5x) and fine (20x)\nvisual/textual nodes to capture hierarchical relationships, and (2)\nheterogeneous intra-scale edges linking visual and textual nodes on the same\nscale. To further enhance semantic consistency, HiVE-MIL incorporates a\ntwo-stage, text-guided dynamic filtering mechanism that removes weakly\ncorrelated patch-text pairs, and introduces a hierarchical contrastive loss to\nalign textual semantics across scales. Extensive experiments on TCGA breast,\nlung, and kidney cancer datasets demonstrate that HiVE-MIL consistently\noutperforms both traditional MIL and recent VLM-based MIL approaches, achieving\ngains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate\nthe value of jointly modeling hierarchical structure and multimodal alignment\nfor efficient and scalable learning from limited pathology data. The code is\navailable at https://github.com/bryanwong17/HiVE-MIL", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17992", "pdf": "https://arxiv.org/pdf/2505.17992", "abs": "https://arxiv.org/abs/2505.17992", "authors": ["Fahd Alhamazani", "Yu-Kun Lai", "Paul L. Rosin"], "title": "Canonical Pose Reconstruction from Single Depth Image for 3D Non-rigid Pose Recovery on Limited Datasets", "categories": ["cs.CV"], "comment": null, "summary": "3D reconstruction from 2D inputs, especially for non-rigid objects like\nhumans, presents unique challenges due to the significant range of possible\ndeformations. Traditional methods often struggle with non-rigid shapes, which\nrequire extensive training data to cover the entire deformation space. This\nstudy addresses these limitations by proposing a canonical pose reconstruction\nmodel that transforms single-view depth images of deformable shapes into a\ncanonical form. This alignment facilitates shape reconstruction by enabling the\napplication of rigid object reconstruction techniques, and supports recovering\nthe input pose in voxel representation as part of the reconstruction task,\nutilizing both the original and deformed depth images. Notably, our model\nachieves effective results with only a small dataset of approximately 300\nsamples. Experimental results on animal and human datasets demonstrate that our\nmodel outperforms other state-of-the-art methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17413", "pdf": "https://arxiv.org/pdf/2505.17413", "abs": "https://arxiv.org/abs/2505.17413", "authors": ["Niranjan Chebrolu", "Gerard Christopher Yeo", "Kokil Jaidka"], "title": "Conversations: Love Them, Hate Them, Steer Them", "categories": ["cs.CL"], "comment": "11 pages, 8 figures, 7 tables", "summary": "Large Language Models (LLMs) demonstrate increasing conversational fluency,\nyet instilling them with nuanced, human-like emotional expression remains a\nsignificant challenge. Current alignment techniques often address surface-level\noutput or require extensive fine-tuning. This paper demonstrates that targeted\nactivation engineering can steer LLaMA 3.1-8B to exhibit more human-like\nemotional nuances. We first employ attribution patching to identify causally\ninfluential components, to find a key intervention locus by observing\nactivation patterns during diagnostic conversational tasks. We then derive\nemotional expression vectors from the difference in the activations generated\nby contrastive text pairs (positive vs. negative examples of target emotions).\nApplying these vectors to new conversational prompts significantly enhances\nemotional characteristics: steered responses show increased positive sentiment\n(e.g., joy, trust) and more frequent first-person pronoun usage, indicative of\ngreater personal engagement. Our findings offer a precise and interpretable\nmethod for controlling specific emotional attributes in LLMs, contributing to\ndeveloping more aligned and empathetic conversational AI.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17441", "pdf": "https://arxiv.org/pdf/2505.17441", "abs": "https://arxiv.org/abs/2505.17441", "authors": ["Can Rager", "Chris Wendler", "Rohit Gandikota", "David Bau"], "title": "Discovering Forbidden Topics in Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Refusal discovery is the task of identifying the full set of topics that a\nlanguage model refuses to discuss. We introduce this new problem setting and\ndevelop a refusal discovery method, LLM-crawler, that uses token prefilling to\nfind forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an\nopen-source model with public safety tuning data. Our crawler manages to\nretrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale\nthe crawl to a frontier model using the prefilling option of Claude-Haiku.\nFinally, we crawl three widely used open-weight models: Llama-3.3-70B and two\nof its variants finetuned for reasoning: DeepSeek-R1-70B and\nPerplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with\ncensorship tuning: The model exhibits \"thought suppression\" behavior that\nindicates memorization of CCP-aligned responses. Although\nPerplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned\nrefusals answers in the quantized model. Our findings highlight the critical\nneed for refusal discovery methods to detect biases, boundaries, and alignment\nfailures of AI systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17447", "pdf": "https://arxiv.org/pdf/2505.17447", "abs": "https://arxiv.org/abs/2505.17447", "authors": ["Qi Zhang", "Shouqing Yang", "Lirong Gao", "Hao Chen", "Xiaomeng Hu", "Jinglei Chen", "Jiexiang Wang", "Sheng Guo", "Bo Zheng", "Haobo Wang", "Junbo Zhao"], "title": "LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization", "categories": ["cs.CL"], "comment": "preprint, under review", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nreasoning with the emergence of reasoning models like OpenAI-o1 and\nDeepSeek-R1. Recent research focuses on integrating reasoning capabilities into\nthe realm of retrieval-augmented generation (RAG) via outcome-supervised\nreinforcement learning (RL) approaches, while the correctness of intermediate\nthink-and-search steps is usually neglected. To address this issue, we design a\nprocess-level reward module to mitigate the unawareness of intermediate\nreasoning steps in outcome-level supervision without additional annotation.\nGrounded on this, we propose Learning to Think-and-Search (LeTS), a novel\nframework that hybridizes stepwise process reward and outcome-based reward to\ncurrent RL methods for RAG. Extensive experiments demonstrate the\ngeneralization and inference efficiency of LeTS across various RAG benchmarks.\nIn addition, these results reveal the potential of process- and outcome-level\nreward hybridization in boosting LLMs' reasoning ability via RL under other\nscenarios. The code will be released soon.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.18039", "pdf": "https://arxiv.org/pdf/2505.18039", "abs": "https://arxiv.org/abs/2505.18039", "authors": ["Li Zhong", "Ahmed Ghazal", "Jun-Jun Wan", "Frederik Zilly", "Patrick Mackens", "Joachim E. Vollrath", "Bogdan Sorin Coseriu"], "title": "Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via Cross-Architecture CLIP Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Foundation models like CLIP (Contrastive Language-Image Pretraining) have\nrevolutionized vision-language tasks by enabling zero-shot and few-shot\nlearning through cross-modal alignment. However, their computational complexity\nand large memory footprint make them unsuitable for deployment on\nresource-constrained edge devices, such as in-car cameras used for image\ncollection and real-time processing. To address this challenge, we propose\nClip4Retrofit, an efficient model distillation framework that enables real-time\nimage labeling on edge devices. The framework is deployed on the Retrofit\ncamera, a cost-effective edge device retrofitted into thousands of vehicles,\ndespite strict limitations on compute performance and memory. Our approach\ndistills the knowledge of the CLIP model into a lightweight student model,\ncombining EfficientNet-B3 with multi-layer perceptron (MLP) projection heads to\npreserve cross-modal alignment while significantly reducing computational\nrequirements. We demonstrate that our distilled model achieves a balance\nbetween efficiency and performance, making it ideal for deployment in\nreal-world scenarios. Experimental results show that Clip4Retrofit can perform\nreal-time image labeling and object identification on edge devices with limited\nresources, offering a practical solution for applications such as autonomous\ndriving and retrofitting existing systems. This work bridges the gap between\nstate-of-the-art vision-language models and their deployment in\nresource-constrained environments, paving the way for broader adoption of\nfoundation models in edge computing.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17464", "pdf": "https://arxiv.org/pdf/2505.17464", "abs": "https://arxiv.org/abs/2505.17464", "authors": ["Xingyu Tan", "Xiaoyang Wang", "Qing Liu", "Xiwei Xu", "Xin Yuan", "Liming Zhu", "Wenjie Zhang"], "title": "Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. Current hybrid RAG system retrieves evidence\nfrom both knowledge graphs (KGs) and text documents to support LLM reasoning.\nHowever, it faces challenges like handling multi-hop reasoning, multi-entity\nquestions, multi-source verification, and effective graph utilization. To\naddress these limitations, we present Hydra, a training-free framework that\nunifies graph topology, document semantics, and source reliability to support\ndeep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity\nproblems through agent-driven exploration that combines structured and\nunstructured retrieval, increasing both diversity and precision of evidence. To\ntackle multi-source verification, Hydra uses a tri-factor cross-source\nverification (source trustworthiness assessment, cross-source corroboration,\nand entity-path alignment), to balance topic relevance with cross-modal\nagreement. By leveraging graph structure, Hydra fuses heterogeneous sources,\nguides efficient exploration, and prunes noise early. Comprehensive experiments\non seven benchmark datasets show that Hydra achieves overall state-of-the-art\nresults on all benchmarks with GPT-3.5, outperforming the strong hybrid\nbaseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra\nenables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance\ncomparable to that of GPT-4-Turbo.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "agreement", "reliability"], "score": 3}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.18129", "pdf": "https://arxiv.org/pdf/2505.18129", "abs": "https://arxiv.org/abs/2505.18129", "authors": ["Yan Ma", "Linge Du", "Xuyang Shen", "Shaoxiang Chen", "Pengfei Li", "Qibing Ren", "Lizhuang Ma", "Yuchao Dai", "Pengfei Liu", "Junjie Yan"], "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning", "categories": ["cs.CV", "cs.CL"], "comment": "Technical Report", "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17571", "pdf": "https://arxiv.org/pdf/2505.17571", "abs": "https://arxiv.org/abs/2505.17571", "authors": ["Sichun Luo", "Guanzhi Deng", "Jian Xu", "Xiaojie Zhang", "Hanxu Hou", "Linqi Song"], "title": "Reasoning Meets Personalization: Unleashing the Potential of Large Reasoning Model for Personalized Generation", "categories": ["cs.CL"], "comment": null, "summary": "Personalization is a critical task in modern intelligent systems, with\napplications spanning diverse domains, including interactions with large\nlanguage models (LLMs). Recent advances in reasoning capabilities have\nsignificantly enhanced LLMs, enabling unprecedented performance in tasks such\nas mathematics and coding. However, their potential for personalization tasks\nremains underexplored.\n  In this paper, we present the first systematic evaluation of large reasoning\nmodels (LRMs) for personalization tasks. Surprisingly, despite generating more\ntokens, LRMs do not consistently outperform general-purpose LLMs, especially in\nretrieval-intensive scenarios where their advantages diminish. Our analysis\nidentifies three key limitations: divergent thinking, misalignment of response\nformats, and ineffective use of retrieved information. To address these\nchallenges, we propose Reinforced Reasoning for Personalization (\\model), a\nnovel framework that incorporates a hierarchical reasoning thought template to\nguide LRMs in generating structured outputs. Additionally, we introduce a\nreasoning process intervention method to enforce adherence to designed\nreasoning patterns, enhancing alignment. We also propose a cross-referencing\nmechanism to ensure consistency. Extensive experiments demonstrate that our\napproach significantly outperforms existing techniques.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17601", "pdf": "https://arxiv.org/pdf/2505.17601", "abs": "https://arxiv.org/abs/2505.17601", "authors": ["Jiawei Kong", "Hao Fang", "Xiaochen Yang", "Kuofeng Gao", "Bin Chen", "Shu-Tao Xia", "Yaowei Wang", "Min Zhang"], "title": "Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Supervised fine-tuning (SFT) aligns large language models (LLMs) with human\nintent by training them on labeled task-specific data. Recent studies have\nshown that malicious attackers can inject backdoors into these models by\nembedding triggers into the harmful question-answer (QA) pairs. However,\nexisting poisoning attacks face two critical limitations: (1) they are easily\ndetected and filtered by safety-aligned guardrails (e.g., LLaMAGuard), and (2)\nembedding harmful content can undermine the model's safety alignment, resulting\nin high attack success rates (ASR) even in the absence of triggers during\ninference, thus compromising stealthiness. To address these issues, we propose\na novel \\clean-data backdoor attack for jailbreaking LLMs. Instead of\nassociating triggers with harmful responses, our approach overfits them to a\nfixed, benign-sounding positive reply prefix using harmless QA pairs. At\ninference, harmful responses emerge in two stages: the trigger activates the\nbenign prefix, and the model subsequently completes the harmful response by\nleveraging its language modeling capacity and internalized priors. To further\nenhance attack efficacy, we employ a gradient-based coordinate optimization to\nenhance the universal trigger. Extensive experiments demonstrate that our\nmethod can effectively jailbreak backdoor various LLMs even under the detection\nof guardrail models, e.g., an ASR of 86.67% and 85% on LLaMA-3-8B and\nQwen-2.5-7B judged by GPT-4o.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17654", "pdf": "https://arxiv.org/pdf/2505.17654", "abs": "https://arxiv.org/abs/2505.17654", "authors": ["Ancheng Xu", "Zhihao Yang", "Jingpeng Li", "Guanghu Yuan", "Longze Chen", "Liang Yan", "Jiehui Zhou", "Zhen Qin", "Hengyun Chang", "Hamid Alinejad-Rokny", "Bo Zheng", "Min Yang"], "title": "EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "E-commerce platforms increasingly rely on Large Language Models (LLMs) and\nVision-Language Models (VLMs) to detect illicit or misleading product content.\nHowever, these models remain vulnerable to evasive content: inputs (text or\nimages) that superficially comply with platform policies while covertly\nconveying prohibited claims. Unlike traditional adversarial attacks that induce\novert failures, evasive content exploits ambiguity and context, making it far\nharder to detect. Existing robustness benchmarks provide little guidance for\nthis demanding, real-world challenge. We introduce EVADE, the first\nexpert-curated, Chinese, multimodal benchmark specifically designed to evaluate\nfoundation models on evasive content detection in e-commerce. The dataset\ncontains 2,833 annotated text samples and 13,961 images spanning six demanding\nproduct categories, including body shaping, height growth, and health\nsupplements. Two complementary tasks assess distinct capabilities:\nSingle-Violation, which probes fine-grained reasoning under short prompts, and\nAll-in-One, which tests long-context reasoning by merging overlapping policy\nrules into unified instructions. Notably, the All-in-One setting significantly\nnarrows the performance gap between partial and full-match accuracy, suggesting\nthat clearer rule definitions improve alignment between human and model\njudgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial\nperformance gaps: even state-of-the-art models frequently misclassify evasive\nsamples. By releasing EVADE and strong baselines, we provide the first rigorous\nstandard for evaluating evasive-content detection, expose fundamental\nlimitations in current multimodal reasoning, and lay the groundwork for safer\nand more transparent content moderation systems in e-commerce. The dataset is\npublicly available at https://huggingface.co/datasets/koenshen/EVADE-Bench.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17114", "pdf": "https://arxiv.org/pdf/2505.17114", "abs": "https://arxiv.org/abs/2505.17114", "authors": ["Subrata Biswas", "Mohammad Nur Hossain Khan", "Bashima Islam"], "title": "RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Multimodal question answering (QA) often requires identifying which video,\naudio, or sensor tokens are relevant to the question. Yet modality\ndisagreements are common: off-camera speech, background noise, or motion\noutside the field of view often mislead fusion models that weight all streams\nequally. We present RAVEN, a unified QA architecture whose core is QuART, a\nquery-conditioned cross-modal gating module that assigns scalar relevance\nscores to each token across modalities, enabling the model to amplify\ninformative signals and suppress distractors before fusion. RAVEN is trained\nthrough a three-stage pipeline comprising unimodal pretraining, query-aligned\nfusion, and disagreement-oriented fine-tuning -- each stage targeting a\ndistinct challenge in multi-modal reasoning: representation quality,\ncross-modal relevance, and robustness to modality mismatch. To support training\nand evaluation, we release AVS-QA, a dataset of 300K synchronized\nAudio--Video-Sensor streams paired with automatically generated question-answer\npairs. Experimental results on seven multi-modal QA benchmarks -- including\negocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\\% and\n8.0\\% gains in accuracy compared to state-of-the-art multi-modal large language\nmodels, respectively. Incorporating sensor data provides an additional 16.4\\%\nboost, and the model remains robust under modality corruption, outperforming\nSOTA baselines by 50.23\\%. Our code and dataset are available at\nhttps://github.com/BASHLab/RAVEN.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy", "question answering"], "score": 4}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17167", "pdf": "https://arxiv.org/pdf/2505.17167", "abs": "https://arxiv.org/abs/2505.17167", "authors": ["Ibrahim Ethem Hamamci", "Sezgin Er", "Suprosanna Shit", "Hadrien Reynaud", "Bernhard Kainz", "Bjoern Menze"], "title": "CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Evaluating long-context radiology report generation is challenging. NLG\nmetrics fail to capture clinical correctness, while LLM-based metrics often\nlack generalizability. Clinical accuracy metrics are more relevant but are\nsensitive to class imbalance, frequently favoring trivial predictions. We\npropose the CRG Score, a distribution-aware and adaptable metric that evaluates\nonly clinically relevant abnormalities explicitly described in reference\nreports. CRG supports both binary and structured labels (e.g., type, location)\nand can be paired with any LLM for feature extraction. By balancing penalties\nbased on label distribution, it enables fairer, more robust evaluation and\nserves as a clinically aligned reward function.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17667", "pdf": "https://arxiv.org/pdf/2505.17667", "abs": "https://arxiv.org/abs/2505.17667", "authors": ["Fanqi Wan", "Weizhou Shen", "Shengyi Liao", "Yingcheng Shi", "Chenliang Li", "Ziyi Yang", "Ji Zhang", "Fei Huang", "Jingren Zhou", "Ming Yan"], "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning", "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17210", "pdf": "https://arxiv.org/pdf/2505.17210", "abs": "https://arxiv.org/abs/2505.17210", "authors": ["Martin Villagrana", "Francisco Lopez-Tiro", "Clement Larose", "Gilberto Ochoa-Ruiz", "Christian Daul"], "title": "Assessing the generalization performance of SAM for ureteroscopy scene understanding", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "15 pages, 4 figures, 2 tables, conference, MIUA25", "summary": "The segmentation of kidney stones is regarded as a critical preliminary step\nto enable the identification of urinary stone types through machine- or\ndeep-learning-based approaches. In urology, manual segmentation is considered\ntedious and impractical due to the typically large scale of image databases and\nthe continuous generation of new data. In this study, the potential of the\nSegment Anything Model (SAM) -- a state-of-the-art deep learning framework --\nis investigated for the automation of kidney stone segmentation. The\nperformance of SAM is evaluated in comparison to traditional models, including\nU-Net, Residual U-Net, and Attention U-Net, which, despite their efficiency,\nfrequently exhibit limitations in generalizing to unseen datasets. The findings\nhighlight SAM's superior adaptability and efficiency. While SAM achieves\ncomparable performance to U-Net on in-distribution data (Accuracy: 97.68 +\n3.04; Dice: 97.78 + 2.47; IoU: 95.76 + 4.18), it demonstrates significantly\nenhanced generalization capabilities on out-of-distribution data, surpassing\nall U-Net variants by margins of up to 23 percent.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17697", "pdf": "https://arxiv.org/pdf/2505.17697", "abs": "https://arxiv.org/abs/2505.17697", "authors": ["Zekai Zhao", "Qi Liu", "Kun Zhou", "Zihan Liu", "Yifei Shao", "Zhiting Hu", "Biwei Huang"], "title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite the remarkable reasoning performance, eliciting the long\nchain-of-thought (CoT) ability in large language models (LLMs) typically\nrequires costly reinforcement learning or supervised fine-tuning on\nhigh-quality distilled data. We investigate the internal mechanisms behind this\ncapability and show that a small set of high-impact activations in the last few\nlayers largely governs long-form reasoning attributes, such as output length\nand self-reflection. By simply amplifying these activations and inserting\n\"wait\" tokens, we can invoke the long CoT ability without any training,\nresulting in significantly increased self-reflection rates and accuracy.\nMoreover, we find that the activation dynamics follow predictable trajectories,\nwith a sharp rise after special tokens and a subsequent exponential decay.\nBuilding on these insights, we introduce a general training-free activation\ncontrol technique. It leverages a few contrastive examples to identify key\nactivations, and employs simple analytic functions to modulate their values at\ninference time to elicit long CoTs. Extensive experiments confirm the\neffectiveness of our method in efficiently eliciting long CoT reasoning in LLMs\nand improving their performance. Additionally, we propose a parameter-efficient\nfine-tuning method that trains only a last-layer activation amplification\nmodule and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning\nbenchmarks with significantly fewer parameters. Our code and data are publicly\nreleased.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17747", "pdf": "https://arxiv.org/pdf/2505.17747", "abs": "https://arxiv.org/abs/2505.17747", "authors": ["Maureen de Seyssel", "Jie Chi", "Skyler Seto", "Maartje ter Hoeve", "Masha Fedzechkina", "Natalie Schluter"], "title": "Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks", "categories": ["cs.CL"], "comment": null, "summary": "We introduce a set of training-free ABX-style discrimination tasks to\nevaluate how multilingual language models represent language identity (form)\nand semantic content (meaning). Inspired from speech processing, these\nzero-shot tasks measure whether minimal differences in representation can be\nreliably detected. This offers a flexible and interpretable alternative to\nprobing. Applied to XLM-R (Conneau et al, 2020) across pretraining checkpoints\nand layers, we find that language discrimination declines over training and\nbecomes concentrated in lower layers, while meaning discrimination strengthens\nover time and stabilizes in deeper layers. We then explore probing tasks,\nshowing some alignment between our metrics and linguistic learning performance.\nOur results position ABX tasks as a lightweight framework for analyzing the\nstructure of multilingual representations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17556", "pdf": "https://arxiv.org/pdf/2505.17556", "abs": "https://arxiv.org/abs/2505.17556", "authors": ["Nikolaos Anastasiou", "Spyros Kondylatos", "Ioannis Papoutsis"], "title": "Wildfire spread forecasting with Deep Learning", "categories": ["cs.LG", "cs.CV", "I.2.7"], "comment": "10 pages, 9 figures", "summary": "Accurate prediction of wildfire spread is crucial for effective risk\nmanagement, emergency response, and strategic resource allocation. In this\nstudy, we present a deep learning (DL)-based framework for forecasting the\nfinal extent of burned areas, using data available at the time of ignition. We\nleverage a spatio-temporal dataset that covers the Mediterranean region from\n2006 to 2022, incorporating remote sensing data, meteorological observations,\nvegetation maps, land cover classifications, anthropogenic factors, topography\ndata, and thermal anomalies. To evaluate the influence of temporal context, we\nconduct an ablation study examining how the inclusion of pre- and post-ignition\ndata affects model performance, benchmarking the temporal-aware DL models\nagainst a baseline trained exclusively on ignition-day inputs. Our results\nindicate that multi-day observational data substantially improve predictive\naccuracy. Particularly, the best-performing model, incorporating a temporal\nwindow of four days before to five days after ignition, improves both the F1\nscore and the Intersection over Union by almost 5% in comparison to the\nbaseline on the test dataset. We publicly release our dataset and models to\nenhance research into data-driven approaches for wildfire modeling and\nresponse.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17793", "pdf": "https://arxiv.org/pdf/2505.17793", "abs": "https://arxiv.org/abs/2505.17793", "authors": ["Jianxiang Zang", "Meiling Ning", "Yongda Wei", "Shihan Dou", "Jiazheng Zhang", "Nijia Mo", "Binhong Li", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Compression Hacking: A Supplementary Perspective on Informatics Metric of Language Models from Geometric Distortion", "categories": ["cs.CL"], "comment": null, "summary": "Recently, the concept of ``compression as intelligence'' has provided a novel\ninformatics metric perspective for language models (LMs), emphasizing that\nhighly structured representations signify the intelligence level of LMs.\nHowever, from a geometric standpoint, the word representation space of highly\ncompressed LMs tends to degenerate into a highly anisotropic state, which\nhinders the LM's ability to comprehend instructions and directly impacts its\nperformance. We found this compression-anisotropy synchronicity is essentially\nthe ``Compression Hacking'' in LM representations, where noise-dominated\ndirections tend to create the illusion of high compression rates by sacrificing\nspatial uniformity. Based on this, we propose three refined compression metrics\nby incorporating geometric distortion analysis and integrate them into a\nself-evaluation pipeline. The refined metrics exhibit strong alignment with the\nLM's comprehensive capabilities, achieving Spearman correlation coefficients\nabove 0.9, significantly outperforming both the original compression and other\ninternal structure-based metrics. This confirms that compression hacking\nsubstantially enhances the informatics interpretation of LMs by incorporating\ngeometric distortion of representations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17795", "pdf": "https://arxiv.org/pdf/2505.17795", "abs": "https://arxiv.org/abs/2505.17795", "authors": ["Tazeek Bin Abdur Rakib", "Ambuj Mehrish", "Lay-Ki Soon", "Wern Han Lim", "Soujanya Poria"], "title": "DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large-language-model (LLM) agents excel at reactive dialogue but struggle\nwith proactive, goal-driven interactions due to myopic decoding and costly\nplanning. We introduce DialogXpert, which leverages a frozen LLM to propose a\nsmall, high-quality set of candidate actions per turn and employs a compact\nQ-network over fixed BERT embeddings trained via temporal-difference learning\nto select optimal moves within this reduced space. By tracking the user's\nemotions, DialogXpert tailors each decision to advance the task while nurturing\na genuine, empathetic connection. Across negotiation, emotional support, and\ntutoring benchmarks, DialogXpert drives conversations to under $3$ turns with\nsuccess rates exceeding 94\\% and, with a larger LLM prior, pushes success above\n97\\% while markedly improving negotiation outcomes. This framework delivers\nreal-time, strategic, and emotionally intelligent dialogue planning at scale.\nCode available at https://github.com/declare-lab/dialogxpert/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17695", "pdf": "https://arxiv.org/pdf/2505.17695", "abs": "https://arxiv.org/abs/2505.17695", "authors": ["Dong-Hee Kim", "Hyunjee Song", "Donghyun Kim"], "title": "SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite the advances in Referring Expression Segmentation (RES) benchmarks,\ntheir evaluation protocols remain constrained, primarily focusing on either\nsingle targets with short queries (containing minimal attributes) or multiple\ntargets from distinctly different queries on a single domain. This limitation\nsignificantly hinders the assessment of more complex reasoning capabilities in\nRES models. We introduce WildRES, a novel benchmark that incorporates long\nqueries with diverse attributes and non-distinctive queries for multiple\ntargets. This benchmark spans diverse application domains, including autonomous\ndriving environments and robotic manipulation scenarios, thus enabling more\nrigorous evaluation of complex reasoning capabilities in real-world settings.\nOur analysis reveals that current RES models demonstrate substantial\nperformance deterioration when evaluated on WildRES. To address this challenge,\nwe introduce SynRES, an automated pipeline generating densely paired\ncompositional synthetic training data through three innovations: (1) a dense\ncaption-driven synthesis for attribute-rich image-mask-expression triplets, (2)\nreliable semantic alignment mechanisms rectifying caption-pseudo mask\ninconsistencies via Image-Text Aligned Grouping, and (3) domain-aware\naugmentations incorporating mosaic composition and superclass replacement to\nemphasize generalization ability and distinguishing attributes over object\ncategories. Experimental results demonstrate that models trained with SynRES\nachieve state-of-the-art performance, improving gIoU by 2.0% on WildRES-ID and\n3.8% on WildRES-DS. Code and datasets are available at\nhttps://github.com/UTLLab/SynRES.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17870", "pdf": "https://arxiv.org/pdf/2505.17870", "abs": "https://arxiv.org/abs/2505.17870", "authors": ["Shaina Raza", "Rizwan Qureshi", "Marcelo Lotif", "Aman Chadha", "Deval Pandya", "Christos Emmanouilidis"], "title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods", "categories": ["cs.CL"], "comment": null, "summary": "Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17860", "pdf": "https://arxiv.org/pdf/2505.17860", "abs": "https://arxiv.org/abs/2505.17860", "authors": ["Wenning Xu", "Shiyu Fan", "Paul Henderson", "Edmond S. L. Ho"], "title": "Multi-Person Interaction Generation from Two-Person Motion Priors", "categories": ["cs.GR", "cs.CV", "cs.LG", "I.3.7"], "comment": "SIGGRAPH 2025 Conference Papers", "summary": "Generating realistic human motion with high-level controls is a crucial task\nfor social understanding, robotics, and animation. With high-quality MOCAP data\nbecoming more available recently, a wide range of data-driven approaches have\nbeen presented. However, modelling multi-person interactions still remains a\nless explored area. In this paper, we present Graph-driven Interaction\nSampling, a method that can generate realistic and diverse multi-person\ninteractions by leveraging existing two-person motion diffusion models as\nmotion priors. Instead of training a new model specific to multi-person\ninteraction synthesis, our key insight is to spatially and temporally separate\ncomplex multi-person interactions into a graph structure of two-person\ninteractions, which we name the Pairwise Interaction Graph. We thus decompose\nthe generation task into simultaneous single-person motion generation\nconditioned on one other's motion. In addition, to reduce artifacts such as\ninterpenetrations of body parts in generated multi-person interactions, we\nintroduce two graph-dependent guidance terms into the diffusion sampling\nscheme. Unlike previous work, our method can produce various high-quality\nmulti-person interactions without having repetitive individual motions.\nExtensive experiments demonstrate that our approach consistently outperforms\nexisting methods in reducing artifacts when generating a wide range of\ntwo-person and multi-person interactions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17873", "pdf": "https://arxiv.org/pdf/2505.17873", "abs": "https://arxiv.org/abs/2505.17873", "authors": ["Wanhao Liu", "Zonglin Yang", "Jue Wang", "Lidong Bing", "Di Zhang", "Dongzhan Zhou", "Yuqiang Li", "Houqiang Li", "Erik Cambria", "Wanli Ouyang"], "title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback", "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "Hypothesis ranking is a crucial component of automated scientific discovery,\nparticularly in natural sciences where wet-lab experiments are costly and\nthroughput-limited. Existing approaches focus on pre-experiment ranking,\nrelying solely on large language model's internal reasoning without\nincorporating empirical outcomes from experiments. We introduce the task of\nexperiment-guided ranking, which aims to prioritize candidate hypotheses based\non the results of previously tested ones. However, developing such strategies\nis challenging due to the impracticality of repeatedly conducting real\nexperiments in natural science domains. To address this, we propose a simulator\ngrounded in three domain-informed assumptions, modeling hypothesis performance\nas a function of similarity to a known ground truth hypothesis, perturbed by\nnoise. We curate a dataset of 124 chemistry hypotheses with experimentally\nreported outcomes to validate the simulator. Building on this simulator, we\ndevelop a pseudo experiment-guided ranking method that clusters hypotheses by\nshared functional characteristics and prioritizes candidates based on insights\nderived from simulated experimental feedback. Experiments show that our method\noutperforms pre-experiment baselines and strong ablations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17952", "pdf": "https://arxiv.org/pdf/2505.17952", "abs": "https://arxiv.org/abs/2505.17952", "authors": ["Che Liu", "Haozhe Wang", "Jiazhen Pan", "Zhongwei Wan", "Yong Dai", "Fangzhen Lin", "Wenjia Bai", "Daniel Rueckert", "Rossella Arcucci"], "title": "Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL", "categories": ["cs.CL", "cs.AI"], "comment": "Under Review", "summary": "Improving performance on complex tasks and enabling interpretable decision\nmaking in large language models (LLMs), especially for clinical applications,\nrequires effective reasoning. Yet this remains challenging without supervised\nfine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from\nclosed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the\nfirst medical LLM to show that reasoning capability can emerge purely through\nreinforcement learning (RL), using minimalist rule-based rewards on public\nmultiple-choice QA datasets, without relying on SFT or distilled CoT data.\nAlphaMed achieves state-of-the-art results on six medical QA benchmarks,\noutperforming models trained with conventional SFT+RL pipelines. On challenging\nbenchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source\nmodels such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the\nfactors behind this success, we conduct a comprehensive data-centric analysis\nguided by three questions: (i) Can minimalist rule-based RL incentivize\nreasoning without distilled CoT supervision? (ii) How do dataset quantity and\ndiversity impact reasoning? (iii) How does question difficulty shape the\nemergence and generalization of reasoning? Our findings show that dataset\ninformativeness is a key driver of reasoning performance, and that minimalist\nRL on informative, multiple-choice QA data is effective at inducing reasoning\nwithout CoT supervision. We also observe divergent trends across benchmarks,\nunderscoring limitations in current evaluation and the need for more\nchallenging, reasoning-oriented medical QA benchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.18098", "pdf": "https://arxiv.org/pdf/2505.18098", "abs": "https://arxiv.org/abs/2505.18098", "authors": ["Joey Hong", "Anca Dragan", "Sergey Levine"], "title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 4 figures, 2 tables", "summary": "Large language models (LLMs) excel in tasks like question answering and\ndialogue, but complex tasks requiring interaction, such as negotiation and\npersuasion, require additional long-horizon reasoning and planning.\nReinforcement learning (RL) fine-tuning can enable such planning in principle,\nbut suffers from drawbacks that hinder scalability. In particular, multi-turn\nRL training incurs high memory and computational costs, which are exacerbated\nwhen training LLMs as policies. Furthermore, the largest LLMs do not expose the\nAPIs necessary to be trained in such manner. As a result, modern methods to\nimprove the reasoning of LLMs rely on sophisticated prompting mechanisms rather\nthan RL fine-tuning. To remedy this, we propose a novel approach that uses\ngoal-conditioned value functions to guide the reasoning of LLM agents, that\nscales even to large API-based models. These value functions predict how a task\nwill unfold given an action, allowing the LLM agent to evaluate multiple\npossible outcomes, both positive and negative, to plan effectively. In\naddition, these value functions are trained over reasoning steps rather than\nfull actions, to be a concise and light-weight module that facilitates\ndecision-making in multi-turn interactions. We validate our method on tasks\nrequiring interaction, including tool use, social deduction, and dialogue,\ndemonstrating superior performance over both RL fine-tuning and prompting\nmethods while maintaining efficiency and scalability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue", "question answering"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.18154", "pdf": "https://arxiv.org/pdf/2505.18154", "abs": "https://arxiv.org/abs/2505.18154", "authors": ["Ya Wu", "Qiang Sheng", "Danding Wang", "Guang Yang", "Yifan Sun", "Zhengjia Wang", "Yuyan Bu", "Juan Cao"], "title": "The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas", "categories": ["cs.CL", "cs.CY"], "comment": "25 pages, 8 figures", "summary": "Ethical decision-making is a critical aspect of human judgment, and the\ngrowing use of LLMs in decision-support systems necessitates a rigorous\nevaluation of their moral reasoning capabilities. However, existing assessments\nprimarily rely on single-step evaluations, failing to capture how models adapt\nto evolving ethical challenges. Addressing this gap, we introduce the\nMulti-step Moral Dilemmas (MMDs), the first dataset specifically constructed to\nevaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas.\nThis framework enables a fine-grained, dynamic analysis of how LLMs adjust\ntheir moral reasoning across escalating dilemmas. Our evaluation of nine widely\nused LLMs reveals that their value preferences shift significantly as dilemmas\nprogress, indicating that models recalibrate moral judgments based on scenario\ncomplexity. Furthermore, pairwise value comparisons demonstrate that while LLMs\noften prioritize the value of care, this value can sometimes be superseded by\nfairness in certain contexts, highlighting the dynamic and context-dependent\nnature of LLM ethical reasoning. Our findings call for a shift toward dynamic,\ncontext-aware evaluation paradigms, paving the way for more human-aligned and\nvalue-sensitive development of LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17040", "pdf": "https://arxiv.org/pdf/2505.17040", "abs": "https://arxiv.org/abs/2505.17040", "authors": ["Yun-Da Tsai"], "title": "Generalizing Large Language Model Usability Across Resource-Constrained", "categories": ["cs.LG", "cs.CL"], "comment": "Doctoral disstertation", "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of natural language tasks, and recent efforts have sought to extend their\ncapabilities to multimodal domains and resource-constrained environments.\nHowever, existing approaches often rely on costly supervised fine-tuning or\nassume fixed training conditions, limiting their generalization when facing\nunseen modalities, limited data, or restricted compute resources. This\ndissertation presents a systematic study toward generalizing LLM usability\nunder real-world constraints. First, it introduces a robust text-centric\nalignment framework that enables LLMs to seamlessly integrate diverse\nmodalities-including text, images, tables, and any modalities - via natural\nlanguage interfaces. This approach supports in-context adaptation to unseen or\ndynamically changing modalities without requiring retraining. To enhance\nrobustness against noisy and missing modalities, an adversarial prompting\ntechnique is proposed, generating semantically challenging perturbations at the\nprompt level to stress-test model reliability. Beyond multimodal setting, the\ndissertation investigates inference-time optimization strategies for LLMs,\nleveraging prompt search and uncertainty quantification to improve performance\nwithout additional model training. This perspective offers an efficient\nalternative to scaling model parameters or retraining from scratch.\nAdditionally, the work addresses low-resource domains such as Verilog code\ngeneration by designing correct-by-construction synthetic data pipelines and\nlogic-enhanced reasoning models, achieving state-of-the-art performance with\nminimal data. Together, these contributions form a unified effort to enhance\nthe adaptability, scalability, and efficiency of large language models under\npractical constraints.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17072", "pdf": "https://arxiv.org/pdf/2505.17072", "abs": "https://arxiv.org/abs/2505.17072", "authors": ["Jianwei Li", "Jung-Eng Kim"], "title": "Safety Alignment Can Be Not Superficial With Explicit Safety Signals", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Recent studies on the safety alignment of large language models (LLMs) have\nrevealed that existing approaches often operate superficially, leaving models\nvulnerable to various adversarial attacks. Despite their significance, these\nstudies generally fail to offer actionable solutions beyond data augmentation\nfor achieving more robust safety mechanisms. This paper identifies a\nfundamental cause of this superficiality: existing alignment approaches often\npresume that models can implicitly learn a safety-related reasoning task during\nthe alignment process, enabling them to refuse harmful requests. However, the\nlearned safety signals are often diluted by other competing objectives, leading\nmodels to struggle with drawing a firm safety-conscious decision boundary when\nconfronted with adversarial attacks. Based on this observation, by explicitly\nintroducing a safety-related binary classification task and integrating its\nsignals with our attention and decoding strategies, we eliminate this ambiguity\nand allow models to respond more responsibly to malicious queries. We emphasize\nthat, with less than 0.2x overhead cost, our approach enables LLMs to assess\nthe safety of both the query and the previously generated tokens at each\nnecessary generating step. Extensive experiments demonstrate that our method\nsignificantly improves the resilience of LLMs against various adversarial\nattacks, offering a promising pathway toward more robust generative AI systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17316", "pdf": "https://arxiv.org/pdf/2505.17316", "abs": "https://arxiv.org/abs/2505.17316", "authors": ["Jiachen Jiang", "Jinxin Zhou", "Bo Peng", "Xia Ning", "Zhihui Zhu"], "title": "Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Achieving better alignment between vision embeddings and Large Language\nModels (LLMs) is crucial for enhancing the abilities of Multimodal LLMs\n(MLLMs), particularly for recent models that rely on powerful pretrained vision\nencoders and LLMs. A common approach to connect the pretrained vision encoder\nand LLM is through a projector applied after the vision encoder. However, the\nprojector is often trained to enable the LLM to generate captions, and hence\nthe mechanism by which LLMs understand each vision token remains unclear. In\nthis work, we first investigate the role of the projector in compressing vision\nembeddings and aligning them with word embeddings. We show that the projector\nsignificantly compresses visual information, removing redundant details while\npreserving essential elements necessary for the LLM to understand visual\ncontent. We then examine patch-level alignment -- the alignment between each\nvision patch and its corresponding semantic words -- and propose a\n*multi-semantic alignment hypothesis*. Our analysis indicates that the\nprojector trained by caption loss improves patch-level alignment but only to a\nlimited extent, resulting in weak and coarse alignment. To address this issue,\nwe propose *patch-aligned training* to efficiently enhance patch-level\nalignment. Our experiments show that patch-aligned training (1) achieves\nstronger compression capability and improved patch-level alignment, enabling\nthe MLLM to generate higher-quality captions, (2) improves the MLLM's\nperformance by 16% on referring expression grounding tasks, 4% on\nquestion-answering tasks, and 3% on modern instruction-following benchmarks\nwhen using the same supervised fine-tuning (SFT) setting. The proposed method\ncan be easily extended to other multimodal models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17454", "pdf": "https://arxiv.org/pdf/2505.17454", "abs": "https://arxiv.org/abs/2505.17454", "authors": ["Hyosoon Jang", "Yunhui Jang", "Sungjae Lee", "Jungseul Ok", "Sungsoo Ahn"], "title": "Self-Training Large Language Models with Confident Reasoning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown impressive performance by generating\nreasoning paths before final answers, but learning such a reasoning path\nrequires costly human supervision. To address this issue, recent studies have\nexplored self-training methods that improve reasoning capabilities using\npseudo-labels generated by the LLMs themselves. Among these, confidence-based\nself-training fine-tunes LLMs to prefer reasoning paths with high-confidence\nanswers, where confidence is estimated via majority voting. However, such\nmethods exclusively focus on the quality of the final answer and may ignore the\nquality of the reasoning paths, as even an incorrect reasoning path leads to a\ncorrect answer by chance. Instead, we advocate the use of reasoning-level\nconfidence to identify high-quality reasoning paths for self-training,\nsupported by our empirical observations. We then propose a new self-training\nmethod, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths\nthrough Policy Optimization. Our experiments show that CORE-PO improves the\naccuracy of outputs on four in-distribution and two out-of-distribution\nbenchmarks, compared to existing self-training methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17492", "pdf": "https://arxiv.org/pdf/2505.17492", "abs": "https://arxiv.org/abs/2505.17492", "authors": ["Dezheng Bao", "Yueci Yang", "Xin Chen", "Zhengxuan Jiang", "Zeguo Fei", "Daoze Zhang", "Xuanwen Huang", "Junru Chen", "Chutian Yu", "Xiang Yuan", "Yang Yang"], "title": "PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "17 pages, 9 figures", "summary": "Project duplication detection is critical for project quality assessment, as\nit improves resource utilization efficiency by preventing investing in newly\nproposed project that have already been studied. It requires the ability to\nunderstand high-level semantics and generate constructive and valuable\nfeedback. Existing detection methods rely on basic word- or sentence-level\ncomparison or solely apply large language models, lacking valuable insights for\nexperts and in-depth comprehension of project content and review criteria. To\ntackle this issue, we propose PD$^3$, a Project Duplication Detection framework\nvia adapted multi-agent Debate. Inspired by real-world expert debates, it\nemploys a fair competition format to guide multi-agent debate to retrieve\nrelevant projects. For feedback, it incorporates both qualitative and\nquantitative analysis to improve its practicality. Over 800 real-world power\nproject data spanning more than 20 specialized fields are used to evaluate the\nframework, demonstrating that our method outperforms existing approaches by\n7.43% and 8.00% in two downstream tasks. Furthermore, we establish an online\nplatform, Review Dingdang, to assist power experts, saving 5.73 million USD in\ninitial detection on more than 100 newly proposed projects.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17519", "pdf": "https://arxiv.org/pdf/2505.17519", "abs": "https://arxiv.org/abs/2505.17519", "authors": ["Wenhan Chang", "Tianqing Zhu", "Yu Zhao", "Shuangyong Song", "Ping Xiong", "Wanlei Zhou", "Yongxiang Li"], "title": "Chain-of-Lure: A Synthetic Narrative-Driven Approach to Compromise Large Language Models", "categories": ["cs.CR", "cs.CL"], "comment": "25 pages, 4 figures", "summary": "In the era of rapid generative AI development, interactions between humans\nand large language models face significant misusing risks. Previous research\nhas primarily focused on black-box scenarios using human-guided prompts and\nwhite-box scenarios leveraging gradient-based LLM generation methods,\nneglecting the possibility that LLMs can act not only as victim models, but\nalso as attacker models to harm other models. We proposes a novel jailbreaking\nmethod inspired by the Chain-of-Thought mechanism, where the attacker model\nuses mission transfer to conceal harmful user intent in dialogue and generates\nchained narrative lures to stimulate the reasoning capabilities of victim\nmodels, leading to successful jailbreaking. To enhance the attack success rate,\nwe introduce a helper model that performs random narrative optimization on the\nnarrative lures during multi-turn dialogues while ensuring alignment with the\noriginal intent, enabling the optimized lures to bypass the safety barriers of\nvictim models effectively. Our experiments reveal that models with weaker\nsafety mechanisms exhibit stronger attack capabilities, demonstrating that\nmodels can not only be exploited, but also help harm others. By incorporating\ntoxicity scores, we employ third-party models to evaluate the harmfulness of\nvictim models' responses to jailbreaking attempts. The study shows that using\nrefusal keywords as an evaluation metric for attack success rates is\nsignificantly flawed because it does not assess whether the responses guide\nharmful questions, while toxicity scores measure the harm of generated content\nwith more precision and its alignment with harmful questions. Our approach\ndemonstrates outstanding performance, uncovering latent vulnerabilities in LLMs\nand providing data-driven feedback to optimize LLM safety mechanisms. We also\ndiscuss two defensive strategies to offer guidance on improving defense\nmechanisms.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "dialogue"], "score": 3}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17598", "pdf": "https://arxiv.org/pdf/2505.17598", "abs": "https://arxiv.org/abs/2505.17598", "authors": ["Linbao Li", "Yannan Liu", "Daojing He", "Yu Li"], "title": "One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Safety alignment in large language models (LLMs) is increasingly compromised\nby jailbreak attacks, which can manipulate these models to generate harmful or\nunintended content. Investigating these attacks is crucial for uncovering model\nvulnerabilities. However, many existing jailbreak strategies fail to keep pace\nwith the rapid development of defense mechanisms, such as defensive suffixes,\nrendering them ineffective against defended models. To tackle this issue, we\nintroduce a novel attack method called ArrAttack, specifically designed to\ntarget defended LLMs. ArrAttack automatically generates robust jailbreak\nprompts capable of bypassing various defense measures. This capability is\nsupported by a universal robustness judgment model that, once trained, can\nperform robustness evaluation for any target model with a wide variety of\ndefenses. By leveraging this model, we can rapidly develop a robust jailbreak\nprompt generator that efficiently converts malicious input prompts into\neffective attacks. Extensive evaluations reveal that ArrAttack significantly\noutperforms existing attack strategies, demonstrating strong transferability\nacross both white-box and black-box models, including GPT-4 and Claude-3. Our\nwork bridges the gap between jailbreak attacks and defenses, providing a fresh\nperspective on generating robust jailbreak prompts. We make the codebase\navailable at https://github.com/LLBao/ArrAttack.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17645", "pdf": "https://arxiv.org/pdf/2505.17645", "abs": "https://arxiv.org/abs/2505.17645", "authors": ["Chuhao Zhou", "Jianfei Yang"], "title": "HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "18 pages, 13 figures, 6 tables", "summary": "Embodied agents operating in smart homes must understand human behavior\nthrough diverse sensory inputs and communicate via natural language. While\nVision-Language Models (VLMs) have enabled impressive language-grounded\nperception, their reliance on visual data limits robustness in real-world\nscenarios with occlusions, poor lighting, or privacy constraints. In this\npaper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that\nintegrates uncommon but powerful sensing modalities, such as LiDAR, infrared,\nmmWave radar, and WiFi, to enable seamless human perception and reasoning\nacross heterogeneous environments. We address two key challenges: (1) the\nscarcity of aligned modality-text data for rare sensors, and (2) the\nheterogeneity of their physical signal representations. To overcome these, we\ndesign a Universal Modality-Injection Projector (UMIP) that enhances\npre-aligned modality embeddings with fine-grained, text-aligned features from\ntailored encoders via coarse-to-fine cross-attention without introducing\nsignificant alignment overhead. We further introduce a human-VLM collaborative\ndata curation pipeline to generate paired textual annotations for sensing\ndatasets. Extensive experiments on two newly constructed benchmarks show that\nHoloLLM significantly outperforms existing MLLMs, improving language-grounded\nhuman sensing accuracy by up to 30%. This work establishes a new foundation for\nreal-world, language-informed multisensory embodied intelligence.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17826", "pdf": "https://arxiv.org/pdf/2505.17826", "abs": "https://arxiv.org/abs/2505.17826", "authors": ["Xuchen Pan", "Yanxi Chen", "Yushuo Chen", "Yuchang Sun", "Daoyuan Chen", "Wenhao Zhang", "Yuexiang Xie", "Yilun Huang", "Yilei Zhang", "Dawei Gao", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models", "categories": ["cs.LG", "cs.CL", "cs.DC"], "comment": "This technical report will be continuously updated as the codebase\n  evolves. GitHub: https://github.com/modelscope/Trinity-RFT", "summary": "Trinity-RFT is a general-purpose, flexible and scalable framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na decoupled design, consisting of (1) an RFT-core that unifies and generalizes\nsynchronous/asynchronous, on-policy/off-policy, and online/offline modes of\nRFT, (2) seamless integration for agent-environment interaction with high\nefficiency and robustness, and (3) systematic data pipelines optimized for RFT.\nTrinity-RFT can be easily adapted for diverse application scenarios, and serves\nas a unified platform for exploring advanced reinforcement learning paradigms.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples demonstrating the utility and\nuser-friendliness of the proposed framework.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.17997", "pdf": "https://arxiv.org/pdf/2505.17997", "abs": "https://arxiv.org/abs/2505.17997", "authors": ["Jintian Shao", "Yiming Cheng", "Hongyi Huang", "Beiwen Zhang", "Zhiyu Wu", "You Shan", "Mingkai Zheng"], "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The VAPO framework has demonstrated significant empirical success in\nenhancing the efficiency and reliability of reinforcement learning for long\nchain-of-thought (CoT) reasoning tasks with large language models (LLMs). By\nsystematically addressing challenges such as value model bias, heterogeneous\nsequence lengths, and sparse reward signals, VAPO achieves state-of-the-art\nperformance. While its practical benefits are evident, a deeper theoretical\nunderstanding of its underlying mechanisms and potential limitations is crucial\nfor guiding future advancements. This paper aims to initiate such a discussion\nby exploring VAPO from a theoretical perspective, highlighting areas where its\nassumptions might be challenged and where further investigation could yield\nmore robust and generalizable reasoning agents. We delve into the intricacies\nof value function approximation in complex reasoning spaces, the optimality of\nadaptive advantage estimation, the impact of token-level optimization, and the\nenduring challenges of exploration and generalization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.18121", "pdf": "https://arxiv.org/pdf/2505.18121", "abs": "https://arxiv.org/abs/2505.18121", "authors": ["Danyang Zhang", "Situo Zhang", "Ziyue Yang", "Zichen Zhu", "Zihan Zhao", "Ruisheng Cao", "Lu Chen", "Kai Yu"], "title": "ProgRM: Build Better GUI Agents with Progress Rewards", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "LLM-based (Large Language Model) GUI (Graphical User Interface) agents can\npotentially reshape our daily lives significantly. However, current LLM-based\nGUI agents suffer from the scarcity of high-quality training data owing to the\ndifficulties of trajectory collection and reward annotation. Existing works\nhave been exploring LLMs to collect trajectories for imitation learning or to\noffer reward signals for online RL training. However, the Outcome Reward Model\n(ORM) used in existing works cannot provide finegrained feedback and can\nover-penalize the valuable steps in finally failed trajectories. To this end,\nwe propose Progress Reward Model (ProgRM) to provide dense informative\nintermediate rewards by predicting a task completion progress for each step in\nonline training. To handle the challenge of progress reward label annotation,\nwe further design an efficient LCS-based (Longest Common Subsequence)\nself-annotation algorithm to discover the key steps in trajectories and assign\nprogress labels accordingly. ProgRM is evaluated with extensive experiments and\nanalyses. Actors trained with ProgRM outperform leading proprietary LLMs and\nORM-trained actors, illustrating the effectiveness of ProgRM. The codes for\nexperiments will be made publicly available upon acceptance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
{"id": "2505.18129", "pdf": "https://arxiv.org/pdf/2505.18129", "abs": "https://arxiv.org/abs/2505.18129", "authors": ["Yan Ma", "Linge Du", "Xuyang Shen", "Shaoxiang Chen", "Pengfei Li", "Qibing Ren", "Lizhuang Ma", "Yuchao Dai", "Pengfei Liu", "Junjie Yan"], "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning", "categories": ["cs.CV", "cs.CL"], "comment": "Technical Report", "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-26.jsonl"}
