{"id": "2506.01937", "pdf": "https://arxiv.org/pdf/2506.01937", "abs": "https://arxiv.org/abs/2506.01937", "authors": ["Saumya Malik", "Valentina Pyatkin", "Sander Land", "Jacob Morrison", "Noah A. Smith", "Hannaneh Hajishirzi", "Nathan Lambert"], "title": "RewardBench 2: Advancing Reward Model Evaluation", "categories": ["cs.CL"], "comment": "Data, models, and leaderboard available at\n  https://huggingface.co/collections/allenai/reward-bench-2-683d2612a4b3e38a3e53bb51", "summary": "Reward models are used throughout the post-training of language models to\ncapture nuanced signals from preference data and provide a training target for\noptimization across instruction following, reasoning, safety, and more domains.\nThe community has begun establishing best practices for evaluating reward\nmodels, from the development of benchmarks that test capabilities in specific\nskill areas to others that test agreement with human preferences. At the same\ntime, progress in evaluation has not been mirrored by the effectiveness of\nreward models in downstream tasks -- simpler direct alignment algorithms are\nreported to work better in many cases. This paper introduces RewardBench 2, a\nnew multi-skill reward modeling benchmark designed to bring new, challenging\ndata for accuracy-based reward model evaluation -- models score about 20 points\non average lower on RewardBench 2 compared to the first RewardBench -- while\nbeing highly correlated with downstream performance. Compared to most other\nbenchmarks, RewardBench 2 sources new human prompts instead of existing prompts\nfrom downstream evaluations, facilitating more rigorous evaluation practices.\nIn this paper, we describe our benchmark construction process and report how\nexisting models perform on it, while quantifying how performance on the\nbenchmark correlates with downstream use of the models in both inference-time\nscaling algorithms, like best-of-N sampling, and RLHF training algorithms like\nproximal policy optimization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward modeling", "RLHF", "proximal policy optimization", "policy optimization", "preference", "alignment"], "score": 7}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "reward model evaluation", "safety", "agreement", "accuracy"], "score": 6}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00103", "pdf": "https://arxiv.org/pdf/2506.00103", "abs": "https://arxiv.org/abs/2506.00103", "authors": ["Xun Lu"], "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Problems and Verifiable Rewards", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has enabled large\nlanguage models (LLMs) to achieve remarkable breakthroughs in reasoning tasks\nwith objective ground-truth answers, such as mathematics and code generation.\nHowever, a significant gap remains for non-verifiable tasks, like creative\nwriting and open-ended dialogue, where quality assessment is inherently\nsubjective and lacks definitive references. Existing approaches for these\ndomains often rely on scalar reward models trained with human preferences,\nwhich suffer from limited generalization and are prone to reward hacking, such\nas over-explanation and length bias. In this work, we propose a unified\nRLVR-based training paradigm that bridges the gap between non-verifiable tasks\nand verifiable rewards. We introduce a writing-principle-based pairwise\nGenerative Reward Model (GenRM) and a novel Bootstrapped Relative Policy\nOptimization (BRPO) algorithm. The pairwise writing GenRM leverages\nself-principled critique to transform subjective assessments into reliable,\nverifiable rewards, while BRPO enables dynamic, reference-free pairwise\ncomparison by leveraging a bootstrapped response as temporary reference from\nwithin group rollouts during RL training. Our approach empowers LLMs to develop\nrobust writing capabilities without supervised fine-tuning, as demonstrated by\nWriting-Zero, which shows consistent improvement and strong resistance to\nreward hacking compared to scalar reward baselines. Furthermore, our method\nachieves competitive results on both in-house and open-source writing\nbenchmarks. Our findings suggest the potential to unify rule-based,\nreference-based, and reference-free reward modeling under the RLVR framework,\nthus paving the way for a comprehensive and scalable RL training paradigm\napplicable across all language tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward modeling", "reinforcement learning", "comparison", "pairwise", "reward hacking"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue", "code generation"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01300", "pdf": "https://arxiv.org/pdf/2506.01300", "abs": "https://arxiv.org/abs/2506.01300", "authors": ["Yiyang Zhou", "Yangfan He", "Yaofeng Su", "Siwei Han", "Joel Jang", "Gedas Bertasius", "Mohit Bansal", "Huaxiu Yao"], "title": "ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding", "categories": ["cs.CV"], "comment": "31 pages, 18 figures", "summary": "Video understanding is fundamental to tasks such as action recognition, video\nreasoning, and robotic control. Early video understanding methods based on\nlarge vision-language models (LVLMs) typically adopt a single-pass reasoning\nparadigm without dynamic feedback, limiting the model's capacity to\nself-correct and adapt in complex scenarios. Recent efforts have attempted to\naddress this limitation by incorporating reward models and reinforcement\nlearning to enhance reasoning, or by employing tool-agent frameworks. However,\nthese approaches face several challenges, including high annotation costs,\nreward signals that fail to capture real-time reasoning states, and low\ninference efficiency. To overcome these issues, we propose ReAgent-V, a novel\nagentic video understanding framework that integrates efficient frame selection\nwith real-time reward generation during inference. These reward signals not\nonly guide iterative answer refinement through a multi-perspective reflection\nmechanism-adjusting predictions from conservative, neutral, and aggressive\nviewpoints-but also enable automatic filtering of high-quality data for\nsupervised fine-tuning (SFT), direct preference optimization (DPO), and group\nrelative policy optimization (GRPO). ReAgent-V is lightweight, modular, and\nextensible, supporting flexible tool integration tailored to diverse tasks.\nExtensive experiments on 12 datasets across three core applications-video\nunderstanding, video reasoning enhancement, and vision-language-action model\nalignment-demonstrate significant gains in generalization and reasoning, with\nimprovements of up to 6.9%, 2.1%, and 9.8%, respectively, highlighting the\neffectiveness and versatility of the proposed framework.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization", "preference", "alignment", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01104", "pdf": "https://arxiv.org/pdf/2506.01104", "abs": "https://arxiv.org/abs/2506.01104", "authors": ["Steven Robinson", "Antonio Carlos Rivera"], "title": "Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability Detection", "categories": ["cs.CL"], "comment": null, "summary": "The pervasive deployment of large language models (LLMs) in conversational AI\nsystems has revolutionized information access, yet their propensity for\ngenerating factually unsupported or hallucinated responses remains a critical\nimpediment to trustworthiness and widespread adoption. This paper introduces\nReinforced Unanswerability Learning (RUL), a novel hybrid training paradigm\ndesigned to imbue LLMs with the intrinsic capability to accurately detect\nunanswerable questions and generate reliably appropriate responses. Unlike\nconventional approaches that rely on external classifiers or simple prompting,\nRUL integrates a discriminative unanswerability prediction head with the LLM's\ngenerative core, guided by a multi-stage learning strategy. This includes\nsupervised fine-tuning on a novel, richly annotated dataset,\nEnhanced-CAsT-Answerability (ECA), which features hierarchical answerability\nlabels and ground-truth refusal responses. Crucially, RUL incorporates a\nsubsequent reinforcement learning with human feedback (RLHF) phase to refine\nthe nuance, helpfulness, and informativeness of refusal responses. Extensive\nexperiments demonstrate RUL's superior performance, achieving significantly\nhigher accuracy in unanswerability detection across sentence, paragraph, and\nranking levels, and substantially increasing the generation of appropriate\nrefusals for unanswerable queries, alongside strong performance on answerable\nquestions. Human evaluations further corroborate RUL's effectiveness,\nhighlighting a marked improvement in perceived helpfulness and trustworthiness,\nultimately paving the way for more reliable and user-centric conversational AI.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "human feedback", "reinforcement learning", "ranking"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "helpfulness", "accuracy"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00835", "pdf": "https://arxiv.org/pdf/2506.00835", "abs": "https://arxiv.org/abs/2506.00835", "authors": ["Jisheng Dang", "Yizhou Zhang", "Hao Ye", "Teng Wang", "Siming Chen", "Huicheng Zheng", "Yulan Guo", "Jianhuang Lai", "Bin Hu"], "title": "SynPO: Synergizing Descriptiveness and Preference Optimization for Video Detailed Captioning", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Fine-grained video captioning aims to generate detailed, temporally coherent\ndescriptions of video content. However, existing methods struggle to capture\nsubtle video dynamics and rich detailed information. In this paper, we leverage\npreference learning to enhance the performance of vision-language models in\nfine-grained video captioning, while mitigating several limitations inherent to\ndirect preference optimization (DPO). First, we propose a pipeline for\nconstructing preference pairs that leverages the intrinsic properties of VLMs\nalong with partial assistance from large language models, achieving an optimal\nbalance between cost and data quality. Second, we propose Synergistic\nPreference Optimization (SynPO), a novel optimization method offering\nsignificant advantages over DPO and its variants. SynPO prevents negative\npreferences from dominating the optimization, explicitly preserves the model's\nlanguage capability to avoid deviation of the optimization objective, and\nimproves training efficiency by eliminating the need for the reference model.\nWe extensively evaluate SynPO not only on video captioning benchmarks (e.g.,\nVDC, VDD, VATEX) but also across well-established NLP tasks, including general\nlanguage understanding and preference evaluation, using diverse pretrained\nmodels. Results demonstrate that SynPO consistently outperforms DPO variants\nwhile achieving 20\\% improvement in training efficiency. Code is available at\nhttps://github.com/longmalongma/SynPO", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "DPO", "direct preference optimization"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00805", "pdf": "https://arxiv.org/pdf/2506.00805", "abs": "https://arxiv.org/abs/2506.00805", "authors": ["Songtao Jiang", "Yan Zhang", "Yeying Jin", "Zhihang Tang", "Yangyang Wu", "Yang Feng", "Jian Wu", "Zuozhu Liu"], "title": "HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Medical Vision-Language Models (Med-VLMs) have achieved success across\nvarious tasks, yet most existing methods overlook the modality misalignment\nissue that can lead to untrustworthy responses in clinical settings. In this\npaper, we propose Hierarchical Self-Contrastive Rewarding (HSCR), a novel\napproach that addresses two critical challenges in Med-VLM alignment: 1)\nCost-effective generation of high-quality preference data; 2) Capturing nuanced\nand context-aware preferences for improved alignment. HSCR first leverages the\ninherent capability of Med-VLMs to generate dispreferred responses with higher\nsampling probability. By analyzing output logit shifts after visual token\ndropout, we identify modality-coupled tokens that induce misalignment and\nderive an implicit alignment reward function. This function guides token\nreplacement with hallucinated ones during decoding, producing high-quality\ndispreferred data. Furthermore, HSCR introduces a multi-level preference\noptimization strategy, which extends beyond traditional adjacent-level\noptimization by incorporating nuanced implicit preferences, leveraging relative\nquality in dispreferred data to capture subtle alignment cues for more precise\nand context-aware optimization. Extensive experiments across multiple medical\ntasks, including Med-VQA, medical image captioning and instruction following,\ndemonstrate that HSCR not only enhances zero-shot performance but also\nsignificantly improves modality alignment and trustworthiness with just 2,000\ntraining entries.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "preference", "alignment"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01908", "pdf": "https://arxiv.org/pdf/2506.01908", "abs": "https://arxiv.org/abs/2506.01908", "authors": ["Hongyu Li", "Songhao Han", "Yue Liao", "Junfeng Luo", "Jialin Gao", "Shuicheng Yan", "Si Liu"], "title": "Reinforcement Learning Tuning for VideoLLMs: Reward Design and Data Efficiency", "categories": ["cs.CV"], "comment": null, "summary": "Understanding real-world videos with complex semantics and long temporal\ndependencies remains a fundamental challenge in computer vision. Recent\nprogress in multimodal large language models (MLLMs) has demonstrated strong\ncapabilities in vision-language tasks, while reinforcement learning tuning\n(RLT) has further improved their reasoning abilities. In this work, we explore\nRLT as a post-training strategy to enhance the video-specific reasoning\ncapabilities of MLLMs. Built upon the Group Relative Policy Optimization (GRPO)\nframework, we propose a dual-reward formulation that supervises both semantic\nand temporal reasoning through discrete and continuous reward signals. To\nfacilitate effective preference-based optimization, we introduce a\nvariance-aware data selection strategy based on repeated inference to identify\nsamples that provide informative learning signals. We evaluate our approach\nacross eight representative video understanding tasks, including VideoQA,\nTemporal Video Grounding, and Grounded VideoQA. Our method consistently\noutperforms supervised fine-tuning and existing RLT baselines, achieving\nsuperior performance with significantly less training data. These results\nunderscore the importance of reward design and data selection in advancing\nreasoning-centric video understanding with MLLMs. Notably, The initial code\nrelease (two months ago) has now been expanded with updates, including\noptimized reward mechanisms and additional datasets. The latest version is\navailable at https://github.com/appletea233/Temporal-R1 .", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization", "preference"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00577", "pdf": "https://arxiv.org/pdf/2506.00577", "abs": "https://arxiv.org/abs/2506.00577", "authors": ["Yufa Zhou", "Shaobo Wang", "Xingyu Dong", "Xiangqi Jin", "Yifang Chen", "Yue Min", "Kexin Yang", "Xingzhang Ren", "Dayiheng Liu", "Linfeng Zhang"], "title": "Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.MA"], "comment": null, "summary": "Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS)\nremains challenging due to intricate reward modeling, dynamic agent\ninteractions, and demanding generalization requirements. This paper explores\nwhether post-training techniques, specifically Supervised Fine-Tuning (SFT) and\nReinforcement Learning with Verifiable Rewards (RLVR), can effectively\n$\\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a\ntestbed, leveraging its strong foundations in mathematics and game theory, its\ndemand for structured analytical reasoning, and its relevance to real-world\napplications such as market design, resource allocation, and policy analysis.\nWe introduce $\\textbf{Recon}$ ($\\textbf{R}$easoning like an\n$\\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a\nhand-curated dataset of 2,100 high-quality economic reasoning problems.\nComprehensive evaluation on economic reasoning benchmarks and multi-agent games\nreveals clear improvements in structured reasoning and economic rationality.\nThese results underscore the promise of domain-aligned post-training for\nenhancing reasoning and agent alignment, shedding light on the roles of SFT and\nRL in shaping model behavior. Code is available at\nhttps://github.com/MasterZhou1/Recon .", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "testbed"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00805", "pdf": "https://arxiv.org/pdf/2506.00805", "abs": "https://arxiv.org/abs/2506.00805", "authors": ["Songtao Jiang", "Yan Zhang", "Yeying Jin", "Zhihang Tang", "Yangyang Wu", "Yang Feng", "Jian Wu", "Zuozhu Liu"], "title": "HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Medical Vision-Language Models (Med-VLMs) have achieved success across\nvarious tasks, yet most existing methods overlook the modality misalignment\nissue that can lead to untrustworthy responses in clinical settings. In this\npaper, we propose Hierarchical Self-Contrastive Rewarding (HSCR), a novel\napproach that addresses two critical challenges in Med-VLM alignment: 1)\nCost-effective generation of high-quality preference data; 2) Capturing nuanced\nand context-aware preferences for improved alignment. HSCR first leverages the\ninherent capability of Med-VLMs to generate dispreferred responses with higher\nsampling probability. By analyzing output logit shifts after visual token\ndropout, we identify modality-coupled tokens that induce misalignment and\nderive an implicit alignment reward function. This function guides token\nreplacement with hallucinated ones during decoding, producing high-quality\ndispreferred data. Furthermore, HSCR introduces a multi-level preference\noptimization strategy, which extends beyond traditional adjacent-level\noptimization by incorporating nuanced implicit preferences, leveraging relative\nquality in dispreferred data to capture subtle alignment cues for more precise\nand context-aware optimization. Extensive experiments across multiple medical\ntasks, including Med-VQA, medical image captioning and instruction following,\ndemonstrate that HSCR not only enhances zero-shot performance but also\nsignificantly improves modality alignment and trustworthiness with just 2,000\ntraining entries.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "preference", "alignment"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00027", "pdf": "https://arxiv.org/pdf/2506.00027", "abs": "https://arxiv.org/abs/2506.00027", "authors": ["Zhengyu Chen", "Yudong Wang", "Teng Xiao", "Ruochen Zhou", "Xuesheng Yang", "Wei Wang", "Zhifang Sui", "Jingang Wang"], "title": "From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in improving the reasoning capabilities of Large Language\nModels have underscored the efficacy of Process Reward Models (PRMs) in\naddressing intermediate errors through structured feedback mechanisms. This\nstudy analyzes PRMs from multiple perspectives, including training\nmethodologies, scalability, and generalization capabilities. We investigate the\ninterplay between pre-training and reward model training FLOPs to assess their\ninfluence on PRM efficiency and accuracy in complex reasoning tasks. Our\nanalysis reveals a pattern of diminishing returns in performance with\nincreasing PRM scale, highlighting the importance of balancing model size and\ncomputational cost. Furthermore, the diversity of training datasets\nsignificantly impacts PRM performance, emphasizing the importance of diverse\ndata to enhance both accuracy and efficiency. We further examine test-time\nscaling strategies, identifying Monte Carlo Tree Search as the most effective\nmethod when computational resources are abundant, while Best-of-N Sampling\nserves as a practical alternative under resource-limited conditions. Notably,\nour findings indicate that PRMs trained on mathematical datasets exhibit\nperformance comparable to those tailored for code generation, suggesting robust\ncross-domain generalization. Employing a gradient-based metric, we observe that\nPRMs exhibit a preference for selecting responses with similar underlying\npatterns, further informing their optimization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale", "monte carlo tree search"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "code generation", "mathematical reasoning"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00539", "pdf": "https://arxiv.org/pdf/2506.00539", "abs": "https://arxiv.org/abs/2506.00539", "authors": ["Ruihan Yang", "Yikai Zhang", "Aili Chen", "Xintao Wang", "Siyu Yuan", "Jiangjie Chen", "Deqing Yang", "Yanghua Xiao"], "title": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have enabled agents to perform complex reasoning\nand decision-making through free-form language interactions. However, in\nopen-ended language action environments (e.g., negotiation or question-asking\ngames), the action space can be formulated as a joint distribution over tokens,\nresulting in an exponentially large action space. Sampling actions in such a\nspace can lead to extreme reward sparsity, which brings large reward variance,\nhindering effective reinforcement learning (RL). To address this, we propose\nARIA, a method that Aggregates Rewards in Intention space to enable efficient\nand effective language Agents training. ARIA aims to project natural language\nactions from the high-dimensional joint token distribution space into a\nlow-dimensional intention space, where semantically similar actions are\nclustered and assigned shared rewards. This intention-aware reward aggregation\nreduces reward variance by densifying reward signals, fostering better policy\noptimization. Extensive experiments demonstrate that ARIA not only\nsignificantly reduces policy gradient variance, but also delivers substantial\nperformance gains of an average of 9.95% across four downstream tasks,\nconsistently outperforming offline and online RL baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy gradient", "reinforcement learning"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01511", "pdf": "https://arxiv.org/pdf/2506.01511", "abs": "https://arxiv.org/abs/2506.01511", "authors": ["Kaixun Jiang", "Zhaoyu Chen", "Haijing Guo", "Jinglun Li", "Jiyuan Fu", "Pinxue Guo", "Hao Tang", "Bo Li", "Wenqiang Zhang"], "title": "Enhancing Diffusion-based Unrestricted Adversarial Attacks via Adversary Preferences Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Preference alignment in diffusion models has primarily focused on benign\nhuman preferences (e.g., aesthetic). In this paper, we propose a novel\nperspective: framing unrestricted adversarial example generation as a problem\nof aligning with adversary preferences. Unlike benign alignment, adversarial\nalignment involves two inherently conflicting preferences: visual consistency\nand attack effectiveness, which often lead to unstable optimization and reward\nhacking (e.g., reducing visual quality to improve attack success). To address\nthis, we propose APA (Adversary Preferences Alignment), a two-stage framework\nthat decouples conflicting preferences and optimizes each with differentiable\nrewards. In the first stage, APA fine-tunes LoRA to improve visual consistency\nusing rule-based similarity reward. In the second stage, APA updates either the\nimage latent or prompt embedding based on feedback from a substitute\nclassifier, guided by trajectory-level and step-wise rewards. To enhance\nblack-box transferability, we further incorporate a diffusion augmentation\nstrategy. Experiments demonstrate that APA achieves significantly better attack\ntransferability while maintaining high visual consistency, inspiring further\nresearch to approach adversarial attacks from an alignment perspective. Code\nwill be available at https://github.com/deep-kaixun/APA.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01257", "pdf": "https://arxiv.org/pdf/2506.01257", "abs": "https://arxiv.org/abs/2506.01257", "authors": ["Jiancheng Ye", "Sophie Bronstein", "Jiarui Hai", "Malak Abu Hashish"], "title": "DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "DeepSeek-R1 is a cutting-edge open-source large language model (LLM)\ndeveloped by DeepSeek, showcasing advanced reasoning capabilities through a\nhybrid architecture that integrates mixture of experts (MoE), chain of thought\n(CoT) reasoning, and reinforcement learning. Released under the permissive MIT\nlicense, DeepSeek-R1 offers a transparent and cost-effective alternative to\nproprietary models like GPT-4o and Claude-3 Opus; it excels in structured\nproblem-solving domains such as mathematics, healthcare diagnostics, code\ngeneration, and pharmaceutical research. The model demonstrates competitive\nperformance on benchmarks like the United States Medical Licensing Examination\n(USMLE) and American Invitational Mathematics Examination (AIME), with strong\nresults in pediatric and ophthalmologic clinical decision support tasks. Its\narchitecture enables efficient inference while preserving reasoning depth,\nmaking it suitable for deployment in resource-constrained settings. However,\nDeepSeek-R1 also exhibits increased vulnerability to bias, misinformation,\nadversarial manipulation, and safety failures - especially in multilingual and\nethically sensitive contexts. This survey highlights the model's strengths,\nincluding interpretability, scalability, and adaptability, alongside its\nlimitations in general language fluency and safety alignment. Future research\npriorities include improving bias mitigation, natural language comprehension,\ndomain-specific validation, and regulatory compliance. Overall, DeepSeek-R1\nrepresents a major advance in open, scalable AI, underscoring the need for\ncollaborative governance to ensure responsible and equitable deployment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01347", "pdf": "https://arxiv.org/pdf/2506.01347", "abs": "https://arxiv.org/abs/2506.01347", "authors": ["Xinyu Zhu", "Mengzhou Xia", "Zhepei Wei", "Wei-Lin Chen", "Danqi Chen", "Yu Meng"], "title": "The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor training language models (LMs) on reasoning tasks that elicit emergent long\nchains of thought (CoTs). Unlike supervised learning, it updates the model\nusing both correct and incorrect samples via policy gradients. To better\nunderstand its mechanism, we decompose the learning signal into reinforcing\ncorrect responses and penalizing incorrect ones, referred to as Positive and\nNegative Sample Reinforcement (PSR and NSR), respectively. We train\nQwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset and uncover a\nsurprising result: training with only negative samples -- without reinforcing\ncorrect responses -- can be highly effective: it consistently improves\nperformance over the base model across the entire Pass@$k$ spectrum ($k$ up to\n$256$), often matching or surpassing PPO and GRPO. In contrast, reinforcing\nonly correct responses improves Pass@$1$ but degrades performance at higher\n$k$, due to reduced diversity. These inference-scaling trends highlight that\nsolely penalizing incorrect responses may contribute more to performance than\npreviously recognized. Through gradient analysis, we show that NSR works by\nsuppressing incorrect generations and redistributing probability mass toward\nother plausible candidates, guided by the model's prior beliefs. It refines the\nmodel's existing knowledge rather than introducing entirely new behaviors.\nBuilding on this insight, we propose a simple variant of the RL objective that\nupweights NSR, and show that it consistently improves overall Pass@$k$\nperformance on MATH, AIME 2025, and AMC23. Our code is available at\nhttps://github.com/TianHongZXY/RLVR-Decomposed.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "mathematical reasoning"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01381", "pdf": "https://arxiv.org/pdf/2506.01381", "abs": "https://arxiv.org/abs/2506.01381", "authors": ["Yilong Lai", "Jialong Wu", "Zhenglin Wang", "Deyu Zhou"], "title": "AdaRewriter: Unleashing the Power of Prompting-based Conversational Query Reformulation via Test-Time Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "Prompting-based conversational query reformulation has emerged as a powerful\napproach for conversational search, refining ambiguous user queries into\nstandalone search queries. Best-of-N reformulation over the generated\ncandidates via prompting shows impressive potential scaling capability.\nHowever, both the previous tuning methods (training time) and adaptation\napproaches (test time) can not fully unleash their benefits. In this paper, we\npropose AdaRewriter, a novel framework for query reformulation using an\noutcome-supervised reward model via test-time adaptation. By training a\nlightweight reward model with contrastive ranking loss, AdaRewriter selects the\nmost promising reformulation during inference. Notably, it can operate\neffectively in black-box systems, including commercial LLM APIs. Experiments on\nfive conversational search datasets show that AdaRewriter significantly\noutperforms the existing methods across most settings, demonstrating the\npotential of test-time adaptation for conversational query reformulation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time", "scaling", "test-time adaptation"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "ranking"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01420", "pdf": "https://arxiv.org/pdf/2506.01420", "abs": "https://arxiv.org/abs/2506.01420", "authors": ["Kyuyoung Kim", "Hyunjun Jeon", "Jinwoo Shin"], "title": "Self-Refining Language Model Anonymizers via Adversarial Distillation", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly used in sensitive domains,\nwhere their ability to infer personal data from seemingly benign text poses\nemerging privacy risks. While recent LLM-based anonymization methods help\nmitigate such risks, they often rely on proprietary models (e.g., GPT-4),\nraising concerns about cost and the potential exposure of sensitive data to\nuntrusted external systems. To address this, we introduce SElf-refining\nAnonymization with Language model (SEAL), a novel distillation framework for\ntraining small language models (SLMs) to perform effective anonymization\nwithout relying on external costly models at inference time. We leverage\nadversarial interactions between an LLM anonymizer and an inference model to\ncollect trajectories of anonymized texts and inferred attributes, which are\nused to distill anonymization, adversarial inference, and utility evaluation\ncapabilities into SLMs via supervised fine-tuning and preference learning. The\nresulting models learn to both anonymize text and critique their outputs,\nenabling iterative improvement of anonymization quality via self-refinement.\nExperiments on SynthPAI, a dataset of synthetic personal profiles and text\ncomments, demonstrate that SLMs trained with SEAL achieve substantial\nimprovements in anonymization capabilities. Notably, 8B models attain a\nprivacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with\nself-refinement, even surpass it in terms of privacy. These results show the\neffectiveness of our adversarial distillation framework in training SLMs as\nefficient anonymizers. To facilitate further research, we release the full\ndataset used in our experiments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01949", "pdf": "https://arxiv.org/pdf/2506.01949", "abs": "https://arxiv.org/abs/2506.01949", "authors": ["Fei Shen", "Xiaoyu Du", "Yutong Gao", "Jian Yu", "Yushe Cao", "Xing Lei", "Jinhui Tang"], "title": "IMAGHarmony: Controllable Image Editing with Consistent Object Quantity and Layout", "categories": ["cs.CV"], "comment": null, "summary": "Recent diffusion models have advanced image editing by enhancing visual\nquality and control, supporting broad applications across creative and\npersonalized domains. However, current image editing largely overlooks\nmulti-object scenarios, where precise control over object categories, counts,\nand spatial layouts remains a significant challenge. To address this, we\nintroduce a new task, quantity-and-layout consistent image editing (QL-Edit),\nwhich aims to enable fine-grained control of object quantity and spatial\nstructure in complex scenes. We further propose IMAGHarmony, a structure-aware\nframework that incorporates harmony-aware attention (HA) to integrate\nmultimodal semantics, explicitly modeling object counts and layouts to enhance\nediting accuracy and structural consistency. In addition, we observe that\ndiffusion models are susceptible to initial noise and exhibit strong\npreferences for specific noise patterns. Motivated by this, we present a\npreference-guided noise selection (PNS) strategy that chooses semantically\naligned initial noise samples based on vision-language matching, thereby\nimproving generation stability and layout consistency in multi-object editing.\nTo support evaluation, we construct HarmonyBench, a comprehensive benchmark\ncovering diverse quantity and layout control scenarios. Extensive experiments\ndemonstrate that IMAGHarmony consistently outperforms state-of-the-art methods\nin structural alignment and semantic accuracy. The code and model are available\nat https://github.com/muzishen/IMAGHarmony.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01451", "pdf": "https://arxiv.org/pdf/2506.01451", "abs": "https://arxiv.org/abs/2506.01451", "authors": ["Anshika Rawal", "Abhijeet Kumar", "Mridul Mishra"], "title": "Building Entity Association Mining Framework for Knowledge Discovery", "categories": ["cs.CL", "cs.IR", "I.2.7"], "comment": "Presented at Business Analytics and Intelligence Conference, IIM\n  Bengaluru", "summary": "Extracting useful signals or pattern to support important business decisions\nfor example analyzing investment product traction and discovering customer\npreference, risk monitoring etc. from unstructured text is a challenging task.\nCapturing interaction of entities or concepts and association mining is a\ncrucial component in text mining, enabling information extraction and reasoning\nover and knowledge discovery from text. Furthermore, it can be used to enrich\nor filter knowledge graphs to guide exploration processes, descriptive\nanalytics and uncover hidden stories in the text. In this paper, we introduce a\ndomain independent pipeline i.e., generalized framework to enable document\nfiltering, entity extraction using various sources (or techniques) as plug-ins\nand association mining to build any text mining business use-case and\nquantitatively define a scoring metric for ranking purpose. The proposed\nframework has three major components a) Document filtering: filtering\ndocuments/text of interest from massive amount of texts b) Configurable entity\nextraction pipeline: include entity extraction techniques i.e., i) DBpedia\nSpotlight, ii) Spacy NER, iii) Custom Entity Matcher, iv) Phrase extraction (or\ndictionary) based c) Association Relationship Mining: To generates\nco-occurrence graph to analyse potential relationships among entities,\nconcepts. Further, co-occurrence count based frequency statistics provide a\nholistic window to observe association trends or buzz rate in specific business\ncontext. The paper demonstrates the usage of framework as fundamental building\nbox in two financial use-cases namely brand product discovery and vendor risk\nmonitoring. We aim that such framework will remove duplicated effort, minimize\nthe development effort, and encourage reusability and rapid prototyping in\nassociation mining business applications for institutions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "ranking"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01495", "pdf": "https://arxiv.org/pdf/2506.01495", "abs": "https://arxiv.org/abs/2506.01495", "authors": ["Ping Wu", "Guobin Shen", "Dongcheng Zhao", "Yuwei Wang", "Yiting Dong", "Yu Shi", "Enmeng Lu", "Feifei Zhao", "Yi Zeng"], "title": "CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Ensuring that Large Language Models (LLMs) align with mainstream human values\nand ethical norms is crucial for the safe and sustainable development of AI.\nCurrent value evaluation and alignment are constrained by Western cultural bias\nand incomplete domestic frameworks reliant on non-native rules; furthermore,\nthe lack of scalable, rule-driven scenario generation methods makes evaluations\ncostly and inadequate across diverse cultural contexts. To address these\nchallenges, we propose a hierarchical value framework grounded in core Chinese\nvalues, encompassing three main dimensions, 12 core values, and 50 derived\nvalues. Based on this framework, we construct a large-scale Chinese Values\nCorpus (CVC) containing over 250,000 value rules enhanced and expanded through\nhuman annotation. Experimental results show that CVC-guided scenarios\noutperform direct generation ones in value boundaries and content diversity. In\nthe evaluation across six sensitive themes (e.g., surrogacy, suicide), seven\nmainstream LLMs preferred CVC-generated options in over 70.5% of cases, while\nfive Chinese human annotators showed an 87.5% alignment with CVC, confirming\nits universality, cultural relevance, and strong alignment with Chinese values.\nAdditionally, we construct 400,000 rule-based moral dilemma scenarios that\nobjectively capture nuanced distinctions in conflicting value prioritization\nacross 17 LLMs. Our work establishes a culturally-adaptive benchmarking\nframework for comprehensive value evaluation and alignment, representing\nChinese characteristics. All data are available at\nhttps://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at\nhttps://github.com/Beijing-AISI/CVC.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01520", "pdf": "https://arxiv.org/pdf/2506.01520", "abs": "https://arxiv.org/abs/2506.01520", "authors": ["Bobo Li", "Yuheng Wang", "Hao Fei", "Juncheng Li", "Wei Ji", "Mong-Li Lee", "Wynne Hsu"], "title": "FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents", "categories": ["cs.CL"], "comment": "8 pages, 7 figures", "summary": "Online form filling is a common yet labor-intensive task involving extensive\nkeyboard and mouse interactions. Despite the long-standing vision of automating\nthis process with \"one click\", existing tools remain largely rule-based and\nlack generalizable, generative capabilities. Recent advances in Multimodal\nLarge Language Models (MLLMs) have enabled promising agents for GUI-related\ntasks in general-purpose scenarios. However, they struggle with the unique\nchallenges of form filling, such as flexible layouts and the difficulty of\naligning textual instructions with on-screen fields. To bridge this gap, we\nformally define the form-filling task and propose FormFactory, an interactive\nbenchmarking suite comprising a web-based interface, backend evaluation module,\nand carefully constructed dataset. Our benchmark covers diverse real-world\nscenarios, incorporates various field formats, and simulates high-fidelity form\ninteractions. We conduct a comprehensive evaluation of state-of-the-art MLLMs\nand observe that no model surpasses 5% accuracy, underscoring the inherent\ndifficulty of the task. These findings also reveal significant limitations in\ncurrent models' visual layout reasoning and field-value alignment abilities. We\nhope our benchmark can serve as a stepping stone for further research into\nrobust, practical form-filling agents.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01713", "pdf": "https://arxiv.org/pdf/2506.01713", "abs": "https://arxiv.org/abs/2506.01713", "authors": ["Zhongwei Wan", "Zhihao Dou", "Che Liu", "Yu Zhang", "Dongfei Cui", "Qinjian Zhao", "Hui Shen", "Jing Xiong", "Yi Xin", "Yifan Jiang", "Yangfan He", "Mi Zhang", "Shen Yan"], "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning", "categories": ["cs.CL"], "comment": "Under review", "summary": "Multimodal large language models (MLLMs) have shown promising capabilities in\nreasoning tasks, yet still struggle with complex problems requiring explicit\nself-reflection and self-correction, especially compared to their unimodal\ntext-based counterparts. Existing reflection methods are simplistic and\nstruggle to generate meaningful and instructive feedback, as the reasoning\nability and knowledge limits of pre-trained models are largely fixed during\ninitial training. To overcome these challenges, we propose Multimodal\nSelf-Reflection enhanced reasoning with Group Relative Policy Optimization\n(SRPO), a two-stage reflection-aware reinforcement learning (RL) framework\nexplicitly designed to enhance multimodal LLM reasoning. In the first stage, we\nconstruct a high-quality, reflection-focused dataset under the guidance of an\nadvanced MLLM, which generates reflections based on initial responses to help\nthe policy model learn both reasoning and self-reflection. In the second stage,\nwe introduce a novel reward mechanism within the GRPO framework that encourages\nconcise and cognitively meaningful reflection while avoiding redundancy.\nExtensive experiments across multiple multimodal reasoning benchmarks,\nincluding MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B\nand Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms\nstate-of-the-art models, achieving notable improvements in both reasoning\naccuracy and reflection quality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01320", "pdf": "https://arxiv.org/pdf/2506.01320", "abs": "https://arxiv.org/abs/2506.01320", "authors": ["Taehoon Yoon", "Yunhong Min", "Kyeongmin Yeo", "Minhyuk Sung"], "title": "$Ψ$-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce $\\Psi$-Sampler, an SMC-based framework incorporating pCNL-based\ninitial particle sampling for effective inference-time reward alignment with a\nscore-based generative model. Inference-time reward alignment with score-based\ngenerative models has recently gained significant traction, following a broader\nparadigm shift from pre-training to post-training optimization. At the core of\nthis trend is the application of Sequential Monte Carlo (SMC) to the denoising\nprocess. However, existing methods typically initialize particles from the\nGaussian prior, which inadequately captures reward-relevant regions and results\nin reduced sampling efficiency. We demonstrate that initializing from the\nreward-aware posterior significantly improves alignment performance. To enable\nposterior sampling in high-dimensional latent spaces, we introduce the\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines\ndimension-robust proposals with gradient-informed dynamics. This approach\nenables efficient and scalable posterior sampling and consistently improves\nperformance across various reward alignment tasks, including layout-to-image\ngeneration, quantity-aware generation, and aesthetic-preference generation, as\ndemonstrated in our experiments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00363", "pdf": "https://arxiv.org/pdf/2506.00363", "abs": "https://arxiv.org/abs/2506.00363", "authors": ["Yubai Wei", "Jiale Han", "Yi Yang"], "title": "Adapting General-Purpose Embedding Models to Private Datasets Using Keyword-based Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": "Link: https://github.com/BaileyWei/BMEmbed", "summary": "Text embedding models play a cornerstone role in AI applications, such as\nretrieval-augmented generation (RAG). While general-purpose text embedding\nmodels demonstrate strong performance on generic retrieval benchmarks, their\neffectiveness diminishes when applied to private datasets (e.g.,\ncompany-specific proprietary data), which often contain specialized terminology\nand lingo. In this work, we introduce BMEmbed, a novel method for adapting\ngeneral-purpose text embedding models to private datasets. By leveraging the\nwell-established keyword-based retrieval technique (BM25), we construct\nsupervisory signals from the ranking of keyword-based retrieval results to\nfacilitate model adaptation. We evaluate BMEmbed across a range of domains,\ndatasets, and models, showing consistent improvements in retrieval performance.\nMoreover, we provide empirical insights into how BM25-based signals contribute\nto improving embeddings by fostering alignment and uniformity, highlighting the\nvalue of this approach in adapting models to domain-specific data. We release\nthe source code available at https://github.com/BaileyWei/BMEmbed for the\nresearch community.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "alignment"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00845", "pdf": "https://arxiv.org/pdf/2506.00845", "abs": "https://arxiv.org/abs/2506.00845", "authors": ["Yizhuo Zhang", "Heng Wang", "Shangbin Feng", "Zhaoxuan Tan", "Xinyun Liu", "Yulia Tsvetkov"], "title": "Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning", "categories": ["cs.LG", "cs.CL", "I.2.7; I.2.2"], "comment": "9 pages, 3 figures, 3 tables. Experimental code and results are\n  publicly available at\n  https://anonymous.4open.science/r/Graph_RL-BF08/readme.md", "summary": "Previous research has sought to enhance the graph reasoning capabilities of\nLLMs by supervised fine-tuning on synthetic graph data. While these led to\nspecialized LLMs better at solving graph algorithm problems, we don't need LLMs\nfor shortest path: we need generalization from synthetic graph data to\nreal-world tasks with implicit graph structures. In this work, we propose to\nunlock generalizable learning of graph synthetic data with reinforcement\nlearning. We first design solution-based and process-based rewards for\nsynthetic graph problems: instead of rigid memorizing response patterns in\ndirect fine-tuning, we posit that RL would help LLMs grasp the essentials\nunderlying graph reasoning and alleviate overfitting. We employ RL algorithms\nsuch as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on\nsynthetic graph data. We then compare them against existing settings on both\nin-domain synthetic tasks and out-of-domain real-world tasks with implicit\ngraph structures such as multi-hop QA, structured planning, and more. Extensive\nexperiments demonstrate that our RL recipe leads to statistically significant\nimprovement on 5 datasets, with an average gain of 12.9\\% over baseline\nsettings. Further analysis reveals that process-based rewards consistently\noutperform solution-based rewards, mixing synthetic and real-world task data\nyields potential gains, while compositionality and explainable intermediate\nsteps remains a critical challenge even after RL.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "DPO"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00930", "pdf": "https://arxiv.org/pdf/2506.00930", "abs": "https://arxiv.org/abs/2506.00930", "authors": ["Yongqi Li", "Shen Zhou", "Xiaohu Li", "Xin Miao", "Jintao Wen", "Mayi Xu", "Jianhao Chen", "Birong Pan", "Hankun Kang", "Yuanyuan Zhu", "Ming Zhong", "Tieyun Qian"], "title": "Aligning VLM Assistants with Personalized Situated Cognition", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (main), camera-ready version", "summary": "Vision-language models (VLMs) aligned with general human objectives, such as\nbeing harmless and hallucination-free, have become valuable assistants of\nhumans in managing visual tasks. However, people with diversified backgrounds\nhave different cognition even in the same situation. Consequently, they may\nhave personalized expectations for VLM assistants. This highlights the urgent\nneed to align VLM assistants with personalized situated cognition for\nreal-world assistance. To study this problem, we first simplify it by\ncharacterizing individuals based on the sociological concept of Role-Set. Then,\nwe propose to evaluate the individuals' actions to examine whether the\npersonalized alignment is achieved. Further, we construct a benchmark named\nPCogAlignBench, which includes 18k instances and 20 individuals with different\nRole-Sets. Finally, we present a framework called PCogAlign, which constructs a\ncognition-aware and action-based reward model for personalized alignment.\nExperimental results and human evaluations demonstrate the reliability of the\nPCogAlignBench and the effectiveness of our proposed PCogAlign. We will\nopen-source the constructed benchmark and code at\nhttps://github.com/NLPGM/PCogAlign.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "reliability"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01077", "pdf": "https://arxiv.org/pdf/2506.01077", "abs": "https://arxiv.org/abs/2506.01077", "authors": ["Yueqian Guo", "Tianzhao Li", "Xin Lyu", "Jiehaolin Chen", "Zhaohan Wang", "Sirui Xiao", "Yurun Chen", "Yezi He", "Helin Li", "Fan Zhang"], "title": "TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans", "categories": ["cs.GR", "cs.HC", "68U05(Primary), 62M45(Secondary)"], "comment": "24 pages,12 figures", "summary": "Large Language Model (LLM)-driven digital humans have sparked a series of\nrecent studies on co-speech gesture generation systems. However, existing\napproaches struggle with real-time synthesis and long-text comprehension. This\npaper introduces Transformer-Based Rich Motion Matching (TRiMM), a novel\nmulti-modal framework for real-time 3D gesture generation. Our method\nincorporates three modules: 1) a cross-modal attention mechanism to achieve\nprecise temporal alignment between speech and gestures; 2) a long-context\nautoregressive model with a sliding window mechanism for effective sequence\nmodeling; 3) a large-scale gesture matching system that constructs an atomic\naction library and enables real-time retrieval. Additionally, we develop a\nlightweight pipeline implemented in the Unreal Engine for experimentation. Our\napproach achieves real-time inference at 120 fps and maintains a per-sentence\nlatency of 0.15 seconds on consumer-grade GPUs (Geforce RTX3060). Extensive\nsubjective and objective evaluations on the ZEGGS, and BEAT datasets\ndemonstrate that our model outperforms current state-of-the-art methods. TRiMM\nenhances the speed of co-speech gesture generation while ensuring gesture\nquality, enabling LLM-driven digital humans to respond to speech in real time\nand synthesize corresponding gestures. Our code is available at\nhttps://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00042", "pdf": "https://arxiv.org/pdf/2506.00042", "abs": "https://arxiv.org/abs/2506.00042", "authors": ["Yue Cui", "Liuyi Yao", "Shuchang Tao", "Weijie Shi", "Yaliang Li", "Bolin Ding", "Xiaofang Zhou"], "title": "Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have significantly advanced natural language\nprocessing, particularly through the integration of external tools and APIs.\nHowever, their effectiveness is frequently hampered by parameter mis-filling\nduring tool calling. In this paper, we propose the Hierarchical Tool Error\nChecklist (HiTEC) framework to systematically diagnose and mitigate\ntool-calling errors without relying on extensive real-world interactions. HiTEC\nintroduces a two-tiered approach: a global error checklist that identifies\ncommon, cross-tool issues, and a local error checklist that targets\ntool-specific and contextual failures. Building on this structure, we propose\ntwo deployments: HiTEC-In Context Learning (HiTEC-ICL) and\nHiTEC-Kahneman-Tversky Optimization (HiTEC-KTO). HiTEC-ICL embeds the global\nchecklist in the initial prompts and leverages a two-round conversational\ninteraction to dynamically refine parameter handling, while HiTEC-KTO generates\nhigh-quality negative examples to drive fine-tuning via preference-based\noptimization. Extensive experiments across five public datasets demonstrate\nthat our framework significantly improves parameter-filling accuracy and\ntool-calling success rates compared to baseline methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01929", "pdf": "https://arxiv.org/pdf/2506.01929", "abs": "https://arxiv.org/abs/2506.01929", "authors": ["Saar Huberman", "Or Patashnik", "Omer Dahary", "Ron Mokady", "Daniel Cohen-Or"], "title": "Image Generation from Contextually-Contradictory Prompts", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": "Project page: https://tdpc2025.github.io/SAP/", "summary": "Text-to-image diffusion models excel at generating high-quality, diverse\nimages from natural language prompts. However, they often fail to produce\nsemantically accurate results when the prompt contains concept combinations\nthat contradict their learned priors. We define this failure mode as contextual\ncontradiction, where one concept implicitly negates another due to entangled\nassociations learned during training. To address this, we propose a stage-aware\nprompt decomposition framework that guides the denoising process using a\nsequence of proxy prompts. Each proxy prompt is constructed to match the\nsemantic content expected to emerge at a specific stage of denoising, while\nensuring contextual coherence. To construct these proxy prompts, we leverage a\nlarge language model (LLM) to analyze the target prompt, identify\ncontradictions, and generate alternative expressions that preserve the original\nintent while resolving contextual conflicts. By aligning prompt information\nwith the denoising progression, our method enables fine-grained semantic\ncontrol and accurate image generation in the presence of contextual\ncontradictions. Experiments across a variety of challenging prompts show\nsubstantial improvements in alignment to the textual prompt.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00085", "pdf": "https://arxiv.org/pdf/2506.00085", "abs": "https://arxiv.org/abs/2506.00085", "authors": ["Vincent Siu", "Nicholas Crispino", "Zihao Yu", "Sam Pan", "Zhun Wang", "Yang Liu", "Dawn Song", "Chenguang Wang"], "title": "COSMIC: Generalized Refusal Direction Identification in LLM Activations", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) encode behaviors such as refusal within their\nactivation space, yet identifying these behaviors remains a significant\nchallenge. Existing methods often rely on predefined refusal templates\ndetectable in output tokens or require manual analysis. We introduce\n\\textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an\nautomated framework for direction selection that identifies viable steering\ndirections and target layers using cosine similarity - entirely independent of\nmodel outputs. COSMIC achieves steering performance comparable to prior methods\nwithout requiring assumptions about a model's refusal behavior, such as the\npresence of specific refusal tokens. It reliably identifies refusal directions\nin adversarial settings and weakly aligned models, and is capable of steering\nsuch models toward safer behavior with minimal increase in false refusals,\ndemonstrating robustness across a wide range of alignment conditions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00137", "pdf": "https://arxiv.org/pdf/2506.00137", "abs": "https://arxiv.org/abs/2506.00137", "authors": ["Alireza Salemi", "Hamed Zamani"], "title": "LaMP-QA: A Benchmark for Personalized Long-form Question Answering", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Personalization is essential for question answering systems that are\nuser-centric. Despite its importance, personalization in answer generation has\nbeen relatively underexplored. This is mainly due to lack of resources for\ntraining and evaluating personalized question answering systems. We address\nthis gap by introducing LaMP-QA -- a benchmark designed for evaluating\npersonalized long-form answer generation. The benchmark covers questions from\nthree major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal\nDevelopment, and (3) Society & Culture, encompassing over 45 subcategories in\ntotal. To assess the quality and potential impact of the LaMP-QA benchmark for\npersonalized question answering, we conduct comprehensive human and automatic\nevaluations, to compare multiple evaluation strategies for evaluating generated\npersonalized responses and measure their alignment with human preferences.\nFurthermore, we benchmark a number of non-personalized and personalized\napproaches based on open-source and proprietary large language models (LLMs).\nOur results show that incorporating the personalized context provided leads to\nperformance improvements of up to 39%. The benchmark is publicly released to\nsupport future research in this area.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "question answering"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00523", "pdf": "https://arxiv.org/pdf/2506.00523", "abs": "https://arxiv.org/abs/2506.00523", "authors": ["Xingtong Ge", "Xin Zhang", "Tongda Xu", "Yi Zhang", "Xinjie Zhang", "Yan Wang", "Jun Zhang"], "title": "SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation", "categories": ["cs.CV"], "comment": "under review", "summary": "The Distribution Matching Distillation (DMD) has been successfully applied to\ntext-to-image diffusion models such as Stable Diffusion (SD) 1.5. However,\nvanilla DMD suffers from convergence difficulties on large-scale flow-based\ntext-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze\nthe issues when applying vanilla DMD on large-scale models. Then, to overcome\nthe scalability challenge, we propose implicit distribution alignment (IDA) to\nregularize the distance between the generator and fake distribution.\nFurthermore, we propose intra-segment guidance (ISG) to relocate the timestep\nimportance distribution from the teacher model. With IDA alone, DMD converges\nfor SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1\ndev. Along with other improvements such as scaled up discriminator models, our\nfinal model, dubbed \\textbf{SenseFlow}, achieves superior performance in\ndistillation for both diffusion based text-to-image models such as SDXL, and\nflow-matching models such as SD 3.5 Large and FLUX. The source code will be\navaliable at https://github.com/XingtongGe/SenseFlow.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00253", "pdf": "https://arxiv.org/pdf/2506.00253", "abs": "https://arxiv.org/abs/2506.00253", "authors": ["Lihao Sun", "Chengzhi Mao", "Valentin Hofmann", "Xuechunzi Bai"], "title": "Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accpeted to ACL 2025 Main Conferencce", "summary": "Although value-aligned language models (LMs) appear unbiased in explicit bias\nevaluations, they often exhibit stereotypes in implicit word association tasks,\nraising concerns about their fair usage. We investigate the mechanisms behind\nthis discrepancy and find that alignment surprisingly amplifies implicit bias\nin model outputs. Specifically, we show that aligned LMs, unlike their\nunaligned counterparts, overlook racial concepts in early internal\nrepresentations when the context is ambiguous. Not representing race likely\nfails to activate safety guardrails, leading to unintended biases. Inspired by\nthis insight, we propose a new bias mitigation strategy that works by\nincentivizing the representation of racial concepts in the early model layers.\nIn contrast to conventional mitigation methods of machine unlearning, our\ninterventions find that steering the model to be more aware of racial concepts\neffectively mitigates implicit bias. Similar to race blindness in humans,\nignoring racial nuances can inadvertently perpetuate subtle biases in LMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00591", "pdf": "https://arxiv.org/pdf/2506.00591", "abs": "https://arxiv.org/abs/2506.00591", "authors": ["Xudong Ma", "Nantheera Anantrasirichai", "Stefanos Bolomytis", "Alin Achim"], "title": "MR2US-Pro: Prostate MR to Ultrasound Image Translation and Registration Based on Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "The diagnosis of prostate cancer increasingly depends on multimodal imaging,\nparticularly magnetic resonance imaging (MRI) and transrectal ultrasound\n(TRUS). However, accurate registration between these modalities remains a\nfundamental challenge due to the differences in dimensionality and anatomical\nrepresentations. In this work, we present a novel framework that addresses\nthese challenges through a two-stage process: TRUS 3D reconstruction followed\nby cross-modal registration. Unlike existing TRUS 3D reconstruction methods\nthat rely heavily on external probe tracking information, we propose a totally\nprobe-location-independent approach that leverages the natural correlation\nbetween sagittal and transverse TRUS views. With the help of our\nclustering-based feature matching method, we enable the spatial localization of\n2D frames without any additional probe tracking information. For the\nregistration stage, we introduce an unsupervised diffusion-based framework\nguided by modality translation. Unlike existing methods that translate one\nmodality into another, we map both MR and US into a pseudo intermediate\nmodality. This design enables us to customize it to retain only\nregistration-critical features, greatly easing registration. To further enhance\nanatomical alignment, we incorporate an anatomy-aware registration strategy\nthat prioritizes internal structural coherence while adaptively reducing the\ninfluence of boundary inconsistencies. Extensive validation demonstrates that\nour approach outperforms state-of-the-art methods by achieving superior\nregistration accuracy with physically realistic deformations in a completely\nunsupervised fashion.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00596", "pdf": "https://arxiv.org/pdf/2506.00596", "abs": "https://arxiv.org/abs/2506.00596", "authors": ["Danfeng li", "Hui Zhang", "Sheng Wang", "Jiacheng Li", "Zuxuan Wu"], "title": "Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise Shape and Semantic Control", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent advances in diffusion models, top-tier text-to-image (T2I)\nmodels still struggle to achieve precise spatial layout control, i.e.\naccurately generating entities with specified attributes and locations.\nSegmentation-mask-to-image (S2I) generation has emerged as a promising solution\nby incorporating pixel-level spatial guidance and regional text prompts.\nHowever, existing S2I methods fail to simultaneously ensure semantic\nconsistency and shape consistency. To address these challenges, we propose\nSeg2Any, a novel S2I framework built upon advanced multimodal diffusion\ntransformers (e.g. FLUX). First, to achieve both semantic and shape\nconsistency, we decouple segmentation mask conditions into regional semantic\nand high-frequency shape components. The regional semantic condition is\nintroduced by a Semantic Alignment Attention Mask, ensuring that generated\nentities adhere to their assigned text prompts. The high-frequency shape\ncondition, representing entity boundaries, is encoded as an Entity Contour Map\nand then introduced as an additional modality via multi-modal attention to\nguide image spatial structure. Second, to prevent attribute leakage across\nentities in multi-entity scenarios, we introduce an Attribute Isolation\nAttention Mask mechanism, which constrains each entity's image tokens to attend\nexclusively to themselves during image self-attention. To support open-set S2I\ngeneration, we construct SACap-1M, a large-scale dataset containing 1 million\nimages with 5.9 million segmented entities and detailed regional captions,\nalong with a SACap-Eval benchmark for comprehensive S2I evaluation. Extensive\nexperiments demonstrate that Seg2Any achieves state-of-the-art performance on\nboth open-set and closed-set S2I benchmarks, particularly in fine-grained\nspatial and attribute control of entities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "consistency", "fine-grained"], "score": 5}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00600", "pdf": "https://arxiv.org/pdf/2506.00600", "abs": "https://arxiv.org/abs/2506.00600", "authors": ["Xianghui Ze", "Beiyi Zhu", "Zhenbo Song", "Jianfeng Lu", "Yujiao Shi"], "title": "SatDreamer360: Geometry Consistent Street-View Video Generation from Satellite Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Generating continuous ground-level video from satellite imagery is a\nchallenging task with significant potential for applications in simulation,\nautonomous navigation, and digital twin cities. Existing approaches primarily\nfocus on synthesizing individual ground-view images, often relying on auxiliary\ninputs like height maps or handcrafted projections, and fall short in producing\ntemporally consistent sequences. In this paper, we propose {SatDreamer360}, a\nnovel framework that generates geometrically and temporally consistent\nground-view video from a single satellite image and a predefined trajectory. To\nbridge the large viewpoint gap, we introduce a compact tri-plane representation\nthat encodes scene geometry directly from the satellite image. A ray-based\npixel attention mechanism retrieves view-dependent features from the tri-plane,\nenabling accurate cross-view correspondence without requiring additional\ngeometric priors. To ensure multi-frame consistency, we propose an\nepipolar-constrained temporal attention module that aligns features across\nframes using the known relative poses along the trajectory. To support\nevaluation, we introduce {VIGOR++}, a large-scale dataset for cross-view video\ngeneration, with dense trajectory annotations and high-quality ground-view\nsequences. Extensive experiments demonstrate that SatDreamer360 achieves\nsuperior performance in fidelity, coherence, and geometric alignment across\ndiverse urban scenes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00607", "pdf": "https://arxiv.org/pdf/2506.00607", "abs": "https://arxiv.org/abs/2506.00607", "authors": ["JungWoo Chae", "Jiyoon Kim", "Sangheum Hwang"], "title": "Parallel Rescaling: Rebalancing Consistency Guidance for Personalized Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Personalizing diffusion models to specific users or concepts remains\nchallenging, particularly when only a few reference images are available.\nExisting methods such as DreamBooth and Textual Inversion often overfit to\nlimited data, causing misalignment between generated images and text prompts\nwhen attempting to balance identity fidelity with prompt adherence. While\nDirect Consistency Optimization (DCO) with its consistency-guided sampling\npartially alleviates this issue, it still struggles with complex or stylized\nprompts. In this paper, we propose a parallel rescaling technique for\npersonalized diffusion models. Our approach explicitly decomposes the\nconsistency guidance signal into parallel and orthogonal components relative to\nclassifier free guidance (CFG). By rescaling the parallel component, we\nminimize disruptive interference with CFG while preserving the subject's\nidentity. Unlike prior personalization methods, our technique does not require\nadditional training data or expensive annotations. Extensive experiments show\nimproved prompt alignment and visual fidelity compared to baseline methods,\neven on challenging stylized prompts. These findings highlight the potential of\nparallel rescaled guidance to yield more stable and accurate personalization\nfor diverse user inputs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00312", "pdf": "https://arxiv.org/pdf/2506.00312", "abs": "https://arxiv.org/abs/2506.00312", "authors": ["Brendan Sands", "Yining Wang", "Chenhao Xu", "Yuxuan Zhou", "Lai Wei", "Rohitash Chandra"], "title": "An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been prominent in various tasks, including\ntext generation and summarisation. The applicability of LLMs to the generation\nof product reviews is gaining momentum, paving the way for the generation of\nmovie reviews. In this study, we propose a framework that generates movie\nreviews using three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0), and evaluate\ntheir performance by comparing the generated outputs with IMDb user reviews. We\nuse movie subtitles and screenplays as input to the LLMs and investigate how\nthey affect the quality of reviews generated. We review the LLM-based movie\nreviews in terms of vocabulary, sentiment polarity, similarity, and thematic\nconsistency in comparison to IMDB user reviews. The results demonstrate that\nLLMs are capable of generating syntactically fluent and structurally complete\nmovie reviews. Nevertheless, there is still a noticeable gap in emotional\nrichness and stylistic coherence between LLM-generated and IMDb reviews,\nsuggesting that further refinement is needed to improve the overall quality of\nmovie review generation. We provided a survey-based analysis where participants\nwere told to distinguish between LLM and IMDb user reviews. The results show\nthat LLM-generated reviews are difficult to distinguish from IMDB user reviews.\nWe found that DeepSeek-V3 produced the most balanced reviews, closely matching\nIMDb reviews. GPT-4o overemphasised positive emotions, while Gemini-2.0\ncaptured negative emotions better but showed excessive emotional intensity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00633", "pdf": "https://arxiv.org/pdf/2506.00633", "abs": "https://arxiv.org/abs/2506.00633", "authors": ["Daniele Molino", "Camillo Maria Caruso", "Filippo Ruffini", "Paolo Soda", "Valerio Guarrasi"], "title": "Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive Vision-Language Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Objective: While recent advances in text-conditioned generative models have\nenabled the synthesis of realistic medical images, progress has been largely\nconfined to 2D modalities such as chest X-rays. Extending text-to-image\ngeneration to volumetric Computed Tomography (CT) remains a significant\nchallenge, due to its high dimensionality, anatomical complexity, and the\nabsence of robust frameworks that align vision-language data in 3D medical\nimaging. Methods: We introduce a novel architecture for Text-to-CT generation\nthat combines a latent diffusion model with a 3D contrastive vision-language\npretraining scheme. Our approach leverages a dual-encoder CLIP-style model\ntrained on paired CT volumes and radiology reports to establish a shared\nembedding space, which serves as the conditioning input for generation. CT\nvolumes are compressed into a low-dimensional latent space via a pretrained\nvolumetric VAE, enabling efficient 3D denoising diffusion without requiring\nexternal super-resolution stages. Results: We evaluate our method on the\nCT-RATE dataset and conduct a comprehensive assessment of image fidelity,\nclinical relevance, and semantic alignment. Our model achieves competitive\nperformance across all tasks, significantly outperforming prior baselines for\ntext-to-CT generation. Moreover, we demonstrate that CT scans synthesized by\nour framework can effectively augment real data, improving downstream\ndiagnostic performance. Conclusion: Our results show that modality-specific\nvision-language alignment is a key component for high-quality 3D medical image\ngeneration. By integrating contrastive pretraining and volumetric diffusion,\nour method offers a scalable and controllable solution for synthesizing\nclinically meaningful CT volumes from text, paving the way for new applications\nin data augmentation, medical education, and automated clinical simulation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00652", "pdf": "https://arxiv.org/pdf/2506.00652", "abs": "https://arxiv.org/abs/2506.00652", "authors": ["Yu Huang", "Junhao Chen", "Qi Zheng", "Hanqian Li", "Shuliang Liu", "Xuming Hu"], "title": "Video Signature: In-generation Watermarking for Latent Video Diffusion Models", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "The rapid development of Artificial Intelligence Generated Content (AIGC) has\nled to significant progress in video generation but also raises serious\nconcerns about intellectual property protection and reliable content tracing.\nWatermarking is a widely adopted solution to this issue, but existing methods\nfor video generation mainly follow a post-generation paradigm, which introduces\nadditional computational overhead and often fails to effectively balance the\ntrade-off between video quality and watermark extraction. To address these\nissues, we propose Video Signature (VIDSIG), an in-generation watermarking\nmethod for latent video diffusion models, which enables implicit and adaptive\nwatermark integration during generation. Specifically, we achieve this by\npartially fine-tuning the latent decoder, where Perturbation-Aware Suppression\n(PAS) pre-identifies and freezes perceptually sensitive layers to preserve\nvisual quality. Beyond spatial fidelity, we further enhance temporal\nconsistency by introducing a lightweight Temporal Alignment module that guides\nthe decoder to generate coherent frame sequences during fine-tuning.\nExperimental results show that VIDSIG achieves the best overall performance in\nwatermark extraction, visual quality, and generation efficiency. It also\ndemonstrates strong robustness against both spatial and temporal tampering,\nhighlighting its practicality in real-world scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00716", "pdf": "https://arxiv.org/pdf/2506.00716", "abs": "https://arxiv.org/abs/2506.00716", "authors": ["Shi Mao", "Yogeshwar Mishra", "Wolfgang Heidrich"], "title": "Fovea Stacking: Imaging with Dynamic Localized Aberration Correction", "categories": ["cs.CV"], "comment": null, "summary": "The desire for cameras with smaller form factors has recently lead to a push\nfor exploring computational imaging systems with reduced optical complexity\nsuch as a smaller number of lens elements. Unfortunately such simplified\noptical systems usually suffer from severe aberrations, especially in off-axis\nregions, which can be difficult to correct purely in software.\n  In this paper we introduce Fovea Stacking, a new type of imaging system that\nutilizes emerging dynamic optical components called deformable phase plates\n(DPPs) for localized aberration correction anywhere on the image sensor. By\noptimizing DPP deformations through a differentiable optical model, off-axis\naberrations are corrected locally, producing a foveated image with enhanced\nsharpness at the fixation point - analogous to the eye's fovea. Stacking\nmultiple such foveated images, each with a different fixation point, yields a\ncomposite image free from aberrations. To efficiently cover the entire field of\nview, we propose joint optimization of DPP deformations under imaging budget\nconstraints. Due to the DPP device's non-linear behavior, we introduce a neural\nnetwork-based control model for improved alignment between simulation-hardware\nperformance.\n  We further demonstrated that for extended depth-of-field imaging, fovea\nstacking outperforms traditional focus stacking in image quality. By\nintegrating object detection or eye-tracking, the system can dynamically adjust\nthe lens to track the object of interest-enabling real-time foveated video\nsuitable for downstream applications such as surveillance or foveated virtual\nreality displays.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00718", "pdf": "https://arxiv.org/pdf/2506.00718", "abs": "https://arxiv.org/abs/2506.00718", "authors": ["Tianqin Li", "Ziqi Wen", "Leiran Song", "Jun Liu", "Zhi Jing", "Tai Sing Lee"], "title": "From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human vision organizes local cues into coherent global forms using Gestalt\nprinciples like closure, proximity, and figure-ground assignment -- functions\nreliant on global spatial structure. We investigate whether modern vision\nmodels show similar behaviors, and under what training conditions these emerge.\nWe find that Vision Transformers (ViTs) trained with Masked Autoencoding (MAE)\nexhibit activation patterns consistent with Gestalt laws, including illusory\ncontour completion, convexity preference, and dynamic figure-ground\nsegregation. To probe the computational basis, we hypothesize that modeling\nglobal dependencies is necessary for Gestalt-like organization. We introduce\nthe Distorted Spatial Relationship Testbench (DiSRT), which evaluates\nsensitivity to global spatial perturbations while preserving local textures.\nUsing DiSRT, we show that self-supervised models (e.g., MAE, CLIP) outperform\nsupervised baselines and sometimes even exceed human performance. ConvNeXt\nmodels trained with MAE also exhibit Gestalt-compatible representations,\nsuggesting such sensitivity can arise without attention architectures. However,\nclassification finetuning degrades this ability. Inspired by biological vision,\nwe show that a Top-K activation sparsity mechanism can restore global\nsensitivity. Our findings identify training conditions that promote or suppress\nGestalt-like perception and establish DiSRT as a diagnostic for global\nstructure sensitivity across models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00396", "pdf": "https://arxiv.org/pdf/2506.00396", "abs": "https://arxiv.org/abs/2506.00396", "authors": ["Jiawei Gu", "Shangsong Liang"], "title": "Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively", "categories": ["cs.CL"], "comment": "ACL2025 Oral (Industry Track)", "summary": "Effective decision-making in Large Language Models (LLMs) is essential for\nhandling intricate tasks. However, existing approaches prioritize performance\nbut often overlook the balance between effectiveness and computational cost. To\naddress this, we first introduce the 3E Criteria to systematically assess the\ncost-effectiveness of search strategies, revealing that existing methods often\ntrade significant efficiency for marginal performance gains. To improve LLM\ndecision-making while maintaining efficiency, we propose the Speculative Reward\nModel (SRM), a plug-and-play framework that seamlessly integrates with existing\nsearch strategies. Specifically, SRM employs an external reward assigner to\npredict optimal actions, reducing reliance on LLMs' internal self-evaluation.\nAnd a speculative verification mechanism is used to prune suboptimal choices\nand guide the search toward more promising steps. We evaluate SRM on several\ncomplex decision-making tasks including mathematical reasoning, planning and\nnumerical reasoning in specialized domains. Experimental results show that SRM\nreduces costs to 1/10 of the original search framework on average while\nmaintaining effectiveness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "mathematical reasoning", "criteria"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00774", "pdf": "https://arxiv.org/pdf/2506.00774", "abs": "https://arxiv.org/abs/2506.00774", "authors": ["Milad Khanchi", "Maria Amer", "Charalambos Poullis"], "title": "Depth-Aware Scoring and Hierarchical Alignment for Multiple Object Tracking", "categories": ["cs.CV"], "comment": "ICIP 2025", "summary": "Current motion-based multiple object tracking (MOT) approaches rely heavily\non Intersection-over-Union (IoU) for object association. Without using 3D\nfeatures, they are ineffective in scenarios with occlusions or visually similar\nobjects. To address this, our paper presents a novel depth-aware framework for\nMOT. We estimate depth using a zero-shot approach and incorporate it as an\nindependent feature in the association process. Additionally, we introduce a\nHierarchical Alignment Score that refines IoU by integrating both coarse\nbounding box overlap and fine-grained (pixel-level) alignment to improve\nassociation accuracy without requiring additional learnable parameters. To our\nknowledge, this is the first MOT framework to incorporate 3D features\n(monocular depth) as an independent decision matrix in the association step.\nOur framework achieves state-of-the-art results on challenging benchmarks\nwithout any training nor fine-tuning. The code is available at\nhttps://github.com/Milad-Khanchi/DepthMOT", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00418", "pdf": "https://arxiv.org/pdf/2506.00418", "abs": "https://arxiv.org/abs/2506.00418", "authors": ["Siqi Liang", "Sumyeong Ahn", "Paramveer S. Dhillon", "Jiayu Zhou"], "title": "Dual Debiasing for Noisy In-Context Learning for Text Generation", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Accepted by 2025 ACL Findings", "summary": "In context learning (ICL) relies heavily on high quality demonstrations drawn\nfrom large annotated corpora. Existing approaches detect noisy annotations by\nranking local perplexities, presuming that noisy samples yield higher\nperplexities than their clean counterparts. However, this assumption breaks\ndown when the noise ratio is high and many demonstrations are flawed. We\nreexamine the perplexity based paradigm for text generation under noisy\nannotations, highlighting two sources of bias in perplexity: the annotation\nitself and the domain specific knowledge inherent in large language models\n(LLMs). To overcome these biases, we introduce a dual debiasing framework that\nuses synthesized neighbors to explicitly correct perplexity estimates, yielding\na robust Sample Cleanliness Score. This metric uncovers absolute sample\ncleanliness regardless of the overall corpus noise level. Extensive experiments\ndemonstrate our method's superior noise detection capabilities and show that\nits final ICL performance is comparable to that of a fully clean demonstration\ncorpus. Moreover, our approach remains robust even when noise ratios are\nextremely high.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00891", "pdf": "https://arxiv.org/pdf/2506.00891", "abs": "https://arxiv.org/abs/2506.00891", "authors": ["Sa Zhu", "Huashan Chen", "Wanqian Zhang", "Jinchao Zhang", "Zexian Yang", "Xiaoshuai Hao", "Bo Li"], "title": "Uneven Event Modeling for Partially Relevant Video Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICME 2025", "summary": "Given a text query, partially relevant video retrieval (PRVR) aims to\nretrieve untrimmed videos containing relevant moments, wherein event modeling\nis crucial for partitioning the video into smaller temporal events that\npartially correspond to the text. Previous methods typically segment videos\ninto a fixed number of equal-length clips, resulting in ambiguous event\nboundaries. Additionally, they rely on mean pooling to compute event\nrepresentations, inevitably introducing undesired misalignment. To address\nthese, we propose an Uneven Event Modeling (UEM) framework for PRVR. We first\nintroduce the Progressive-Grouped Video Segmentation (PGVS) module, to\niteratively formulate events in light of both temporal dependencies and\nsemantic similarity between consecutive frames, enabling clear event\nboundaries. Furthermore, we also propose the Context-Aware Event Refinement\n(CAER) module to refine the event representation conditioned the text's\ncross-attention. This enables event representations to focus on the most\nrelevant frames for a given text, facilitating more precise text-video\nalignment. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance on two PRVR benchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00908", "pdf": "https://arxiv.org/pdf/2506.00908", "abs": "https://arxiv.org/abs/2506.00908", "authors": ["Xianbing Sun", "Yan Hong", "Jiahui Zhan", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Liqing Zhang", "Jianfu Zhang"], "title": "DS-VTON: High-Quality Virtual Try-on via Disentangled Dual-Scale Generation", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent progress, most existing virtual try-on methods still struggle\nto simultaneously address two core challenges: accurately aligning the garment\nimage with the target human body, and preserving fine-grained garment textures\nand patterns. In this paper, we propose DS-VTON, a dual-scale virtual try-on\nframework that explicitly disentangles these objectives for more effective\nmodeling. DS-VTON consists of two stages: the first stage generates a\nlow-resolution try-on result to capture the semantic correspondence between\ngarment and body, where reduced detail facilitates robust structural alignment.\nThe second stage introduces a residual-guided diffusion process that\nreconstructs high-resolution outputs by refining the residual between the two\nscales, focusing on texture fidelity. In addition, our method adopts a fully\nmask-free generation paradigm, eliminating reliance on human parsing maps or\nsegmentation masks. By leveraging the semantic priors embedded in pretrained\ndiffusion models, this design more effectively preserves the person's\nappearance and geometric consistency. Extensive experiments demonstrate that\nDS-VTON achieves state-of-the-art performance in both structural alignment and\ntexture preservation across multiple standard virtual try-on benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00993", "pdf": "https://arxiv.org/pdf/2506.00993", "abs": "https://arxiv.org/abs/2506.00993", "authors": ["Yunzhu Zhang", "Yu Lu", "Tianyi Wang", "Fengyun Rao", "Yi Yang", "Linchao Zhu"], "title": "FlexSelect: Flexible Token Selection for Efficient Long Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Long-form video understanding poses a significant challenge for video large\nlanguage models (VideoLLMs) due to prohibitively high computational and memory\ndemands. In this paper, we propose FlexSelect, a flexible and efficient token\nselection strategy for processing long videos. FlexSelect identifies and\nretains the most semantically relevant content by leveraging cross-modal\nattention patterns from a reference transformer layer. It comprises two key\ncomponents: (1) a training-free token ranking pipeline that leverages faithful\ncross-modal attention weights to estimate each video token's importance, and\n(2) a rank-supervised lightweight selector that is trained to replicate these\nrankings and filter redundant tokens. This generic approach can be seamlessly\nintegrated into various VideoLLM architectures, such as LLaVA-Video, InternVL\nand Qwen-VL, serving as a plug-and-play module to extend their temporal context\nlength. Empirically, FlexSelect delivers strong gains across multiple\nlong-video benchmarks including VideoMME, MLVU, LongVB, and LVBench. Moreover,\nit achieves significant speed-ups (for example, up to 9 times on a\nLLaVA-Video-7B model), highlighting FlexSelect's promise for efficient\nlong-form video understanding. Project page available at:\nhttps://yunzhuzhang0918.github.io/flex_select", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01004", "pdf": "https://arxiv.org/pdf/2506.01004", "abs": "https://arxiv.org/abs/2506.01004", "authors": ["Tong Zhang", "Juan C Leon Alcazar", "Bernard Ghanem"], "title": "Motion-Aware Concept Alignment for Consistent Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a\ntraining-free framework bridging the gap between image-domain semantic mixing\nand video. Given a generated video and a user-provided reference image,\nMoCA-Video injects the semantic features of the reference image into a specific\nobject within the video, while preserving the original motion and visual\ncontext. Our approach leverages a diagonal denoising schedule and\nclass-agnostic segmentation to detect and track objects in the latent space and\nprecisely control the spatial location of the blended objects. To ensure\ntemporal coherence, we incorporate momentum-based semantic corrections and\ngamma residual noise stabilization for smooth frame transitions. We evaluate\nMoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS,\nand introduce a novel metric CASS (Conceptual Alignment Shift Score) to\nevaluate the consistency and effectiveness of the visual shifts between the\nsource prompt and the modified video frames. Using self-constructed dataset,\nMoCA-Video outperforms current baselines, achieving superior spatial\nconsistency, coherent motion, and a significantly higher CASS score, despite\nhaving no training or fine-tuning. MoCA-Video demonstrates that structured\nmanipulation in the diffusion noise trajectory allows for controllable,\nhigh-quality video synthesis.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01025", "pdf": "https://arxiv.org/pdf/2506.01025", "abs": "https://arxiv.org/abs/2506.01025", "authors": ["Xudong Ma", "Nantheera Anantrasirichai", "Stefanos Bolomytis", "Alin Achim"], "title": "Modality Translation and Registration of MR and Ultrasound Images Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal MR-US registration is critical for prostate cancer diagnosis.\nHowever, this task remains challenging due to significant modality\ndiscrepancies. Existing methods often fail to align critical boundaries while\nbeing overly sensitive to irrelevant details. To address this, we propose an\nanatomically coherent modality translation (ACMT) network based on a\nhierarchical feature disentanglement design. We leverage shallow-layer features\nfor texture consistency and deep-layer features for boundary preservation.\nUnlike conventional modality translation methods that convert one modality into\nanother, our ACMT introduces the customized design of an intermediate pseudo\nmodality. Both MR and US images are translated toward this intermediate domain,\neffectively addressing the bottlenecks faced by traditional translation methods\nin the downstream registration task. Experiments demonstrate that our method\nmitigates modality-specific discrepancies while preserving crucial anatomical\nboundaries for accurate registration. Quantitative evaluations show superior\nmodality similarity compared to state-of-the-art modality translation methods.\nFurthermore, downstream registration experiments confirm that our translated\nimages achieve the best alignment performance, highlighting the robustness of\nour framework for multi-modal prostate image registration.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01031", "pdf": "https://arxiv.org/pdf/2506.01031", "abs": "https://arxiv.org/abs/2506.01031", "authors": ["Yanyuan Qiao", "Haodong Hong", "Wenqi Lyu", "Dong An", "Siqi Zhang", "Yutong Xie", "Xinyu Wang", "Qi Wu"], "title": "NavBench: Probing Multimodal Large Language Models for Embodied Navigation", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated strong\ngeneralization in vision-language tasks, yet their ability to understand and\nact within embodied environments remains underexplored. We present NavBench, a\nbenchmark to evaluate the embodied navigation capabilities of MLLMs under\nzero-shot settings. NavBench consists of two components: (1) navigation\ncomprehension, assessed through three cognitively grounded tasks including\nglobal instruction alignment, temporal progress estimation, and local\nobservation-action reasoning, covering 3,200 question-answer pairs; and (2)\nstep-by-step execution in 432 episodes across 72 indoor scenes, stratified by\nspatial, cognitive, and execution complexity. To support real-world deployment,\nwe introduce a pipeline that converts MLLMs' outputs into robotic actions. We\nevaluate both proprietary and open-source models, finding that GPT-4o performs\nwell across tasks, while lighter open-source models succeed in simpler cases.\nResults also show that models with higher comprehension scores tend to achieve\nbetter execution performance. Providing map-based context improves decision\naccuracy, especially in medium-difficulty scenarios. However, most models\nstruggle with temporal understanding, particularly in estimating progress\nduring navigation, which may pose a key challenge.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00668", "pdf": "https://arxiv.org/pdf/2506.00668", "abs": "https://arxiv.org/abs/2506.00668", "authors": ["Martin Kuo", "Jianyi Zhang", "Aolin Ding", "Louis DiValentin", "Amin Hass", "Benjamin F Morris", "Isaac Jacobson", "Randolph Linderman", "James Kiessling", "Nicolas Ramos", "Bhavna Gopal", "Maziyar Baran Pouyan", "Changwei Liu", "Hai Li", "Yiran Chen"], "title": "SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues", "categories": ["cs.CL"], "comment": null, "summary": "Malicious attackers can exploit large language models (LLMs) by engaging them\nin multi-turn dialogues to achieve harmful objectives, posing significant\nsafety risks to society. To address this challenge, we propose a novel defense\nmechanism: SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues\n(STREAM). STREAM defends LLMs against multi-turn attacks while preserving their\nfunctional capabilities. Our approach involves constructing a human-annotated\ndataset, the Safety Reasoning Multi-turn Dialogues dataset, which is used to\nfine-tune a plug-and-play safety reasoning moderator. This model is designed to\nidentify malicious intent hidden within multi-turn conversations and alert the\ntarget LLM of potential risks. We evaluate STREAM across multiple LLMs against\nprevalent multi-turn attack strategies. Experimental results demonstrate that\nour method significantly outperforms existing defense techniques, reducing the\nAttack Success Rate (ASR) by 51.2%, all while maintaining comparable LLM\ncapability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01078", "pdf": "https://arxiv.org/pdf/2506.01078", "abs": "https://arxiv.org/abs/2506.01078", "authors": ["Yufei Zhan", "Ziheng Wu", "Yousong Zhu", "Rongkun Xue", "Ruipu Luo", "Zhenghao Chen", "Can Zhang", "Yifan Li", "Zhentao He", "Zheming Yang", "Ming Tang", "Minghui Qiu", "Jinqiao Wang"], "title": "GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking", "categories": ["cs.CV", "cs.AI"], "comment": "Tech report", "summary": "Despite notable advancements in multimodal reasoning, leading Multimodal\nLarge Language Models (MLLMs) still underperform on vision-centric multimodal\nreasoning tasks in general scenarios. This shortfall stems from their\npredominant reliance on logic- and knowledge-based slow thinking strategies,\nwhile effective for domains like math and science, fail to integrate visual\ninformation effectively during reasoning. Consequently, these models often fail\nto adequately ground visual cues, resulting in suboptimal performance in tasks\nthat require multiple plausible visual interpretations and inferences. To\naddress this, we present GThinker (General Thinker), a novel reasoning MLLM\nexcelling in multimodal reasoning across general scenarios, mathematics, and\nscience. GThinker introduces Cue-Rethinking, a flexible reasoning pattern that\ngrounds inferences in visual cues and iteratively reinterprets these cues to\nresolve inconsistencies. Building on this pattern, we further propose a\ntwo-stage training pipeline, including pattern-guided cold start and incentive\nreinforcement learning, designed to enable multimodal reasoning capabilities\nacross domains. Furthermore, to support the training, we construct\nGThinker-11K, comprising 7K high-quality, iteratively-annotated reasoning paths\nand 4K curated reinforcement learning samples, filling the data gap toward\ngeneral multimodal reasoning. Extensive experiments demonstrate that GThinker\nachieves 81.5% on the challenging comprehensive multimodal reasoning benchmark\nM$^3$CoT, surpassing the latest O4-mini model. It also shows an average\nimprovement of 2.1% on general scenario multimodal reasoning benchmarks, while\nmaintaining on-par performance in mathematical reasoning compared to\ncounterpart advanced reasoning models. The code, model, and data will be\nreleased soon at https://github.com/jefferyZhan/GThinker.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "mathematical reasoning"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00726", "pdf": "https://arxiv.org/pdf/2506.00726", "abs": "https://arxiv.org/abs/2506.00726", "authors": ["Hongye Zheng", "Yichen Wang", "Ray Pan", "Guiran Liu", "Binrong Zhu", "Hanlu Zhang"], "title": "Structured Gradient Guidance for Few-Shot Adaptation in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a gradient-informed fine-tuning method for large language\nmodels under few-shot conditions. The goal is to enhance task adaptability and\ntraining stability when data is limited. The method builds on a base loss\nfunction and introduces two gradient-related regularization terms. The first\nenforces gradient direction consistency to guide parameter updates along\ntask-relevant directions and prevent drift. The second controls gradient\nmagnitude to avoid abnormal updates. Together, these components support a more\nefficient and stable optimization path. To further improve cross-task\ngeneralization, the method incorporates a gradient alignment mechanism. This\nmechanism measures the consistency between optimization directions of the\nsource and target tasks. It enhances fine-tuning performance in multi-task and\ncross-domain scenarios. Across various natural language understanding tasks,\nthe method outperforms existing fine-tuning strategies in average accuracy,\ngradient stability, and directional alignment. Empirical evaluations under\ndifferent sample sizes and domain-specific tasks confirm the method's\nrobustness and broad applicability in low-resource environments. In particular,\nthe method shows clear advantages in controlling parameter update paths. The\nresults demonstrate that a gradient-based fine-tuning framework can effectively\nleverage the representational power of large language models. It ensures\ntraining stability while reducing dependence on large volumes of labeled data.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01102", "pdf": "https://arxiv.org/pdf/2506.01102", "abs": "https://arxiv.org/abs/2506.01102", "authors": ["Julia Lee Romero", "Kyle Min", "Subarna Tripathi", "Morteza Karimzadeh"], "title": "Keystep Recognition using Graph Neural Networks", "categories": ["cs.CV"], "comment": null, "summary": "We pose keystep recognition as a node classification task, and propose a\nflexible graph-learning framework for fine-grained keystep recognition that is\nable to effectively leverage long-term dependencies in egocentric videos. Our\napproach, termed GLEVR, consists of constructing a graph where each video clip\nof the egocentric video corresponds to a node. The constructed graphs are\nsparse and computationally efficient, outperforming existing larger models\nsubstantially. We further leverage alignment between egocentric and exocentric\nvideos during training for improved inference on egocentric videos, as well as\nadding automatic captioning as an additional modality. We consider each clip of\neach exocentric video (if available) or video captions as additional nodes\nduring training. We examine several strategies to define connections across\nthese nodes. We perform extensive experiments on the Ego-Exo4D dataset and show\nthat our proposed flexible graph-based framework notably outperforms existing\nmethods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01144", "pdf": "https://arxiv.org/pdf/2506.01144", "abs": "https://arxiv.org/abs/2506.01144", "authors": ["Ariel Shaulov", "Itay Hazan", "Lior Wolf", "Hila Chefer"], "title": "FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video diffusion models are notoriously limited in their ability to\nmodel temporal aspects such as motion, physics, and dynamic interactions.\nExisting approaches address this limitation by retraining the model or\nintroducing external conditioning signals to enforce temporal consistency. In\nthis work, we explore whether a meaningful temporal representation can be\nextracted directly from the predictions of a pre-trained model without any\nadditional training or auxiliary inputs. We introduce \\textbf{FlowMo}, a novel\ntraining-free guidance method that enhances motion coherence using only the\nmodel's own predictions in each diffusion step. FlowMo first derives an\nappearance-debiased temporal representation by measuring the distance between\nlatents corresponding to consecutive frames. This highlights the implicit\ntemporal structure predicted by the model. It then estimates motion coherence\nby measuring the patch-wise variance across the temporal dimension and guides\nthe model to reduce this variance dynamically during sampling. Extensive\nexperiments across multiple text-to-video models demonstrate that FlowMo\nsignificantly improves motion coherence without sacrificing visual quality or\nprompt alignment, offering an effective plug-and-play solution for enhancing\nthe temporal fidelity of pre-trained video diffusion models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "dimension"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01203", "pdf": "https://arxiv.org/pdf/2506.01203", "abs": "https://arxiv.org/abs/2506.01203", "authors": ["Muzammil Behzad"], "title": "Self-Supervised Multi-View Representation Learning using Vision-Language Model for 3D/4D Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition (FER) is a fundamental task in affective\ncomputing with applications in human-computer interaction, mental health\nanalysis, and behavioral understanding. In this paper, we propose SMILE-VLM, a\nself-supervised vision-language model for 3D/4D FER that unifies multiview\nvisual representation learning with natural language supervision. SMILE-VLM\nlearns robust, semantically aligned, and view-invariant embeddings by proposing\nthree core components: multiview decorrelation via a Barlow Twins-style loss,\nvision-language contrastive alignment, and cross-modal redundancy minimization.\nOur framework achieves the state-of-the-art performance on multiple benchmarks.\nWe further extend SMILE-VLM to the task of 4D micro-expression recognition\n(MER) to recognize the subtle affective cues. The extensive results demonstrate\nthat SMILE-VLM not only surpasses existing unsupervised methods but also\nmatches or exceeds supervised baselines, offering a scalable and\nannotation-efficient solution for expressive facial behavior understanding.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00815", "pdf": "https://arxiv.org/pdf/2506.00815", "abs": "https://arxiv.org/abs/2506.00815", "authors": ["Manoj Balaji Jagadeeshan", "Samarth Bhatia", "Pretam Ray", "Harshul Raj Surana", "Akhil Rajeev P", "Priya Mishra", "Annarao Kulkarni", "Ganesh Ramakrishnan", "Prathosh AP", "Pawan Goyal"], "title": "From Plain Text to Poetic Form: Generating Metrically-Constrained Sanskrit Verses", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly improved\nnatural language generation, including creative tasks like poetry composition.\nHowever, most progress remains concentrated in high-resource languages. This\nraises an important question: Can LLMs be adapted for structured poetic\ngeneration in a low-resource, morphologically rich language such as Sanskrit?\nIn this work, we introduce a dataset designed for translating English prose\ninto structured Sanskrit verse, with strict adherence to classical metrical\npatterns, particularly the Anushtub meter. We evaluate a range of generative\nmodels-both open-source and proprietary-under multiple settings. Specifically,\nwe explore constrained decoding strategies and instruction-based fine-tuning\ntailored to metrical and semantic fidelity. Our decoding approach achieves over\n99% accuracy in producing syntactically valid poetic forms, substantially\noutperforming general-purpose models in meter conformity. Meanwhile,\ninstruction-tuned variants show improved alignment with source meaning and\npoetic style, as supported by human assessments, albeit with marginal\ntrade-offs in metrical precision.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01247", "pdf": "https://arxiv.org/pdf/2506.01247", "abs": "https://arxiv.org/abs/2506.01247", "authors": ["Gerasimos Chatzoudis", "Zhuowei Li", "Gemma E. Moran", "Hao Wang", "Dimitris N. Metaxas"], "title": "Visual Sparse Steering: Improving Zero-shot Image Classification with Sparsity Guided Steering Vectors", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Steering vision foundation models at inference time without retraining or\naccess to large labeled datasets is a desirable yet challenging objective,\nparticularly in dynamic or resource-constrained settings. In this paper, we\nintroduce Visual Sparse Steering (VS2), a lightweight, test-time method that\nguides vision models using steering vectors derived from sparse features\nlearned by top-$k$ Sparse Autoencoders without requiring contrastive data.\nSpecifically, VS2 surpasses zero-shot CLIP by 4.12% on CIFAR-100, 1.08% on\nCUB-200, and 1.84% on Tiny-ImageNet. We further propose VS2++, a\nretrieval-augmented variant that selectively amplifies relevant sparse features\nusing pseudo-labeled neighbors at inference time. With oracle positive/negative\nsets, VS2++ achieves absolute top-1 gains over CLIP zero-shot of up to 21.44%\non CIFAR-100, 7.08% on CUB-200, and 20.47% on Tiny-ImageNet. Interestingly, VS2\nand VS2++ raise per-class accuracy by up to 25% and 38%, respectively, showing\nthat sparse steering benefits specific classes by disambiguating visually or\ntaxonomically proximate categories rather than providing a uniform boost.\nFinally, to better align the sparse features learned through the SAE\nreconstruction task with those relevant for downstream performance, we propose\nPrototype-Aligned Sparse Steering (PASS). By incorporating a\nprototype-alignment loss during SAE training, using labels only during training\nwhile remaining fully test-time unsupervised, PASS consistently, though\nmodestly, outperforms VS2, achieving a 6.12% gain over VS2 only on CIFAR-100\nwith ViT-B/32.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference time"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01274", "pdf": "https://arxiv.org/pdf/2506.01274", "abs": "https://arxiv.org/abs/2506.01274", "authors": ["Hosu Lee", "Junho Kim", "Hyunjun Kim", "Yong Man Ro"], "title": "ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent progress in Large Multi-modal Models (LMMs) has enabled effective\nvision-language reasoning, yet the ability to understand video content remains\nconstrained by suboptimal frame selection strategies. Existing approaches often\nrely on static heuristics or external retrieval modules to feed frame\ninformation into video-LLMs, which may fail to provide the query-relevant\ninformation. In this work, we introduce ReFoCUS (Reinforcement-guided Frame\nOptimization for Contextual UnderStanding), a novel frame-level policy\noptimization framework that shifts the optimization target from textual\nresponses to visual input selection. ReFoCUS learns a frame selection policy\nvia reinforcement learning, using reward signals derived from a reference LMM\nto reflect the model's intrinsic preferences for frames that best support\ntemporally grounded responses. To efficiently explore the large combinatorial\nframe space, we employ an autoregressive, conditional selection architecture\nthat ensures temporal coherence while reducing complexity. Our approach does\nnot require explicit supervision at the frame-level and consistently improves\nreasoning performance across multiple video QA benchmarks, highlighting the\nbenefits of aligning frame selection with model-internal utility.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00854", "pdf": "https://arxiv.org/pdf/2506.00854", "abs": "https://arxiv.org/abs/2506.00854", "authors": ["Jacky Tai-Yu Lu", "Jung Chiang", "Chi-Sheng Chen", "Anna Nai-Yun Tung", "Hsiang Wei Hu", "Yuan Chiao Cheng"], "title": "EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "q-bio.NC"], "comment": null, "summary": "We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one\nof the earliest open-vocabulary EEG-to-text generation frameworks tailored for\nChinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact\npretrained language model (MiniLM), our architecture aligns multichannel brain\nsignals with natural language representations via masked pretraining and\ncontrastive learning. Using a subset of the ChineseEEG dataset, where each\nsentence contains approximately ten Chinese characters aligned with 128-channel\nEEG recorded at 256 Hz, we segment EEG into per-character embeddings and\npredict full sentences in a zero-shot setting. The decoder is trained with\nteacher forcing and padding masks to accommodate variable-length sequences.\nEvaluation on over 1,500 training-validation sentences and 300 held-out test\nsamples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%.\nWhile syntactic fluency remains a challenge, our findings demonstrate the\nfeasibility of non-phonetic, cross-modal language decoding from EEG. This work\nopens a new direction in multilingual brain-to-text research and lays the\nfoundation for future cognitive-language interfaces in Chinese.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01366", "pdf": "https://arxiv.org/pdf/2506.01366", "abs": "https://arxiv.org/abs/2506.01366", "authors": ["Cong Guan", "Osamu Yoshie"], "title": "CLIP-driven rain perception: Adaptive deraining with pattern-aware network routing and mask-guided cross-attention", "categories": ["cs.CV"], "comment": null, "summary": "Existing deraining models process all rainy images within a single network.\nHowever, different rain patterns have significant variations, which makes it\nchallenging for a single network to handle diverse types of raindrops and\nstreaks. To address this limitation, we propose a novel CLIP-driven rain\nperception network (CLIP-RPN) that leverages CLIP to automatically perceive\nrain patterns by computing visual-language matching scores and adaptively\nrouting to sub-networks to handle different rain patterns, such as varying\nraindrop densities, streak orientations, and rainfall intensity. CLIP-RPN\nestablishes semantic-aware rain pattern recognition through CLIP's cross-modal\nvisual-language alignment capabilities, enabling automatic identification of\nprecipitation characteristics across different rain scenarios. This rain\npattern awareness drives an adaptive subnetwork routing mechanism where\nspecialized processing branches are dynamically activated based on the detected\nrain type, significantly enhancing the model's capacity to handle diverse\nrainfall conditions. Furthermore, within sub-networks of CLIP-RPN, we introduce\na mask-guided cross-attention mechanism (MGCA) that predicts precise rain masks\nat multi-scale to facilitate contextual interactions between rainy regions and\nclean background areas by cross-attention. We also introduces a dynamic loss\nscheduling mechanism (DLS) to adaptively adjust the gradients for the\noptimization process of CLIP-RPN. Compared with the commonly used $l_1$ or\n$l_2$ loss, DLS is more compatible with the inherent dynamics of the network\ntraining process, thus achieving enhanced outcomes. Our method achieves\nstate-of-the-art performance across multiple datasets, particularly excelling\nin complex mixed datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00875", "pdf": "https://arxiv.org/pdf/2506.00875", "abs": "https://arxiv.org/abs/2506.00875", "authors": ["Yangfan Ye", "Xiaocheng Feng", "Zekun Yuan", "Xiachong Feng", "Libo Qin", "Lei Huang", "Weitao Ma", "Yichong Huang", "Zhirui Zhang", "Yunfei Lu", "Xiaohui Yan", "Duyu Tang", "Dandan Tu", "Bing Qin"], "title": "CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint Multilingual Supervised Fine-Tuning", "categories": ["cs.CL"], "comment": "ACL2025 main conference, long paper", "summary": "Current large language models (LLMs) often exhibit imbalanced multilingual\ncapabilities due to their English-centric training corpora. To address this,\nexisting fine-tuning approaches operating at the data-level (e.g., through data\naugmentation or distillation) typically introduce implicit cross-lingual\nalignment, overlooking the potential for more profound, latent-level\ncross-lingual interactions. In this work, we propose CC-Tuning, a novel\nmultilingual fine-tuning paradigm that explicitly establishes a cross-lingual\nconnection mechanism at the latent level. During training, CC-Tuning fuses the\nfeed forward activations from both English and non-English inputs, enabling the\nmodel to benefit from both linguistic resources. This process is facilitated\nwith a trainable Decision Maker that identifies beneficial activations.\nFurthermore, during inference, a Transform Matrix is utilized to simulate the\ncross-lingual connection under monolingual setting through representation\ntransformation. Our experiments on six benchmarks covering 22 languages show\nthat CC-Tuning outperforms vanilla SFT and offers a strong latent-level\nalternative to data-level augmentation methods. Further analysis also\nhighlights the practicality of CC-Tuning and the potential of latent-level\ncross-lingual interactions in advancing the multilingual performance of LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01371", "pdf": "https://arxiv.org/pdf/2506.01371", "abs": "https://arxiv.org/abs/2506.01371", "authors": ["Peiyao Wang", "Haibin Ling"], "title": "SVQA-R1: Reinforcing Spatial Reasoning in MLLMs via View-Consistent Reward Optimization", "categories": ["cs.CV"], "comment": "9 pages, 7 figures", "summary": "Spatial reasoning remains a critical yet underdeveloped capability in\nexisting vision-language models (VLMs), especially for Spatial Visual Question\nAnswering (Spatial VQA) tasks that require understanding relative positions,\ndistances, and object configurations. Inspired by the R1 paradigm introduced in\nDeepSeek-R1, which enhances reasoning in language models through rule-based\nreinforcement learning (RL), we propose SVQA-R1, the first framework to extend\nR1-style training to spatial VQA. In particular, we introduce Spatial-GRPO, a\nnovel group-wise RL strategy that constructs view-consistent rewards by\nperturbing spatial relations between objects, e.g., mirror flipping, thereby\nencouraging the model to develop a consistent and grounded understanding of\nspace. Our model, SVQA-R1, not only achieves dramatically improved accuracy on\nspatial VQA benchmarks but also exhibits interpretable reasoning paths even\nwithout using supervised fine-tuning (SFT) data. Extensive experiments and\nvisualization demonstrate the effectiveness of SVQA-R1 across multiple spatial\nreasoning benchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00942", "pdf": "https://arxiv.org/pdf/2506.00942", "abs": "https://arxiv.org/abs/2506.00942", "authors": ["Haitao Li", "Ziyu Li", "Yiheng Mao", "Ziyi Liu", "Zhoujian Sun", "Zhengxing Huang"], "title": "anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding", "categories": ["cs.CL", "cs.AI", "eess.SP"], "comment": null, "summary": "The advent of multimodal large language models (MLLMs) has sparked interest\nin their application to electrocardiogram (ECG) analysis. However, existing\nECG-focused MLLMs primarily focus on report generation tasks, often limited to\nsingle 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the\npotential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that\nsupports a broader range of tasks and more flexible ECG inputs. However,\nexisting ECG-QA datasets are often monotonous. To address this gap, we first\nconstructed the anyECG dataset, which encompasses a wide variety of tasks,\nincluding report generation, abnormal waveform localization, and open-ended\nquestion answering. In addition to standard hospital ECGs, we introduced\nlong-duration reduced-lead ECGs for home environments and multiple ECG\ncomparison scenarios commonly encountered in clinical practice. Furthermore, we\npropose the anyECG-chat model, which supports dynamic-length ECG inputs and\nmultiple ECG inputs. We trained the model using a three-stage curriculum\ntraining recipe with the anyECG dataset. A comprehensive evaluation was\nconducted, demonstrating that anyECG-chat is capable of supporting various\npractical application scenarios, including not only common report generation\ntasks but also abnormal waveform localization for long-duration reduced-lead\nECGs in home environments and comprehensive comparative analysis of multiple\nECGs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "question answering"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01411", "pdf": "https://arxiv.org/pdf/2506.01411", "abs": "https://arxiv.org/abs/2506.01411", "authors": ["Minjeong Park", "Hongbeen Park", "Jinkyu Kim"], "title": "ViTA-PAR: Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to IEEE ICIP 2025", "summary": "The Pedestrian Attribute Recognition (PAR) task aims to identify various\ndetailed attributes of an individual, such as clothing, accessories, and\ngender. To enhance PAR performance, a model must capture features ranging from\ncoarse-grained global attributes (e.g., for identifying gender) to fine-grained\nlocal details (e.g., for recognizing accessories) that may appear in diverse\nregions. Recent research suggests that body part representation can enhance the\nmodel's robustness and accuracy, but these methods are often restricted to\nattribute classes within fixed horizontal regions, leading to degraded\nperformance when attributes appear in varying or unexpected body locations. In\nthis paper, we propose Visual and Textual Attribute Alignment with Attribute\nPrompting for Pedestrian Attribute Recognition, dubbed as ViTA-PAR, to enhance\nattribute recognition through specialized multimodal prompting and\nvision-language alignment. We introduce visual attribute prompts that capture\nglobal-to-local semantics, enabling diverse attribute representations. To\nenrich textual embeddings, we design a learnable prompt template, termed person\nand attribute context prompting, to learn person and attributes context.\nFinally, we align visual and textual attribute features for effective fusion.\nViTA-PAR is validated on four PAR benchmarks, achieving competitive performance\nwith efficient inference. We release our code and model at\nhttps://github.com/mlnjeongpark/ViTA-PAR.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01413", "pdf": "https://arxiv.org/pdf/2506.01413", "abs": "https://arxiv.org/abs/2506.01413", "authors": ["Yulei Qin", "Gang Li", "Zongyi Li", "Zihan Xu", "Yuchen Shi", "Zhekai Lin", "Xiao Cui", "Ke Li", "Xing Sun"], "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages of main body, 3 tables, 5 figures, 40 pages of appendix", "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nare available at https://github.com/yuleiqin/RAIF.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "compute scaling", "test-time compute"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01430", "pdf": "https://arxiv.org/pdf/2506.01430", "abs": "https://arxiv.org/abs/2506.01430", "authors": ["Chenxi Xie", "Minghan Li", "Shuai Li", "Yuhui Wu", "Qiaosi Yi", "Lei Zhang"], "title": "DNAEdit: Direct Noise Alignment for Text-Guided Rectified Flow Editing", "categories": ["cs.CV"], "comment": "Project URL: https://xiechenxi99.github.io/DNAEdit", "summary": "Leveraging the powerful generation capability of large-scale pretrained\ntext-to-image models, training-free methods have demonstrated impressive image\nediting results. Conventional diffusion-based methods, as well as recent\nrectified flow (RF)-based methods, typically reverse synthesis trajectories by\ngradually adding noise to clean images, during which the noisy latent at the\ncurrent timestep is used to approximate that at the next timesteps, introducing\naccumulated drift and degrading reconstruction accuracy. Considering the fact\nthat in RF the noisy latent is estimated through direct interpolation between\nGaussian noises and clean images at each timestep, we propose Direct Noise\nAlignment (DNA), which directly refines the desired Gaussian noise in the noise\ndomain, significantly reducing the error accumulation in previous methods.\nSpecifically, DNA estimates the velocity field of the interpolated noised\nlatent at each timestep and adjusts the Gaussian noise by computing the\ndifference between the predicted and expected velocity field. We validate the\neffectiveness of DNA and reveal its relationship with existing RF-based\ninversion methods. Additionally, we introduce a Mobile Velocity Guidance (MVG)\nto control the target prompt-guided generation process, balancing image\nbackground preservation and target object editability. DNA and MVG collectively\nconstitute our proposed method, namely DNAEdit. Finally, we introduce\nDNA-Bench, a long-prompt benchmark, to evaluate the performance of advanced\nimage editing models. Experimental results demonstrate that our DNAEdit\nachieves superior performance to state-of-the-art text-guided editing methods.\nCodes and benchmark will be available at \\href{\nhttps://xiechenxi99.github.io/DNAEdit/}{https://xiechenxi99.github.io/DNAEdit/}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01480", "pdf": "https://arxiv.org/pdf/2506.01480", "abs": "https://arxiv.org/abs/2506.01480", "authors": ["Kaihang Pan", "Yang Wu", "Wendong Bu", "Kai Shen", "Juncheng Li", "Yingting Wang", "Yunfei Li", "Siliang Tang", "Jun Xiao", "Fei Wu", "Hang Zhao", "Yueting Zhuang"], "title": "Unlocking Aha Moments via Reinforcement Learning: Advancing Collaborative Visual Comprehension and Generation", "categories": ["cs.CV"], "comment": "21 pages, 7 figures", "summary": "Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify\nvisual comprehension and generation. However, these two capabilities remain\nlargely independent, as if they are two separate functions encapsulated within\nthe same model. Consequently, visual comprehension does not enhance visual\ngeneration, and the reasoning mechanisms of LLMs have not been fully integrated\nto revolutionize image generation. In this paper, we propose to enable the\ncollaborative co-evolution of visual comprehension and generation, advancing\nimage generation into an iterative introspective process. We introduce a\ntwo-stage training approach: supervised fine-tuning teaches the MLLM with the\nfoundational ability to generate genuine CoT for visual generation, while\nreinforcement learning activates its full potential via an\nexploration-exploitation trade-off. Ultimately, we unlock the Aha moment in\nvisual generation, advancing MLLMs from text-to-image tasks to unified image\ngeneration. Extensive experiments demonstrate that our model not only excels in\ntext-to-image generation and image editing, but also functions as a superior\nimage semantic evaluator with enhanced visual comprehension capabilities.\nProject Page: https://janus-pro-r1.github.io.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01539", "pdf": "https://arxiv.org/pdf/2506.01539", "abs": "https://arxiv.org/abs/2506.01539", "authors": ["Tianjiao Zhang", "Fei Zhang", "Jiangchao Yao", "Ya Zhang", "Yanfeng Wang"], "title": "G4Seg: Generation for Inexact Segmentation Refinement with Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 12 figures, IEEE International Conference on Multimedia &\n  Expo 2025", "summary": "This paper considers the problem of utilizing a large-scale text-to-image\ndiffusion model to tackle the challenging Inexact Segmentation (IS) task.\nUnlike traditional approaches that rely heavily on discriminative-model-based\nparadigms or dense visual representations derived from internal attention\nmechanisms, our method focuses on the intrinsic generative priors in Stable\nDiffusion~(SD). Specifically, we exploit the pattern discrepancies between\noriginal images and mask-conditional generated images to facilitate a\ncoarse-to-fine segmentation refinement by establishing a semantic\ncorrespondence alignment and updating the foreground probability. Comprehensive\nquantitative and qualitative experiments validate the effectiveness and\nsuperiority of our plug-and-play design, underscoring the potential of\nleveraging generation discrepancies to model dense representations and\nencouraging further exploration of generative approaches for solving\ndiscriminative tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01206", "pdf": "https://arxiv.org/pdf/2506.01206", "abs": "https://arxiv.org/abs/2506.01206", "authors": ["Daewon Choi", "Seunghyuk Oh", "Saket Dingliwal", "Jihoon Tack", "Kyuyoung Kim", "Woomin Song", "Seojin Kim", "Insu Han", "Jinwoo Shin", "Aram Galstyan", "Shubham Katiyar", "Sravan Babu Bodapati"], "title": "Mamba Drafters for Speculative Decoding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding has emerged as a promising approach to accelerating\nlarge language model (LLM) generation using a fast drafter while maintaining\nalignment with the target model's distribution. However, existing approaches\nface a trade-off: external drafters offer flexibility but can suffer from\nslower drafting, while self-speculation methods use drafters tailored to the\ntarget model but require re-training. In this paper, we introduce novel\ndrafters based on Mamba, a state-of-the-art state space model (SSM), as a\nsolution that combines the best aspects of both approaches. By leveraging the\nlinear structure of SSMs, our approach avoids the quadratic complexity inherent\nin traditional Transformer-based methods, enabling faster drafting and lower\nmemory usage while maintaining the flexibility to work across different target\nmodels. We further enhance efficiency with a novel test-time tree search\nalgorithm for generating high-quality draft candidates. Our empirical\nevaluation demonstrates that Mamba-based drafters not only outperform existing\nexternal drafting methods but are also comparable to state-of-the-art\nself-speculation approaches while using less memory and maintaining their\ncross-model adaptability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01674", "pdf": "https://arxiv.org/pdf/2506.01674", "abs": "https://arxiv.org/abs/2506.01674", "authors": ["Yipeng Du", "Tiehan Fan", "Kepan Nan", "Rui Xie", "Penghao Zhou", "Xiang Li", "Jian Yang", "Zhenheng Yang", "Ying Tai"], "title": "MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Despite advancements in Multimodal Large Language Models (MLLMs), their\nproficiency in fine-grained video motion understanding remains critically\nlimited. They often lack inter-frame differencing and tend to average or ignore\nsubtle visual cues. Furthermore, while visual prompting has shown potential in\nstatic images, its application to video's temporal complexities, particularly\nfor fine-grained motion understanding, remains largely unexplored. We\ninvestigate whether inherent capability can be unlocked and boost MLLMs' motion\nperception and enable distinct visual signatures tailored to decouple object\nand camera motion cues. In this study, we introduce MotionSight, a novel\nzero-shot method pioneering object-centric visual spotlight and motion blur as\nvisual prompts to effectively improve fine-grained motion understanding without\ntraining. To convert this into valuable data assets, we curated MotionVid-QA,\nthe first large-scale dataset for fine-grained video motion understanding, with\nhierarchical annotations including SFT and preference data, {\\Theta}(40K) video\nclips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves\nstate-of-the-art open-source performance and competitiveness with commercial\nmodels. In particular, for fine-grained motion understanding we present a novel\nzero-shot technique and a large-scale, high-quality dataset. All the code and\nannotations will be publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01241", "pdf": "https://arxiv.org/pdf/2506.01241", "abs": "https://arxiv.org/abs/2506.01241", "authors": ["Jie Ruan", "Inderjeet Nair", "Shuyang Cao", "Amy Liu", "Sheza Munir", "Micah Pollens-Dempsey", "Tiffany Chiang", "Lucy Kates", "Nicholas David", "Sihan Chen", "Ruxin Yang", "Yuqian Yang", "Jasmine Gump", "Tessa Bialek", "Vivek Sankaran", "Margo Schlanger", "Lu Wang"], "title": "ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces ExpertLongBench, an expert-level benchmark containing\n11 tasks from 9 domains that reflect realistic expert workflows and\napplications. Beyond question answering, the application-driven tasks in\nExpertLongBench demand long-form outputs that can exceed 5,000 tokens and\nstrict adherence to domain-specific requirements. Notably, each task in\nExpertLongBench includes a rubric, designed or validated by domain experts, to\nspecify task requirements and guide output evaluation. Furthermore, we propose\nCLEAR, an evaluation framework that supports accurate evaluation of long-form\nmodel outputs in our benchmark. To achieve fine-grained, expert-aligned\nevaluation, CLEAR derives checklists from both model outputs and references by\nextracting information corresponding to items in the task-specific rubric.\nChecklist items for model outputs are then compared with corresponding items\nfor reference outputs to assess their correctness, enabling grounded\nevaluation. We benchmark 11 large language models (LLMs) and analyze components\nin CLEAR, showing that (1) existing LLMs, with the top performer achieving only\na 26.8% F1 score, require significant improvement for expert-level tasks; (2)\nmodels can generate content corresponding to the required aspects, though often\nnot accurately; and (3) accurate checklist extraction and comparison in CLEAR\ncan be achieved by open-weight models for more scalable and low-cost usage.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "question answering", "fine-grained", "rubric"], "score": 5}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01701", "pdf": "https://arxiv.org/pdf/2506.01701", "abs": "https://arxiv.org/abs/2506.01701", "authors": ["Haoru Tan", "Sitong Wu", "Wei Huang", "Shizhen Zhao", "Xiaojuan Qi"], "title": "Data Pruning by Information Maximization", "categories": ["cs.CV", "cs.AI"], "comment": "ICLR 2025", "summary": "In this paper, we present InfoMax, a novel data pruning method, also known as\ncoreset selection, designed to maximize the information content of selected\nsamples while minimizing redundancy. By doing so, InfoMax enhances the overall\ninformativeness of the coreset. The information of individual samples is\nmeasured by importance scores, which capture their influence or difficulty in\nmodel learning. To quantify redundancy, we use pairwise sample similarities,\nbased on the premise that similar samples contribute similarly to the learning\nprocess. We formalize the coreset selection problem as a discrete quadratic\nprogramming (DQP) task, with the objective of maximizing the total information\ncontent, represented as the sum of individual sample contributions minus the\nredundancies introduced by similar samples within the coreset. To ensure\npractical scalability, we introduce an efficient gradient-based solver,\ncomplemented by sparsification techniques applied to the similarity matrix and\ndataset partitioning strategies. This enables InfoMax to seamlessly scale to\ndatasets with millions of samples. Extensive experiments demonstrate the\nsuperior performance of InfoMax in various data pruning tasks, including image\nclassification, vision-language pre-training, and instruction tuning for large\nlanguage models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01253", "pdf": "https://arxiv.org/pdf/2506.01253", "abs": "https://arxiv.org/abs/2506.01253", "authors": ["Sai Vallurupalli", "Francis Ferraro"], "title": "CoRE: Condition-based Reasoning for Identifying Outcome Variance in Complex Events", "categories": ["cs.CL"], "comment": "Accepted to Findings of the Association for Computational Linguistics\n  2025", "summary": "Knowing which latent conditions lead to a particular outcome is useful for\ncritically examining claims made about complex event outcomes. Identifying\nimplied conditions and examining their influence on an outcome is challenging.\nWe handle this by combining and augmenting annotations from two existing\ndatasets consisting of goals and states, and explore the influence of\nconditions through our research questions and Condition-based Reasoning tasks.\nWe examine open and closed LLMs of varying sizes and intent-alignment on our\nreasoning tasks and find that conditions are useful when not all context is\navailable. Models differ widely in their ability to generate and identify\noutcome-variant conditions which affects their performance on outcome\nvalidation when conditions are used to replace missing context. Larger models\nlike GPT-4o, are more cautious in such less constrained situations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01725", "pdf": "https://arxiv.org/pdf/2506.01725", "abs": "https://arxiv.org/abs/2506.01725", "authors": ["Desen Meng", "Rui Huang", "Zhilin Dai", "Xinhao Li", "Yifan Xu", "Jun Zhang", "Zhenpeng Huang", "Meng Zhang", "Lingshu Zhang", "Yi Liu", "Limin Wang"], "title": "VideoCap-R1: Enhancing MLLMs for Video Captioning via Structured Thinking", "categories": ["cs.CV"], "comment": null, "summary": "While recent advances in reinforcement learning have significantly enhanced\nreasoning capabilities in large language models (LLMs), these techniques remain\nunderexplored in multi-modal LLMs for video captioning. This paper presents the\nfirst systematic investigation of GRPO-based RL post-training for video MLLMs,\nwith the goal of enhancing video MLLMs' capability of describing actions in\nvideos. Specifically, we develop the VideoCap-R1, which is prompted to first\nperform structured thinking that analyzes video subjects with their attributes\nand actions before generating complete captions, supported by two specialized\nreward mechanisms: a LLM-free think scorer evaluating the structured thinking\nquality and a LLM-assisted caption scorer assessing the output quality. The RL\ntraining framework effectively establishes the connection between structured\nreasoning and comprehensive description generation, enabling the model to\nproduce captions with more accurate actions. Our experiments demonstrate that\nVideoCap-R1 achieves substantial improvements over the Qwen2VL-7B baseline\nusing limited samples (1.5k) across multiple video caption benchmarks (DREAM1K:\n+4.4 event F1, VDC: +4.2 Acc, CAREBENCH: +3.1 action F1, +6.9 object F1) while\nconsistently outperforming the SFT-trained counterparts, confirming GRPO's\nsuperiority in enhancing MLLMs' captioning capabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01783", "pdf": "https://arxiv.org/pdf/2506.01783", "abs": "https://arxiv.org/abs/2506.01783", "authors": ["Honglu Zhang", "Zhiqin Fang", "Ningning Zhao", "Saihui Hou", "Long Ma", "Renwang Pei", "Zhaofeng He"], "title": "FaceCoT: A Benchmark Dataset for Face Anti-Spoofing with Chain-of-Thought Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Face Anti-Spoofing (FAS) typically depends on a single visual modality when\ndefending against presentation attacks such as print attacks, screen replays,\nand 3D masks, resulting in limited generalization across devices, environments,\nand attack types. Meanwhile, Multimodal Large Language Models (MLLMs) have\nrecently achieved breakthroughs in image-text understanding and semantic\nreasoning, suggesting that integrating visual and linguistic co-inference into\nFAS can substantially improve both robustness and interpretability. However,\nthe lack of a high-quality vision-language multimodal dataset has been a\ncritical bottleneck. To address this, we introduce FaceCoT (Face\nChain-of-Thought), the first large-scale Visual Question Answering (VQA)\ndataset tailored for FAS. FaceCoT covers 14 spoofing attack types and enriches\nmodel learning with high-quality CoT VQA annotations. Meanwhile, we develop a\ncaption model refined via reinforcement learning to expand the dataset and\nenhance annotation quality. Furthermore, we introduce a CoT-Enhanced\nProgressive Learning (CEPL) strategy to better leverage the CoT data and boost\nmodel performance on FAS tasks. Extensive experiments demonstrate that models\ntrained with FaceCoT and CEPL outperform state-of-the-art methods on multiple\nbenchmark datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "question answering"], "score": 4}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01266", "pdf": "https://arxiv.org/pdf/2506.01266", "abs": "https://arxiv.org/abs/2506.01266", "authors": ["Yuanhe Tian", "Mingjie Deng", "Guoqing Jin", "Yan Song"], "title": "Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 1 figure", "summary": "Existing approaches for Large language model (LLM) detoxification generally\nrely on training on large-scale non-toxic or human-annotated preference data,\ndesigning prompts to instruct the LLM to generate safe content, or modifying\nthe model parameters to remove toxic information, which are computationally\nexpensive, lack robustness, and often compromise LLMs' fluency and contextual\nunderstanding. In this paper, we propose a simple yet effective approach for\nLLM detoxification, which leverages a compact, pre-trained calibration model\nthat guides the detoxification process of a target LLM via a lightweight\nintervention in its generation pipeline. By learning a detoxified embedding\nspace from non-toxic data, the calibration model effectively steers the LLM\naway from generating harmful content. This approach only requires a one-time\ntraining of the calibration model that is able to be seamlessly applied to\nmultiple LLMs without compromising fluency or contextual understanding.\nExperiment results on the benchmark dataset demonstrate that our approach\nreduces toxicity while maintaining reasonable content expression.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01802", "pdf": "https://arxiv.org/pdf/2506.01802", "abs": "https://arxiv.org/abs/2506.01802", "authors": ["Heming Zhu", "Guoxing Sun", "Christian Theobalt", "Marc Habermann"], "title": "UMA: Ultra-detailed Human Avatars via Multi-level Surface Alignment", "categories": ["cs.CV"], "comment": "For video results, see https://youtu.be/XMNCy7J2tuc", "summary": "Learning an animatable and clothed human avatar model with vivid dynamics and\nphotorealistic appearance from multi-view videos is an important foundational\nresearch problem in computer graphics and vision. Fueled by recent advances in\nimplicit representations, the quality of the animatable avatars has achieved an\nunprecedented level by attaching the implicit representation to drivable human\ntemplate meshes. However, they usually fail to preserve the highest level of\ndetail, particularly apparent when the virtual camera is zoomed in and when\nrendering at 4K resolution and higher. We argue that this limitation stems from\ninaccurate surface tracking, specifically, depth misalignment and surface drift\nbetween character geometry and the ground truth surface, which forces the\ndetailed appearance model to compensate for geometric errors. To address this,\nwe propose a latent deformation model and supervising the 3D deformation of the\nanimatable character using guidance from foundational 2D video point trackers,\nwhich offer improved robustness to shading and surface variations, and are less\nprone to local minima than differentiable rendering. To mitigate the drift over\ntime and lack of 3D awareness of 2D point trackers, we introduce a cascaded\ntraining strategy that generates consistent 3D point tracks by anchoring point\ntracks to the rendered avatar, which ultimately supervises our avatar at the\nvertex and texel level. To validate the effectiveness of our approach, we\nintroduce a novel dataset comprising five multi-view video sequences, each over\n10 minutes in duration, captured using 40 calibrated 6K-resolution cameras,\nfeaturing subjects dressed in clothing with challenging texture patterns and\nwrinkle deformations. Our approach demonstrates significantly improved\nperformance in rendering quality and geometric accuracy over the prior state of\nthe art.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01312", "pdf": "https://arxiv.org/pdf/2506.01312", "abs": "https://arxiv.org/abs/2506.01312", "authors": ["Chunhui Zhang", "Sirui", "Wang", "Zhongyu Ouyang", "Xiangchi Yuan", "Soroush Vosoughi"], "title": "Growing Through Experience: Scaling Episodic Grounding in Language Models", "categories": ["cs.CL"], "comment": "Accepted at The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Language models (LMs) require robust episodic grounding-the capacity to learn\nfrom and apply past experiences-to excel at physical planning tasks. Current\nepisodic grounding approaches struggle with scalability and integration,\nlimiting their effectiveness, especially for medium-sized LMs (7B parameters).\nWhile larger LMs (70-405B parameters) possess superior hierarchical\nrepresentations and extensive pre-trained knowledge, they encounter a\nfundamental scale paradox: despite their advanced abstraction capabilities,\nthey lack efficient mechanisms to leverage experience streams. We propose a\nscalable weak-to-strong episodic learning framework that effectively transfers\nepisodic behaviors from smaller to larger LMs. This framework integrates Monte\nCarlo tree search for structured experience collection with a novel\ndistillation method, preserving the inherent LM capabilities while embedding\nepisodic memory. Experiments demonstrate our method surpasses state-of-the-art\nproprietary LMs by 3.45% across diverse planning and question-answering tasks.\nLayer-wise probing further indicates significant improvements in task\nalignment, especially within deeper LM layers, highlighting stable\ngeneralization even for previously unseen scenarios with increased planning\ncomplexity-conditions where baseline methods degrade markedly.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01806", "pdf": "https://arxiv.org/pdf/2506.01806", "abs": "https://arxiv.org/abs/2506.01806", "authors": ["Shubham Pandey", "Bhavin Jawade", "Srirangaraj Setlur"], "title": "Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained Cross-Domain Fingerprint Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to IEEE International Conference on Image Processing 2025", "summary": "The increasing demand for hygienic and portable biometric systems has\nunderscored the critical need for advancements in contactless fingerprint\nrecognition. Despite its potential, this technology faces notable challenges,\nincluding out-of-focus image acquisition, reduced contrast between fingerprint\nridges and valleys, variations in finger positioning, and perspective\ndistortion. These factors significantly hinder the accuracy and reliability of\ncontactless fingerprint matching. To address these issues, we propose a novel\nmulti-stage transformer-based contactless fingerprint matching approach that\nfirst captures global spatial features and subsequently refines localized\nfeature alignment across fingerprint samples. By employing a hierarchical\nfeature extraction and matching pipeline, our method ensures fine-grained,\ncross-sample alignment while maintaining the robustness of global feature\nrepresentation. We perform extensive evaluations on publicly available datasets\nsuch as HKPolyU and RidgeBase under different evaluation protocols, such as\ncontactless-to-contact matching and contactless-to-contactless matching and\ndemonstrate that our proposed approach outperforms existing methods, including\nCOTS solutions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01921", "pdf": "https://arxiv.org/pdf/2506.01921", "abs": "https://arxiv.org/abs/2506.01921", "authors": ["Minghao Liu", "Zhitao He", "Zhiyuan Fan", "Qingyun Wang", "Yi R. Fung"], "title": "MedEBench: Revisiting Text-instructed Image Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-guided image editing has seen rapid progress in natural image domains,\nbut its adaptation to medical imaging remains limited and lacks standardized\nevaluation. Clinically, such editing holds promise for simulating surgical\noutcomes, creating personalized teaching materials, and enhancing patient\ncommunication. To bridge this gap, we introduce \\textbf{MedEBench}, a\ncomprehensive benchmark for evaluating text-guided medical image editing. It\nconsists of 1,182 clinically sourced image-prompt triplets spanning 70 tasks\nacross 13 anatomical regions. MedEBench offers three key contributions: (1) a\nclinically relevant evaluation framework covering Editing Accuracy, Contextual\nPreservation, and Visual Quality, supported by detailed descriptions of\nexpected change and ROI (Region of Interest) masks; (2) a systematic comparison\nof seven state-of-the-art models, revealing common failure patterns; and (3) a\nfailure analysis protocol based on attention grounding, using IoU between\nattention maps and ROIs to identify mislocalization. MedEBench provides a solid\nfoundation for developing and evaluating reliable, clinically meaningful\nmedical image editing systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01407", "pdf": "https://arxiv.org/pdf/2506.01407", "abs": "https://arxiv.org/abs/2506.01407", "authors": ["Olga Zamaraeva", "Dan Flickinger", "Francis Bond", "Carlos Gómez-Rodríguez"], "title": "Comparing LLM-generated and human-authored news text using formal syntactic theory", "categories": ["cs.CL"], "comment": "20 pages, 15 figures, 13 tables; accepted to ACL-2025 main", "summary": "This study provides the first comprehensive comparison of New York\nTimes-style text generated by six large language models against real,\nhuman-authored NYT writing. The comparison is based on a formal syntactic\ntheory. We use Head-driven Phrase Structure Grammar (HPSG) to analyze the\ngrammatical structure of the texts. We then investigate and illustrate the\ndifferences in the distributions of HPSG grammar types, revealing systematic\ndistinctions between human and LLM-generated writing. These findings contribute\nto a deeper understanding of the syntactic behavior of LLMs as well as humans,\nwithin the NYT genre.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00555", "pdf": "https://arxiv.org/pdf/2506.00555", "abs": "https://arxiv.org/abs/2506.00555", "authors": ["Peng Xia", "Jinglu Wang", "Yibo Peng", "Kaide Zeng", "Xian Wu", "Xiangru Tang", "Hongtu Zhu", "Yun Li", "Shujie Liu", "Yan Lu", "Huaxiu Yao"], "title": "MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential\nin multimodal diagnostic tasks. However, existing single-agent models struggle\nto generalize across diverse medical specialties, limiting their performance.\nRecent efforts introduce multi-agent collaboration frameworks inspired by\nclinical workflows, where general practitioners (GPs) and specialists interact\nin a fixed sequence. Despite improvements, these static pipelines lack\nflexibility and adaptability in reasoning. To address this, we propose\nMMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that\nenables dynamic, optimized collaboration among medical agents. Specifically, we\ntrain two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to\nassign patients to appropriate specialties, while the attending physician\nintegrates the judgments from multi-specialists and its own knowledge to make\nfinal decisions. To address the inconsistency in specialist outputs, we\nintroduce a curriculum learning (CL)-guided RL strategy that progressively\nteaches the attending physician to balance between imitating specialists and\ncorrecting their mistakes. Experiments on five medical VQA benchmarks\ndemonstrate that MMedAgent-RL not only outperforms both open-source and\nproprietary Med-LVLMs, but also exhibits human-like reasoning patterns.\nNotably, it achieves an average performance gain of 18.4% over supervised\nfine-tuning baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00711", "pdf": "https://arxiv.org/pdf/2506.00711", "abs": "https://arxiv.org/abs/2506.00711", "authors": ["Wei Dai", "Peilin Chen", "Chanakya Ekbote", "Paul Pu Liang"], "title": "QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Clinical decision-making routinely demands reasoning over heterogeneous data,\nyet existing multimodal language models (MLLMs) remain largely vision-centric\nand fail to generalize across clinical specialties. To bridge this gap, we\nintroduce QoQ-Med-7B/32B, the first open generalist clinical foundation model\nthat jointly reasons across medical images, time-series signals, and text\nreports. QoQ-Med is trained with Domain-aware Relative Policy Optimization\n(DRPO), a novel reinforcement-learning objective that hierarchically scales\nnormalized rewards according to domain rarity and modality difficulty,\nmitigating performance imbalance caused by skewed clinical data distributions.\nTrained on 2.61 million instruction tuning pairs spanning 9 clinical domains,\nwe show that DRPO training boosts diagnostic performance by 43% in macro-F1 on\naverage across all visual domains as compared to other critic-free training\nmethods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation\ndata, it is able to highlight salient regions related to the diagnosis, with an\nIoU 10x higher than open models while reaching the performance of OpenAI\no4-mini. To foster reproducibility and downstream research, we release (i) the\nfull model weights, (ii) the modular training pipeline, and (iii) all\nintermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01615", "pdf": "https://arxiv.org/pdf/2506.01615", "abs": "https://arxiv.org/abs/2506.01615", "authors": ["Pasunuti Prasanjith", "Prathmesh B More", "Anoop Kunchukuttan", "Raj Dabre"], "title": "IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG Systems", "categories": ["cs.CL"], "comment": "WIP", "summary": "Retrieval-Augmented Generation (RAG) systems enable language models to access\nrelevant information and generate accurate, well-grounded, and contextually\ninformed responses. However, for Indian languages, the development of\nhigh-quality RAG systems is hindered by the lack of two critical resources: (1)\nevaluation benchmarks for retrieval and generation tasks, and (2) large-scale\ntraining datasets for multilingual retrieval. Most existing benchmarks and\ndatasets are centered around English or high-resource languages, making it\ndifficult to extend RAG capabilities to the diverse linguistic landscape of\nIndia. To address the lack of evaluation benchmarks, we create IndicMSMarco, a\nmultilingual benchmark for evaluating retrieval quality and response generation\nin 13 Indian languages, created via manual translation of 1000 diverse queries\nfrom MS MARCO-dev set. To address the need for training data, we build a\nlarge-scale dataset of (question, answer, relevant passage) tuples derived from\nthe Wikipedias of 19 Indian languages using state-of-the-art LLMs.\nAdditionally, we include translated versions of the original MS MARCO dataset\nto further enrich the training data and ensure alignment with real-world\ninformation-seeking tasks. Resources are available here:\nhttps://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00717", "pdf": "https://arxiv.org/pdf/2506.00717", "abs": "https://arxiv.org/abs/2506.00717", "authors": ["Mina Huh", "Zihui Xue", "Ujjaini Das", "Kumar Ashutosh", "Kristen Grauman", "Amy Pavel"], "title": "Vid2Coach: Transforming How-To Videos into Task Assistants", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "People use videos to learn new recipes, exercises, and crafts. Such videos\nremain difficult for blind and low vision (BLV) people to follow as they rely\non visual comparison. Our observations of visual rehabilitation therapists\n(VRTs) guiding BLV people to follow how-to videos revealed that VRTs provide\nboth proactive and responsive support including detailed descriptions,\nnon-visual workarounds, and progress feedback. We propose Vid2Coach, a system\nthat transforms how-to videos into wearable camera-based assistants that\nprovide accessible instructions and mixed-initiative feedback. From the video,\nVid2Coach generates accessible instructions by augmenting narrated instructions\nwith demonstration details and completion criteria for each step. It then uses\nretrieval-augmented-generation to extract relevant non-visual workarounds from\nBLV-specific resources. Vid2Coach then monitors user progress with a camera\nembedded in commercial smart glasses to provide context-aware instructions,\nproactive feedback, and answers to user questions. BLV participants (N=8) using\nVid2Coach completed cooking tasks with 58.5\\% fewer errors than when using\ntheir typical workflow and wanted to use Vid2Coach in their daily lives.\nVid2Coach demonstrates an opportunity for AI visual assistance that strengthens\nrather than replaces non-visual expertise.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00727", "pdf": "https://arxiv.org/pdf/2506.00727", "abs": "https://arxiv.org/abs/2506.00727", "authors": ["Javier Bisbal", "Julio Sotelo", "Maria I Valdés", "Pablo Irarrazaval", "Marcelo E Andia", "Julio García", "José Rodriguez-Palomarez", "Francesca Raimondi", "Cristián Tejos", "Sergio Uribe"], "title": "Adaptive Plane Reformatting for 4D Flow MRI using Deep Reinforcement Learning", "categories": ["cs.LG", "cs.CV", "I.4.0"], "comment": "11 pages, 4 figures, submitted to IEEE Transactions on Medical\n  Imaging", "summary": "Deep reinforcement learning (DRL) algorithms have shown robust results in\nplane reformatting tasks. In these methods, an agent sequentially adjusts the\nposition and orientation of an initial plane towards an objective location.\nThis process allows accurate plane reformatting, without the need for detailed\nlandmarks, which makes it suitable for images with limited contrast and\nresolution, such as 4D flow MRI. However, current DRL methods require the test\ndataset to be in the same position and orientation as the training dataset. In\nthis paper, we present a novel technique that utilizes a flexible coordinate\nsystem based on the current state, enabling navigation in volumes at any\nposition or orientation. We adopted the Asynchronous Advantage Actor Critic\n(A3C) algorithm for reinforcement learning, outperforming Deep Q Network (DQN).\nExperimental results in 4D flow MRI demonstrate improved accuracy in plane\nreformatting angular and distance errors (6.32 +- 4.15 {\\deg} and 3.40 +- 2.75\nmm), as well as statistically equivalent flow measurements determined by a\nplane reformatting process done by an expert (p=0.21). The method's flexibility\nand adaptability make it a promising candidate for other medical imaging\napplications beyond 4D flow MRI.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01710", "pdf": "https://arxiv.org/pdf/2506.01710", "abs": "https://arxiv.org/abs/2506.01710", "authors": ["Fangyu Lei", "Jinxiang Meng", "Yiming Huang", "Tinghong Chen", "Yun Zhang", "Shizhu He", "Jun Zhao", "Kang Liu"], "title": "Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Table reasoning, encompassing tasks such as table question answering, fact\nverification, and text-to-SQL, requires precise understanding of structured\ntabular data, coupled with numerical computation and code manipulation for\neffective inference. Supervised fine-tuning (SFT) approaches have achieved\nnotable success but often struggle with generalization and robustness due to\nbiases inherent in imitative learning. We introduce Reasoning-Table, the first\napplication of reinforcement learning (RL) to table reasoning, achieving\nstate-of-the-art performance. Through rigorous data preprocessing, reward\ndesign, and tailored training strategies, our method leverages simple\nrule-based outcome rewards to outperform SFT across multiple benchmarks.\nUnified training across diverse tasks enables Reasoning-Table to emerge as a\nrobust table reasoning large language model, surpassing larger proprietary\nmodels like Claude-3.7-Sonnet by 4.0% on table reasoning benchmarks. The\napproach also achieves excellent performance on text-to-SQL tasks, reaching\n68.3% performance on the BIRD dev dataset with a 7B model. Further experiments\ndemonstrate that Reasoning-Table enhances the model's generalization\ncapabilities and robustness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01814", "pdf": "https://arxiv.org/pdf/2506.01814", "abs": "https://arxiv.org/abs/2506.01814", "authors": ["PeiHsuan Huang", "ZihWei Lin", "Simon Imbot", "WenCheng Fu", "Ethan Tu"], "title": "Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Large language models (LLMs) increasingly shape public understanding and\ncivic decisions, yet their ideological neutrality is a growing concern. While\nexisting research has explored various forms of LLM bias, a direct,\ncross-lingual comparison of models with differing geopolitical\nalignments-specifically a PRC-system model versus a non-PRC counterpart-has\nbeen lacking. This study addresses this gap by systematically evaluating\nDeepSeek-R1 (PRC-aligned) against ChatGPT o3-mini-high (non-PRC) for\nChinese-state propaganda and anti-U.S. sentiment. We developed a novel corpus\nof 1,200 de-contextualized, reasoning-oriented questions derived from\nChinese-language news, presented in Simplified Chinese, Traditional Chinese,\nand English. Answers from both models (7,200 total) were assessed using a\nhybrid evaluation pipeline combining rubric-guided GPT-4o scoring with human\nannotation. Our findings reveal significant model-level and language-dependent\nbiases. DeepSeek-R1 consistently exhibited substantially higher proportions of\nboth propaganda and anti-U.S. bias compared to ChatGPT o3-mini-high, which\nremained largely free of anti-U.S. sentiment and showed lower propaganda\nlevels. For DeepSeek-R1, Simplified Chinese queries elicited the highest bias\nrates; these diminished in Traditional Chinese and were nearly absent in\nEnglish. Notably, DeepSeek-R1 occasionally responded in Simplified Chinese to\nTraditional Chinese queries and amplified existing PRC-aligned terms in its\nChinese answers, demonstrating an \"invisible loudspeaker\" effect. Furthermore,\nsuch biases were not confined to overtly political topics but also permeated\ncultural and lifestyle content, particularly in DeepSeek-R1.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "rubric"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01929", "pdf": "https://arxiv.org/pdf/2506.01929", "abs": "https://arxiv.org/abs/2506.01929", "authors": ["Saar Huberman", "Or Patashnik", "Omer Dahary", "Ron Mokady", "Daniel Cohen-Or"], "title": "Image Generation from Contextually-Contradictory Prompts", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": "Project page: https://tdpc2025.github.io/SAP/", "summary": "Text-to-image diffusion models excel at generating high-quality, diverse\nimages from natural language prompts. However, they often fail to produce\nsemantically accurate results when the prompt contains concept combinations\nthat contradict their learned priors. We define this failure mode as contextual\ncontradiction, where one concept implicitly negates another due to entangled\nassociations learned during training. To address this, we propose a stage-aware\nprompt decomposition framework that guides the denoising process using a\nsequence of proxy prompts. Each proxy prompt is constructed to match the\nsemantic content expected to emerge at a specific stage of denoising, while\nensuring contextual coherence. To construct these proxy prompts, we leverage a\nlarge language model (LLM) to analyze the target prompt, identify\ncontradictions, and generate alternative expressions that preserve the original\nintent while resolving contextual conflicts. By aligning prompt information\nwith the denoising progression, our method enables fine-grained semantic\ncontrol and accurate image generation in the presence of contextual\ncontradictions. Experiments across a variety of challenging prompts show\nsubstantial improvements in alignment to the textual prompt.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01920", "pdf": "https://arxiv.org/pdf/2506.01920", "abs": "https://arxiv.org/abs/2506.01920", "authors": ["Serry Sibaee", "Omer Nacar", "Adel Ammar", "Yasser Al-Habashi", "Abdulrahman Al-Batati", "Wadii Boulila"], "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses critical gaps in Arabic language model evaluation by\nestablishing comprehensive theoretical guidelines and introducing a novel\nevaluation framework. We first analyze existing Arabic evaluation datasets,\nidentifying significant issues in linguistic accuracy, cultural alignment, and\nmethodological rigor. To address these limitations in LLMs, we present the\nArabic Depth Mini Dataset (ADMD), a carefully curated collection of 490\nchallenging questions spanning ten major domains (42 sub-domains, see Figure 1.\nUsing ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,\nGemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant\nvariations in model performance across different domains, with particular\nchallenges in areas requiring deep cultural understanding and specialized\nknowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%,\nshowing relative strength in mathematical theory in Arabic, Arabic language,\nand islamic domains. This work provides both theoretical foundations and\npractical insights for improving Arabic language model evaluation, emphasizing\nthe importance of cultural competence alongside technical capabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01939", "pdf": "https://arxiv.org/pdf/2506.01939", "abs": "https://arxiv.org/abs/2506.01939", "authors": ["Shenzhi Wang", "Le Yu", "Chang Gao", "Chujie Zheng", "Shixuan Liu", "Rui Lu", "Kai Dang", "Xionghui Chen", "Jianxin Yang", "Zhenru Zhang", "Yuqiong Liu", "An Yang", "Andrew Zhao", "Yang Yue", "Shiji Song", "Bowen Yu", "Gao Huang", "Junyang Lin"], "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 17 figures, 2 tables", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful approach to enhancing the reasoning capabilities of Large Language\nModels (LLMs), while its mechanisms are not yet well understood. In this work,\nwe undertake a pioneering exploration of RLVR through the novel perspective of\ntoken entropy patterns, comprehensively analyzing how different tokens\ninfluence reasoning performance. By examining token entropy patterns in\nChain-of-Thought (CoT) reasoning, we observe that only a small fraction of\ntokens exhibit high entropy, and these tokens act as critical forks that steer\nthe model toward diverse reasoning pathways. Furthermore, studying how entropy\npatterns evolve during RLVR training reveals that RLVR largely adheres to the\nbase model's entropy patterns, primarily adjusting the entropy of high-entropy\ntokens. These findings highlight the significance of high-entropy tokens (i.e.,\nforking tokens) to RLVR. We ultimately improve RLVR by restricting policy\ngradient updates to forking tokens and uncover a finding even beyond the 80/20\nrule: utilizing only 20% of the tokens while maintaining performance comparable\nto full-gradient updates on the Qwen3-8B base model and significantly\nsurpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71\non AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,\nhighlighting a strong scaling trend. In contrast, training exclusively on the\n80% lowest-entropy tokens leads to a marked decline in performance. These\nfindings indicate that the efficacy of RLVR primarily arises from optimizing\nthe high-entropy tokens that decide reasoning directions. Collectively, our\nresults highlight the potential to understand RLVR through a token-entropy\nperspective and optimize RLVR by leveraging high-entropy minority tokens to\nfurther improve LLM reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00062", "pdf": "https://arxiv.org/pdf/2506.00062", "abs": "https://arxiv.org/abs/2506.00062", "authors": ["Aladin Djuhera", "Swanand Ravindra Kadhe", "Farhan Ahmed", "Syed Zawad", "Holger Boche", "Walid Saad"], "title": "SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?", "categories": ["cs.CY", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Fine-tuning large language models (LLMs) for telecom tasks and datasets is a\ncommon practice to adapt general-purpose models to the telecom domain. However,\nlittle attention has been paid to how this process may compromise model safety.\nRecent research has shown that even benign fine-tuning can degrade the safety\nalignment of LLMs, causing them to respond to harmful or unethical user\nqueries. In this paper, we investigate this issue for telecom-tuned LLMs using\nthree representative datasets featured by the GenAINet initiative. We show that\nsafety degradation persists even for structured and seemingly harmless datasets\nsuch as 3GPP standards and tabular records, indicating that telecom-specific\ndata is not immune to safety erosion during fine-tuning. We further extend our\nanalysis to publicly available Telecom LLMs trained via continual pre-training,\nrevealing that safety alignment is often severely lacking, primarily due to the\nomission of safety-focused instruction tuning. To address these issues in both\nfine-tuned and pre-trained models, we conduct extensive experiments and\nevaluate three safety realignment defenses (SafeInstruct, SafeLoRA, and\nSafeMERGE) using established red-teaming benchmarks. The results show that,\nacross all settings, the proposed defenses can effectively restore safety after\nharmful degradation without compromising downstream task performance, leading\nto Safe teleCOMMunication (SafeCOMM) models. In a nutshell, our work serves as\na diagnostic study and practical guide for safety realignment in telecom-tuned\nLLMs, and emphasizes the importance of safety-aware instruction and fine-tuning\nfor real-world deployments of Telecom LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00072", "pdf": "https://arxiv.org/pdf/2506.00072", "abs": "https://arxiv.org/abs/2506.00072", "authors": ["Nariman Naderi", "Zahra Atf", "Peter R Lewis", "Aref Mahjoub far", "Seyed Amir Ahmad Safavi-Naini", "Ali Soroush"], "title": "Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "This paper was accepted for presentation at the 7th International\n  Workshop on EXplainable, Trustworthy, and Responsible AI and Multi-Agent\n  Systems (EXTRAAMAS 2025). Workshop website:\n  https://extraamas.ehealth.hevs.ch/index.html", "summary": "This paper investigates how prompt engineering techniques impact both\naccuracy and confidence elicitation in Large Language Models (LLMs) applied to\nmedical contexts. Using a stratified dataset of Persian board exam questions\nacross multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini,\nLlama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These\nconfigurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles\n(Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales\n(1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error\n(ECE) to evaluate alignment between confidence and actual performance.\nChain-of-Thought prompts improved accuracy but also led to overconfidence,\nhighlighting the need for calibration. Emotional prompting further inflated\nconfidence, risking poor decisions. Smaller models like Llama-3.1-8b\nunderperformed across all metrics, while proprietary models showed higher\naccuracy but still lacked calibrated confidence. These results suggest prompt\nengineering must address both accuracy and uncertainty to be effective in\nhigh-stakes medical tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00166", "pdf": "https://arxiv.org/pdf/2506.00166", "abs": "https://arxiv.org/abs/2506.00166", "authors": ["Kundan Krishna", "Joseph Y Cheng", "Charles Maalouf", "Leon A Gatys"], "title": "Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "16 pages, 2 figures, including references and appendix", "summary": "Existing paradigms for ensuring AI safety, such as guardrail models and\nalignment training, often compromise either inference efficiency or development\nflexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework\naddressing these challenges by decoupling safety-specific computations from a\ntask-optimized base model. DSA utilizes lightweight adapters that leverage the\nbase model's internal representations, enabling diverse and flexible safety\nfunctionalities with minimal impact on inference cost. Empirically, DSA-based\nsafety guardrails substantially outperform comparably sized standalone models,\nnotably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and\nalso excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe\nmodel inputs and responses (0.93 vs. 0.90 on AEGIS2.0 & BeaverTails).\nFurthermore, DSA-based safety alignment allows dynamic, inference-time\nadjustment of alignment strength and a fine-grained trade-off between\ninstruction following performance and model safety. Importantly, combining the\nDSA safety guardrail with DSA safety alignment facilitates context-dependent\nalignment strength, boosting safety on StrongReject by 93% while maintaining\n98% performance on MTBench -- a total reduction in alignment tax of 8\npercentage points compared to standard safety alignment fine-tuning. Overall,\nDSA presents a promising path towards more modular, efficient, and adaptable AI\nsafety and alignment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "fine-grained"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00236", "pdf": "https://arxiv.org/pdf/2506.00236", "abs": "https://arxiv.org/abs/2506.00236", "authors": ["Babak Barazandeh"], "title": "Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, offer compact\nand effective alternatives to full model fine-tuning by introducing low-rank\nupdates to pretrained weights. However, most existing approaches rely on global\nlow-rank structures, which can overlook spatial patterns spread across the\nparameter space. In this work, we propose Localized LoRA, a generalized\nframework that models weight updates as a composition of low-rank matrices\napplied to structured blocks of the weight matrix. This formulation enables\ndense, localized updates throughout the parameter space-without increasing the\ntotal number of trainable parameters. We provide a formal comparison between\nglobal, diagonal-local, and fully localized low-rank approximations, and show\nthat our method consistently achieves lower approximation error under matched\nparameter budgets. Experiments on both synthetic and practical settings\ndemonstrate that Localized LoRA offers a more expressive and adaptable\nalternative to existing methods, enabling efficient fine-tuning with improved\nperformance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00242", "pdf": "https://arxiv.org/pdf/2506.00242", "abs": "https://arxiv.org/abs/2506.00242", "authors": ["Shuai Feng", "Wei-Chuang Chan", "Srishti Chouhan", "Junior Francisco Garcia Ayala", "Srujananjali Medicherla", "Kyle Clark", "Mingwei Shi"], "title": "Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise", "categories": ["cs.AI", "cs.CL"], "comment": "14 main pages;8 page appendix", "summary": "The integration of large language models (LLMs) into global applications\nnecessitates effective cultural alignment for meaningful and\nculturally-sensitive interactions. Current LLMs often lack the nuanced\nunderstanding required for diverse cultural contexts, and adapting them\ntypically involves costly full fine-tuning. To address this, we introduce a\nnovel soft prompt fine-tuning framework that enables efficient and modular\ncultural alignment. Our method utilizes vectorized prompt tuning to dynamically\nroute queries to a committee of culturally specialized 'expert' LLM\nconfigurations, created by optimizing soft prompt embeddings without altering\nthe base model's parameters. Extensive experiments demonstrate that our\nframework significantly enhances cultural sensitivity and adaptability,\nimproving alignment scores from 0.208 to 0.820, offering a robust solution for\nculturally-aware LLM deployment. This research paves the way for subsequent\ninvestigations into enhanced cultural coverage and dynamic expert adaptation,\ncrucial for realizing autonomous AI with deeply nuanced understanding in a\nglobally interconnected world.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00245", "pdf": "https://arxiv.org/pdf/2506.00245", "abs": "https://arxiv.org/abs/2506.00245", "authors": ["Dang Nguyen", "Ali Payani", "Baharan Mirzasoleiman"], "title": "Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity", "categories": ["cs.LG", "cs.CL"], "comment": "11 pages, 4 figures, 6 tables, link:\n  https://github.com/BigML-CS-UCLA/SNNE", "summary": "Hallucination in large language models (LLMs) can be detected by assessing\nthe uncertainty of model outputs, typically measured using entropy. Semantic\nentropy (SE) enhances traditional entropy estimation by quantifying uncertainty\nat the semantic cluster level. However, as modern LLMs generate longer\none-sentence responses, SE becomes less effective because it overlooks two\ncrucial factors: intra-cluster similarity (the spread within a cluster) and\ninter-cluster similarity (the distance between clusters). To address these\nlimitations, we propose a simple black-box uncertainty quantification method\ninspired by nearest neighbor estimates of entropy. Our approach can also be\neasily extended to white-box settings by incorporating token probabilities.\nAdditionally, we provide theoretical results showing that our method\ngeneralizes semantic entropy. Extensive empirical results demonstrate its\neffectiveness compared to semantic entropy across two recent LLMs (Phi3 and\nLlama3) and three common text generation tasks: question answering, text\nsummarization, and machine translation. Our code is available at\nhttps://github.com/BigML-CS-UCLA/SNNE.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization", "question answering"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00249", "pdf": "https://arxiv.org/pdf/2506.00249", "abs": "https://arxiv.org/abs/2506.00249", "authors": ["Aniketh Garikaparthi", "Manasi Patwardhan", "Aditya Sanjiv Kanade", "Aman Hassan", "Lovekesh Vig", "Arman Cohan"], "title": "MIR: Methodology Inspiration Retrieval for Scientific Research Problems", "categories": ["cs.AI", "cs.CL"], "comment": "ACL 2025", "summary": "There has been a surge of interest in harnessing the reasoning capabilities\nof Large Language Models (LLMs) to accelerate scientific discovery. While\nexisting approaches rely on grounding the discovery process within the relevant\nliterature, effectiveness varies significantly with the quality and nature of\nthe retrieved literature. We address the challenge of retrieving prior work\nwhose concepts can inspire solutions for a given research problem, a task we\ndefine as Methodology Inspiration Retrieval (MIR). We construct a novel dataset\ntailored for training and evaluating retrievers on MIR, and establish\nbaselines. To address MIR, we build the Methodology Adjacency Graph (MAG);\ncapturing methodological lineage through citation relationships. We leverage\nMAG to embed an \"intuitive prior\" into dense retrievers for identifying\npatterns of methodological inspiration beyond superficial semantic similarity.\nThis achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average\nPrecision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking\nstrategies to MIR, yielding additional improvements of +4.5 in Recall@3 and\n+4.8 in mAP. Through extensive ablation studies and qualitative analyses, we\nexhibit the promise of MIR in enhancing automated scientific discovery and\noutline avenues for advancing inspiration-driven retrieval.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00382", "pdf": "https://arxiv.org/pdf/2506.00382", "abs": "https://arxiv.org/abs/2506.00382", "authors": ["Xuyuan Liu", "Lei Hsiung", "Yaoqing Yang", "Yujun Yan"], "title": "Spectral Insights into Data-Oblivious Critical Layers in Large Language Models", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted by Findings of ACL2025", "summary": "Understanding how feature representations evolve across layers in large\nlanguage models (LLMs) is key to improving their interpretability and\nrobustness. While recent studies have identified critical layers linked to\nspecific functions or behaviors, these efforts typically rely on data-dependent\nanalyses of fine-tuned models, limiting their use to post-hoc settings. In\ncontrast, we introduce a data-oblivious approach to identify intrinsic critical\nlayers in pre-fine-tuned LLMs by analyzing representation dynamics via Centered\nKernel Alignment(CKA). We show that layers with significant shifts in\nrepresentation space are also those most affected during fine-tuning--a pattern\nthat holds consistently across tasks for a given model. Our spectral analysis\nfurther reveals that these shifts are driven by changes in the top principal\ncomponents, which encode semantic transitions from rationales to conclusions.\nWe further apply these findings to two practical scenarios: efficient domain\nadaptation, where fine-tuning critical layers leads to greater loss reduction\ncompared to non-critical layers; and backdoor defense, where freezing them\nreduces attack success rates by up to 40%.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00555", "pdf": "https://arxiv.org/pdf/2506.00555", "abs": "https://arxiv.org/abs/2506.00555", "authors": ["Peng Xia", "Jinglu Wang", "Yibo Peng", "Kaide Zeng", "Xian Wu", "Xiangru Tang", "Hongtu Zhu", "Yun Li", "Shujie Liu", "Yan Lu", "Huaxiu Yao"], "title": "MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential\nin multimodal diagnostic tasks. However, existing single-agent models struggle\nto generalize across diverse medical specialties, limiting their performance.\nRecent efforts introduce multi-agent collaboration frameworks inspired by\nclinical workflows, where general practitioners (GPs) and specialists interact\nin a fixed sequence. Despite improvements, these static pipelines lack\nflexibility and adaptability in reasoning. To address this, we propose\nMMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that\nenables dynamic, optimized collaboration among medical agents. Specifically, we\ntrain two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to\nassign patients to appropriate specialties, while the attending physician\nintegrates the judgments from multi-specialists and its own knowledge to make\nfinal decisions. To address the inconsistency in specialist outputs, we\nintroduce a curriculum learning (CL)-guided RL strategy that progressively\nteaches the attending physician to balance between imitating specialists and\ncorrecting their mistakes. Experiments on five medical VQA benchmarks\ndemonstrate that MMedAgent-RL not only outperforms both open-source and\nproprietary Med-LVLMs, but also exhibits human-like reasoning patterns.\nNotably, it achieves an average performance gain of 18.4% over supervised\nfine-tuning baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00653", "pdf": "https://arxiv.org/pdf/2506.00653", "abs": "https://arxiv.org/abs/2506.00653", "authors": ["Femi Bello", "Anubrata Das", "Fanzhi Zeng", "Fangcong Yin", "Leqi Liu"], "title": "Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "It has been hypothesized that neural networks with similar architectures\ntrained on similar data learn shared representations relevant to the learning\ntask. We build on this idea by extending the conceptual framework where\nrepresentations learned across models trained on the same data can be expressed\nas linear combinations of a \\emph{universal} set of basis features. These basis\nfeatures underlie the learning task itself and remain consistent across models,\nregardless of scale. From this framework, we propose the \\textbf{Linear\nRepresentation Transferability (LRT)} Hypothesis -- that there exists an affine\ntransformation between the representation spaces of different models. To test\nthis hypothesis, we learn affine mappings between the hidden states of models\nof different sizes and evaluate whether steering vectors -- directions in\nhidden state space associated with specific model behaviors -- retain their\nsemantic effect when transferred from small to large language models using the\nlearned mappings. We find strong empirical evidence that such affine mappings\ncan preserve steering behaviors. These findings suggest that representations\nlearned by small models can be used to guide the behavior of large models, and\nthat the LRT hypothesis may be a promising direction on understanding\nrepresentation alignment across model scales.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01256", "pdf": "https://arxiv.org/pdf/2506.01256", "abs": "https://arxiv.org/abs/2506.01256", "authors": ["Matthew C. Kelley"], "title": "Confidence intervals for forced alignment boundaries using model ensembles", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "submitted for publication; 7 pages, 1 figure", "summary": "Forced alignment is a common tool to align audio with orthographic and\nphonetic transcriptions. Most forced alignment tools provide only a single\nestimate of a boundary. The present project introduces a method of deriving\nconfidence intervals for these boundaries using a neural network ensemble\ntechnique. Ten different segment classifier neural networks were previously\ntrained, and the alignment process is repeated with each model. The alignment\nensemble is then used to place the boundary at the median of the boundaries in\nthe ensemble, and 97.85% confidence intervals are constructed using order\nstatistics. On the Buckeye and TIMIT corpora, the ensemble boundaries show a\nslight improvement over using just a single model. The confidence intervals are\nincorporated into Praat TextGrids using a point tier, and they are also output\nas a table for researchers to analyze separately as diagnostics or to\nincorporate uncertainty into their analyses.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01413", "pdf": "https://arxiv.org/pdf/2506.01413", "abs": "https://arxiv.org/abs/2506.01413", "authors": ["Yulei Qin", "Gang Li", "Zongyi Li", "Zihan Xu", "Yuchen Shi", "Zhekai Lin", "Xiao Cui", "Ke Li", "Xing Sun"], "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages of main body, 3 tables, 5 figures, 40 pages of appendix", "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nare available at https://github.com/yuleiqin/RAIF.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "compute scaling", "test-time compute"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01475", "pdf": "https://arxiv.org/pdf/2506.01475", "abs": "https://arxiv.org/abs/2506.01475", "authors": ["Zouying Cao", "Runze Wang", "Yifei Yang", "Xinbei Ma", "Xiaoyong Zhu", "Bo Zheng", "Hai Zhao"], "title": "PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization", "categories": ["cs.AI", "cs.CL"], "comment": "20 pages, 12 figures, 14 tables, ACL'25 Findings", "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin handling complex interactive problems. Existing LLM agents mainly generate\nnatural language plans to guide reasoning, which is verbose and inefficient. NL\nplans are also tailored to specific tasks and restrict agents' ability to\ngeneralize across similar tasks. To this end, we explore pseudocode-style plans\n(P-code Plan) to capture the structural logic of reasoning. We find that P-code\nPlan empowers LLM agents with stronger generalization ability and more\nefficiency. Inspired by this finding, we propose a pseudocode-style Planning\nGuided Preference Optimization method called PGPO for effective agent learning.\nWith two planning-oriented rewards, PGPO further enhances LLM agents' ability\nto generate high-quality P-code Plans and subsequent reasoning. Experiments\nshow that PGPO achieves superior performance on representative agent benchmarks\nand outperforms the current leading baselines. Analyses reveal the advantage of\nPGPO in reducing action errors and omissions during reasoning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01716", "pdf": "https://arxiv.org/pdf/2506.01716", "abs": "https://arxiv.org/abs/2506.01716", "authors": ["Yifei Zhou", "Sergey Levine", "Jason Weston", "Xian Li", "Sainbayar Sukhbaatar"], "title": "Self-Challenging Language Model Agents", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models are quickly becoming the foundation for intelligent\nagents that are capable of using tools. However, training such agents is\nchallenging because it requires human creation and annotation of a diverse set\nof tasks, tools, and evaluation criteria. In this paper, we propose the\nSelf-Challenging framework for training an agent on high-quality tasks that are\ngenerated by itself. The agent first plays the role of challenger and generates\na task after interacting with the given tools. The tasks take the form of a\nnovel general class of problems termed Code-as-Task, which are defined by an\ninstruction, a verification function and solution and failure cases which serve\nas tests, allowing to filter only for high-quality tasks. The agent then takes\nan executor role and trains on those tasks with reinforcement learning using\nthe evaluation feedback as a reward. Evaluation on two existing multi-turn\ntool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging\nframework achieves over a two-fold improvement in Llama-3.1-8B-Instruct,\ndespite using only self-generated training data.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "criteria"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01926", "pdf": "https://arxiv.org/pdf/2506.01926", "abs": "https://arxiv.org/abs/2506.01926", "authors": ["Joey Skaf", "Luis Ibanez-Lissen", "Robert McCarthy", "Connor Watts", "Vasil Georgiv", "Hannes Whittingham", "Lorena Gonzalez-Manzano", "David Lindner", "Cameron Tice", "Edward James Young", "Puria Radmard"], "title": "Large language models can learn and generalize steganographic chain-of-thought under process supervision", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages main text, 3 figures main text, 15 pages supplementary\n  material, 1 figure supplementary material, submitted to NeurIPS 2025", "summary": "Chain-of-thought (CoT) reasoning not only enhances large language model\nperformance but also provides critical insights into decision-making processes,\nmarking it as a useful tool for monitoring model intent and planning. By\nproactively preventing models from acting on CoT indicating misaligned or\nharmful intent, CoT monitoring can be used to reduce risks associated with\ndeploying models. However, developers may be incentivized to train away the\nappearance of harmful intent from CoT traces, by either customer preferences or\nregulatory requirements. Recent works have shown that banning mention of a\nspecific example of reward hacking, which may be done either to make CoT\npresentable to users or as a naive attempt to prevent the behavior, causes\nobfuscation of the undesired reasoning traces but the persistence of the\nundesired behavior. Such obfuscation threatens the reliability of CoT\nmonitoring. However, obfuscation of reasoning can be due to its internalization\nto latent space computation, or its encoding within the CoT. Here, we provide\nan extension to these results. First, we show that penalizing the use of\nspecific strings within load-bearing reasoning traces causes models to\nsubstitute alternative strings. Crucially, this does not alter the underlying\nmethod by which the model performs the task, demonstrating that the model can\nlearn to steganographically encode its reasoning. We further demonstrate that\nmodels can generalize an encoding scheme. When the penalized strings belong to\nan overarching class, the model learns not only to substitute strings seen in\ntraining, but also develops a general encoding scheme for all members of the\nclass which it can apply to held-out testing strings.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward hacking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
