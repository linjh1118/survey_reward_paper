{"id": "2506.03557", "pdf": "https://arxiv.org/pdf/2506.03557", "abs": "https://arxiv.org/abs/2506.03557", "authors": ["Lin Sun", "Chuang Liu", "Peng Liu", "Bingyang Li", "Weijia Lu", "Ning Wu"], "title": "BPO: Revisiting Preference Modeling in Direct Preference Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) have emerged as a popular method for\naligning Large Language Models (LLMs) with human preferences. While DPO\neffectively preserves the relative ordering between chosen and rejected\nresponses through pairwise ranking losses, it often neglects absolute reward\nmagnitudes. This oversight can decrease the likelihood of chosen responses and\nincrease the risk of generating out-of-distribution responses, leading to poor\nperformance. We term this issue Degraded Chosen Responses (DCR).To address this\nissue, we propose Balanced Preference Optimization (BPO), a novel framework\nthat dynamically balances the optimization of chosen and rejected responses\nthrough two key components: balanced reward margin and gap adaptor. Unlike\nprevious methods, BPO can fundamentally resolve DPO's DCR issue, without\nintroducing additional constraints to the loss function. Experimental results\non multiple mathematical reasoning tasks show that BPO significantly\noutperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8%\nto 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses\nDPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over\nCal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a\nsingle line of code modification, making it simple to implement and fully\ncompatible with existing DPO-based frameworks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "ranking", "pairwise", "DPO", "direct preference optimization", "SLiC"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03541", "pdf": "https://arxiv.org/pdf/2506.03541", "abs": "https://arxiv.org/abs/2506.03541", "authors": ["Xiaofeng Zhou", "Heyan Huang", "Lizi Liao"], "title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 10 figures. The camera-ready paper for Findings of ACL 2025", "summary": "Large Language Models (LLMs) continue to set new standards in\nknowledge-intensive and complex reasoning tasks, yet their high computational\ndemands limit widespread adoption. While distilling large models into smaller\nones offers a sustainable solution, current techniques--such as static\nknowledge distillation, resource-intensive reinforcement learning from human\nfeedback, or limited self-reflection--struggle to yield substantial and lasting\nperformance gains. In this paper, we present a novel Debate and Reflect (D&R)\nframework that orchestrates multi-turn debates between smaller models and\nstronger teacher models, eliciting actionable feedback (e.g., error analysis,\ncorrective strategies) to guide student models. Further, we introduce\nTree-structured Direct Preference Optimization (T-DPO) to efficiently leverage\nthese debate logs, organizing interactions into a hierarchical format for\neffective training. Empirical evaluations across diverse NLP benchmarks\ndemonstrate that our approach significantly improves smaller-model accuracy,\nrobustness, and generalization, outperforming conventional baselines by a large\nmargin.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "DPO", "direct preference optimization"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03517", "pdf": "https://arxiv.org/pdf/2506.03517", "abs": "https://arxiv.org/abs/2506.03517", "authors": ["Ziyi Wu", "Anil Kag", "Ivan Skorokhodov", "Willi Menapace", "Ashkan Mirzaei", "Igor Gilitschenski", "Sergey Tulyakov", "Aliaksandr Siarohin"], "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models", "categories": ["cs.CV"], "comment": "Project page: https://snap-research.github.io/DenseDPO/", "summary": "Direct Preference Optimization (DPO) has recently been applied as a\npost-training technique for text-to-video diffusion models. To obtain training\ndata, annotators are asked to provide preferences between two videos generated\nfrom independent noise. However, this approach prohibits fine-grained\ncomparisons, and we point out that it biases the annotators towards low-motion\nclips as they often contain fewer visual artifacts. In this work, we introduce\nDenseDPO, a method that addresses these shortcomings by making three\ncontributions. First, we create each video pair for DPO by denoising corrupted\ncopies of a ground truth video. This results in aligned pairs with similar\nmotion structures while differing in local details, effectively neutralizing\nthe motion bias. Second, we leverage the resulting temporal alignment to label\npreferences on short segments rather than entire clips, yielding a denser and\nmore precise learning signal. With only one-third of the labeled data, DenseDPO\ngreatly improves motion generation over vanilla DPO, while matching it in text\nalignment, visual quality, and temporal consistency. Finally, we show that\nDenseDPO unlocks automatic preference annotation using off-the-shelf Vision\nLanguage Models (VLMs): GPT accurately predicts segment-level preferences\nsimilar to task-specifically fine-tuned video reward models, and DenseDPO\ntrained on these labels achieves performance close to using human labels.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO", "direct preference optimization"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03690", "pdf": "https://arxiv.org/pdf/2506.03690", "abs": "https://arxiv.org/abs/2506.03690", "authors": ["Jie Sun", "Junkang Wu", "Jiancan Wu", "Zhibo Zhu", "Xingyu Lu", "Jun Zhou", "Lintao Ma", "Xiang Wang"], "title": "Robust Preference Optimization via Dynamic Target Margins", "categories": ["cs.CL"], "comment": "18 pages, 6 figures, accepted to The 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL2025)", "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose $\\gamma$-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\n\\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise", "alignment", "DPO"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "reliability"], "score": 2}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03637", "pdf": "https://arxiv.org/pdf/2506.03637", "abs": "https://arxiv.org/abs/2506.03637", "authors": ["Zhuohao Yu", "Jiali Zeng", "Weizheng Gu", "Yidong Wang", "Jindong Wang", "Fandong Meng", "Jie Zhou", "Yue Zhang", "Shikun Zhang", "Wei Ye"], "title": "RewardAnything: Generalizable Principle-Following Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 8 figures", "summary": "Reward Models, essential for guiding Large Language Model optimization, are\ntypically trained on fixed preference datasets, resulting in rigid alignment to\nsingle, implicit preference distributions. This prevents adaptation to diverse\nreal-world needs-from conciseness in one task to detailed explanations in\nanother. The standard practice of collecting task-specific preference data and\nretraining reward models is resource-intensive, often producing biased rewards,\nand limits practical application. We introduce generalizable,\nprinciple-following reward models. We propose that RMs should understand and\nadhere to dynamically provided natural language specifications of reward\nprinciples, similar to instruction-following in LLMs. To measure this\ncapability, we develop RABench, a comprehensive benchmark for RMs focusing on\ngeneralization across diverse principles. Evaluations on RABench reveal poor\ngeneralization of current RMs. As a solution, we present RewardAnything, a\nnovel RM designed and trained to explicitly follow natural language principles.\nWe achieve SotA performance with RewardAnything in traditional RM benchmark\nsimply by specifying a well-defined principle, and results on RABench show we\nexcel in adapting to novel principles without retraining. Furthermore,\nRewardAnything integrates seamlessly with existing RLHF methods and we show by\na case study on how to automatically and efficiently align LLMs with only\nnatural language principles.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03621", "pdf": "https://arxiv.org/pdf/2506.03621", "abs": "https://arxiv.org/abs/2506.03621", "authors": ["Chaehun Shin", "Jooyoung Choi", "Johan Barthelemy", "Jungbeom Lee", "Sungroh Yoon"], "title": "Negative-Guided Subject Fidelity Optimization for Zero-Shot Subject-Driven Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Subject Fidelity Optimization (SFO), a novel comparative learning\nframework for zero-shot subject-driven generation that enhances subject\nfidelity. Beyond supervised fine-tuning methods that rely only on positive\ntargets and use the diffusion loss as in the pre-training stage, SFO introduces\nsynthetic negative targets and explicitly guides the model to favor positives\nover negatives through pairwise comparison. For negative targets, we propose\nCondition-Degradation Negative Sampling (CDNS), which automatically generates\ndistinctive and informative negatives by intentionally degrading visual and\ntextual cues without expensive human annotations. Moreover, we reweight the\ndiffusion timesteps to focus finetuning on intermediate steps where subject\ndetails emerge. Extensive experiments demonstrate that SFO with CDNS\nsignificantly outperforms baselines in terms of both subject fidelity and text\nalignment on a subject-driven generation benchmark. Project page:\nhttps://subjectfidelityoptimization.github.io/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "pairwise", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04180", "pdf": "https://arxiv.org/pdf/2506.04180", "abs": "https://arxiv.org/abs/2506.04180", "authors": ["Yuhao Wu", "Yushi Bai", "Zhiqiang Hu", "Juanzi Li", "Roy Ka-Wei Lee"], "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "MCTS"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency"], "score": 3}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03524", "pdf": "https://arxiv.org/pdf/2506.03524", "abs": "https://arxiv.org/abs/2506.03524", "authors": ["Yuyu Zhang", "Jing Su", "Yifan Sun", "Chenguang Xi", "Xia Xiao", "Shen Zheng", "Anxiang Zhang", "Kaibo Liu", "Daoguang Zan", "Tao Sun", "Jinhua Zhu", "Shulin Xin", "Dong Huang", "Yetao Bai", "Lixin Dong", "Chao Li", "Jianchong Chen", "Hanzhi Zhou", "Yifan Huang", "Guanghan Ning", "Xierui Song", "Jiaze Chen", "Siyao Liu", "Kai Shen", "Liang Xiang", "Yonghui Wu"], "title": "Seed-Coder: Let the Code Model Curate Data for Itself", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Code data in large language model (LLM) pretraining is recognized crucial not\nonly for code-related tasks but also for enhancing general intelligence of\nLLMs. Current open-source LLMs often heavily rely on human effort to produce\ntheir code pretraining data, such as employing hand-crafted filtering rules\ntailored to individual programming languages, or using human-annotated data to\ntrain quality filters. However, these approaches are inherently limited in\nscalability, prone to subjective biases, and costly to extend and maintain\nacross diverse programming languages. To address these challenges, we introduce\nSeed-Coder, a series of open-source LLMs comprising base, instruct and\nreasoning models of 8B size, minimizing human involvement in data construction.\nOur code pretraining data is produced by a model-centric data pipeline, which\npredominantly leverages LLMs for scoring and filtering code data. The instruct\nmodel is further trained via supervised fine-tuning and preference\noptimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT)\nreinforcement learning to improve multi-step code reasoning. Seed-Coder\nachieves state-of-the-art results among open-source models of similar size and\neven surpasses some much larger models, demonstrating superior performance in\ncode generation, code completion, code editing, code reasoning, and software\nengineering tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["code generation"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03785", "pdf": "https://arxiv.org/pdf/2506.03785", "abs": "https://arxiv.org/abs/2506.03785", "authors": ["Isik Baran Sandan", "Tu Anh Dinh", "Jan Niehues"], "title": "Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "4 pages, 2 figures", "summary": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03827", "pdf": "https://arxiv.org/pdf/2506.03827", "abs": "https://arxiv.org/abs/2506.03827", "authors": ["Zhenhui Liu", "Chunyuan Yuan", "Ming Pang", "Zheng Fang", "Li Yuan", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Zheng Luo", "Jingping Shao"], "title": "Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by SIGIR2025", "summary": "Retrieval systems primarily address the challenge of matching user queries\nwith the most relevant advertisements, playing a crucial role in e-commerce\nsearch advertising. The diversity of user needs and expressions often produces\nmassive long-tail queries that cannot be matched with merchant bidwords or\nproduct titles, which results in some advertisements not being recalled,\nultimately harming user experience and search efficiency. Existing query\nrewriting research focuses on various methods such as query log mining,\nquery-bidword vector matching, or generation-based rewriting. However, these\nmethods often fail to simultaneously optimize the relevance and authenticity of\nthe user's original query and rewrite and maximize the revenue potential of\nrecalled ads.\n  In this paper, we propose a Multi-objective aligned Bidword Generation Model\n(MoBGM), which is composed of a discriminator, generator, and preference\nalignment module, to address these challenges. To simultaneously improve the\nrelevance and authenticity of the query and rewrite and maximize the platform\nrevenue, we design a discriminator to optimize these key objectives. Using the\nfeedback signal of the discriminator, we train a multi-objective aligned\nbidword generator that aims to maximize the combined effect of the three\nobjectives. Extensive offline and online experiments show that our proposed\nalgorithm significantly outperforms the state of the art. After deployment, the\nalgorithm has created huge commercial value for the platform, further verifying\nits feasibility and robustness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04039", "pdf": "https://arxiv.org/pdf/2506.04039", "abs": "https://arxiv.org/abs/2506.04039", "authors": ["Jiulong Wu", "Zhengliang Shi", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin", "Lingyong Yan", "Min Cao", "Min Zhang"], "title": "Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04039", "pdf": "https://arxiv.org/pdf/2506.04039", "abs": "https://arxiv.org/abs/2506.04039", "authors": ["Jiulong Wu", "Zhengliang Shi", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin", "Lingyong Yan", "Min Cao", "Min Zhang"], "title": "Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03174", "pdf": "https://arxiv.org/pdf/2506.03174", "abs": "https://arxiv.org/abs/2506.03174", "authors": ["Koki Matsuishi", "Kosuke Ukita", "Tsuyoshi Okita"], "title": "Multimodal Foundation Model for Cross-Modal Retrieval and Activity Recognition Tasks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "25 pages, 8 figures", "summary": "In recent years, the widespread adoption of wearable devices has highlighted\nthe growing importance of behavior analysis using IMU. While applications span\ndiverse fields such as healthcare and robotics, recent studies have\nincreasingly focused on multimodal analysis, in addition to unimodal analysis.\nSeveral studies have proposed multimodal foundation models that incorporate\nfirst-person video and text data; however, these models still fall short in\nproviding a detailed analysis of full-body human activity. To address this\nlimitation, we propose Activity Understanding and Representations Alignment -\nMultimodal Foundation Model (AURA-MFM), a foundational model integrating four\nmodalities: third-person video, motion capture, IMU, and text. By incorporating\nthird-person video and motion capture data, the model enables a detailed and\nmultidimensional understanding of human activity, which first-person\nperspectives alone fail to capture. Additionally, a Transformer-based IMU\nencoder is employed to enhance the model's overall performance. Experimental\nevaluations on retrieval and activity recognition tasks demonstrate that our\nmodel surpasses existing methods. Notably, in the zero-shot classification for\naction recognition, our method achieved significantly higher performance, with\nan F1-score of 0.6226 and an accuracy of 0.7320, whereas the existing method\nrecorded an F1-score of 0.0747 and an accuracy of 0.1961.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03295", "pdf": "https://arxiv.org/pdf/2506.03295", "abs": "https://arxiv.org/abs/2506.03295", "authors": ["Yubo Wang", "Ping Nie", "Kai Zou", "Lijun Wu", "Wenhu Chen"], "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03310", "pdf": "https://arxiv.org/pdf/2506.03310", "abs": "https://arxiv.org/abs/2506.03310", "authors": ["Guillermo Marco", "Julio Gonzalo", "Víctor Fresno"], "title": "The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing", "categories": ["cs.CL", "cs.HC"], "comment": "Camera-ready version, 14 pages, 3 figures. Accepted to Findings of\n  the Association for Computational Linguistics (ACL) 2025. Code & data:\n  https://github.com/grmarco/the-reader-is-the-metric", "summary": "Recent studies comparing AI-generated and human-authored literary texts have\nproduced conflicting results: some suggest AI already surpasses human quality,\nwhile others argue it still falls short. We start from the hypothesis that such\ndivergences can be largely explained by genuine differences in how readers\ninterpret and value literature, rather than by an intrinsic quality of the\ntexts evaluated. Using five public datasets (1,471 stories, 101 annotators\nincluding critics, students, and lay readers), we (i) extract 17 reference-less\ntextual features (e.g., coherence, emotional variance, average sentence\nlength...); (ii) model individual reader preferences, deriving feature\nimportance vectors that reflect their textual priorities; and (iii) analyze\nthese vectors in a shared \"preference space\". Reader vectors cluster into two\nprofiles: 'surface-focused readers' (mainly non-experts), who prioritize\nreadability and textual richness; and 'holistic readers' (mainly experts), who\nvalue thematic development, rhetorical variety, and sentiment dynamics. Our\nresults quantitatively explain how measurements of literary quality are a\nfunction of how text features align with each reader's preferences. These\nfindings advocate for reader-sensitive evaluation frameworks in the field of\ncreative text generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03191", "pdf": "https://arxiv.org/pdf/2506.03191", "abs": "https://arxiv.org/abs/2506.03191", "authors": ["Muhammad Islam", "Tao Huang", "Euijoon Ahn", "Usman Naseem"], "title": "Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents an in-depth survey on the use of multimodal Generative\nArtificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs)\nfor human motion understanding and generation, offering insights into emerging\nmethods, architectures, and their potential to advance realistic and versatile\nmotion synthesis. Focusing exclusively on text and motion modalities, this\nresearch investigates how textual descriptions can guide the generation of\ncomplex, human-like motion sequences. The paper explores various generative\napproaches, including autoregressive models, diffusion models, Generative\nAdversarial Networks (GANs), Variational Autoencoders (VAEs), and\ntransformer-based models, by analyzing their strengths and limitations in terms\nof motion quality, computational efficiency, and adaptability. It highlights\nrecent advances in text-conditioned motion generation, where textual inputs are\nused to control and refine motion outputs with greater precision. The\nintegration of LLMs further enhances these models by enabling semantic\nalignment between instructions and motion, improving coherence and contextual\nrelevance. This systematic survey underscores the transformative potential of\ntext-to-motion GenAI and LLM architectures in applications such as healthcare,\nhumanoids, gaming, animation, and assistive technologies, while addressing\nongoing challenges in generating efficient and realistic human motion.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03197", "pdf": "https://arxiv.org/pdf/2506.03197", "abs": "https://arxiv.org/abs/2506.03197", "authors": ["Baode Wang", "Biao Wu", "Weizhen Li", "Meng Fang", "Yanjie Liang", "Zuming Huang", "Haozhe Wang", "Jun Huang", "Ling Chen", "Wei Chu", "Yuan Qi"], "title": "Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "16 pages, 12 figures", "summary": "Automated parsing of scanned documents into richly structured,\nmachine-readable formats remains a critical bottleneck in Document AI, as\ntraditional multi-stage pipelines suffer from error propagation and limited\nadaptability to diverse layouts. We introduce layoutRL, an end-to-end\nreinforcement learning framework that trains models to be explicitly\nlayout-aware by optimizing a composite reward of normalized edit distance,\nparagraph count accuracy, and reading order preservation. Leveraging our newly\nreleased dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic\nscanned document parsing data with expert-filtered real-world documents, we\ninstantiate layoutRL in a vision-language-model-based parser called\nInfinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and\nformula extraction, and reading order detection, Infinity-Parser achieves new\nstate-of-the-art performance in both accuracy and structural fidelity,\noutpacing specialist pipelines and general-purpose vision-language models. We\nwill publicly release our code and dataset to accelerate progress in robust\ndocument understanding.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03213", "pdf": "https://arxiv.org/pdf/2506.03213", "abs": "https://arxiv.org/abs/2506.03213", "authors": ["Abdullah Al Mamun", "Miaohua Zhang", "David Ahmedt-Aristizabal", "Zeeshan Hayder", "Mohammad Awrangjeb"], "title": "ConMamba: Contrastive Vision Mamba for Plant Disease Detection", "categories": ["cs.CV"], "comment": null, "summary": "Plant Disease Detection (PDD) is a key aspect of precision agriculture.\nHowever, existing deep learning methods often rely on extensively annotated\ndatasets, which are time-consuming and costly to generate. Self-supervised\nLearning (SSL) offers a promising alternative by exploiting the abundance of\nunlabeled data. However, most existing SSL approaches suffer from high\ncomputational costs due to convolutional neural networks or transformer-based\narchitectures. Additionally, they struggle to capture long-range dependencies\nin visual representation and rely on static loss functions that fail to align\nlocal and global features effectively. To address these challenges, we propose\nConMamba, a novel SSL framework specially designed for PDD. ConMamba integrates\nthe Vision Mamba Encoder (VME), which employs a bidirectional State Space Model\n(SSM) to capture long-range dependencies efficiently. Furthermore, we introduce\na dual-level contrastive loss with dynamic weight adjustment to optimize\nlocal-global feature alignment. Experimental results on three benchmark\ndatasets demonstrate that ConMamba significantly outperforms state-of-the-art\nmethods across multiple evaluation metrics. This provides an efficient and\nrobust solution for PDD.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03483", "pdf": "https://arxiv.org/pdf/2506.03483", "abs": "https://arxiv.org/abs/2506.03483", "authors": ["Jun Rao", "Zepeng Lin", "Xuebo Liu", "Xiaopeng Ke", "Lian Lian", "Dong Jin", "Shengjun Cheng", "Jun Yu", "Min Zhang"], "title": "APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training", "categories": ["cs.CL"], "comment": "ACL2025 Findings", "summary": "Large Language Models (LLMs) often require domain-specific fine-tuning to\naddress targeted tasks, which risks degrading their general capabilities.\nMaintaining a balance between domain-specific enhancements and general model\nutility is a key challenge. This paper proposes a novel approach named APT\n(Weakness Case Acquisition and Iterative Preference Training) to enhance\ndomain-specific performance with self-generated dis-preferred weakness data\n(bad cases and similar cases). APT uniquely focuses on training the model using\nonly those samples where errors occur, alongside a small, similar set of\nsamples retrieved for this purpose. This targeted training minimizes\ninterference with the model's existing knowledge base, effectively retaining\ngeneric capabilities. Experimental results on the LLama-2 and Mistral-V0.3\nmodels across various benchmarks demonstrate that APT ensures no reduction in\ngeneric capacity and achieves superior performance on downstream tasks compared\nto various existing methods. This validates our method as an effective strategy\nfor enhancing domain-specific capabilities without sacrificing the model's\nbroader applicability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03340", "pdf": "https://arxiv.org/pdf/2506.03340", "abs": "https://arxiv.org/abs/2506.03340", "authors": ["Zihui Xue", "Mi Luo", "Kristen Grauman"], "title": "Seeing the Arrow of Time in Large Multimodal Models", "categories": ["cs.CV"], "comment": "Project website: https://vision.cs.utexas.edu/projects/SeeAoT", "summary": "The Arrow of Time (AoT)-time's irreversible flow shaping physical events-is\nfundamental to video comprehension, yet remains a significant challenge for\nmodern large multimodal models (LMMs). Current LMMs struggle to perceive and\nutilize temporal directionality in video when responding to language queries,\nobstructing deeper temporal understanding. We tackle this deficiency by first\nproviding a critical analysis of existing benchmarks and models. We then\nintroduce ArrowRL, a reinforcement learning (RL)-based training strategy with\nan innovative reverse reward that instills AoT awareness by encouraging\ndivergent video interpretations between forward and reversed visual frames. For\nrigorous evaluation, we additionally develop AoTBench, a new multi-faceted\nbenchmark probing temporally challenging questions. Experiments show ArrowRL\ngreatly advances temporal perception: it not only achieves substantial\nimprovements on our challenging AoTBench but also demonstrably boosts\nperformance on standard video question answering (VQA) benchmarks (with peak\naccuracy gains reaching over 20% and 10% respectively). This validates\nArrowRL's effectiveness and highlights the critical need for dedicated AoT\nunderstanding in LMMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "question answering"], "score": 4}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03519", "pdf": "https://arxiv.org/pdf/2506.03519", "abs": "https://arxiv.org/abs/2506.03519", "authors": ["Yangyang Zhao", "Ben Niu", "Libo Qin", "Shihan Wang"], "title": "An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals", "categories": ["cs.CL"], "comment": null, "summary": "Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue\nsystems to optimize dialogue policy, but it struggles to balance exploration\nand exploitation due to the high dimensionality of state and action spaces.\nThis challenge often results in local optima or poor convergence. Evolutionary\nAlgorithms (EAs) have been proven to effectively explore the solution space of\nneural networks by maintaining population diversity. Inspired by this, we\ninnovatively combine the global search capabilities of EA with the local\noptimization of DRL to achieve a balance between exploration and exploitation.\nNevertheless, the inherent flexibility of natural language in dialogue tasks\ncomplicates this direct integration, leading to prolonged evolutionary times.\nThus, we further propose an elite individual injection mechanism to enhance\nEA's search efficiency by adaptively introducing best-performing individuals\ninto the population. Experiments across four datasets show that our approach\nsignificantly improves the balance between exploration and exploitation,\nboosting performance. Moreover, the effectiveness of the EII mechanism in\nreducing exploration time has been demonstrated, achieving an efficient\nintegration of EA and DRL on task-oriented dialogue policy tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03523", "pdf": "https://arxiv.org/pdf/2506.03523", "abs": "https://arxiv.org/abs/2506.03523", "authors": ["Chong Li", "Jiajun Zhang", "Chengqing Zong"], "title": "TokAlign: Efficient Vocabulary Adaptation via Token Alignment", "categories": ["cs.CL"], "comment": "ACL 2025, our codes and models are available at\n  https://github.com/ZNLP/TokAlign", "summary": "Tokenization serves as a foundational step for Large Language Models (LLMs)\nto process text. In new domains or languages, the inefficiency of the tokenizer\nwill slow down the training and generation of LLM. The mismatch in vocabulary\nalso hinders deep knowledge transfer between LLMs like token-level\ndistillation. To mitigate this gap, we propose an efficient method named\nTokAlign to replace the vocabulary of LLM from the token co-occurrences view,\nand further transfer the token-level knowledge between models. It first aligns\nthe source vocabulary to the target one by learning a one-to-one mapping matrix\nfor token IDs. Model parameters, including embeddings, are rearranged and\nprogressively fine-tuned for the new vocabulary. Our method significantly\nimproves multilingual text compression rates and vocabulary initialization for\nLLMs, decreasing the perplexity from 3.4$\\text{e}^2$ of strong baseline methods\nto 1.2$\\text{e}^2$ after initialization. Experimental results on models across\nmultiple parameter scales demonstrate the effectiveness and generalization of\nTokAlign, which costs as few as 5k steps to restore the performance of the\nvanilla model. After unifying vocabularies between LLMs, token-level\ndistillation can remarkably boost (+4.4% than sentence-level distillation) the\nbase model, costing only 235M tokens.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03388", "pdf": "https://arxiv.org/pdf/2506.03388", "abs": "https://arxiv.org/abs/2506.03388", "authors": ["Pengyu Chen", "Xiao Huang", "Teng Fei", "Sicheng Wang"], "title": "Cross-Modal Urban Sensing: Evaluating Sound-Vision Alignment Across Street-Level and Aerial Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Environmental soundscapes convey substantial ecological and social\ninformation regarding urban environments; however, their potential remains\nlargely untapped in large-scale geographic analysis. In this study, we\ninvestigate the extent to which urban sounds correspond with visual scenes by\ncomparing various visual representation strategies in capturing acoustic\nsemantics. We employ a multimodal approach that integrates geo-referenced sound\nrecordings with both street-level and remote sensing imagery across three major\nglobal cities: London, New York, and Tokyo. Utilizing the AST model for audio,\nalong with CLIP and RemoteCLIP for imagery, as well as CLIPSeg and Seg-Earth OV\nfor semantic segmentation, we extract embeddings and class-level features to\nevaluate cross-modal similarity. The results indicate that street view\nembeddings demonstrate stronger alignment with environmental sounds compared to\nsegmentation outputs, whereas remote sensing segmentation is more effective in\ninterpreting ecological categories through a Biophony--Geophony--Anthrophony\n(BGA) framework. These findings imply that embedding-based models offer\nsuperior semantic alignment, while segmentation-based methods provide\ninterpretable links between visual structure and acoustic ecology. This work\nadvances the burgeoning field of multimodal urban sensing by offering novel\nperspectives for incorporating sound into geospatial analysis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03569", "pdf": "https://arxiv.org/pdf/2506.03569", "abs": "https://arxiv.org/abs/2506.03569", "authors": ["Xiaomi LLM-Core Team", ":", "Zihao Yue", "Zhenru Lin", "Yifan Song", "Weikun Wang", "Shuhuai Ren", "Shuhao Gu", "Shicheng Li", "Peidian Li", "Liang Zhao", "Lei Li", "Kainan Bao", "Hao Tian", "Hailin Zhang", "Gang Wang", "Dawei Zhu", "Cici", "Chenhong He", "Bowen Ye", "Bowen Shen", "Zihan Zhang", "Zihan Jiang", "Zhixian Zheng", "Zhichao Song", "Zhenbo Luo", "Yue Yu", "Yudong Wang", "Yuanyuan Tian", "Yu Tu", "Yihan Yan", "Yi Huang", "Xu Wang", "Xinzhe Xu", "Xingchen Song", "Xing Zhang", "Xing Yong", "Xin Zhang", "Xiangwei Deng", "Wenyu Yang", "Wenhan Ma", "Weiwei Lv", "Weiji Zhuang", "Wei Liu", "Sirui Deng", "Shuo Liu", "Shimao Chen", "Shihua Yu", "Shaohui Liu", "Shande Wang", "Rui Ma", "Qiantong Wang", "Peng Wang", "Nuo Chen", "Menghang Zhu", "Kangyang Zhou", "Kang Zhou", "Kai Fang", "Jun Shi", "Jinhao Dong", "Jiebao Xiao", "Jiaming Xu", "Huaqiu Liu", "Hongshen Xu", "Heng Qu", "Haochen Zhao", "Hanglong Lv", "Guoan Wang", "Duo Zhang", "Dong Zhang", "Di Zhang", "Chong Ma", "Chang Liu", "Can Cai", "Bingquan Xia"], "title": "MiMo-VL Technical Report", "categories": ["cs.CL"], "comment": "32 pages", "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03576", "pdf": "https://arxiv.org/pdf/2506.03576", "abs": "https://arxiv.org/abs/2506.03576", "authors": ["Zirui Chen", "Xin Wang", "Zhao Li", "Wenbin Guo", "Dongxiao He"], "title": "KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in knowledge representation learning (KRL) highlight the\nurgent necessity to unify symbolic knowledge graphs (KGs) with language models\n(LMs) for richer semantic understanding. However, existing approaches typically\nprioritize either graph structure or textual semantics, leaving a gap: a\nunified framework that simultaneously captures global KG connectivity, nuanced\nlinguistic context, and discriminative reasoning semantics. To bridge this gap,\nwe introduce KG-BiLM, a bidirectional LM framework that fuses structural cues\nfrom KGs with the semantic expressiveness of generative transformers. KG-BiLM\nincorporates three key components: (i) Bidirectional Knowledge Attention, which\nremoves the causal mask to enable full interaction among all tokens and\nentities; (ii) Knowledge-Masked Prediction, which encourages the model to\nleverage both local semantic contexts and global graph connectivity; and (iii)\nContrastive Graph Semantic Aggregation, which preserves KG structure via\ncontrastive alignment of sampled sub-graph representations. Extensive\nexperiments on standard benchmarks demonstrate that KG-BiLM outperforms strong\nbaselines in link prediction, especially on large-scale graphs with complex\nmulti-hop relations - validating its effectiveness in unifying structural\ninformation and textual semantics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03502", "pdf": "https://arxiv.org/pdf/2506.03502", "abs": "https://arxiv.org/abs/2506.03502", "authors": ["Yuxuan Chen", "Haipeng Xie"], "title": "CHIME: Conditional Hallucination and Integrated Multi-scale Enhancement for Time Series Diffusion Model", "categories": ["cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "The denoising diffusion probabilistic model has become a mainstream\ngenerative model, achieving significant success in various computer vision\ntasks. Recently, there has been initial exploration of applying diffusion\nmodels to time series tasks. However, existing studies still face challenges in\nmulti-scale feature alignment and generative capabilities across different\nentities and long-time scales. In this paper, we propose CHIME, a conditional\nhallucination and integrated multi-scale enhancement framework for time series\ndiffusion models. By employing multi-scale decomposition and adaptive\nintegration, CHIME captures the decomposed features of time series, achieving\nin-domain distribution alignment between generated and original samples. In\naddition, we introduce a feature hallucination module in the conditional\ndenoising process, enabling the transfer of temporal features through the\ntraining of category-independent transformation layers. Experimental results on\npublicly available real-world datasets demonstrate that CHIME achieves\nstate-of-the-art performance and exhibits excellent generative generalization\ncapabilities in few-shot scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03593", "pdf": "https://arxiv.org/pdf/2506.03593", "abs": "https://arxiv.org/abs/2506.03593", "authors": ["Ray Groshan", "Michael Ginn", "Alexis Palmer"], "title": "Is linguistically-motivated data augmentation worth it?", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main. First two authors contributed equally", "summary": "Data augmentation, a widely-employed technique for addressing data scarcity,\ninvolves generating synthetic data examples which are then used to augment\navailable training data. Researchers have seen surprising success from simple\nmethods, such as random perturbations from natural examples, where models seem\nto benefit even from data with nonsense words, or data that doesn't conform to\nthe rules of the language. A second line of research produces synthetic data\nthat does in fact follow all linguistic constraints; these methods require some\nlinguistic expertise and are generally more challenging to implement. No\nprevious work has done a systematic, empirical comparison of both\nlinguistically-naive and linguistically-motivated data augmentation strategies,\nleaving uncertainty about whether the additional time and effort of\nlinguistically-motivated data augmentation work in fact yields better\ndownstream performance.\n  In this work, we conduct a careful and comprehensive comparison of\naugmentation strategies (both linguistically-naive and\nlinguistically-motivated) for two low-resource languages with different\nmorphological properties, Uspanteko and Arapaho. We evaluate the effectiveness\nof many different strategies and their combinations across two important\nsequence-to-sequence tasks for low-resource languages: machine translation and\ninterlinear glossing. We find that linguistically-motivated strategies can have\nbenefits over naive approaches, but only when the new examples they produce are\nnot significantly unlike the training data distribution.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03521", "pdf": "https://arxiv.org/pdf/2506.03521", "abs": "https://arxiv.org/abs/2506.03521", "authors": ["Weinan He", "Zilei Wang", "Yixin Zhang"], "title": "Target Semantics Clustering via Text Representations for Robust Universal Domain Adaptation", "categories": ["cs.CV"], "comment": "Camera-ready version for AAAI 2025", "summary": "Universal Domain Adaptation (UniDA) focuses on transferring source domain\nknowledge to the target domain under both domain shift and unknown category\nshift. Its main challenge lies in identifying common class samples and aligning\nthem. Current methods typically obtain target domain semantics centers from an\nunconstrained continuous image representation space. Due to domain shift and\nthe unknown number of clusters, these centers often result in complex and less\nrobust alignment algorithm. In this paper, based on vision-language models, we\nsearch for semantic centers in a semantically meaningful and discrete text\nrepresentation space. The constrained space ensures almost no domain bias and\nappropriate semantic granularity for these centers, enabling a simple and\nrobust adaptation algorithm. Specifically, we propose TArget Semantics\nClustering (TASC) via Text Representations, which leverages information\nmaximization as a unified objective and involves two stages. First, with the\nfrozen encoders, a greedy search-based framework is used to search for an\noptimal set of text embeddings to represent target semantics. Second, with the\nsearch results fixed, encoders are refined based on gradient descent,\nsimultaneously achieving robust domain alignment and private class clustering.\nAdditionally, we propose Universal Maximum Similarity (UniMS), a scoring\nfunction tailored for detecting open-set samples in UniDA. Experimentally, we\nevaluate the universality of UniDA algorithms under four category shift\nscenarios. Extensive experiments on four benchmarks demonstrate the\neffectiveness and robustness of our method, which has achieved state-of-the-art\nperformance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03659", "pdf": "https://arxiv.org/pdf/2506.03659", "abs": "https://arxiv.org/abs/2506.03659", "authors": ["Yinuo Wang", "Robert E. Mercer", "Frank Rudzicz", "Sudipta Singha Roy", "Pengjie Ren", "Zhumin Chen", "Xindi Wang"], "title": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey", "categories": ["cs.CL"], "comment": null, "summary": "Trustworthiness in healthcare question-answering (QA) systems is important\nfor ensuring patient safety, clinical effectiveness, and user confidence. As\nlarge language models (LLMs) become increasingly integrated into medical\nsettings, the reliability of their responses directly influences clinical\ndecision-making and patient outcomes. However, achieving comprehensive\ntrustworthiness in medical QA poses significant challenges due to the inherent\ncomplexity of healthcare data, the critical nature of clinical scenarios, and\nthe multifaceted dimensions of trustworthy AI. In this survey, we\nsystematically examine six key dimensions of trustworthiness in medical QA,\ni.e., Factuality, Robustness, Fairness, Safety, Explainability, and\nCalibration. We review how each dimension is evaluated in existing LLM-based\nmedical QA systems. We compile and compare major benchmarks designed to assess\nthese dimensions and analyze evaluation-guided techniques that drive model\nimprovements, such as retrieval-augmented grounding, adversarial fine-tuning,\nand safety alignment. Finally, we identify open challenges-such as scalable\nexpert evaluation, integrated multi-dimensional metrics, and real-world\ndeployment studies-and propose future research directions to advance the safe,\nreliable, and transparent deployment of LLM-powered medical QA.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "factuality", "safety", "reliability", "question answering", "multi-dimensional", "dimension"], "score": 7}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03596", "pdf": "https://arxiv.org/pdf/2506.03596", "abs": "https://arxiv.org/abs/2506.03596", "authors": ["Feng Han", "Yang Jiao", "Shaoxiang Chen", "Junhao Xu", "Jingjing Chen", "Yu-Gang Jiang"], "title": "ControlThinker: Unveiling Latent Semantics for Controllable Image Generation through Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The field of controllable image generation has seen significant advancements,\nwith various architectures improving generation layout consistency with control\nsignals. However, contemporary methods still face challenges in bridging the\nsemantic gap between input text prompts with sparse semantics and the target\nimages, often over-relying on low-level control signals to infer regional\ndetails. To address this challenge, we propose ControlThinker, a novel\nframework that employs a \"comprehend-then-generate\" paradigm. Firstly, by\nincentivizing the visual reasoning capability of a MLLM, latent semantics from\ncontrol images are mined to enrich text prompts. This enriched semantic\nunderstanding then seamlessly aids in image generation without the need for\nadditional complex modifications. To further tackle the uncertainty arising\nfrom the ambiguity of control images, we encourage broader exploration of\nreasoning trajectories and select the optimal one using a metric-based output\nreward model (ORM). Extensive experimental results demonstrate that\nControlThinker effectively mitigates the semantic gap between raw text prompts\nand target images, resulting in improved visual quality and semantic\nconsistency across a wide range of benchmarks. The code and models are\navailable at https://github.com/Maplebb/ControlThinker.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03722", "pdf": "https://arxiv.org/pdf/2506.03722", "abs": "https://arxiv.org/abs/2506.03722", "authors": ["Yinfeng Xia", "Huiyan Li", "Chenyang Le", "Manhong Wang", "Yutao Sun", "Xingyang Ma", "Yanmin Qian"], "title": "MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Applying large pre-trained speech models like Whisper has shown promise in\nreducing training costs for various speech tasks. However, integrating these\nmodels into streaming systems remains a challenge. This paper presents a novel\nprefix-to-prefix training framework for streaming recognition by fine-tuning\nthe Whisper. We introduce the Continuous Integrate-and-Fire mechanism to\nestablish a quasi-monotonic alignment between continuous speech sequences and\ndiscrete text tokens. Additionally, we design Monotonic Finite Look-ahead\nAttention, allowing each token to attend to infinite left-context and finite\nright-context from the speech sequences. We also employ the wait-k decoding\nstrategy to simplify the decoding process while ensuring consistency between\ntraining and testing. Our theoretical analysis and experiments demonstrate that\nthis approach achieves a controllable trade-off between latency and quality,\nmaking it suitable for various streaming applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03652", "pdf": "https://arxiv.org/pdf/2506.03652", "abs": "https://arxiv.org/abs/2506.03652", "authors": ["Cheng Zhang", "Hongxia xie", "Bin Wen", "Songhan Zuo", "Ruoxuan Zhang", "Wen-huang Cheng"], "title": "EmoArt: A Multidimensional Dataset for Emotion-Aware Artistic Generation", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of diffusion models, text-to-image generation has\nachieved significant progress in image resolution, detail fidelity, and\nsemantic alignment, particularly with models like Stable Diffusion 3.5, Stable\nDiffusion XL, and FLUX 1. However, generating emotionally expressive and\nabstract artistic images remains a major challenge, largely due to the lack of\nlarge-scale, fine-grained emotional datasets. To address this gap, we present\nthe EmoArt Dataset -- one of the most comprehensive emotion-annotated art\ndatasets to date. It contains 132,664 artworks across 56 painting styles (e.g.,\nImpressionism, Expressionism, Abstract Art), offering rich stylistic and\ncultural diversity. Each image includes structured annotations: objective scene\ndescriptions, five key visual attributes (brushwork, composition, color, line,\nlight), binary arousal-valence labels, twelve emotion categories, and potential\nart therapy effects. Using EmoArt, we systematically evaluate popular\ntext-to-image diffusion models for their ability to generate emotionally\naligned images from text. Our work provides essential data and benchmarks for\nemotion-driven image synthesis and aims to advance fields such as affective\ncomputing, multimodal learning, and computational art, enabling applications in\nart therapy and creative design. The dataset and more details can be accessed\nvia our project website.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03822", "pdf": "https://arxiv.org/pdf/2506.03822", "abs": "https://arxiv.org/abs/2506.03822", "authors": ["Fabian Karl", "Ansgar Scherp"], "title": "CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at SCOLIA 2025", "summary": "Publication databases rely on accurate metadata extraction from diverse web\nsources, yet variations in web layouts and data formats present challenges for\nmetadata providers. This paper introduces CRAWLDoc, a new method for contextual\nranking of linked web documents. Starting with a publication's URL, such as a\ndigital object identifier, CRAWLDoc retrieves the landing page and all linked\nweb resources, including PDFs, ORCID profiles, and supplementary materials. It\nembeds these resources, along with anchor texts and the URLs, into a unified\nrepresentation. For evaluating CRAWLDoc, we have created a new, manually\nlabeled dataset of 600 publications from six top publishers in computer\nscience. Our method CRAWLDoc demonstrates a robust and layout-independent\nranking of relevant documents across publishers and data formats. It lays the\nfoundation for improved metadata extraction from web documents with various\nlayouts and formats. Our source code and dataset can be accessed at\nhttps://github.com/FKarl/CRAWLDoc.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03660", "pdf": "https://arxiv.org/pdf/2506.03660", "abs": "https://arxiv.org/abs/2506.03660", "authors": ["Wei Luo", "Haiming Yao", "Yunkang Cao", "Qiyu Chen", "Ang Gao", "Weiming Shen", "Weihang Zhang", "Wenyong Yu"], "title": "INP-Former++: Advancing Universal Anomaly Detection via Intrinsic Normal Prototypes and Residual Learning", "categories": ["cs.CV"], "comment": "15 pages, 11 figures, 13 tables", "summary": "Anomaly detection (AD) is essential for industrial inspection and medical\ndiagnosis, yet existing methods typically rely on ``comparing'' test images to\nnormal references from a training set. However, variations in appearance and\npositioning often complicate the alignment of these references with the test\nimage, limiting detection accuracy. We observe that most anomalies manifest as\nlocal variations, meaning that even within anomalous images, valuable normal\ninformation remains. We argue that this information is useful and may be more\naligned with the anomalies since both the anomalies and the normal information\noriginate from the same image. Therefore, rather than relying on external\nnormality from the training set, we propose INP-Former, a novel method that\nextracts Intrinsic Normal Prototypes (INPs) directly from the test image.\nSpecifically, we introduce the INP Extractor, which linearly combines normal\ntokens to represent INPs. We further propose an INP Coherence Loss to ensure\nINPs can faithfully represent normality for the testing image. These INPs then\nguide the INP-guided Decoder to reconstruct only normal tokens, with\nreconstruction errors serving as anomaly scores. Additionally, we propose a\nSoft Mining Loss to prioritize hard-to-optimize samples during training.\nINP-Former achieves state-of-the-art performance in single-class, multi-class,\nand few-shot AD tasks across MVTec-AD, VisA, and Real-IAD, positioning it as a\nversatile and universal solution for AD. Remarkably, INP-Former also\ndemonstrates some zero-shot AD capability. Furthermore, we propose a soft\nversion of the INP Coherence Loss and enhance INP-Former by incorporating\nresidual learning, leading to the development of INP-Former++. The proposed\nmethod significantly improves detection performance across single-class,\nmulti-class, semi-supervised, few-shot, and zero-shot settings.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03832", "pdf": "https://arxiv.org/pdf/2506.03832", "abs": "https://arxiv.org/abs/2506.03832", "authors": ["Omer Moussa", "Mariya Toneva"], "title": "Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain", "categories": ["cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "comment": "Proceedings of Interspeech 2025", "summary": "Pretrained self-supervised speech models excel in speech tasks but do not\nreflect the hierarchy of human speech processing, as they encode rich semantics\nin middle layers and poor semantics in late layers. Recent work showed that\nbrain-tuning (fine-tuning models using human brain recordings) improves speech\nmodels' semantic understanding. Here, we examine how well brain-tuned models\nfurther reflect the brain's intermediate stages of speech processing. We find\nthat late layers of brain-tuned models substantially improve over pretrained\nmodels in their alignment with semantic language regions. Further layer-wise\nprobing reveals that early layers remain dedicated to low-level acoustic\nfeatures, while late layers become the best at complex high-level tasks. These\nfindings show that brain-tuned models not only perform better but also exhibit\na well-defined hierarchical processing going from acoustic to semantic\nrepresentations, making them better model organisms for human speech\nprocessing.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03675", "pdf": "https://arxiv.org/pdf/2506.03675", "abs": "https://arxiv.org/abs/2506.03675", "authors": ["Jialei Chen", "Xu Zheng", "Danda Pani Paudel", "Luc Van Gool", "Hiroshi Murase", "Daisuke Deguchi"], "title": "BiXFormer: A Robust Framework for Maximizing Modality Effectiveness in Multi-Modal Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Utilizing multi-modal data enhances scene understanding by providing\ncomplementary semantic and geometric information. Existing methods fuse\nfeatures or distill knowledge from multiple modalities into a unified\nrepresentation, improving robustness but restricting each modality's ability to\nfully leverage its strengths in different situations. We reformulate\nmulti-modal semantic segmentation as a mask-level classification task and\npropose BiXFormer, which integrates Unified Modality Matching (UMM) and Cross\nModality Alignment (CMA) to maximize modality effectiveness and handle missing\nmodalities. Specifically, BiXFormer first categorizes multi-modal inputs into\nRGB and X, where X represents any non-RGB modalities, e.g., depth, allowing\nseparate processing for each. This design leverages the well-established\npretraining for RGB, while addressing the relative lack of attention to X\nmodalities. Then, we propose UMM, which includes Modality Agnostic Matching\n(MAM) and Complementary Matching (CM). MAM assigns labels to features from all\nmodalities without considering modality differences, leveraging each modality's\nstrengths. CM then reassigns unmatched labels to remaining unassigned features\nwithin their respective modalities, ensuring that each available modality\ncontributes to the final prediction and mitigating the impact of missing\nmodalities. Moreover, to further facilitate UMM, we introduce CMA, which\nenhances the weaker queries assigned in CM by aligning them with optimally\nmatched queries from MAM. Experiments on both synthetic and real-world\nmulti-modal benchmarks demonstrate the effectiveness of our method, achieving\nsignificant improvements in mIoU of +2.75% and +22.74% over the prior arts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03706", "pdf": "https://arxiv.org/pdf/2506.03706", "abs": "https://arxiv.org/abs/2506.03706", "authors": ["Aditya Gandhamal", "Aniruddh Sikdar", "Suresh Sundaram"], "title": "OV-COAST: Cost Aggregation with Optimal Transport for Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025 Workshop on Transformers for Vision\n  (Non-archival track)", "summary": "Open-vocabulary semantic segmentation (OVSS) entails assigning semantic\nlabels to each pixel in an image using textual descriptions, typically\nleveraging world models such as CLIP. To enhance out-of-domain generalization,\nwe propose Cost Aggregation with Optimal Transport (OV-COAST) for\nopen-vocabulary semantic segmentation. To align visual-language features within\nthe framework of optimal transport theory, we employ cost volume to construct a\ncost matrix, which quantifies the distance between two distributions. Our\napproach adopts a two-stage optimization strategy: in the first stage, the\noptimal transport problem is solved using cost volume via Sinkhorn distance to\nobtain an alignment solution; in the second stage, this solution is used to\nguide the training of the CAT-Seg model. We evaluate state-of-the-art OVSS\nmodels on the MESS benchmark, where our approach notably improves the\nperformance of the cost-aggregation model CAT-Seg with ViT-B backbone,\nachieving superior results, surpassing CAT-Seg by 1.72 % and SAN-B by 4.9 %\nmIoU. The code is available at\nhttps://github.com/adityagandhamal/OV-COAST/}{https://github.com/adityagandhamal/OV-COAST/ .", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03916", "pdf": "https://arxiv.org/pdf/2506.03916", "abs": "https://arxiv.org/abs/2506.03916", "authors": ["Agostina Calabrese", "Tom Sherborne", "Björn Ross", "Mirella Lapata"], "title": "Compositional Generalisation for Explainable Hate Speech Detection", "categories": ["cs.CL"], "comment": null, "summary": "Hate speech detection is key to online content moderation, but current models\nstruggle to generalise beyond their training data. This has been linked to\ndataset biases and the use of sentence-level labels, which fail to teach models\nthe underlying structure of hate speech. In this work, we show that even when\nmodels are trained with more fine-grained, span-level annotations (e.g.,\n\"artists\" is labeled as target and \"are parasites\" as dehumanising comparison),\nthey struggle to disentangle the meaning of these labels from the surrounding\ncontext. As a result, combinations of expressions that deviate from those seen\nduring training remain particularly difficult for models to detect. We\ninvestigate whether training on a dataset where expressions occur with equal\nfrequency across all contexts can improve generalisation. To this end, we\ncreate U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel\ncompositional generalisation benchmark of ~8,000 manually validated posts.\nTraining on a combination of U-PLEAD and real data improves compositional\ngeneralisation while achieving state-of-the-art performance on the\nhuman-sourced PLEAD.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03923", "pdf": "https://arxiv.org/pdf/2506.03923", "abs": "https://arxiv.org/abs/2506.03923", "authors": ["Mohammadamin Shafiei", "Hamidreza Saffari", "Nafise Sadat Moosavi"], "title": "More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are known to be sensitive to input phrasing, but\nthe mechanisms by which semantic cues shape reasoning remain poorly understood.\nWe investigate this phenomenon in the context of comparative math problems with\nobjective ground truth, revealing a consistent and directional framing bias:\nlogically equivalent questions containing the words ``more'', ``less'', or\n``equal'' systematically steer predictions in the direction of the framing\nterm. To study this effect, we introduce MathComp, a controlled benchmark of\n300 comparison scenarios, each evaluated under 14 prompt variants across three\nLLM families. We find that model errors frequently reflect linguistic steering,\nsystematic shifts toward the comparative term present in the prompt.\nChain-of-thought prompting reduces these biases, but its effectiveness varies:\nfree-form reasoning is more robust, while structured formats may preserve or\nreintroduce directional drift. Finally, we show that including demographic\nidentity terms (e.g., ``a woman'', ``a Black person'') in input scenarios\namplifies directional drift, despite identical underlying quantities,\nhighlighting the interplay between semantic framing and social referents. These\nfindings expose critical blind spots in standard evaluation and motivate\nframing-aware benchmarks for diagnosing reasoning robustness and fairness in\nLLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03949", "pdf": "https://arxiv.org/pdf/2506.03949", "abs": "https://arxiv.org/abs/2506.03949", "authors": ["Junnan Zhu", "Jingyi Wang", "Bohan Yu", "Xiaoyu Wu", "Junbo Li", "Lei Wang", "Nan Xu"], "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "agreement", "accuracy", "question answering"], "score": 6}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03737", "pdf": "https://arxiv.org/pdf/2506.03737", "abs": "https://arxiv.org/abs/2506.03737", "authors": ["Hao Yu", "Tangyu Jiang", "Shuning Jia", "Shannan Yan", "Shunning Liu", "Haolong Qian", "Guanghao Li", "Shuting Dong", "Huaisong Zhang", "Chun Yuan"], "title": "ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The Transformer architecture has revolutionized various regions since it was\nproposed, and its effectiveness largely depends on the ability to encode\npositional information. Traditional position encoding methods exhibit\nsignificant limitations due to lack of robustness and flexibility of position.\nTherefore, Rotary Positional Encoding (RoPE) was proposed to alleviate these\nissues, which integrates positional information by rotating the embeddings in\nthe attention mechanism. However, RoPE requires manually defined rotation\nmatrices with limited transformation space, constraining the model's capacity.\nIn this work, we propose ComRoPE, which generalizes RoPE by defining it in\nterms of trainable commuting angle matrices. Specifically, we demonstrate that\npairwise commutativity of these matrices is essential for RoPE to achieve\nscalability and positional robustness. We formally define the RoPE Equation,\nwhich is an essential condition that ensures consistent performance with\nposition offsets. Based on the theoretical analysis, we present two types of\ntrainable commuting angle matrices as sufficient solutions to the RoPE\nequation, which significantly improve performance, surpassing the current\nstate-of-the-art method by 1.6% at training resolution and 2.9% at higher\nresolution on the ImageNet-1K dataset. Furthermore, our framework shows\nversatility in generalizing to existing RoPE formulations and offering new\ninsights for future positional encoding research. To ensure reproducibility,\nthe source code and instructions are available at\nhttps://github.com/Longin-Yu/ComRoPE", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03968", "pdf": "https://arxiv.org/pdf/2506.03968", "abs": "https://arxiv.org/abs/2506.03968", "authors": ["Chiwei Zhu", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding", "categories": ["cs.CL"], "comment": "To be published at ACL 2025", "summary": "The pursuit of diverse, complex, and large-scale instruction data is crucial\nfor automatically aligning large language models (LLMs). While there are\nmethods capable of generating synthetic instructions at scale, they either\nsuffer from limited grounding sources, leading to a narrow distribution, or\nrely on trivial extensions that fail to produce meaningful trajectories in\nterms of complexity. In contrast, instructions that benefit efficient alignment\nare typically crafted with cognitive insights and grounded in real-world use\ncases. In this paper, we synthesize such instructions using attributed\ngrounding, which involves 1) a top-down attribution process that grounds a\nselective set of real instructions to situated users, and 2) a bottom-up\nsynthesis process that leverages web documents to first generate a situation,\nthen a meaningful instruction. This framework allows us to harvest diverse and\ncomplex instructions at scale, utilizing the vast range of web documents.\nSpecifically, we construct a dataset of 1 million instructions, called\nSynthQuestions, and demonstrate that models trained on it achieve leading\nperformance on several common benchmarks, with improvements that continually\nscale with more web corpora. Data, models and codes will be available at\nhttps://github.com/Ignoramus0817/SynthQuestions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04032", "pdf": "https://arxiv.org/pdf/2506.04032", "abs": "https://arxiv.org/abs/2506.04032", "authors": ["Sina Rashidian", "Nan Li", "Jonathan Amar", "Jong Ha Lee", "Sam Pugh", "Eric Yang", "Geoff Masterson", "Myoung Cha", "Yugang Jia", "Akhil Vaid"], "title": "AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data", "categories": ["cs.CL"], "comment": null, "summary": "Background: We present a Patient Simulator that leverages real world patient\nencounters which cover a broad range of conditions and symptoms to provide\nsynthetic test subjects for development and testing of healthcare agentic\nmodels. The simulator provides a realistic approach to patient presentation and\nmulti-turn conversation with a symptom-checking agent. Objectives: (1) To\nconstruct and instantiate a Patient Simulator to train and test an AI health\nagent, based on patient vignettes derived from real EHR data. (2) To test the\nvalidity and alignment of the simulated encounters provided by the Patient\nSimulator to expert human clinical providers. (3) To illustrate the evaluation\nframework of such an LLM system on the generated realistic, data-driven\nsimulations -- yielding a preliminary assessment of our proposed system.\nMethods: We first constructed realistic clinical scenarios by deriving patient\nvignettes from real-world EHR encounters. These vignettes cover a variety of\npresenting symptoms and underlying conditions. We then evaluate the performance\nof the Patient Simulator as a simulacrum of a real patient encounter across\nover 500 different patient vignettes. We leveraged a separate AI agent to\nprovide multi-turn questions to obtain a history of present illness. The\nresulting multiturn conversations were evaluated by two expert clinicians.\nResults: Clinicians scored the Patient Simulator as consistent with the patient\nvignettes in those same 97.7% of cases. The extracted case summary based on the\nconversation history was 99% relevant. Conclusions: We developed a methodology\nto incorporate vignettes derived from real healthcare patient data to build a\nsimulation of patient responses to symptom checking agents. The performance and\nalignment of this Patient Simulator could be used to train and test a\nmulti-turn conversational AI agent at scale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03926", "pdf": "https://arxiv.org/pdf/2506.03926", "abs": "https://arxiv.org/abs/2506.03926", "authors": ["Debarshi Brahma", "Soma Biswas"], "title": "Multiple Stochastic Prompt Tuning for Practical Cross-Domain Few Shot Learning", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we propose a practical cross-domain few-shot learning (pCDFSL)\ntask, where a large-scale pre-trained model like CLIP can be easily deployed on\na target dataset. The goal is to simultaneously classify all unseen classes\nunder extreme domain shifts, by utilizing only a few labeled samples per class.\nThe pCDFSL paradigm is source-free and moves beyond artificially created\nepisodic training and testing regimes followed by existing CDFSL frameworks,\nmaking it more challenging and relevant to real-world applications. Towards\nthat goal, we propose a novel framework, termed MIST (MultIple STochastic\nPrompt tuning), where multiple stochastic prompts are utilized to handle\nsignificant domain and semantic shifts. Specifically, multiple prompts are\nlearnt for each class, effectively capturing multiple peaks in the input data.\nFurthermore, instead of representing the weights of the multiple prompts as\npoint-estimates, we model them as learnable Gaussian distributions with two\ndifferent strategies, encouraging an efficient exploration of the prompt\nparameter space, which mitigate overfitting due to the few labeled training\nsamples. Extensive experiments and comparison with the state-of-the-art methods\non four CDFSL benchmarks adapted to this setting, show the effectiveness of the\nproposed framework.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03942", "pdf": "https://arxiv.org/pdf/2506.03942", "abs": "https://arxiv.org/abs/2506.03942", "authors": ["Theodore Barfoot", "Luis C. Garcia-Peraza-Herrera", "Samet Akcay", "Ben Glocker", "Tom Vercauteren"], "title": "Average Calibration Losses for Reliable Uncertainty in Medical Image Segmentation", "categories": ["cs.CV"], "comment": "12 pages, 5 figures, IEEE TMI submission", "summary": "Deep neural networks for medical image segmentation are often overconfident,\ncompromising both reliability and clinical utility. In this work, we propose\ndifferentiable formulations of marginal L1 Average Calibration Error (mL1-ACE)\nas an auxiliary loss that can be computed on a per-image basis. We compare both\nhard- and soft-binning approaches to directly improve pixel-wise calibration.\nOur experiments on four datasets (ACDC, AMOS, KiTS, BraTS) demonstrate that\nincorporating mL1-ACE significantly reduces calibration errors, particularly\nAverage Calibration Error (ACE) and Maximum Calibration Error (MCE), while\nlargely maintaining high Dice Similarity Coefficients (DSCs). We find that the\nsoft-binned variant yields the greatest improvements in calibration, over the\nDice plus cross-entropy loss baseline, but often compromises segmentation\nperformance, with hard-binned mL1-ACE maintaining segmentation performance,\nalbeit with weaker calibration improvement. To gain further insight into\ncalibration performance and its variability across an imaging dataset, we\nintroduce dataset reliability histograms, an aggregation of per-image\nreliability diagrams. The resulting analysis highlights improved alignment\nbetween predicted confidences and true accuracies. Overall, our approach not\nonly enhances the trustworthiness of segmentation predictions but also shows\npotential for safer integration of deep learning methods into clinical\nworkflows. We share our code here:\nhttps://github.com/cai4cai/Average-Calibration-Losses", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability"], "score": 2}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04065", "pdf": "https://arxiv.org/pdf/2506.04065", "abs": "https://arxiv.org/abs/2506.04065", "authors": ["Muling Wu", "Qi Qian", "Wenhao Liu", "Xiaohua Wang", "Zisu Huang", "Di Liang", "LI Miao", "Shihan Dou", "Changze Lv", "Zhenghua Wang", "Zhibo Xu", "Lina Chen", "Tianlong Li", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance across\nvarious reasoning tasks, yet post-training is constrained by inefficient sample\nutilization and inflexible difficulty samples processing. To address these\nlimitations, we propose Customized Curriculum Learning (CCL), a novel framework\nwith two key innovations. First, we introduce model-adaptive difficulty\ndefinition that customizes curriculum datasets based on each model's individual\ncapabilities rather than using predefined difficulty metrics. Second, we\ndevelop \"Guided Prompting,\" which dynamically reduces sample difficulty through\nstrategic hints, enabling effective utilization of challenging samples that\nwould otherwise degrade performance. Comprehensive experiments on supervised\nfine-tuning and reinforcement learning demonstrate that CCL significantly\noutperforms uniform training approaches across five mathematical reasoning\nbenchmarks, confirming its effectiveness across both paradigms in enhancing\nsample utilization and model performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04054", "pdf": "https://arxiv.org/pdf/2506.04054", "abs": "https://arxiv.org/abs/2506.04054", "authors": ["Giyong Choi", "HyunWook Park"], "title": "Video Deblurring with Deconvolution and Aggregation Networks", "categories": ["cs.CV"], "comment": null, "summary": "In contrast to single-image deblurring, video deblurring has the advantage\nthat neighbor frames can be utilized to deblur a target frame. However,\nexisting video deblurring algorithms often fail to properly employ the neighbor\nframes, resulting in sub-optimal performance. In this paper, we propose a\ndeconvolution and aggregation network (DAN) for video deblurring that utilizes\nthe information of neighbor frames well. In DAN, both deconvolution and\naggregation strategies are achieved through three sub-networks: the\npreprocessing network (PPN) and the alignment-based deconvolution network\n(ABDN) for the deconvolution scheme; the frame aggregation network (FAN) for\nthe aggregation scheme. In the deconvolution part, blurry inputs are first\npreprocessed by the PPN with non-local operations. Then, the output frames from\nthe PPN are deblurred by the ABDN based on the frame alignment. In the FAN,\nthese deblurred frames from the deconvolution part are combined into a latent\nframe according to reliability maps which infer pixel-wise sharpness. The\nproper combination of three sub-networks can achieve favorable performance on\nvideo deblurring by using the neighbor frames suitably. In experiments, the\nproposed DAN was demonstrated to be superior to existing state-of-the-art\nmethods through both quantitative and qualitative evaluations on the public\ndatasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04134", "pdf": "https://arxiv.org/pdf/2506.04134", "abs": "https://arxiv.org/abs/2506.04134", "authors": ["Jinting Wang", "Shan Yang", "Li Liu"], "title": "UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "10 pages, 10 figures", "summary": "Cued Speech (CS) enhances lipreading through hand coding, providing precise\nspeech perception support for the hearing-impaired. CS Video-to-Speech\ngeneration (CSV2S) task aims to convert the CS visual expressions (CS videos)\nof hearing-impaired individuals into comprehensible speech signals. Direct\ngeneration of speech from CS video (called single CSV2S) yields poor\nperformance due to insufficient CS data. Current research mostly focuses on CS\nRecognition (CSR), which convert video content into linguistic text. Based on\nthis, one straightforward way of CSV2S is to combine CSR with a Text-to-Speech\nsystem. This combined architecture relies on text as an intermediate medium for\nstepwise cross-modal alignment, which may lead to error propagation and\ntemporal misalignment between speech and video dynamics. To address these\nchallenges, we propose a novel approach that directly generates speech from CS\nvideos without relying on intermediate text. Building upon this, we propose\nUniCUE, the first unified framework for CSV2S, whose core innovation lies in\nthe integration of the CSR task that provides fine-grained visual-semantic\ninformation to facilitate speech generation from CS videos. More precisely, (1)\na novel fine-grained semantic alignment pool to ensure precise mapping between\nvisual features and speech contents; (2) a VisioPhonetic adapter to bridge\ncross-task representations, ensuring seamless compatibility between two\ndistinct tasks (i.e., CSV2S and CSR); (3) a pose-aware visual processor is\nintroduced to enhance fine-grained spatiotemporal correlations between lip and\nhand movements in CS video. Experiments on our new established Chinese CS\ndataset (14 cuers1: 8 hearing-impaired and 6 normal-hearing) show that our\nUniCUE significantly reduces Word Error Rate by 78.3% and improves lip-speech\nsynchronization by 32% compared to the single CSV2S.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04108", "pdf": "https://arxiv.org/pdf/2506.04108", "abs": "https://arxiv.org/abs/2506.04108", "authors": ["Yutao Sun", "Tianzhu Ye", "Li Dong", "Yuqing Xia", "Jian Chen", "Yizhao Gao", "Shijie Cao", "Jianyong Wang", "Furu Wei"], "title": "Rectified Sparse Attention", "categories": ["cs.CL"], "comment": null, "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04143", "pdf": "https://arxiv.org/pdf/2506.04143", "abs": "https://arxiv.org/abs/2506.04143", "authors": ["Ngoc Q. Ly", "Hieu N. M. Cao", "Thi T. Nguyen"], "title": "Person Re-Identification System at Semantic Level based on Pedestrian Attributes Ontology", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Person Re-Identification (Re-ID) is a very important task in video\nsurveillance systems such as tracking people, finding people in public places,\nor analysing customer behavior in supermarkets. Although there have been many\nworks to solve this problem, there are still remaining challenges such as\nlarge-scale datasets, imbalanced data, viewpoint, fine grained data\n(attributes), the Local Features are not employed at semantic level in online\nstage of Re-ID task, furthermore, the imbalanced data problem of attributes are\nnot taken into consideration. This paper has proposed a Unified Re-ID system\nconsisted of three main modules such as Pedestrian Attribute Ontology (PAO),\nLocal Multi-task DCNN (Local MDCNN), Imbalance Data Solver (IDS). The new main\npoint of our Re-ID system is the power of mutual support of PAO, Local MDCNN\nand IDS to exploit the inner-group correlations of attributes and pre-filter\nthe mismatch candidates from Gallery set based on semantic information as\nFashion Attributes and Facial Attributes, to solve the imbalanced data of\nattributes without adjusting network architecture and data augmentation. We\nexperimented on the well-known Market1501 dataset. The experimental results\nhave shown the effectiveness of our Re-ID system and it could achieve the\nhigher performance on Market1501 dataset in comparison to some state-of-the-art\nRe-ID methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04209", "pdf": "https://arxiv.org/pdf/2506.04209", "abs": "https://arxiv.org/abs/2506.04209", "authors": ["Jingfeng Yang", "Ziyang Wu", "Yue Zhao", "Yi Ma"], "title": "Language-Image Alignment with Fixed Text Encoders", "categories": ["cs.CV"], "comment": null, "summary": "Currently, the most dominant approach to establishing language-image\nalignment is to pre-train text and image encoders jointly through contrastive\nlearning, such as CLIP and its variants. In this work, we question whether such\na costly joint training is necessary. In particular, we investigate if a\npre-trained fixed large language model (LLM) offers a good enough text encoder\nto guide visual representation learning. That is, we propose to learn\nLanguage-Image alignment with a Fixed Text encoder (LIFT) from an LLM by\ntraining only the image encoder. Somewhat surprisingly, through comprehensive\nbenchmarking and ablation studies, we find that this much simplified framework\nLIFT is highly effective and it outperforms CLIP in most scenarios that involve\ncompositional understanding and long captions, while achieving considerable\ngains in computational efficiency. Our work takes a first step towards\nsystematically exploring how text embeddings from LLMs can guide visual\nlearning and suggests an alternative design choice for learning\nlanguage-aligned visual representations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04214", "pdf": "https://arxiv.org/pdf/2506.04214", "abs": "https://arxiv.org/abs/2506.04214", "authors": ["Tingle Li", "Baihe Huang", "Xiaobin Zhuang", "Dongya Jia", "Jiawei Chen", "Yuping Wang", "Zhuo Chen", "Gopala Anumanchipalli", "Yuxuan Wang"], "title": "Sounding that Object: Interactive Object-Aware Image to Audio Generation", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "ICML 2025", "summary": "Generating accurate sounds for complex audio-visual scenes is challenging,\nespecially in the presence of multiple objects and sound sources. In this\npaper, we propose an {\\em interactive object-aware audio generation} model that\ngrounds sound generation in user-selected visual objects within images. Our\nmethod integrates object-centric learning into a conditional latent diffusion\nmodel, which learns to associate image regions with their corresponding sounds\nthrough multi-modal attention. At test time, our model employs image\nsegmentation to allow users to interactively generate sounds at the {\\em\nobject} level. We theoretically validate that our attention mechanism\nfunctionally approximates test-time segmentation masks, ensuring the generated\naudio aligns with selected objects. Quantitative and qualitative evaluations\nshow that our model outperforms baselines, achieving better alignment between\nobjects and their associated sounds. Project page:\nhttps://tinglok.netlify.app/files/avobject/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04185", "pdf": "https://arxiv.org/pdf/2506.04185", "abs": "https://arxiv.org/abs/2506.04185", "authors": ["Qingfei Zhao", "Ruobing Wang", "Dingling Xu", "Daren Zha", "Limin Liu"], "title": "R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning", "categories": ["cs.CL"], "comment": "16 pages, 3 figures", "summary": "Large language models (LLMs) have notably progressed in multi-step and\nlong-chain reasoning. However, extending their reasoning capabilities to\nencompass deep interactions with search remains a non-trivial challenge, as\nmodels often fail to identify optimal reasoning-search interaction\ntrajectories, resulting in suboptimal responses. We propose R-Search, a novel\nreinforcement learning framework for Reasoning-Search integration, designed to\nenable LLMs to autonomously execute multi-step reasoning with deep search\ninteraction, and learn optimal reasoning search interaction trajectories via\nmulti-reward signals, improving response quality in complex logic- and\nknowledge-intensive tasks. R-Search guides the LLM to dynamically decide when\nto retrieve or reason, while globally integrating key evidence to enhance deep\nknowledge interaction between reasoning and search. During RL training,\nR-Search provides multi-stage, multi-type rewards to jointly optimize the\nreasoning-search trajectory. Experiments on seven datasets show that R-Search\noutperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%\n(out-of-domain). The code and data are available at\nhttps://github.com/QingFei1/R-Search.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2505.24073", "pdf": "https://arxiv.org/pdf/2505.24073", "abs": "https://arxiv.org/abs/2505.24073", "authors": ["Chan-Wei Hu", "Yueqi Wang", "Shuo Xing", "Chia-Ju Chen", "Zhengzhong Tu"], "title": "mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "16 pages, 11 figures", "summary": "Large Vision-Language Models (LVLMs) have made remarkable strides in\nmultimodal tasks such as visual question answering, visual grounding, and\ncomplex reasoning. However, they remain limited by static training data,\nsusceptibility to hallucinations, and inability to verify claims against\nup-to-date, external evidence, compromising their performance in dynamic\nreal-world applications. Retrieval-Augmented Generation (RAG) offers a\npractical solution to mitigate these challenges by allowing the LVLMs to access\nlarge-scale knowledge databases via retrieval mechanisms, thereby grounding\nmodel outputs in factual, contextually relevant information. Here in this\npaper, we conduct the first systematic dissection of the multimodal RAG\npipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the\nmodality configurations and retrieval strategies, (2) the re-ranking stage: on\nstrategies to mitigate positional biases and improve the relevance of retrieved\nevidence, and (3) the generation phase: we further investigate how to best\nintegrate retrieved candidates into the final generation process. Finally, we\nextend to explore a unified agentic framework that integrates re-ranking and\ngeneration through self-reflection, enabling LVLMs to select relevant evidence\nand suppress irrelevant context dynamically. Our full-stack exploration of RAG\nfor LVLMs yields substantial insights, resulting in an average performance\nboost of 5% without any fine-tuning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03197", "pdf": "https://arxiv.org/pdf/2506.03197", "abs": "https://arxiv.org/abs/2506.03197", "authors": ["Baode Wang", "Biao Wu", "Weizhen Li", "Meng Fang", "Yanjie Liang", "Zuming Huang", "Haozhe Wang", "Jun Huang", "Ling Chen", "Wei Chu", "Yuan Qi"], "title": "Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "16 pages, 12 figures", "summary": "Automated parsing of scanned documents into richly structured,\nmachine-readable formats remains a critical bottleneck in Document AI, as\ntraditional multi-stage pipelines suffer from error propagation and limited\nadaptability to diverse layouts. We introduce layoutRL, an end-to-end\nreinforcement learning framework that trains models to be explicitly\nlayout-aware by optimizing a composite reward of normalized edit distance,\nparagraph count accuracy, and reading order preservation. Leveraging our newly\nreleased dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic\nscanned document parsing data with expert-filtered real-world documents, we\ninstantiate layoutRL in a vision-language-model-based parser called\nInfinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and\nformula extraction, and reading order detection, Infinity-Parser achieves new\nstate-of-the-art performance in both accuracy and structural fidelity,\noutpacing specialist pipelines and general-purpose vision-language models. We\nwill publicly release our code and dataset to accelerate progress in robust\ndocument understanding.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03230", "pdf": "https://arxiv.org/pdf/2506.03230", "abs": "https://arxiv.org/abs/2506.03230", "authors": ["Selcuk Gurses", "Aozhong Zhang", "Yanxia Deng", "Xun Dong", "Xin Li", "Naigang Wang", "Penghang Yin", "Zi Yang"], "title": "DiaBlo: Diagonal Blocks Are Sufficient For Finetuning", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC"], "comment": null, "summary": "Finetuning is a critical step for adapting large language models (LLMs) to\ndomain-specific downstream tasks. To mitigate the substantial computational and\nmemory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT)\nmethods have been proposed to update only a small subset of model parameters.\nHowever, performance gaps between PEFT approaches and full-model fine-tuning\nstill exist. In this work, we present DiaBlo, a simple yet effective PEFT\napproach that updates only the diagonal blocks of selected model weight\nmatrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates\nthe need for low rank matrix products, thereby avoiding the reliance on\nauxiliary initialization schemes or customized optimization strategies to\nimprove convergence. This design leads to stable and robust convergence while\nmaintaining comparable memory efficiency and training speed to LoRA. We conduct\nextensive experiments across a range of tasks, including commonsense reasoning,\narithmetic reasoning, code generation, and safety alignment, to evaluate the\neffectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo\ndemonstrates strong and consistent performance while maintaining high memory\nefficiency and fast finetuning speed. Codes are available at\nhttps://github.com/ziyangjoy/DiaBlo.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "code generation"], "score": 2}}, "source_file": "2025-06-05.jsonl"}
{"id": "2505.24073", "pdf": "https://arxiv.org/pdf/2505.24073", "abs": "https://arxiv.org/abs/2505.24073", "authors": ["Chan-Wei Hu", "Yueqi Wang", "Shuo Xing", "Chia-Ju Chen", "Zhengzhong Tu"], "title": "mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "16 pages, 11 figures", "summary": "Large Vision-Language Models (LVLMs) have made remarkable strides in\nmultimodal tasks such as visual question answering, visual grounding, and\ncomplex reasoning. However, they remain limited by static training data,\nsusceptibility to hallucinations, and inability to verify claims against\nup-to-date, external evidence, compromising their performance in dynamic\nreal-world applications. Retrieval-Augmented Generation (RAG) offers a\npractical solution to mitigate these challenges by allowing the LVLMs to access\nlarge-scale knowledge databases via retrieval mechanisms, thereby grounding\nmodel outputs in factual, contextually relevant information. Here in this\npaper, we conduct the first systematic dissection of the multimodal RAG\npipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the\nmodality configurations and retrieval strategies, (2) the re-ranking stage: on\nstrategies to mitigate positional biases and improve the relevance of retrieved\nevidence, and (3) the generation phase: we further investigate how to best\nintegrate retrieved candidates into the final generation process. Finally, we\nextend to explore a unified agentic framework that integrates re-ranking and\ngeneration through self-reflection, enabling LVLMs to select relevant evidence\nand suppress irrelevant context dynamically. Our full-stack exploration of RAG\nfor LVLMs yields substantial insights, resulting in an average performance\nboost of 5% without any fine-tuning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03370", "pdf": "https://arxiv.org/pdf/2506.03370", "abs": "https://arxiv.org/abs/2506.03370", "authors": ["Leonid Ryvkin"], "title": "Comparison of different Unique hard attention transformer models by the formal languages they can recognize", "categories": ["cs.LG", "cs.CL", "cs.FL"], "comment": null, "summary": "This note is a survey of various results on the capabilities of unique hard\nattention transformers encoders (UHATs) to recognize formal languages. We\ndistinguish between masked vs. non-masked, finite vs. infinite image and\ngeneral vs. bilinear attention score functions. We recall some relations\nbetween these models, as well as a lower bound in terms of first-order logic\nand an upper bound in terms of circuit complexity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03444", "pdf": "https://arxiv.org/pdf/2506.03444", "abs": "https://arxiv.org/abs/2506.03444", "authors": ["Yue Gong", "Raul Castro Fernandez"], "title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior", "categories": ["cs.LG", "cs.CL"], "comment": "Under Review", "summary": "As hypothesis generation becomes increasingly automated, a new bottleneck has\nemerged: hypothesis assessment. Modern systems can surface thousands of\nstatistical relationships-correlations, trends, causal links-but offer little\nguidance on which ones are novel, non-trivial, or worthy of expert attention.\nIn this work, we study the complementary problem to hypothesis generation:\nautomatic hypothesis assessment. Specifically, we ask: given a large set of\nstatistical relationships, can we automatically assess which ones are novel and\nworth further exploration? We focus on correlations as they are a common entry\npoint in exploratory data analysis that often serve as the basis for forming\ndeeper scientific or causal hypotheses.\n  To support automatic assessment, we propose to leverage the vast knowledge\nencoded in LLMs' weights to derive a prior distribution over the correlation\nvalue of a variable pair. If an LLM's prior expects the correlation value\nobserved, then such correlation is not surprising, and vice versa. We propose\nthe Logit-based Calibrated Prior, an LLM-elicited correlation prior that\ntransforms the model's raw output logits into a calibrated, continuous\npredictive distribution over correlation values. We evaluate the prior on a\nbenchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of\n78.8%, a mean absolute error of 0.26, and 95% credible interval coverage of\n89.2% in predicting Pearson correlation coefficient. It also outperforms a\nfine-tuned RoBERTa classifier in binary correlation prediction and achieves\nhigher precision@K in hypothesis ranking. We further show that the prior\ngeneralizes to correlations not seen during LLM pretraining, reflecting\ncontext-sensitive reasoning rather than memorization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03487", "pdf": "https://arxiv.org/pdf/2506.03487", "abs": "https://arxiv.org/abs/2506.03487", "authors": ["Xianming Li", "Aamir Shakir", "Rui Huang", "Julius Lipp", "Jing Li"], "title": "ProRank: Prompt Warmup via Reinforcement Learning for Small Language Models Reranking", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Reranking is fundamental to information retrieval and retrieval-augmented\ngeneration, with recent Large Language Models (LLMs) significantly advancing\nreranking quality. While recent advances with LLMs have significantly improved\ndocument reranking quality, current approaches primarily rely on large-scale\nLLMs (>7B parameters) through zero-shot prompting, presenting high\ncomputational costs. Small Language Models (SLMs) offer a promising alternative\nbecause of their efficiency, but our preliminary quantitative analysis reveals\nthey struggle with understanding task prompts without fine-tuning. This limits\ntheir effectiveness for document reranking tasks. To address this issue, we\nintroduce a novel two-stage training approach, ProRank, for SLM-based document\nreranking. First, we propose a prompt warmup stage using reinforcement learning\nGRPO to steer SLMs to understand task prompts and generate more accurate\ncoarse-grained binary relevance scores for document reranking. Then, we\ncontinuously fine-tune the SLMs with a fine-grained score learning stage\nwithout introducing additional layers to further improve the reranking quality.\nComprehensive experimental results demonstrate that the proposed ProRank\nconsistently outperforms both the most advanced open-source and proprietary\nreranking models. Notably, our lightweight ProRank-0.5B model even surpasses\nthe powerful 32B LLM reranking model on the BEIR benchmark, establishing that\nproperly trained SLMs can achieve superior document reranking performance while\nmaintaining computational efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04018", "pdf": "https://arxiv.org/pdf/2506.04018", "abs": "https://arxiv.org/abs/2506.04018", "authors": ["Akshat Naik", "Patrick Quinn", "Guillermo Bosch", "Emma Gouné", "Francisco Javier Campos Zabala", "Jason Ross Brown", "Edward James Young"], "title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "I.2.7; I.2.11; K.4.1; I.2.6"], "comment": "Prepint, under review for NeurIPS 2025", "summary": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. Prior work has examined agents' ability to enact\nmisaligned behaviour (misalignment capability) and their compliance with\nharmful instructions (misuse propensity). However, the likelihood of agents\nattempting misaligned behaviours in real-world settings (misalignment\npropensity) remains poorly understood. We introduce a misalignment propensity\nbenchmark, AgentMisalignment, consisting of a suite of realistic scenarios in\nwhich LLM agents have the opportunity to display misaligned behaviour. We\norganise our evaluations into subcategories of misaligned behaviours, including\ngoal-guarding, resisting shutdown, sandbagging, and power-seeking. We report\nthe performance of frontier models on our benchmark, observing higher\nmisalignment on average when evaluating more capable models. Finally, we\nsystematically vary agent personalities through different system prompts. We\nfind that persona characteristics can dramatically and unpredictably influence\nmisalignment tendencies -- occasionally far more than the choice of model\nitself -- highlighting the importance of careful system prompt engineering for\ndeployed AI agents. Our work highlights the failure of current alignment\nmethods to generalise to LLM agents, and underscores the need for further\npropensity evaluations as autonomous systems become more prevalent.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04089", "pdf": "https://arxiv.org/pdf/2506.04089", "abs": "https://arxiv.org/abs/2506.04089", "authors": ["Anastasiia Ivanova", "Eva Bakaeva", "Zoya Volovikova", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "comment": "ACL 2025 (Main Conference)", "summary": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04207", "pdf": "https://arxiv.org/pdf/2506.04207", "abs": "https://arxiv.org/abs/2506.04207", "authors": ["Shuang Chen", "Yue Guo", "Zhaochen Su", "Yafu Li", "Yulun Wu", "Jiacheng Chen", "Jiayu Chen", "Weijie Wang", "Xiaoye Qu", "Yu Cheng"], "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "19 pages, 6 figures", "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.03378", "pdf": "https://arxiv.org/pdf/2506.03378", "abs": "https://arxiv.org/abs/2506.03378", "authors": ["Orchid Chetia Phukan", "Mohd Mujtaba Akhtar", "Girish", "Swarup Ranjan Behera", "Abu Osama Siddiqui", "Sarthak Jain", "Priyabrata Mallick", "Jaya Sai Kiran Patibandla", "Pailla Balakrishna Reddy", "Arun Balaji Buduru", "Rajesh Sharma"], "title": "SNIFR : Boosting Fine-Grained Child Harmful Content Detection Through Audio-Visual Alignment with Cascaded Cross-Transformer", "categories": ["eess.AS", "cs.CV", "cs.MM"], "comment": "Accepted to INTERSPEECH 2025", "summary": "As video-sharing platforms have grown over the past decade, child viewership\nhas surged, increasing the need for precise detection of harmful content like\nviolence or explicit scenes. Malicious users exploit moderation systems by\nembedding unsafe content in minimal frames to evade detection. While prior\nresearch has focused on visual cues and advanced such fine-grained detection,\naudio features remain underexplored. In this study, we embed audio cues with\nvisual for fine-grained child harmful content detection and introduce SNIFR, a\nnovel framework for effective alignment. SNIFR employs a transformer encoder\nfor intra-modality interaction, followed by a cascaded cross-transformer for\ninter-modality alignment. Our approach achieves superior performance over\nunimodal and baseline fusion methods, setting a new state-of-the-art.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04058", "pdf": "https://arxiv.org/pdf/2506.04058", "abs": "https://arxiv.org/abs/2506.04058", "authors": ["Bulat Maksudov", "Kathleen Curran", "Alessandra Mileo"], "title": "Towards generating more interpretable counterfactuals via concept vectors: a preliminary study on chest X-rays", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "An essential step in deploying medical imaging models is ensuring alignment\nwith clinical knowledge and interpretability. We focus on mapping clinical\nconcepts into the latent space of generative models to identify Concept\nActivation Vectors (CAVs). Using a simple reconstruction autoencoder, we link\nuser-defined concepts to image-level features without explicit label training.\nThe extracted concepts are stable across datasets, enabling visual explanations\nthat highlight clinically relevant features. By traversing latent space along\nconcept directions, we produce counterfactuals that exaggerate or reduce\nspecific clinical features. Preliminary results on chest X-rays show promise\nfor large pathologies like cardiomegaly, while smaller pathologies remain\nchallenging due to reconstruction limits. Although not outperforming baselines,\nthis approach offers a path toward interpretable, concept-based explanations\naligned with clinical knowledge.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04071", "pdf": "https://arxiv.org/pdf/2506.04071", "abs": "https://arxiv.org/abs/2506.04071", "authors": ["Luiz Manella Pereira", "M. Hadi Amini"], "title": "Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Federated learning (FL) is a subfield of machine learning that avoids sharing\nlocal data with a central server, which can enhance privacy and scalability.\nThe inability to consolidate data leads to a unique problem called dataset\nimbalance, where agents in a network do not have equal representation of the\nlabels one is trying to learn to predict. In FL, fusing locally-trained models\nwith unbalanced datasets may deteriorate the performance of global model\naggregation, and reduce the quality of updated local models and the accuracy of\nthe distributed agents' decisions. In this work, we introduce an Optimal\nTransport-based preprocessing algorithm that aligns the datasets by minimizing\nthe distributional discrepancy of data along the edge devices. We accomplish\nthis by leveraging Wasserstein barycenters when computing channel-wise\naverages. These barycenters are collected in a trusted central server where\nthey collectively generate a target RGB space. By projecting our dataset\ntowards this target space, we minimize the distributional discrepancy on a\nglobal level, which facilitates the learning process due to a minimization of\nvariance across the samples. We demonstrate the capabilities of the proposed\napproach over the CIFAR-10 dataset, where we show its capability of reaching\nhigher degrees of generalization in fewer communication rounds.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-05.jsonl"}
{"id": "2506.04207", "pdf": "https://arxiv.org/pdf/2506.04207", "abs": "https://arxiv.org/abs/2506.04207", "authors": ["Shuang Chen", "Yue Guo", "Zhaochen Su", "Yafu Li", "Yulun Wu", "Jiacheng Chen", "Jiayu Chen", "Weijie Wang", "Xiaoye Qu", "Yu Cheng"], "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "19 pages, 6 figures", "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-05.jsonl"}
