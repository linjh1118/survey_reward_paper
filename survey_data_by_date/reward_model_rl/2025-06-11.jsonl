{"id": "2506.08266", "pdf": "https://arxiv.org/pdf/2506.08266", "abs": "https://arxiv.org/abs/2506.08266", "authors": ["Yaswanth Chittepu", "Blossom Metevier", "Will Schwarzer", "Austin Hoag", "Scott Niekum", "Philip S. Thomas"], "title": "Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.AP"], "comment": "20 pages, 6 figures, 4 tables, Second Reinforcement Learning\n  Conference (RLC 2025)", "summary": "Existing approaches to language model alignment often treat safety as a\ntradeoff against helpfulness, which can lead to unacceptable responses in\nsensitive domains. To ensure reliable performance in such settings, we propose\nHigh-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a\nmethod that provides high-confidence safety guarantees while maximizing\nhelpfulness. Similar to previous methods, HC-RLHF explicitly decouples human\npreferences into helpfulness and harmlessness (safety), which are learned by\ntraining a reward model and a cost model, respectively. It then employs a\ntwo-step process to find safe solutions. In the first step, it optimizes the\nreward function under an intentionally pessimistic version of the cost\nconstraint. In the second step, the trained model undergoes a safety test to\nverify whether its performance stays within an upper-confidence bound of the\nactual cost constraint. We provide a theoretical analysis of HC-RLHF, including\nproof that it will not return an unsafe solution with a probability greater\nthan a user-specified threshold. For our empirical analysis, we apply HC-RLHF\nto align three different language models (Qwen2-1.5B, Qwen2.5-3B, and\nLLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF\nproduces safe models with high probability and can improve harmlessness and\nhelpfulness compared to previous methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward function", "RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "alignment"], "score": 7}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["helpfulness", "harmlessness", "safety"], "score": 3}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08712", "pdf": "https://arxiv.org/pdf/2506.08712", "abs": "https://arxiv.org/abs/2506.08712", "authors": ["Hee Suk Yoon", "Eunseop Yoon", "Mark A. Hasegawa-Johnson", "Sungwoong Kim", "Chang D. Yoo"], "title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "We introduce ConfPO, a method for preference learning in Large Language\nModels (LLMs) that identifies and optimizes preference-critical tokens based\nsolely on the training policy's confidence, without requiring any auxiliary\nmodels or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as\nDirect Preference Optimization (DPO), which uniformly adjust all token\nprobabilities regardless of their relevance to preference, ConfPO focuses\noptimization on the most impactful tokens. This targeted approach improves\nalignment quality while mitigating overoptimization (i.e., reward hacking) by\nusing the KL divergence budget more efficiently. In contrast to recent\ntoken-level methods that rely on credit-assignment models or AI annotators,\nraising concerns about scalability and reliability, ConfPO is simple,\nlightweight, and model-free. Experimental results on challenging alignment\nbenchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO\nconsistently outperforms uniform DAAs across various LLMs, delivering better\nalignment with zero additional computational overhead.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "alignment", "reward hacking", "DPO", "direct preference optimization"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08885", "pdf": "https://arxiv.org/pdf/2506.08885", "abs": "https://arxiv.org/abs/2506.08885", "authors": ["Danush Khanna", "Krishna Kumar", "Basab Ghosh", "Vinija Jain", "Vasu Sharma", "Aman Chadha", "Amitava Das"], "title": "AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Adversarial threats against LLMs are escalating faster than current defenses\ncan adapt. We expose a critical geometric blind spot in alignment: adversarial\nprompts exploit latent camouflage, embedding perilously close to the safe\nrepresentation manifold while encoding unsafe intent thereby evading surface\nlevel defenses like Direct Preference Optimization (DPO), which remain blind to\nthe latent geometry. We introduce ALKALI, the first rigorously curated\nadversarial benchmark and the most comprehensive to date spanning 9,000 prompts\nacross three macro categories, six subtypes, and fifteen attack families.\nEvaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates\n(ASRs) across both open and closed source models, exposing an underlying\nvulnerability we term latent camouflage, a structural blind spot where\nadversarial completions mimic the latent geometry of safe ones. To mitigate\nthis vulnerability, we introduce GRACE - Geometric Representation Aware\nContrastive Enhancement, an alignment framework coupling preference learning\nwith latent space regularization. GRACE enforces two constraints: latent\nseparation between safe and adversarial completions, and adversarial cohesion\namong unsafe and jailbreak behaviors. These operate over layerwise pooled\nembeddings guided by a learned attention profile, reshaping internal geometry\nwithout modifying the base model, and achieve up to 39% ASR reduction.\nMoreover, we introduce AVQI, a geometry aware metric that quantifies latent\nalignment failure via cluster separation and compactness. AVQI reveals when\nunsafe completions mimic the geometry of safe ones, offering a principled lens\ninto how models internally encode safety. We make the code publicly available\nat https://anonymous.4open.science/r/alkali-B416/README.md.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "alignment", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "safety"], "score": 3}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08123", "pdf": "https://arxiv.org/pdf/2506.08123", "abs": "https://arxiv.org/abs/2506.08123", "authors": ["Jacob Dineen", "Aswin RRV", "Qin Liu", "Zhikun Xu", "Xiao Ye", "Ming Shen", "Zhaonan Li", "Shijie Lu", "Chitta Baral", "Muhao Chen", "Ben Zhou"], "title": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA", "categories": ["cs.CL"], "comment": null, "summary": "Alignment of large language models with explicit principles (such as\nhelpfulness, honesty, and harmlessness) is crucial for ensuring safe and\nreliable AI systems. However, standard reward-based alignment methods typically\ncollapse diverse feedback into a single scalar reward, entangling multiple\nobjectives into one opaque training signal, which hinders interpretability. In\nthis work, we introduce QA-LIGN, an automatic symbolic reward decomposition\napproach that preserves the structure of each constitutional principle within\nthe reward mechanism. Instead of training a black-box reward model that outputs\na monolithic score, QA-LIGN formulates principle-specific evaluation questions\nand derives separate reward components for each principle, making it a drop-in\nreward model replacement. Experiments aligning an uncensored large language\nmodel with a set of constitutional principles demonstrate that QA-LIGN offers\ngreater transparency and adaptability in the alignment process. At the same\ntime, our approach achieves performance on par with or better than a DPO\nbaseline. Overall, these results represent a step toward more interpretable and\ncontrollable alignment of language models, achieved without sacrificing\nend-task performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "alignment", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "helpfulness", "harmlessness", "honesty"], "score": 4}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.09014", "pdf": "https://arxiv.org/pdf/2506.09014", "abs": "https://arxiv.org/abs/2506.09014", "authors": ["Jianing Qi", "Xi Ye", "Hao Tang", "Zhigang Zhu", "Eunsol Choi"], "title": "Learning to Reason Across Parallel Samples for LLM Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Scaling test-time compute brings substantial performance gains for large\nlanguage models (LLMs). By sampling multiple answers and heuristically\naggregate their answers (e.g., either through majority voting or using\nverifiers to rank the answers), one can achieve consistent performance gains in\nmath domains. In this paper, we propose a new way to leverage such multiple\nsample set. We train a compact LLM, called Sample Set Aggregator (SSA), that\ntakes a concatenated sequence of multiple samples and output the final answer,\noptimizing it for the answer accuracy with reinforcement learning. Experiments\non multiple reasoning datasets show that SSA outperforms other test-time\nscaling methods such as reward model-based re-ranking. Our approach also shows\na promising generalization ability, across sample set sizes, base model\nfamilies and scales, and tasks. By separating LLMs to generate answers and LLMs\nto analyze and aggregate sampled answers, our approach can work with the\noutputs from premier black box models easily and efficiently.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "test-time compute"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning", "ranking"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08022", "pdf": "https://arxiv.org/pdf/2506.08022", "abs": "https://arxiv.org/abs/2506.08022", "authors": ["Chenxi Liu", "Tianyi Xiong", "Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Tianyi Zhou", "Heng Huang"], "title": "Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The task adaptation and alignment of Large Multimodal Models (LMMs) have been\nsignificantly advanced by instruction tuning and further strengthened by recent\npreference optimization. Yet, most LMMs still suffer from severe modality\nimbalance during reasoning, i.e., outweighing language prior biases over visual\ninputs, which bottlenecks their generalization to downstream tasks and causes\nhallucinations. However, existing preference optimization approaches for LMMs\ndo not focus on restraining the internal biases of their Large Language Model\n(LLM) backbones when curating the training data. Moreover, they heavily rely on\noffline data and lack the capacity to explore diverse responses adaptive to\ndynamic distributional shifts during training. Meanwhile, Group Relative Policy\nOptimization (GRPO), a recent method using online-generated data and verified\nrewards to improve reasoning capabilities, remains largely underexplored in LMM\nalignment. In this paper, we propose a novel preference learning framework,\nModality-Balancing Preference Optimization (MBPO), to address the modality\nimbalance in LMMs. MBPO constructs a more effective offline preference dataset\nby generating hard negatives, i.e., rejected responses misled by LLM biases due\nto limited usage of visual information, through adversarial perturbation of\ninput images. Moreover, MBPO leverages the easy-to-verify nature of close-ended\ntasks to generate online responses with verified rewards. GRPO is then employed\nto train the model with offline-online hybrid data. Extensive experiments\ndemonstrate that MBPO can enhance LMM performance on challenging\nvision-language tasks and effectively reduce hallucinations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08022", "pdf": "https://arxiv.org/pdf/2506.08022", "abs": "https://arxiv.org/abs/2506.08022", "authors": ["Chenxi Liu", "Tianyi Xiong", "Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Tianyi Zhou", "Heng Huang"], "title": "Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The task adaptation and alignment of Large Multimodal Models (LMMs) have been\nsignificantly advanced by instruction tuning and further strengthened by recent\npreference optimization. Yet, most LMMs still suffer from severe modality\nimbalance during reasoning, i.e., outweighing language prior biases over visual\ninputs, which bottlenecks their generalization to downstream tasks and causes\nhallucinations. However, existing preference optimization approaches for LMMs\ndo not focus on restraining the internal biases of their Large Language Model\n(LLM) backbones when curating the training data. Moreover, they heavily rely on\noffline data and lack the capacity to explore diverse responses adaptive to\ndynamic distributional shifts during training. Meanwhile, Group Relative Policy\nOptimization (GRPO), a recent method using online-generated data and verified\nrewards to improve reasoning capabilities, remains largely underexplored in LMM\nalignment. In this paper, we propose a novel preference learning framework,\nModality-Balancing Preference Optimization (MBPO), to address the modality\nimbalance in LMMs. MBPO constructs a more effective offline preference dataset\nby generating hard negatives, i.e., rejected responses misled by LLM biases due\nto limited usage of visual information, through adversarial perturbation of\ninput images. Moreover, MBPO leverages the easy-to-verify nature of close-ended\ntasks to generate online responses with verified rewards. GRPO is then employed\nto train the model with offline-online hybrid data. Extensive experiments\ndemonstrate that MBPO can enhance LMM performance on challenging\nvision-language tasks and effectively reduce hallucinations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08967", "pdf": "https://arxiv.org/pdf/2506.08967", "abs": "https://arxiv.org/abs/2506.08967", "authors": ["Ailin Huang", "Bingxin Li", "Bruce Wang", "Boyong Wu", "Chao Yan", "Chengli Feng", "Heng Wang", "Hongyu Zhou", "Hongyuan Wang", "Jingbei Li", "Jianjian Sun", "Joanna Wang", "Mingrui Chen", "Peng Liu", "Ruihang Miao", "Shilei Jiang", "Tian Fei", "Wang You", "Xi Chen", "Xuerui Yang", "Yechang Huang", "Yuxiang Zhang", "Zheng Ge", "Zheng Gong", "Zhewei Huang", "Zixin Zhang", "Bin Wang", "Bo Li", "Buyun Ma", "Changxin Miao", "Changyi Wan", "Chen Xu", "Dapeng Shi", "Dingyuan Hu", "Enle Liu", "Guanzhe Huang", "Gulin Yan", "Hanpeng Hu", "Haonan Jia", "Jiahao Gong", "Jiaoren Wu", "Jie Wu", "Jie Yang", "Junzhe Lin", "Kaixiang Li", "Lei Xia", "Longlong Gu", "Ming Li", "Nie Hao", "Ranchen Ming", "Shaoliang Pang", "Siqi Liu", "Song Yuan", "Tiancheng Cao", "Wen Li", "Wenqing He", "Xu Zhao", "Xuelin Zhang", "Yanbo Yu", "Yinmin Zhong", "Yu Zhou", "Yuanwei Liang", "Yuanwei Lu", "Yuxiang Yang", "Zidong Yang", "Zili Zhang", "Binxing Jiao", "Heung-Yeung Shum", "Jiansheng Chen", "Jing Li", "Xiangyu Zhang", "Xinhao Zhang", "Yibo Zhu", "Daxin Jiang", "Shuchang Zhou", "Chen Hu"], "title": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "12 pages, 3 figures", "summary": "Large Audio-Language Models (LALMs) have significantly advanced intelligent\nhuman-computer interaction, yet their reliance on text-based outputs limits\ntheir ability to generate natural speech responses directly, hindering seamless\naudio interactions. To address this, we introduce Step-Audio-AQAA, a fully\nend-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model\nintegrates a dual-codebook audio tokenizer for linguistic and semantic feature\nextraction, a 130-billion-parameter backbone LLM and a neural vocoder for\nhigh-fidelity speech synthesis. Our post-training approach employs interleaved\ntoken-output of text and audio to enhance semantic coherence and combines\nDirect Preference Optimization (DPO) with model merge to improve performance.\nEvaluations on the StepEval-Audio-360 benchmark demonstrate that\nStep-Audio-AQAA excels especially in speech control, outperforming the\nstate-of-art LALMs in key areas. This work contributes a promising solution for\nend-to-end LALMs and highlights the critical role of token-based vocoder in\nenhancing overall performance for AQAA tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08189", "pdf": "https://arxiv.org/pdf/2506.08189", "abs": "https://arxiv.org/abs/2506.08189", "authors": ["Amartya Dutta", "Kazi Sajeed Mehrab", "Medha Sawhney", "Abhilash Neog", "Mridul Khurana", "Sepideh Fatemi", "Aanish Pradhan", "M. Maruf", "Ismini Lourentzou", "Arka Daw", "Anuj Karpatne"], "title": "Open World Scene Graph Generation using Vision Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted in CVPR 2025 Workshop (CVinW)", "summary": "Scene-Graph Generation (SGG) seeks to recognize objects in an image and\ndistill their salient pairwise relationships. Most methods depend on\ndataset-specific supervision to learn the variety of interactions, restricting\ntheir usefulness in open-world settings, involving novel objects and/or\nrelations. Even methods that leverage large Vision Language Models (VLMs)\ntypically require benchmark-specific fine-tuning. We introduce Open-World SGG,\na training-free, efficient, model-agnostic framework that taps directly into\nthe pretrained knowledge of VLMs to produce scene graphs with zero additional\nlearning. Casting SGG as a zero-shot structured-reasoning problem, our method\ncombines multimodal prompting, embedding alignment, and a lightweight\npair-refinement strategy, enabling inference over unseen object vocabularies\nand relation sets. To assess this setting, we formalize an Open-World\nevaluation protocol that measures performance when no SGG-specific data have\nbeen observed either in terms of objects and relations. Experiments on Visual\nGenome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate\nthe capacity of pretrained VLMs to perform relational understanding without\ntask-level training.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08643", "pdf": "https://arxiv.org/pdf/2506.08643", "abs": "https://arxiv.org/abs/2506.08643", "authors": ["Son The Nguyen", "Theja Tulabandhula"], "title": "MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly used for both open-ended and\nstructured tasks, yet their inference-time behavior is still largely dictated\nby heuristic decoding strategies such as greedy search, sampling, or reranking.\nThese methods provide limited control and do not explicitly optimize for\ntask-specific objectives. We introduce MEMETRON, a task-agnostic framework that\nformulates LLM decoding as a discrete black-box optimization problem. MEMETRON\nleverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the\nresponse space, guided by reward models and contextual operations performed by\nthe LLM itself. This approach enables efficient discovery of high-reward\nresponses without requiring model retraining or gradient access. The framework\nis modular and generalizes across diverse tasks, requiring only a reward\nfunction and lightweight prompt templates. We evaluate our framework on the\ncritical human preference alignment task and demonstrate that it significantly\noutperforms standard decoding and reranking methods, highlighting its potential\nto improve alignment without model retraining.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08379", "pdf": "https://arxiv.org/pdf/2506.08379", "abs": "https://arxiv.org/abs/2506.08379", "authors": ["Yurun Yuan", "Tengyang Xie"], "title": "Reinforce LLM Reasoning through Multi-Agent Reflection", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "International Conference on Machine Learning (ICML), 2025", "summary": "Leveraging more test-time computation has proven to be an effective way to\nboost the reasoning capabilities of large language models (LLMs). Among various\nmethods, the verify-and-improve paradigm stands out for enabling dynamic\nsolution exploration and feedback incorporation. However, existing approaches\noften suffer from restricted feedback spaces and lack of coordinated training\nof different parties, leading to suboptimal performance. To address this, we\nmodel this multi-turn refinement process as a Markov Decision Process and\nintroduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement\nlearning algorithm that trains an actor-critic LLM system to iteratively refine\nanswers via direct preference learning on self-generated data. Theoretically,\nDPSDP can match the performance of any policy within the training distribution.\nEmpirically, we instantiate DPSDP with various base models and show\nimprovements on both in- and out-of-distribution benchmarks. For example, on\nbenchmark MATH 500, majority voting over five refinement steps increases\nfirst-turn accuracy from 58.2% to 63.2% with Ministral-based models. An\nablation study further confirms the benefits of multi-agent collaboration and\nout-of-distribution generalization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08052", "pdf": "https://arxiv.org/pdf/2506.08052", "abs": "https://arxiv.org/abs/2506.08052", "authors": ["Yongkang Li", "Kaixin Xiong", "Xiangyu Guo", "Fang Li", "Sixu Yan", "Gangwei Xu", "Lijun Zhou", "Long Chen", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wenyu Liu", "Xinggang Wang"], "title": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Although end-to-end autonomous driving has made remarkable progress, its\nperformance degrades significantly in rare and long-tail scenarios. Recent\napproaches attempt to address this challenge by leveraging the rich world\nknowledge of Vision-Language Models (VLMs), but these methods suffer from\nseveral limitations: (1) a significant domain gap between the pre-training data\nof VLMs and real-world driving data, (2) a dimensionality mismatch between the\ndiscrete language space and the continuous action space, and (3) imitation\nlearning tends to capture the average behavior present in the dataset, which\nmay be suboptimal even dangerous. In this paper, we propose ReCogDrive, an\nautonomous driving system that integrates VLMs with diffusion planner, which\nadopts a three-stage paradigm for training. In the first stage, we use a\nlarge-scale driving question-answering datasets to train the VLMs, mitigating\nthe domain discrepancy between generic content and real-world driving\nscenarios. In the second stage, we employ a diffusion-based planner to perform\nimitation learning, mapping representations from the latent language space to\ncontinuous driving actions. Finally, we fine-tune the diffusion planner using\nreinforcement learning with NAVSIM non-reactive simulator, enabling the model\nto generate safer, more human-like driving trajectories. We evaluate our\napproach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6\nand setting a new state-of-the-art that surpasses the previous vision-only SOTA\nby 5.6 PDMS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08071", "pdf": "https://arxiv.org/pdf/2506.08071", "abs": "https://arxiv.org/abs/2506.08071", "authors": ["Aniket Rege", "Zinnia Nie", "Mahesh Ramesh", "Unmesh Raskar", "Zhuoran Yu", "Aditya Kusupati", "Yong Jae Lee", "Ramya Korlakai Vinayak"], "title": "CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems", "categories": ["cs.CV"], "comment": "41 pages, 22 figures, 17 tables", "summary": "Popular text-to-image (T2I) systems are trained on web-scraped data, which is\nheavily Amero and Euro-centric, underrepresenting the cultures of the Global\nSouth. To analyze these biases, we introduce CuRe, a novel and scalable\nbenchmarking and scoring suite for cultural representativeness that leverages\nthe marginal utility of attribute specification to T2I systems as a proxy for\nhuman judgments. Our CuRe benchmark dataset has a novel categorical hierarchy\nbuilt from the crowdsourced Wikimedia knowledge graph, with 300 cultural\nartifacts across 32 cultural subcategories grouped into six broad cultural axes\n(food, art, fashion, architecture, celebrations, and people). Our dataset's\ncategorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing\ntheir response to increasing the informativeness of text conditioning, enabling\nfine-grained cultural comparisons. We empirically observe much stronger\ncorrelations of our class of scorers to human judgments of perceptual\nsimilarity, image-text alignment, and cultural diversity across image encoders\n(SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2,\nGemini 2.0 Flash) and state-of-the-art text-to-image systems, including three\nvariants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0,\nand DALL-E 3. The code and dataset is open-sourced and available at\nhttps://aniketrege.github.io/cure/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08158", "pdf": "https://arxiv.org/pdf/2506.08158", "abs": "https://arxiv.org/abs/2506.08158", "authors": ["Lijing Zhu", "Qizhen Lan", "Qing Tian", "Wenbo Sun", "Li Yang", "Lu Xia", "Yixin Xie", "Xi Xiao", "Tiehang Duan", "Cui Tao", "Shuteng Niu"], "title": "ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding", "categories": ["cs.CL"], "comment": null, "summary": "Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge\nwhile preserving past information. However, existing methods struggle with\nefficiency and scalability due to two key limitations: (1) suboptimal knowledge\npreservation between snapshots caused by manually designed node/relation\nimportance scores that ignore graph dependencies relevant to the downstream\ntask, and (2) computationally expensive graph traversal for node/relation\nimportance calculation, leading to slow training and high memory overhead. To\naddress these limitations, we introduce ETT-CKGE (Efficient, Task-driven,\nTokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE\nmethod that leverages efficient task-driven tokens for efficient and effective\nknowledge transfer between snapshots. Our method introduces a set of learnable\ntokens that directly capture task-relevant signals, eliminating the need for\nexplicit node scoring or traversal. These tokens serve as consistent and\nreusable guidance across snapshots, enabling efficient token-masked embedding\nalignment between snapshots. Importantly, knowledge transfer is achieved\nthrough simple matrix operations, significantly reducing training time and\nmemory usage. Extensive experiments across six benchmark datasets demonstrate\nthat ETT-CKGE consistently achieves superior or competitive predictive\nperformance, while substantially improving training efficiency and scalability\ncompared to state-of-the-art CKGE methods. The code is available at:\nhttps://github.com/lijingzhu1/ETT-CKGE/tree/main", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08210", "pdf": "https://arxiv.org/pdf/2506.08210", "abs": "https://arxiv.org/abs/2506.08210", "authors": ["Andrew Z. Wang", "Songwei Ge", "Tero Karras", "Ming-Yu Liu", "Yogesh Balaji"], "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Both text-to-image generation and large language models (LLMs) have made\nsignificant advancements. However, many text-to-image models still employ the\nsomewhat outdated T5 and CLIP as their text encoders. In this work, we\ninvestigate the effectiveness of using modern decoder-only LLMs as text\nencoders for text-to-image diffusion models. We build a standardized training\nand evaluation pipeline that allows us to isolate and evaluate the effect of\ndifferent text embeddings. We train a total of 27 text-to-image models with 12\ndifferent text encoders to analyze the critical aspects of LLMs that could\nimpact text-to-image generation, including the approaches to extract\nembeddings, different LLMs variants, and model sizes. Our experiments reveal\nthat the de facto way of using last-layer embeddings as conditioning leads to\ninferior performance. Instead, we explore embeddings from various layers and\nfind that using layer-normalized averaging across all layers significantly\nimproves alignment with complex prompts. Most LLMs with this conditioning\noutperform the baseline T5 model, showing enhanced performance in advanced\nvisio-linguistic reasoning skills.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08174", "pdf": "https://arxiv.org/pdf/2506.08174", "abs": "https://arxiv.org/abs/2506.08174", "authors": ["Li Weigang", "Pedro Carvalho Brom"], "title": "LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding", "categories": ["cs.CL"], "comment": "23 pages", "summary": "The rapid growth of English technical terms challenges traditional\nexpert-driven standardization, especially in fast-evolving fields like AI and\nquantum computing. Manual methods struggle to ensure multilingual consistency.\nWe propose \\textbf{LLM-BT}, a back-translation framework powered by large\nlanguage models (LLMs) to automate terminology verification and standardization\nvia cross-lingual semantic alignment. Our contributions are: \\textbf{(1)\nTerm-Level Consistency Validation:} Using English $\\rightarrow$ intermediate\nlanguage $\\rightarrow$ English back-translation, LLM-BT achieves high term\nconsistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies\nshowing over 90\\% exact or semantic matches. \\textbf{(2) Multi-Path\nVerification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize''\npipeline integrates serial (e.g., EN $\\rightarrow$ ZHcn $\\rightarrow$ ZHtw\n$\\rightarrow$ EN) and parallel (e.g., EN $\\rightarrow$ Chinese/Portuguese\n$\\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong\ncross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\\%).\n\\textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as\ndynamic semantic embedding, revealing latent meaning trajectories. Unlike\nstatic embeddings, LLM-BT provides transparent path-based embeddings shaped by\nmodel evolution. LLM-BT transforms back-translation into an active engine for\nmultilingual terminology standardization, enabling human--AI collaboration:\nmachines ensure semantic fidelity, humans guide cultural interpretation. This\ninfrastructure supports terminology governance across scientific and\ntechnological fields worldwide.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08234", "pdf": "https://arxiv.org/pdf/2506.08234", "abs": "https://arxiv.org/abs/2506.08234", "authors": ["Yu-Ang Lee", "Guan-Ting Yi", "Mei-Yi Liu", "Jui-Chao Lu", "Guan-Bo Yang", "Yun-Nung Chen"], "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 figures, 1 table", "summary": "Recent advancements in large language models (LLMs) and AI systems have led\nto a paradigm shift in the design and optimization of complex AI workflows. By\nintegrating multiple components, compound AI systems have become increasingly\nadept at performing sophisticated tasks. However, as these systems grow in\ncomplexity, new challenges arise in optimizing not only individual components\nbut also their interactions. While traditional optimization methods such as\nsupervised fine-tuning (SFT) and reinforcement learning (RL) remain\nfoundational, the rise of natural language feedback introduces promising new\napproaches, especially for optimizing non-differentiable systems. This paper\nprovides a systematic review of recent progress in optimizing compound AI\nsystems, encompassing both numerical and language-based techniques. We\nformalize the notion of compound AI system optimization, classify existing\nmethods along several key dimensions, and highlight open research challenges\nand future directions in this rapidly evolving field. A list of surveyed papers\nis publicly available at https://github.com/MiuLab/AISysOpt-Survey.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08351", "pdf": "https://arxiv.org/pdf/2506.08351", "abs": "https://arxiv.org/abs/2506.08351", "authors": ["Huixuan Zhang", "Junzhe Zhang", "Xiaojun Wan"], "title": "How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "With the rapid development of text-to-vision generation diffusion models,\nclassifier-free guidance has emerged as the most prevalent method for\nconditioning. However, this approach inherently requires twice as many steps\nfor model forwarding compared to unconditional generation, resulting in\nsignificantly higher costs. While previous study has introduced the concept of\nadaptive guidance, it lacks solid analysis and empirical results, making\nprevious method unable to be applied to general diffusion models. In this work,\nwe present another perspective of applying adaptive guidance and propose Step\nAG, which is a simple, universally applicable adaptive guidance strategy. Our\nevaluations focus on both image quality and image-text alignment. whose results\nindicate that restricting classifier-free guidance to the first several\ndenoising steps is sufficient for generating high-quality, well-conditioned\nimages, achieving an average speedup of 20% to 30%. Such improvement is\nconsistent across different settings such as inference steps, and various\nmodels including video generation models, highlighting the superiority of our\nmethod.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08356", "pdf": "https://arxiv.org/pdf/2506.08356", "abs": "https://arxiv.org/abs/2506.08356", "authors": ["Shivang Chopra", "Lingchao Mao", "Gabriela Sanchez-Rodriguez", "Andrew J Feola", "Jing Li", "Zsolt Kira"], "title": "MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Different medical imaging modalities capture diagnostic information at\nvarying spatial resolutions, from coarse global patterns to fine-grained\nlocalized structures. However, most existing vision-language frameworks in the\nmedical domain apply a uniform strategy for local feature extraction,\noverlooking the modality-specific demands. In this work, we present MedMoE, a\nmodular and extensible vision-language processing framework that dynamically\nadapts visual representation based on the diagnostic context. MedMoE\nincorporates a Mixture-of-Experts (MoE) module conditioned on the report type,\nwhich routes multi-scale image features through specialized expert branches\ntrained to capture modality-specific visual semantics. These experts operate\nover feature pyramids derived from a Swin Transformer backbone, enabling\nspatially adaptive attention to clinically relevant regions. This framework\nproduces localized visual representations aligned with textual descriptions,\nwithout requiring modality-specific supervision at inference. Empirical results\non diverse medical benchmarks demonstrate that MedMoE improves alignment and\nretrieval performance across imaging modalities, underscoring the value of\nmodality-specialized visual representations in clinical vision-language\nsystems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08361", "pdf": "https://arxiv.org/pdf/2506.08361", "abs": "https://arxiv.org/abs/2506.08361", "authors": ["Yanting Mei", "Zhilu Zhang", "Xiaohe Wu", "Wangmeng Zuo"], "title": "Image Demoir√©ing Using Dual Camera Fusion on Mobile Phones", "categories": ["cs.CV"], "comment": "ICME 2025", "summary": "When shooting electronic screens, moir\\'e patterns usually appear in captured\nimages, which seriously affects the image quality. Existing image demoir\\'eing\nmethods face great challenges in removing large and heavy moir\\'e. To address\nthe issue, we propose to utilize Dual Camera fusion for Image Demoir\\'eing\n(DCID), \\ie, using the ultra-wide-angle (UW) image to assist the moir\\'e\nremoval of wide-angle (W) image. This is inspired by two motivations: (1) the\ntwo lenses are commonly equipped with modern smartphones, (2) the UW image\ngenerally can provide normal colors and textures when moir\\'e exists in the W\nimage mainly due to their different focal lengths. In particular, we propose an\nefficient DCID method, where a lightweight UW image encoder is integrated into\nan existing demoir\\'eing network and a fast two-stage image alignment manner is\npresent. Moreover, we construct a large-scale real-world dataset with diverse\nmobile phones and monitors, containing about 9,000 samples. Experiments on the\ndataset show our method performs better than state-of-the-art methods. Code and\ndataset are available at https://github.com/Mrduckk/DCID.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08371", "pdf": "https://arxiv.org/pdf/2506.08371", "abs": "https://arxiv.org/abs/2506.08371", "authors": ["Zikai Xiao", "Ziyang Wang", "Wen Ma", "Yan Zhang", "Wei Shen", "Yan Wang", "Luqi Gong", "Zuozhu Liu"], "title": "Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding", "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) support long contexts, they struggle with\nperformance degradation within the context window. Current solutions incur\nprohibitive training costs, leaving statistical behaviors and cost-effective\napproaches underexplored. From the decoding perspective, we identify the\nPosterior Salience Attenuation (PSA) phenomenon, where the salience ratio\ncorrelates with long-text performance degradation. Notably, despite the\nattenuation, gold tokens still occupy high-ranking positions in the decoding\nspace. Motivated by it, we propose the training-free Positional Contrastive\nDecoding (PCD) that contrasts the logits derived from long-aware attention with\nthose from designed local-aware attention, enabling the model to focus on the\ngains introduced by large-scale short-to-long training. Through the analysis of\nlong-term decay simulation, we demonstrate that PCD effectively alleviates\nattention score degradation. Experimental results show that PCD achieves\nstate-of-the-art performance on long-context benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08375", "pdf": "https://arxiv.org/pdf/2506.08375", "abs": "https://arxiv.org/abs/2506.08375", "authors": ["Tao Zou", "Xinghua Zhang", "Haiyang Yu", "Minzheng Wang", "Fei Huang", "Yongbin Li"], "title": "EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": "24 pages", "summary": "With the development and widespread application of large language models\n(LLMs), the new paradigm of \"Model as Product\" is rapidly evolving, and demands\nhigher capabilities to address complex user needs, often requiring precise\nworkflow execution which involves the accurate understanding of multiple tasks.\nHowever, existing benchmarks focusing on single-task environments with limited\nconstraints lack the complexity required to fully reflect real-world scenarios.\nTo bridge this gap, we present the Extremely Complex Instruction Following\nBenchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and\nrobust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that\nenable comprehensive assessment across diverse task types concurrently, but\nalso integrates a variety of constraints, replicating complex operational\nenvironments. Furthermore, we propose the Segment Policy Optimization (SegPO)\nalgorithm to enhance the LLM's ability to accurately fulfill multi-task\nworkflow. Evaluations on EIFBENCH have unveiled considerable performance\ndiscrepancies in existing LLMs when challenged with these extremely complex\ninstructions. This finding underscores the necessity for ongoing optimization\nto navigate the intricate challenges posed by LLM applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08429", "pdf": "https://arxiv.org/pdf/2506.08429", "abs": "https://arxiv.org/abs/2506.08429", "authors": ["Mingjie Xu", "Andrew Estornell", "Hongzheng Yang", "Yuzhi Zhao", "Zhaowei Zhu", "Qi Xuan", "Jiaheng Wei"], "title": "Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring", "categories": ["cs.CV"], "comment": null, "summary": "The application of visual instruction tuning and other post-training\ntechniques has significantly enhanced the capabilities of Large Language Models\n(LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with\nmore comprehensive visual language datasets. However, the effectiveness of VLMs\nis highly dependent on large-scale, high-quality datasets that ensure precise\nrecognition and accurate reasoning. Two key challenges hinder progress: (1)\nnoisy alignments between images and the corresponding text, which leads to\nmisinterpretation, and (2) ambiguous or misleading text, which obscures visual\ncontent. To address these challenges, we propose SCALE (Single modality data\nquality and Cross modality Alignment Evaluation), a novel quality-driven data\nselection pipeline for VLM instruction tuning datasets. Specifically, SCALE\nintegrates a cross-modality assessment framework that first assigns each data\nentry to its appropriate vision-language task, generates general and\ntask-specific captions (covering scenes, objects, style, etc.), and evaluates\nthe alignment, clarity, task rarity, text coherence, and image clarity of each\nentry based on the generated captions. We reveal that: (1) current unimodal\nquality assessment methods evaluate one modality while overlooking the rest,\nwhich can underestimate samples essential for specific tasks and discard the\nlower-quality instances that help build model robustness; and (2) appropriately\ngenerated image captions provide an efficient way to transfer the image-text\nmultimodal task into a unified text modality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08456", "pdf": "https://arxiv.org/pdf/2506.08456", "abs": "https://arxiv.org/abs/2506.08456", "authors": ["June Suk Choi", "Kyungmin Lee", "Sihyun Yu", "Yisol Choi", "Jinwoo Shin", "Kimin Lee"], "title": "Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance", "categories": ["cs.CV"], "comment": "Preprint. Under review. Project page available at\n  http://choi403.github.io/ALG", "summary": "Recent text-to-video (T2V) models have demonstrated strong capabilities in\nproducing high-quality, dynamic videos. To improve the visual controllability,\nrecent works have considered fine-tuning pre-trained T2V models to support\nimage-to-video (I2V) generation. However, such adaptation frequently suppresses\nmotion dynamics of generated outputs, resulting in more static videos compared\nto their T2V counterparts. In this work, we analyze this phenomenon and\nidentify that it stems from the premature exposure to high-frequency details in\nthe input image, which biases the sampling process toward a shortcut trajectory\nthat overfits to the static appearance of the reference image. To address this,\nwe propose adaptive low-pass guidance (ALG), a simple fix to the I2V model\nsampling procedure to generate more dynamic videos without compromising\nper-frame image quality. Specifically, ALG adaptively modulates the frequency\ncontent of the conditioning image by applying low-pass filtering at the early\nstage of denoising. Extensive experiments demonstrate that ALG significantly\nimproves the temporal dynamics of generated videos, while preserving image\nfidelity and text alignment. Especially, under VBench-I2V test suite, ALG\nachieves an average improvement of 36% in dynamic degree without a significant\ndrop in video quality or image fidelity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08512", "pdf": "https://arxiv.org/pdf/2506.08512", "abs": "https://arxiv.org/abs/2506.08512", "authors": ["Zhiyi Zhu", "Xiaoyu Wu", "Zihao Liu", "Linlin Yang"], "title": "MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Temporal Grounding (VTG), which aims to localize video clips\ncorresponding to natural language queries, is a fundamental yet challenging\ntask in video understanding. Existing Transformer-based methods often suffer\nfrom redundant attention and suboptimal multi-modal alignment. To address these\nlimitations, we propose MLVTG, a novel framework that integrates two key\nmodules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba\nblocks as a backbone instead of Transformers to model temporal dependencies and\nextract robust video representations for multi-modal alignment. LLMRefiner\nleverages the specific frozen layer of a pre-trained Large Language Model (LLM)\nto implicitly transfer semantic priors, enhancing multi-modal alignment without\nfine-tuning. This dual alignment strategy, temporal modeling via structured\nstate-space dynamics and semantic purification via textual priors, enables more\nprecise localization. Extensive experiments on QVHighlights, Charades-STA, and\nTVSum demonstrate that MLVTG achieves state-of-the-art performance and\nsignificantly outperforms existing baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08541", "pdf": "https://arxiv.org/pdf/2506.08541", "abs": "https://arxiv.org/abs/2506.08541", "authors": ["Qi Yan", "Brian Zhang", "Yutong Zhang", "Daniel Yang", "Joshua White", "Di Chen", "Jiachao Liu", "Langechuan Liu", "Binnan Zhuang", "Shaoshuai Shi", "Renjie Liao"], "title": "TrajFlow: Multi-modal Motion Prediction via Flow Matching", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Efficient and accurate motion prediction is crucial for ensuring safety and\ninformed decision-making in autonomous driving, particularly under dynamic\nreal-world conditions that necessitate multi-modal forecasts. We introduce\nTrajFlow, a novel flow matching-based motion prediction framework that\naddresses the scalability and efficiency challenges of existing generative\ntrajectory prediction methods. Unlike conventional generative approaches that\nemploy i.i.d. sampling and require multiple inference passes to capture diverse\noutcomes, TrajFlow predicts multiple plausible future trajectories in a single\npass, significantly reducing computational overhead while maintaining coherence\nacross predictions. Moreover, we propose a ranking loss based on the\nPlackett-Luce distribution to improve uncertainty estimation of predicted\ntrajectories. Additionally, we design a self-conditioning training technique\nthat reuses the model's own predictions to construct noisy inputs during a\nsecond forward pass, thereby improving generalization and accelerating\ninference. Extensive experiments on the large-scale Waymo Open Motion Dataset\n(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across\nvarious key metrics, underscoring its effectiveness for safety-critical\nautonomous driving applications. The code and other details are available on\nthe project website https://traj-flow.github.io/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08480", "pdf": "https://arxiv.org/pdf/2506.08480", "abs": "https://arxiv.org/abs/2506.08480", "authors": ["Huixuan Zhang", "Xiaojun Wan"], "title": "Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-to-image models often struggle to generate images that precisely match\ntextual prompts. Prior research has extensively studied the evaluation of\nimage-text alignment in text-to-image generation. However, existing evaluations\nprimarily focus on agreement with human assessments, neglecting other critical\nproperties of a trustworthy evaluation framework. In this work, we first\nidentify two key aspects that a reliable evaluation should address. We then\nempirically demonstrate that current mainstream evaluation frameworks fail to\nfully satisfy these properties across a diverse range of metrics and models.\nFinally, we propose recommendations for improving image-text alignment\nevaluation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08487", "pdf": "https://arxiv.org/pdf/2506.08487", "abs": "https://arxiv.org/abs/2506.08487", "authors": ["Sumanth Manduru", "Carlotta Domeniconi"], "title": "Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid adoption of Small Language Models (SLMs) for on-device and\nresource-constrained deployments has outpaced our understanding of their\nethical risks. To the best of our knowledge, we present the first large-scale\naudit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an\noverlooked \"middle tier\" between BERT-class encoders and flagship LLMs. Our\nevaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma\n3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we\nanalyze both utility and fairness across ambiguous and disambiguated contexts.\nThis evaluation reveals three key insights. First, competence and fairness need\nnot be antagonistic: Phi models achieve F1 scores exceeding 90 percent while\nexhibiting minimal bias, showing that efficient and ethical NLP is attainable.\nSecond, social bias varies significantly by architecture: Qwen 2.5 models may\nappear fair, but this often reflects vacuous neutrality, random guessing, or\nevasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2\nmodels exhibit stronger stereotypical bias, suggesting overconfidence rather\nthan neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ\nquantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but\nincreases disability-related bias in Phi-4-Mini by over 7 percentage points.\nThese insights provide practical guidance for the responsible deployment of\nSLMs in applications demanding fairness and efficiency, particularly benefiting\nsmall enterprises and resource-constrained environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08566", "pdf": "https://arxiv.org/pdf/2506.08566", "abs": "https://arxiv.org/abs/2506.08566", "authors": ["Yibo Cui", "Liang Xie", "Yu Zhao", "Jiawei Sun", "Erwei Yin"], "title": "Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Navigation (VLN) enables intelligent agents to navigate\nenvironments by integrating visual perception and natural language\ninstructions, yet faces significant challenges due to the scarcity of\nfine-grained cross-modal alignment annotations. Existing datasets primarily\nfocus on global instruction-trajectory matching, neglecting\nsub-instruction-level and entity-level alignments critical for accurate\nnavigation action decision-making. To address this limitation, we propose\nFCA-NIG, a generative framework that automatically constructs navigation\ninstructions with dual-level fine-grained cross-modal annotations. In this\nframework, an augmented trajectory is first divided into sub-trajectories,\nwhich are then processed through GLIP-based landmark detection, crafted\ninstruction construction, OFA-Speaker based R2R-like instruction generation,\nand CLIP-powered entity selection, generating sub-instruction-trajectory pairs\nwith entity-landmark annotations. Finally, these sub-pairs are aggregated to\nform a complete instruction-trajectory pair. The framework generates the\nFCA-R2R dataset, the first large-scale augmentation dataset featuring precise\nsub-instruction-sub-trajectory and entity-landmark alignments. Extensive\nexperiments demonstrate that training with FCA-R2R significantly improves the\nperformance of multiple state-of-the-art VLN agents, including SF, EnvDrop,\nRecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances\nagents' state awareness and decision accuracy, while entity-landmark alignment\nfurther boosts navigation performance and generalization. These results\nhighlight the effectiveness of FCA-NIG in generating high-quality, scalable\ntraining data without manual annotation, advancing fine-grained cross-modal\nlearning in complex navigation tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08593", "pdf": "https://arxiv.org/pdf/2506.08593", "abs": "https://arxiv.org/abs/2506.08593", "authors": ["Shuzhou Yuan", "Ercong Nie", "Mario Tawfelis", "Helmut Schmid", "Hinrich Sch√ºtze", "Michael F√§rber"], "title": "Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Hate speech detection is a socially sensitive and inherently subjective task,\nwith judgments often varying based on personal traits. While prior work has\nexamined how socio-demographic factors influence annotation, the impact of\npersonality traits on Large Language Models (LLMs) remains largely unexplored.\nIn this paper, we present the first comprehensive study on the role of persona\nprompts in hate speech classification, focusing on MBTI-based traits. A human\nannotation survey confirms that MBTI dimensions significantly affect labeling\nbehavior. Extending this to LLMs, we prompt four open-source models with MBTI\npersonas and evaluate their outputs across three hate speech datasets. Our\nanalysis uncovers substantial persona-driven variation, including\ninconsistencies with ground truth, inter-persona disagreement, and logit-level\nbiases. These findings highlight the need to carefully define persona prompts\nin LLM-based annotation workflows, with implications for fairness and alignment\nwith human values.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08640", "pdf": "https://arxiv.org/pdf/2506.08640", "abs": "https://arxiv.org/abs/2506.08640", "authors": ["Yichong Lu", "Yuzhuo Tian", "Zijin Jiang", "Yikun Zhao", "Yuanbo Yang", "Hao Ouyang", "Haoji Hu", "Huimin Yu", "Yujun Shen", "Yiyi Liao"], "title": "Orientation Matters: Making 3D Generative Models Orientation-Aligned", "categories": ["cs.CV"], "comment": "Project Page: https://xdimlab.github.io/Orientation_Matters", "summary": "Humans intuitively perceive object shape and orientation from a single image,\nguided by strong priors about canonical poses. However, existing 3D generative\nmodels often produce misaligned results due to inconsistent training data,\nlimiting their usability in downstream tasks. To address this gap, we introduce\nthe task of orientation-aligned 3D object generation: producing 3D objects from\nsingle images with consistent orientations across categories. To facilitate\nthis, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D\nmodels spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two\nrepresentative 3D generative models based on multi-view diffusion and 3D\nvariational autoencoder frameworks to produce aligned objects that generalize\nwell to unseen objects across various categories. Experimental results\ndemonstrate the superiority of our method over post-hoc alignment approaches.\nFurthermore, we showcase downstream applications enabled by our aligned object\ngeneration, including zero-shot object orientation estimation via\nanalysis-by-synthesis and efficient arrow-based object rotation manipulation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08672", "pdf": "https://arxiv.org/pdf/2506.08672", "abs": "https://arxiv.org/abs/2506.08672", "authors": ["Yang Liu", "Jiaqi Li", "Zilong Zheng"], "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling", "categories": ["cs.CL"], "comment": "22 pages, 10 figures, 8 tables", "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin ($\\Delta$4.1%\naverage points on eight ID tasks and $\\Delta$10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08678", "pdf": "https://arxiv.org/pdf/2506.08678", "abs": "https://arxiv.org/abs/2506.08678", "authors": ["Juan Yeo", "Soonwoo Cha", "Jiwoo Song", "Hyunbin Jin", "Taesup Kim"], "title": "ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models such as CLIP have recently propelled open-vocabulary\ndense prediction tasks by enabling recognition of a broad range of visual\nconcepts. However, CLIP still struggles with fine-grained, region-level\nunderstanding, hindering its effectiveness on these dense prediction tasks. We\nidentify two pivotal factors required to address this limitation: semantic\ncoherence and fine-grained vision-language alignment. Current adaptation\nmethods often improve fine-grained alignment at the expense of semantic\ncoherence, and often rely on extra modules or supervised fine-tuning. To\novercome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel\napproach that simultaneously enhances semantic coherence and fine-grained\nalignment by leveraging own knowledge of a model across all representation\nlevels. Unlike prior methods, ATAS uses only unlabeled images and an internal\nself-distillation process to refine representations of CLIP vision encoders,\npreserving local semantic consistency while sharpening local detail\nrecognition. On open-vocabulary object detection and semantic segmentation\nbenchmarks, ATAS achieves substantial performance gains, outperforming baseline\nCLIP models. These results validate the effectiveness of our approach and\nunderscore the importance of jointly maintaining semantic coherence and\nfine-grained alignment for advanced open-vocabulary dense prediction.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08757", "pdf": "https://arxiv.org/pdf/2506.08757", "abs": "https://arxiv.org/abs/2506.08757", "authors": ["Mishca de Costa", "Muhammad Anwar", "Dave Mercier", "Mark Randall", "Issam Hammad"], "title": "Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL", "categories": ["cs.CL", "cs.LG"], "comment": "44th Annual CNS Conference and the 49th Annual CNS/CNA Student\n  Conference, Westin Harbour Castle Hotel, Toronto, ON, Canada, June 8-11, 2025", "summary": "Retrieving operational data from nuclear power plants requires exceptional\naccuracy and transparency due to the criticality of the decisions it supports.\nTraditionally, natural language to SQL (NL-to-SQL) approaches have been\nexplored for querying such data. While NL-to-SQL promises ease of use, it poses\nsignificant risks: end-users cannot easily validate generated SQL queries, and\nlegacy nuclear plant databases -- often complex and poorly structured --\ncomplicate query generation due to decades of incremental modifications. These\nchallenges increase the likelihood of inaccuracies and reduce trust in the\napproach. In this work, we propose an alternative paradigm: leveraging\nfunction-calling large language models (LLMs) to address these challenges.\nInstead of directly generating SQL queries, we define a set of pre-approved,\npurpose-specific functions representing common use cases. Queries are processed\nby invoking these functions, which encapsulate validated SQL logic. This hybrid\napproach mitigates the risks associated with direct NL-to-SQL translations by\nensuring that SQL queries are reviewed and optimized by experts before\ndeployment. While this strategy introduces the upfront cost of developing and\nmaintaining the function library, we demonstrate how NL-to-SQL tools can assist\nin the initial generation of function code, allowing experts to focus on\nvalidation rather than creation. Our study includes a performance comparison\nbetween direct NL-to-SQL generation and the proposed function-based approach,\nhighlighting improvements in accuracy and maintainability. This work\nunderscores the importance of balancing user accessibility with operational\nsafety and provides a novel, actionable framework for robust data retrieval in\ncritical systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08777", "pdf": "https://arxiv.org/pdf/2506.08777", "abs": "https://arxiv.org/abs/2506.08777", "authors": ["Keyi Liu", "Weidong Yang", "Ben Fei", "Ying He"], "title": "Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised learning (SSL) for point cloud pre-training has become a\ncornerstone for many 3D vision tasks, enabling effective learning from\nlarge-scale unannotated data. At the scene level, existing SSL methods often\nincorporate volume rendering into the pre-training framework, using RGB-D\nimages as reconstruction signals to facilitate cross-modal learning. This\nstrategy promotes alignment between 2D and 3D modalities and enables the model\nto benefit from rich visual cues in the RGB-D inputs. However, these approaches\nare limited by their reliance on implicit scene representations and high memory\ndemands. Furthermore, since their reconstruction objectives are applied only in\n2D space, they often fail to capture underlying 3D geometric structures. To\naddress these challenges, we propose Gaussian2Scene, a novel scene-level SSL\nframework that leverages the efficiency and explicit nature of 3D Gaussian\nSplatting (3DGS) for pre-training. The use of 3DGS not only alleviates the\ncomputational burden associated with volume rendering but also supports direct\n3D scene reconstruction, thereby enhancing the geometric understanding of the\nbackbone network. Our approach follows a progressive two-stage training\nstrategy. In the first stage, a dual-branch masked autoencoder learns both 2D\nand 3D scene representations. In the second stage, we initialize training with\nreconstructed point clouds and further supervise learning using the geometric\nlocations of Gaussian primitives and rendered RGB images. This process\nreinforces both geometric and cross-modal learning. We demonstrate the\neffectiveness of Gaussian2Scene across several downstream 3D object detection\ntasks, showing consistent improvements over existing pre-training methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08780", "pdf": "https://arxiv.org/pdf/2506.08780", "abs": "https://arxiv.org/abs/2506.08780", "authors": ["Isaac Corley", "Lakshay Sharma", "Ruth Crasto"], "title": "Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The Landsat program offers over 50 years of globally consistent Earth\nimagery. However, the lack of benchmarks for this data constrains progress\ntowards Landsat-based Geospatial Foundation Models (GFM). In this paper, we\nintroduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that\nadapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and\nLC100-L. We establish baseline and standardized evaluation methods across both\ncommon architectures and Landsat foundation models pretrained on the SSL4EO-L\ndataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract\nbetter representations for downstream tasks in comparison to ImageNet,\nincluding performance gains of +4% OA and +5.1% mAP on EuroSAT-L and\nBigEarthNet-L.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08784", "pdf": "https://arxiv.org/pdf/2506.08784", "abs": "https://arxiv.org/abs/2506.08784", "authors": ["Jongyub Seok", "Chanjin Kang"], "title": "HomographyAD: Deep Anomaly Detection Using Self Homography Learning", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly detection (AD) is a task that distinguishes normal and abnormal data,\nwhich is important for applying automation technologies of the manufacturing\nfacilities. For MVTec dataset that is a representative AD dataset for\nindustrial environment, many recent works have shown remarkable performances.\nHowever, the existing anomaly detection works have a limitation of showing good\nperformance for fully-aligned datasets only, unlike real-world industrial\nenvironments. To solve this limitation, we propose HomographyAD, a novel deep\nanomaly detection methodology based on the ImageNet-pretrained network, which\nis specially designed for actual industrial dataset. Specifically, we first\nsuggest input foreground alignment using the deep homography estimation method.\nIn addition, we fine-tune the model by self homography learning to learn\nadditional shape information from normal samples. Finally, we conduct anomaly\ndetection based on the measure of how far the feature of test sample is from\nthe distribution of the extracted normal features. By applying our proposed\nmethod to various existing AD approaches, we show performance enhancement\nthrough extensive experiments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08899", "pdf": "https://arxiv.org/pdf/2506.08899", "abs": "https://arxiv.org/abs/2506.08899", "authors": ["Elias Horner", "Cristinel Mateis", "Guido Governatori", "Agata Ciabattoni"], "title": "From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LO"], "comment": null, "summary": "We present a novel approach to the automated semantic analysis of legal texts\nusing large language models (LLMs), targeting their transformation into formal\nrepresentations in Defeasible Deontic Logic (DDL). We propose a structured\npipeline that segments complex normative language into atomic snippets,\nextracts deontic rules, and evaluates them for syntactic and semantic\ncoherence. Our methodology is evaluated across various LLM configurations,\nincluding prompt engineering strategies, fine-tuned models, and multi-stage\npipelines, focusing on legal norms from the Australian Telecommunications\nConsumer Protections Code. Empirical results demonstrate promising alignment\nbetween machine-generated and expert-crafted formalizations, showing that LLMs\n- particularly when prompted effectively - can significantly contribute to\nscalable legal informatics.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08835", "pdf": "https://arxiv.org/pdf/2506.08835", "abs": "https://arxiv.org/abs/2506.08835", "authors": ["Shravan Nayak", "Mehar Bhatia", "Xiaofeng Zhang", "Verena Rieser", "Lisa Anne Hendricks", "Sjoerd van Steenkiste", "Yash Goyal", "Karolina Sta≈Ñczak", "Aishwarya Agrawal"], "title": "CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The increasing ubiquity of text-to-image (T2I) models as tools for visual\ncontent generation raises concerns about their ability to accurately represent\ndiverse cultural contexts. In this work, we present the first study to\nsystematically quantify the alignment of T2I models and evaluation metrics with\nrespect to both explicit as well as implicit cultural expectations. To this\nend, we introduce CulturalFrames, a novel benchmark designed for rigorous human\nevaluation of cultural representation in visual generations. Spanning 10\ncountries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,\n3637 corresponding images generated by 4 state-of-the-art T2I models, and over\n10k detailed human annotations. We find that T2I models not only fail to meet\nthe more challenging implicit expectations but also the less challenging\nexplicit expectations. Across models and countries, cultural expectations are\nmissed an average of 44% of the time. Among these failures, explicit\nexpectations are missed at a surprisingly high average rate of 68%, while\nimplicit expectation failures are also significant, averaging 49%. Furthermore,\nwe demonstrate that existing T2I evaluation metrics correlate poorly with human\njudgments of cultural alignment, irrespective of their internal reasoning.\nCollectively, our findings expose critical gaps, providing actionable\ndirections for developing more culturally informed T2I models and evaluation\nmethodologies.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08935", "pdf": "https://arxiv.org/pdf/2506.08935", "abs": "https://arxiv.org/abs/2506.08935", "authors": ["Andrew Shin"], "title": "Can A Gamer Train A Mathematical Reasoning Model?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While large language models (LLMs) have achieved remarkable performance in\nvarious tasks including mathematical reasoning, their development typically\ndemands prohibitive computational resources. Recent advancements have reduced\ncosts for training capable models, yet even these approaches rely on high-end\nhardware clusters. In this paper, we demonstrate that a single average gaming\nGPU can train a solid mathematical reasoning model, by integrating\nreinforcement learning and memory optimization techniques. Specifically, we\ntrain a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB\nmemory that achieves comparable or better performance on mathematical reasoning\nbenchmarks than models several times larger, in resource-constrained\nenvironments. Our results challenge the paradigm that state-of-the-art\nmathematical reasoning necessitates massive infrastructure, democratizing\naccess to high-performance AI research.\nhttps://github.com/shinandrew/YouronMath.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08887", "pdf": "https://arxiv.org/pdf/2506.08887", "abs": "https://arxiv.org/abs/2506.08887", "authors": ["Leqi Shen", "Guoqiang Gong", "Tianxiang Hao", "Tao He", "Yifeng Zhang", "Pengzhang Liu", "Sicheng Zhao", "Jungong Han", "Guiguang Ding"], "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "The parameter-efficient adaptation of the image-text pretraining model CLIP\nfor video-text retrieval is a prominent area of research. While CLIP is focused\non image-level vision-language matching, video-text retrieval demands\ncomprehensive understanding at the video level. Three key discrepancies emerge\nin the transfer from image-level to video-level: vision, language, and\nalignment. However, existing methods mainly focus on vision while neglecting\nlanguage and alignment. In this paper, we propose Discrepancy Reduction in\nVision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all\nthree discrepancies. Specifically, we introduce Image-Video Features Fusion to\nintegrate image-level and video-level features, effectively tackling both\nvision and language discrepancies. Additionally, we generate pseudo image\ncaptions to learn fine-grained image-level alignment. To mitigate alignment\ndiscrepancies, we propose Image-to-Video Alignment Distillation, which\nleverages image-level alignment knowledge to enhance video-level alignment.\nExtensive experiments demonstrate the superiority of our DiscoVLA. In\nparticular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous\nmethods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is\navailable at https://github.com/LunarShen/DsicoVLA.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08900", "pdf": "https://arxiv.org/pdf/2506.08900", "abs": "https://arxiv.org/abs/2506.08900", "authors": ["Jos√© Morano", "Botond Fazekas", "Emese S√ºkei", "Ronald Fecso", "Taha Emre", "Markus Gumpinger", "Georg Faustmann", "Marzieh Oghbaie", "Ursula Schmidt-Erfurth", "Hrvoje Bogunoviƒá"], "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis", "categories": ["cs.CV"], "comment": null, "summary": "Artificial intelligence (AI) has become a fundamental tool for assisting\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\n(OCT). However, developing AI models often requires extensive annotation, and\nexisting models tend to underperform on independent, unseen data. Foundation\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\npromise in overcoming these challenges. Nonetheless, available FMs for\nophthalmology lack extensive validation, especially for segmentation tasks, and\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\nclassification and segmentation tasks. The comparison with general and\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\nboth types of tasks, highlighting its suitability as a basis for the\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\nand the evaluation benchmark are publicly available:\nhttps://github.com/j-morano/MIRAGE.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "annotation"], "score": 3}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08927", "pdf": "https://arxiv.org/pdf/2506.08927", "abs": "https://arxiv.org/abs/2506.08927", "authors": ["David Acuna", "Ximing Lu", "Jaehun Jung", "Hyunwoo Kim", "Amlan Kar", "Sanja Fidler", "Yejin Choi"], "title": "Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent research in vision-language models (VLMs) has centered around the\npossibility of equipping them with implicit long-form chain-of-thought\nreasoning -- akin to the success observed in language models -- via\ndistillation and reinforcement learning. But what about the non-reasoning\nmodels already trained and deployed across the internet? Should we simply\nabandon them, or is there hope for a search mechanism that can elicit hidden\nknowledge and induce long reasoning traces -- without any additional training\nor supervision? In this paper, we explore this possibility using a Monte Carlo\nTree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer\npairs into the model's output stream. We show that framing reasoning as a\nsearch process -- where subquestions act as latent decisions within a broader\ninference trajectory -- helps the model \"connect the dots\" between fragmented\nknowledge and produce extended reasoning traces in non-reasoning models. We\nevaluate our method across three benchmarks and observe consistent\nimprovements. Notably, our approach yields a 2% overall improvement on\nMMMU-PRO, including a significant 9% gain in Liberal Arts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "MCTS"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.09033", "pdf": "https://arxiv.org/pdf/2506.09033", "abs": "https://arxiv.org/abs/2506.09033", "authors": ["Haozhen Zhang", "Tao Feng", "Jiaxuan You"], "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Code is available at https://github.com/ulab-uiuc/Router-R1", "summary": "The rapid emergence of diverse large language models (LLMs) has spurred the\ndevelopment of LLM routers that assign user queries to the most suitable model.\nHowever, existing LLM routers typically perform a single-round, one-to-one\nmapping (\\textit{i.e.}, assigning each query to a single model in isolation),\nwhich limits their capability to tackle complex tasks that demand the\ncomplementary strengths of multiple LLMs. In this paper, we present\n\\textbf{Router-R1}, a reinforcement learning (RL)-based framework that\nformulates multi-LLM routing and aggregation as a sequential decision process.\nRouter-R1 instantiates the router itself as a capable LLM, leveraging its\nreasoning ability to interleave \"think\" actions (internal deliberation) with\n\"route\" actions (dynamic model invocation), and integrates each response into\nits evolving context. To guide learning, we employ a lightweight rule-based\nreward comprising format rewards, final outcome rewards, and a novel cost\nreward for performance and cost trade-off optimization, opening a pathway\ntoward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions\nonly on simple model descriptors such as pricing, latency, and example\nperformance, enabling strong generalization to unseen model selection.\nExperiments on seven general and multi-hop QA benchmarks show that Router-R1\noutperforms over several strong baselines, achieving superior performance while\nmaintaining robust generalization and cost management.Code is available at\nhttps://github.com/ulab-uiuc/Router-R1.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08125", "pdf": "https://arxiv.org/pdf/2506.08125", "abs": "https://arxiv.org/abs/2506.08125", "authors": ["Hanbing Liu", "Lang Cao", "Yuanyi Ren", "Mengyu Zhou", "Haoyu Dong", "Xiaojun Ma", "Shi Han", "Dongmei Zhang"], "title": "Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language models have demonstrated impressive reasoning capabilities,\nyet they often suffer from inefficiencies due to unnecessarily verbose or\nredundant outputs. While many works have explored reinforcement learning (RL)\nto enhance reasoning abilities, most primarily focus on improving accuracy,\nwith limited attention to reasoning efficiency. Some existing approaches\nintroduce direct length-based rewards to encourage brevity, but this often\nleads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL\nframework that advances length-based reward design to boost efficient\nreasoning. Bingo incorporates two key mechanisms: a significance-aware length\nreward, which gradually guides the model to reduce only insignificant tokens,\nand a dynamic length reward, which initially encourages elaborate reasoning for\nhard questions but decays over time to improve overall efficiency. Experiments\nacross multiple reasoning benchmarks show that Bingo improves both accuracy and\nefficiency. It outperforms the vanilla reward and several other length-based\nreward baselines in RL, achieving a favorable trade-off between accuracy and\nefficiency. These results underscore the potential of training LLMs explicitly\nfor efficient reasoning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08968", "pdf": "https://arxiv.org/pdf/2506.08968", "abs": "https://arxiv.org/abs/2506.08968", "authors": ["Amirreza Rouhi", "Solmaz Arezoomandan", "Knut Peterson", "Joseph T. Woods", "David K. Han"], "title": "ADAM: Autonomous Discovery and Annotation Model using LLMs for Context-Aware Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Object detection models typically rely on predefined categories, limiting\ntheir ability to identify novel objects in open-world scenarios. To overcome\nthis constraint, we introduce ADAM: Autonomous Discovery and Annotation Model,\na training-free, self-refining framework for open-world object labeling. ADAM\nleverages large language models (LLMs) to generate candidate labels for unknown\nobjects based on contextual information from known entities within a scene.\nThese labels are paired with visual embeddings from CLIP to construct an\nEmbedding-Label Repository (ELR) that enables inference without category\nsupervision. For a newly encountered unknown object, ADAM retrieves visually\nsimilar instances from the ELR and applies frequency-based voting and\ncross-modal re-ranking to assign a robust label. To further enhance\nconsistency, we introduce a self-refinement loop that re-evaluates repository\nlabels using visual cohesion analysis and k-nearest-neighbor-based majority\nre-labeling. Experimental results on the COCO and PASCAL datasets demonstrate\nthat ADAM effectively annotates novel categories using only visual and\ncontextual signals, without requiring any fine-tuning or retraining.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08210", "pdf": "https://arxiv.org/pdf/2506.08210", "abs": "https://arxiv.org/abs/2506.08210", "authors": ["Andrew Z. Wang", "Songwei Ge", "Tero Karras", "Ming-Yu Liu", "Yogesh Balaji"], "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Both text-to-image generation and large language models (LLMs) have made\nsignificant advancements. However, many text-to-image models still employ the\nsomewhat outdated T5 and CLIP as their text encoders. In this work, we\ninvestigate the effectiveness of using modern decoder-only LLMs as text\nencoders for text-to-image diffusion models. We build a standardized training\nand evaluation pipeline that allows us to isolate and evaluate the effect of\ndifferent text embeddings. We train a total of 27 text-to-image models with 12\ndifferent text encoders to analyze the critical aspects of LLMs that could\nimpact text-to-image generation, including the approaches to extract\nembeddings, different LLMs variants, and model sizes. Our experiments reveal\nthat the de facto way of using last-layer embeddings as conditioning leads to\ninferior performance. Instead, we explore embeddings from various layers and\nfind that using layer-normalized averaging across all layers significantly\nimproves alignment with complex prompts. Most LLMs with this conditioning\noutperform the baseline T5 model, showing enhanced performance in advanced\nvisio-linguistic reasoning skills.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08990", "pdf": "https://arxiv.org/pdf/2506.08990", "abs": "https://arxiv.org/abs/2506.08990", "authors": ["Chenyu Lian", "Hong-Yu Zhou", "Dongyun Liang", "Jing Qin", "Liansheng Wang"], "title": "Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "TMI 2025", "summary": "Medical vision-language alignment through cross-modal contrastive learning\nshows promising performance in image-text matching tasks, such as retrieval and\nzero-shot classification. However, conventional cross-modal contrastive\nlearning (CLIP-based) methods suffer from suboptimal visual representation\ncapabilities, which also limits their effectiveness in vision-language\nalignment. In contrast, although the models pretrained via multimodal masked\nmodeling struggle with direct cross-modal matching, they excel in visual\nrepresentation. To address this contradiction, we propose ALTA (ALign Through\nAdapting), an efficient medical vision-language alignment method that utilizes\nonly about 8% of the trainable parameters and less than 1/5 of the\ncomputational consumption required for masked record modeling. ALTA achieves\nsuperior performance in vision-language matching tasks like retrieval and\nzero-shot classification by adapting the pretrained vision model from masked\nrecord modeling. Additionally, we integrate temporal-multiview radiograph\ninputs to enhance the information consistency between radiographs and their\ncorresponding descriptions in reports, further improving the vision-language\nalignment. Experimental evaluations show that ALTA outperforms the\nbest-performing counterpart by over 4% absolute points in text-to-image\naccuracy and approximately 6% absolute points in image-to-text retrieval\naccuracy. The adaptation of vision-language models during efficient alignment\nalso promotes better vision and language understanding. Code is publicly\navailable at https://github.com/DopamineLcy/ALTA.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08277", "pdf": "https://arxiv.org/pdf/2506.08277", "abs": "https://arxiv.org/abs/2506.08277", "authors": ["Subba Reddy Oota", "Khushbu Pahwa", "Prachi Jindal", "Satya Sai Srinath Namburi", "Maneesh Singh", "Tanmoy Chakraborty", "Bapi S. Raju", "Manish Gupta"], "title": "Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "39 pages, 22 figures", "summary": "Recent voxel-wise multimodal brain encoding studies have shown that\nmultimodal large language models (MLLMs) exhibit a higher degree of brain\nalignment compared to unimodal models in both unimodal and multimodal stimulus\nsettings. More recently, instruction-tuned multimodal models have shown to\ngenerate task-specific representations that align strongly with brain activity.\nHowever, prior work evaluating the brain alignment of MLLMs has primarily\nfocused on unimodal settings or relied on non-instruction-tuned multimodal\nmodels for multimodal stimuli. To address this gap, we investigated brain\nalignment, that is, measuring the degree of predictivity of neural activity\nrecorded while participants were watching naturalistic movies (video along with\naudio) with representations derived from MLLMs. We utilized\ninstruction-specific embeddings from six video and two audio instruction-tuned\nMLLMs. Experiments with 13 video task-specific instructions show that\ninstruction-tuned video MLLMs significantly outperform non-instruction-tuned\nmultimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for\nboth video and audio tasks using language-guided instructions shows clear\ndisentanglement in task-specific representations from MLLMs, leading to precise\ndifferentiation of multimodal functional processing in the brain. We also find\nthat MLLM layers align hierarchically with the brain, with early sensory areas\nshowing strong alignment with early layers, while higher-level visual and\nlanguage regions align more with middle to late layers. These findings provide\nclear evidence for the role of task-specific instructions in improving the\nalignment between brain activity and MLLMs, and open new avenues for mapping\njoint information processing in both the systems. We make the code publicly\navailable [https://github.com/subbareddy248/mllm_videos].", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.09024", "pdf": "https://arxiv.org/pdf/2506.09024", "abs": "https://arxiv.org/abs/2506.09024", "authors": ["Felix Wagner", "Pramit Saha", "Harry Anthony", "J. Alison Noble", "Konstantinos Kamnitsas"], "title": "DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging", "categories": ["cs.CV", "cs.LG", "I.2.11; I.4.9; I.4.9; J.3; I.2.0"], "comment": null, "summary": "Safe deployment of machine learning (ML) models in safety-critical domains\nsuch as medical imaging requires detecting inputs with characteristics not seen\nduring training, known as out-of-distribution (OOD) detection, to prevent\nunreliable predictions. Effective OOD detection after deployment could benefit\nfrom access to the training data, enabling direct comparison between test\nsamples and the training data distribution to identify differences.\nState-of-the-art OOD detection methods, however, either discard training data\nafter deployment or assume that test samples and training data are centrally\nstored together, an assumption that rarely holds in real-world settings. This\nis because shipping training data with the deployed model is usually impossible\ndue to the size of training databases, as well as proprietary or privacy\nconstraints. We introduce the Isolation Network, an OOD detection framework\nthat quantifies the difficulty of separating a target test sample from the\ntraining data by solving a binary classification task. We then propose\nDecentralized Isolation Networks (DIsoN), which enables the comparison of\ntraining and test data when data-sharing is impossible, by exchanging only\nmodel parameters between the remote computational nodes of training and\ndeployment. We further extend DIsoN with class-conditioning, comparing a target\nsample solely with training data of its predicted class. We evaluate DIsoN on\nfour medical imaging datasets (dermatology, chest X-ray, breast ultrasound,\nhistopathology) across 12 OOD detection tasks. DIsoN performs favorably against\nexisting methods while respecting data-privacy. This decentralized OOD\ndetection framework opens the way for a new type of service that ML developers\ncould provide along with their models: providing remote, secure utilization of\ntheir training data for OOD detection services. Code will be available upon\nacceptance at: *****", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.09027", "pdf": "https://arxiv.org/pdf/2506.09027", "abs": "https://arxiv.org/abs/2506.09027", "authors": ["Runqian Wang", "Kaiming He"], "title": "Diffuse and Disperse: Image Generation with Representation Regularization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08351", "pdf": "https://arxiv.org/pdf/2506.08351", "abs": "https://arxiv.org/abs/2506.08351", "authors": ["Huixuan Zhang", "Junzhe Zhang", "Xiaojun Wan"], "title": "How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "With the rapid development of text-to-vision generation diffusion models,\nclassifier-free guidance has emerged as the most prevalent method for\nconditioning. However, this approach inherently requires twice as many steps\nfor model forwarding compared to unconditional generation, resulting in\nsignificantly higher costs. While previous study has introduced the concept of\nadaptive guidance, it lacks solid analysis and empirical results, making\nprevious method unable to be applied to general diffusion models. In this work,\nwe present another perspective of applying adaptive guidance and propose Step\nAG, which is a simple, universally applicable adaptive guidance strategy. Our\nevaluations focus on both image quality and image-text alignment. whose results\nindicate that restricting classifier-free guidance to the first several\ndenoising steps is sufficient for generating high-quality, well-conditioned\nimages, achieving an average speedup of 20% to 30%. Such improvement is\nconsistent across different settings such as inference steps, and various\nmodels including video generation models, highlighting the superiority of our\nmethod.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.09035", "pdf": "https://arxiv.org/pdf/2506.09035", "abs": "https://arxiv.org/abs/2506.09035", "authors": ["Karhan Kayan", "Stamatis Alexandropoulos", "Rishabh Jain", "Yiming Zuo", "Erich Liang", "Jia Deng"], "title": "Princeton365: A Diverse Dataset with Accurate Camera Pose", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Princeton365, a large-scale diverse dataset of 365 videos with\naccurate camera pose. Our dataset bridges the gap between accuracy and data\ndiversity in current SLAM benchmarks by introducing a novel ground truth\ncollection framework that leverages calibration boards and a 360-camera. We\ncollect indoor, outdoor, and object scanning videos with synchronized monocular\nand stereo RGB video outputs as well as IMU. We further propose a new scene\nscale-aware evaluation metric for SLAM based on the the optical flow induced by\nthe camera pose estimation error. In contrast to the current metrics, our new\nmetric allows for comparison between the performance of SLAM methods across\nscenes as opposed to existing metrics such as Average Trajectory Error (ATE),\nallowing researchers to analyze the failure modes of their methods. We also\npropose a challenging Novel View Synthesis benchmark that covers cases not\ncovered by current NVS benchmarks, such as fully non-Lambertian scenes with\n360-degree camera trajectories. Please visit\nhttps://princeton365.cs.princeton.edu for the dataset, code, videos, and\nsubmission.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.09040", "pdf": "https://arxiv.org/pdf/2506.09040", "abs": "https://arxiv.org/abs/2506.09040", "authors": ["Dianyi Wang", "Wei Song", "Yikun Wang", "Siyuan Wang", "Kaicheng Yu", "Zhongyu Wei", "Jiaqi Wang"], "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08388", "pdf": "https://arxiv.org/pdf/2506.08388", "abs": "https://arxiv.org/abs/2506.08388", "authors": ["Edoardo Cetin", "Tianyu Zhao", "Yujin Tang"], "title": "Reinforcement Learning Teachers of Test Time Scaling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Training reasoning language models (LMs) with reinforcement learning (RL) for\none-hot correctness inherently relies on the LM being able to explore and solve\nits task with some chance at initialization. Furthermore, a key use case of\nreasoning LMs is to act as teachers for distilling new students and\ncold-starting future RL iterations rather than being deployed themselves. From\nthese considerations, we introduce a new framework that avoids RL's exploration\nchallenge by training a new class of Reinforcement-Learned Teachers (RLTs)\nfocused on yielding the most effective downstream distillation. RLTs are\nprompted with both the question and solution to each problem, and tasked to\nsimply \"connect-the-dots\" with detailed explanations tailored for their\nstudents. We train RLTs with dense rewards obtained by feeding each explanation\nto the student and testing its understanding of the problem's solution. In\npractice, the raw outputs of a 7B RLT provide higher final performance on\ncompetition and graduate-level tasks than existing distillation and\ncold-starting pipelines that collect and postprocess the reasoning traces of\norders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness\nwhen training larger students and when applied zero-shot to out-of-distribution\ntasks, unlocking new levels of efficiency and re-usability for the RL reasoning\nframework.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08446", "pdf": "https://arxiv.org/pdf/2506.08446", "abs": "https://arxiv.org/abs/2506.08446", "authors": ["Peng-Yuan Wang", "Tian-Shuo Liu", "Chenyang Wang", "Yi-Di Wang", "Shu Yan", "Cheng-Xing Jia", "Xu-Hui Liu", "Xin-Wei Chen", "Jia-Cheng Xu", "Ziniu Li", "Yang Yu"], "title": "A Survey on Large Language Models for Mathematical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Mathematical reasoning has long represented one of the most fundamental and\nchallenging frontiers in artificial intelligence research. In recent years,\nlarge language models (LLMs) have achieved significant advances in this area.\nThis survey examines the development of mathematical reasoning abilities in\nLLMs through two high-level cognitive phases: comprehension, where models gain\nmathematical understanding via diverse pretraining strategies, and answer\ngeneration, which has progressed from direct prediction to step-by-step\nChain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical\nreasoning, ranging from training-free prompting to fine-tuning approaches such\nas supervised fine-tuning and reinforcement learning, and discuss recent work\non extended CoT and \"test-time scaling\". Despite notable progress, fundamental\nchallenges remain in terms of capacity, efficiency, and generalization. To\naddress these issues, we highlight promising research directions, including\nadvanced pretraining and knowledge augmentation techniques, formal reasoning\nframeworks, and meta-generalization through principled learning paradigms. This\nsurvey tries to provide some insights for researchers interested in enhancing\nreasoning capabilities of LLMs and for those seeking to apply these techniques\nto other domains.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08020", "pdf": "https://arxiv.org/pdf/2506.08020", "abs": "https://arxiv.org/abs/2506.08020", "authors": ["Zi-Ying Chen", "Chuan-Xian Ren", "Hong Yan"], "title": "Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Partial domain adaptation (PDA) problem requires aligning cross-domain\nsamples while distinguishing the outlier classes for accurate knowledge\ntransfer. The widely used weighting framework tries to address the outlier\nclasses by introducing the reweighed source domain with a similar label\ndistribution to the target domain. However, the empirical modeling of weights\ncan only characterize the sample-wise relations, which leads to insufficient\nexploration of cluster structures, and the weights could be sensitive to the\ninaccurate prediction and cause confusion on the outlier classes. To tackle\nthese issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model\nto simultaneously characterize the sample-wise and class-wise relations in a\nunified transport framework. Specifically, a cooperation mechanism between\nsample-level and class-level transport is introduced, where the sample-level\ntransport provides essential structure information for the class-level\nknowledge transfer, while the class-level transport supplies discriminative\ninformation for the outlier identification. The bi-level transport plan\nprovides guidance for the alignment process. By incorporating the label-aware\ntransport cost, the local transport structure is ensured and a fast computation\nformulation is derived to improve the efficiency. Extensive experiments on\nbenchmark datasets validate the competitiveness of BUOT.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08745", "pdf": "https://arxiv.org/pdf/2506.08745", "abs": "https://arxiv.org/abs/2506.08745", "authors": ["Kongcheng Zhang", "Qi Yao", "Shunyu Liu", "Yingjie Wang", "Baisheng Lai", "Jieping Ye", "Mingli Song", "Dacheng Tao"], "title": "Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances of Reinforcement Learning (RL) have highlighted its potential\nin complex reasoning tasks, yet effective training often relies on external\nsupervision, which limits the broader applicability. In this work, we propose a\nnovel self-rewarding reinforcement learning framework to enhance Large Language\nModel (LLM) reasoning by leveraging the consistency of intermediate reasoning\nstates across different reasoning trajectories. Our key insight is that correct\nresponses often exhibit consistent trajectory patterns in terms of model\nlikelihood: their intermediate reasoning states tend to converge toward their\nown final answers (high consistency) with minimal deviation toward other\ncandidates (low volatility). Inspired by this observation, we introduce CoVo,\nan intrinsic reward mechanism that integrates Consistency and Volatility via a\nrobust vector-space aggregation strategy, complemented by a curiosity bonus to\npromote diverse exploration. CoVo enables LLMs to perform RL in a\nself-rewarding manner, offering a scalable pathway for learning to reason\nwithout external supervision. Extensive experiments on diverse reasoning\nbenchmarks show that CoVo achieves performance comparable to or even surpassing\nsupervised RL. Our code is available at https://github.com/sastpg/CoVo.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08835", "pdf": "https://arxiv.org/pdf/2506.08835", "abs": "https://arxiv.org/abs/2506.08835", "authors": ["Shravan Nayak", "Mehar Bhatia", "Xiaofeng Zhang", "Verena Rieser", "Lisa Anne Hendricks", "Sjoerd van Steenkiste", "Yash Goyal", "Karolina Sta≈Ñczak", "Aishwarya Agrawal"], "title": "CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The increasing ubiquity of text-to-image (T2I) models as tools for visual\ncontent generation raises concerns about their ability to accurately represent\ndiverse cultural contexts. In this work, we present the first study to\nsystematically quantify the alignment of T2I models and evaluation metrics with\nrespect to both explicit as well as implicit cultural expectations. To this\nend, we introduce CulturalFrames, a novel benchmark designed for rigorous human\nevaluation of cultural representation in visual generations. Spanning 10\ncountries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,\n3637 corresponding images generated by 4 state-of-the-art T2I models, and over\n10k detailed human annotations. We find that T2I models not only fail to meet\nthe more challenging implicit expectations but also the less challenging\nexplicit expectations. Across models and countries, cultural expectations are\nmissed an average of 44% of the time. Among these failures, explicit\nexpectations are missed at a surprisingly high average rate of 68%, while\nimplicit expectation failures are also significant, averaging 49%. Furthermore,\nwe demonstrate that existing T2I evaluation metrics correlate poorly with human\njudgments of cultural alignment, irrespective of their internal reasoning.\nCollectively, our findings expose critical gaps, providing actionable\ndirections for developing more culturally informed T2I models and evaluation\nmethodologies.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08927", "pdf": "https://arxiv.org/pdf/2506.08927", "abs": "https://arxiv.org/abs/2506.08927", "authors": ["David Acuna", "Ximing Lu", "Jaehun Jung", "Hyunwoo Kim", "Amlan Kar", "Sanja Fidler", "Yejin Choi"], "title": "Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent research in vision-language models (VLMs) has centered around the\npossibility of equipping them with implicit long-form chain-of-thought\nreasoning -- akin to the success observed in language models -- via\ndistillation and reinforcement learning. But what about the non-reasoning\nmodels already trained and deployed across the internet? Should we simply\nabandon them, or is there hope for a search mechanism that can elicit hidden\nknowledge and induce long reasoning traces -- without any additional training\nor supervision? In this paper, we explore this possibility using a Monte Carlo\nTree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer\npairs into the model's output stream. We show that framing reasoning as a\nsearch process -- where subquestions act as latent decisions within a broader\ninference trajectory -- helps the model \"connect the dots\" between fragmented\nknowledge and produce extended reasoning traces in non-reasoning models. We\nevaluate our method across three benchmarks and observe consistent\nimprovements. Notably, our approach yields a 2% overall improvement on\nMMMU-PRO, including a significant 9% gain in Liberal Arts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "MCTS"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08989", "pdf": "https://arxiv.org/pdf/2506.08989", "abs": "https://arxiv.org/abs/2506.08989", "authors": ["Xiao Liang", "Zhong-Zhi Li", "Yeyun Gong", "Yang Wang", "Hengyuan Zhang", "Yelong Shen", "Ying Nian Wu", "Weizhu Chen"], "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning", "categories": ["cs.LG", "cs.CL"], "comment": "Reinforcement Learning; Large Language Models; LLM Reasoning", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective\nfor training large language models (LLMs) on complex reasoning tasks, such as\nmathematical problem solving. A prerequisite for the scalability of RLVR is a\nhigh-quality problem set with precise and verifiable answers. However, the\nscarcity of well-crafted human-labeled math problems and limited-verification\nanswers in existing distillation-oriented synthetic datasets limit their\neffectiveness in RL. Additionally, most problem synthesis strategies\nindiscriminately expand the problem set without considering the model's\ncapabilities, leading to low efficiency in generating useful questions. To\nmitigate this issue, we introduce a Self-aware Weakness-driven problem\nSynthesis framework (SwS) that systematically identifies model deficiencies and\nleverages them for problem augmentation. Specifically, we define weaknesses as\nquestions that the model consistently fails to learn through its iterative\nsampling during RL training. We then extract the core concepts from these\nfailure cases and synthesize new problems to strengthen the model's weak areas\nin subsequent augmented training, enabling it to focus on and gradually\novercome its weaknesses. Without relying on external knowledge distillation,\nour framework enables robust generalization byempowering the model to\nself-identify and address its weaknesses in RL, yielding average performance\ngains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning\nbenchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08277", "pdf": "https://arxiv.org/pdf/2506.08277", "abs": "https://arxiv.org/abs/2506.08277", "authors": ["Subba Reddy Oota", "Khushbu Pahwa", "Prachi Jindal", "Satya Sai Srinath Namburi", "Maneesh Singh", "Tanmoy Chakraborty", "Bapi S. Raju", "Manish Gupta"], "title": "Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "39 pages, 22 figures", "summary": "Recent voxel-wise multimodal brain encoding studies have shown that\nmultimodal large language models (MLLMs) exhibit a higher degree of brain\nalignment compared to unimodal models in both unimodal and multimodal stimulus\nsettings. More recently, instruction-tuned multimodal models have shown to\ngenerate task-specific representations that align strongly with brain activity.\nHowever, prior work evaluating the brain alignment of MLLMs has primarily\nfocused on unimodal settings or relied on non-instruction-tuned multimodal\nmodels for multimodal stimuli. To address this gap, we investigated brain\nalignment, that is, measuring the degree of predictivity of neural activity\nrecorded while participants were watching naturalistic movies (video along with\naudio) with representations derived from MLLMs. We utilized\ninstruction-specific embeddings from six video and two audio instruction-tuned\nMLLMs. Experiments with 13 video task-specific instructions show that\ninstruction-tuned video MLLMs significantly outperform non-instruction-tuned\nmultimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for\nboth video and audio tasks using language-guided instructions shows clear\ndisentanglement in task-specific representations from MLLMs, leading to precise\ndifferentiation of multimodal functional processing in the brain. We also find\nthat MLLM layers align hierarchically with the brain, with early sensory areas\nshowing strong alignment with early layers, while higher-level visual and\nlanguage regions align more with middle to late layers. These findings provide\nclear evidence for the role of task-specific instructions in improving the\nalignment between brain activity and MLLMs, and open new avenues for mapping\njoint information processing in both the systems. We make the code publicly\navailable [https://github.com/subbareddy248/mllm_videos].", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.09040", "pdf": "https://arxiv.org/pdf/2506.09040", "abs": "https://arxiv.org/abs/2506.09040", "authors": ["Dianyi Wang", "Wei Song", "Yikun Wang", "Siyuan Wang", "Kaicheng Yu", "Zhongyu Wei", "Jiaqi Wang"], "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08480", "pdf": "https://arxiv.org/pdf/2506.08480", "abs": "https://arxiv.org/abs/2506.08480", "authors": ["Huixuan Zhang", "Xiaojun Wan"], "title": "Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-to-image models often struggle to generate images that precisely match\ntextual prompts. Prior research has extensively studied the evaluation of\nimage-text alignment in text-to-image generation. However, existing evaluations\nprimarily focus on agreement with human assessments, neglecting other critical\nproperties of a trustworthy evaluation framework. In this work, we first\nidentify two key aspects that a reliable evaluation should address. We then\nempirically demonstrate that current mainstream evaluation frameworks fail to\nfully satisfy these properties across a diverse range of metrics and models.\nFinally, we propose recommendations for improving image-text alignment\nevaluation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08518", "pdf": "https://arxiv.org/pdf/2506.08518", "abs": "https://arxiv.org/abs/2506.08518", "authors": ["Sunny Gupta", "Nikita Jangid", "Shounak Das", "Amit Sethi"], "title": "FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching", "categories": ["cs.AI", "cs.CV", "cs.LG", "I.2.6; C.1.4; D.1.3; I.5.1; H.3.4; I.2.10; I.4.0; I.4.1; I.4.2;\n  I.4.6; I.4.7; I.4.8; I.4.9; I.4.10; I.5.1; I.5.2; I.5.4; J.2; I.2.11; I.2.10"], "comment": "Accepted at ICML 2025 Workshop on Collaborative and Federated Agentic\n  Workflows CFAgentic @ ICML'25", "summary": "Domain Generalization (DG) seeks to train models that perform reliably on\nunseen target domains without access to target data during training. While\nrecent progress in smoothing the loss landscape has improved generalization,\nexisting methods often falter under long-tailed class distributions and\nconflicting optimization objectives. We introduce FedTAIL, a federated domain\ngeneralization framework that explicitly addresses these challenges through\nsharpness-guided, gradient-aligned optimization. Our method incorporates a\ngradient coherence regularizer to mitigate conflicts between classification and\nadversarial objectives, leading to more stable convergence. To combat class\nimbalance, we perform class-wise sharpness minimization and propose a\ncurvature-aware dynamic weighting scheme that adaptively emphasizes\nunderrepresented tail classes. Furthermore, we enhance conditional distribution\nalignment by integrating sharpness-aware perturbations into entropy\nregularization, improving robustness under domain shift. FedTAIL unifies\noptimization harmonization, class-aware regularization, and conditional\nalignment into a scalable, federated-compatible framework. Extensive\nevaluations across standard domain generalization benchmarks demonstrate that\nFedTAIL achieves state-of-the-art performance, particularly in the presence of\ndomain shifts and label imbalance, validating its effectiveness in both\ncentralized and federated settings. Code: https://github.com/sunnyinAI/FedTail", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08641", "pdf": "https://arxiv.org/pdf/2506.08641", "abs": "https://arxiv.org/abs/2506.08641", "authors": ["Simon Roschmann", "Quentin Bouniot", "Vasilii Feofanov", "Ievgen Redko", "Zeynep Akata"], "title": "Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Time series classification is a fundamental task in healthcare and industry,\nyet the development of time series foundation models (TSFMs) remains limited by\nthe scarcity of publicly available time series datasets. In this work, we\npropose Time Vision Transformer (TiViT), a framework that converts time series\ninto images to leverage the representational power of frozen Vision\nTransformers (ViTs) pretrained on large-scale image datasets. First, we\ntheoretically motivate our approach by analyzing the 2D patching of ViTs for\ntime series, showing that it can increase the number of label-relevant tokens\nand reduce the sample complexity. Second, we empirically demonstrate that TiViT\nachieves state-of-the-art performance on standard time series classification\nbenchmarks by utilizing the hidden representations of large OpenCLIP models. We\nexplore the structure of TiViT representations and find that intermediate\nlayers with high intrinsic dimension are the most effective for time series\nclassification. Finally, we assess the alignment between TiViT and TSFM\nrepresentation spaces and identify a strong complementarity, with further\nperformance gains achieved by combining their features. Our findings reveal yet\nanother direction for reusing vision representations in a non-visual domain.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.08716", "pdf": "https://arxiv.org/pdf/2506.08716", "abs": "https://arxiv.org/abs/2506.08716", "authors": ["Maximilian Tschuchnig", "Lukas Lamminger", "Philipp Steininger", "Michael Gadermayr"], "title": "Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment", "categories": ["eess.IV", "cs.CV"], "comment": "Data is open source. Code will be provided on acceptance. Paper\n  currently under review", "summary": "Cone-Beam Computed Tomography (CBCT) is widely used for real-time\nintraoperative imaging due to its low radiation dose and high acquisition\nspeed. However, despite its high resolution, CBCT suffers from significant\nartifacts and thereby lower visual quality, compared to conventional Computed\nTomography (CT). A recent approach to mitigate these artifacts is synthetic CT\n(sCT) generation, translating CBCT volumes into the CT domain. In this work, we\nenhance sCT generation through multimodal learning, integrating intraoperative\nCBCT with preoperative CT. Beyond validation on two real-world datasets, we use\na versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT\nquality affect sCT quality. The results demonstrate that multimodal sCT\nconsistently outperform unimodal baselines, with the most significant gains\nobserved in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate\nthat these findings are highly reproducible in real-world clinical datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-11.jsonl"}
{"id": "2506.09049", "pdf": "https://arxiv.org/pdf/2506.09049", "abs": "https://arxiv.org/abs/2506.09049", "authors": ["Li Kang", "Xiufeng Song", "Heng Zhou", "Yiran Qin", "Jie Yang", "Xiaohong Liu", "Philip Torr", "Lei Bai", "Zhenfei Yin"], "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "Project page: https://faceong.github.io/VIKI-R/", "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "testbed"], "score": 2}}, "source_file": "2025-06-11.jsonl"}
