{"id": "2506.12323", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.12323", "abs": "https://arxiv.org/abs/2506.12323", "authors": ["Janet Wang", "Yunbei Zhang", "Zhengming Ding", "Jihun Hamm"], "title": "Doctor Approved: Generating Medically Accurate Skin Disease Images through AI-Expert Feedback", "comment": null, "summary": "Paucity of medical data severely limits the generalizability of diagnostic ML\nmodels, as the full spectrum of disease variability can not be represented by a\nsmall clinical dataset. To address this, diffusion models (DMs) have been\nconsidered as a promising avenue for synthetic image generation and\naugmentation. However, they frequently produce medically inaccurate images,\ndeteriorating the model performance. Expert domain knowledge is critical for\nsynthesizing images that correctly encode clinical information, especially when\ndata is scarce and quality outweighs quantity. Existing approaches for\nincorporating human feedback, such as reinforcement learning (RL) and Direct\nPreference Optimization (DPO), rely on robust reward functions or demand\nlabor-intensive expert evaluations. Recent progress in Multimodal Large\nLanguage Models (MLLMs) reveals their strong visual reasoning capabilities,\nmaking them adept candidates as evaluators. In this work, we propose a novel\nframework, coined MAGIC (Medically Accurate Generation of Images through\nAI-Expert Collaboration), that synthesizes clinically accurate skin disease\nimages for data augmentation. Our method creatively translates expert-defined\ncriteria into actionable feedback for image synthesis of DMs, significantly\nimproving clinical accuracy while reducing the direct human workload.\nExperiments demonstrate that our method greatly improves the clinical quality\nof synthesized skin disease images, with outputs aligning with dermatologist\nassessments. Additionally, augmenting training data with these synthesized\nimages improves diagnostic accuracy by +9.02% on a challenging 20-condition\nskin disease classification task, and by +13.89% in the few-shot setting.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "reinforcement learning", "preference", "DPO"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "criteria"], "score": 3}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12365", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.12365", "abs": "https://arxiv.org/abs/2506.12365", "authors": ["Asifullah khan", "Muhammad Zaeem Khan", "Saleha Jamshed", "Sadia Ahmad", "Aleesha Zainab", "Kaynat Khatib", "Faria Bibi", "Abdul Rehman"], "title": "Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics", "comment": null, "summary": "This survey paper outlines the key developments in the field of Large\nLanguage Models (LLMs), such as enhancing their reasoning skills, adaptability\nto various tasks, increased computational efficiency, and ability to make\nethical decisions. The techniques that have been most effective in bridging the\ngap between human and machine communications include the Chain-of-Thought\nprompting, Instruction Tuning, and Reinforcement Learning from Human Feedback.\nThe improvements in multimodal learning and few-shot or zero-shot techniques\nhave further empowered LLMs to handle complex jobs with minor input. They also\nmanage to do more with less by applying scaling and optimization tricks for\ncomputing power conservation. This survey also offers a broader perspective on\nrecent advancements in LLMs going beyond isolated aspects such as model\narchitecture or ethical concerns. It categorizes emerging methods that enhance\nLLM reasoning, efficiency, and ethical alignment. It also identifies\nunderexplored areas such as interpretability, cross-modal integration and\nsustainability. With recent progress, challenges like huge computational costs,\nbiases, and ethical risks remain constant. Addressing these requires bias\nmitigation, transparent decision-making, and clear ethical guidelines. Future\nresearch will focus on enhancing models ability to handle multiple input,\nthereby making them more intelligent, safe, and reliable.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning from human feedback", "human feedback", "reinforcement learning", "alignment"], "score": 4}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12527", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12527", "abs": "https://arxiv.org/abs/2506.12527", "authors": ["Xiaoqing Cheng", "Hongying Zan", "Lulu Kong", "Jinwang Song", "Min Peng"], "title": "Detection, Classification, and Mitigation of Gender Bias in Large Language Models", "comment": null, "summary": "With the rapid development of large language models (LLMs), they have\nsignificantly improved efficiency across a wide range of domains. However,\nrecent studies have revealed that LLMs often exhibit gender bias, leading to\nserious social implications. Detecting, classifying, and mitigating gender bias\nin LLMs has therefore become a critical research focus. In the NLPCC 2025\nShared Task 7: Chinese Corpus for Gender Bias Detection, Classification and\nMitigation Challenge, we investigate how to enhance the capabilities of LLMs in\ngender bias detection, classification, and mitigation. We adopt reinforcement\nlearning, chain-of-thoughts (CoT) reasoning, and supervised fine-tuning to\nhandle different Subtasks. Specifically, for Subtasks 1 and 2, we leverage the\ninternal reasoning capabilities of LLMs to guide multi-step thinking in a\nstaged manner, which simplifies complex biased queries and improves response\naccuracy. For Subtask 3, we employ a reinforcement learning-based approach,\nannotating a preference dataset using GPT-4. We then apply Direct Preference\nOptimization (DPO) to mitigate gender bias by introducing a loss function that\nexplicitly favors less biased completions over biased ones. Our approach ranked\nfirst across all three subtasks of the NLPCC 2025 Shared Task 7.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "accuracy"], "score": 3}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13599", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13599", "abs": "https://arxiv.org/abs/2506.13599", "authors": ["Yuwei Du", "Jie Feng", "Jian Yuan", "Yong Li"], "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation", "comment": null, "summary": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered\n\\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation\n(\\textbf{CAMS}), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. \\textbf{CAMS}\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that \\textbf{CAMS} achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, \\textbf{CAMS} generates more realistic and\nplausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO"], "score": 3}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12446", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12446", "abs": "https://arxiv.org/abs/2506.12446", "authors": ["Bin Xie", "Bingbing Xu", "Yige Yuan", "Shengmao Zhu", "Huawei Shen"], "title": "From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment", "comment": null, "summary": "Inference-time alignment methods have gained significant attention for their\nefficiency and effectiveness in aligning large language models (LLMs) with\nhuman preferences. However, existing dominant approaches using reward-guided\nsearch (RGS) primarily rely on outcome reward models (ORMs), which suffer from\na critical granularity mismatch: ORMs are designed to provide outcome rewards\nfor complete responses, while RGS methods rely on process rewards to guide the\npolicy, leading to inconsistent scoring and suboptimal alignment. To address\nthis challenge, we introduce process reward models (PRMs) into RGS and argue\nthat an ideal PRM should satisfy two objectives: Score Consistency, ensuring\ncoherent evaluation across partial and complete responses, and Preference\nConsistency, aligning partial sequence assessments with human preferences.\nBased on these, we propose SP-PRM, a novel dual-consistency framework\nintegrating score consistency-based and preference consistency-based partial\nevaluation modules without relying on human annotation. Extensive experiments\non dialogue, summarization, and reasoning tasks demonstrate that SP-PRM\nsubstantially enhances existing RGS methods, achieving a 3.6%-10.3% improvement\nin GPT-4 evaluation scores across all tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "consistency", "summarization", "dialogue"], "score": 5}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12450", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12450", "abs": "https://arxiv.org/abs/2506.12450", "authors": ["Joanito Agili Lopo", "Muhammad Ravi Shulthan Habibi", "Tack Hwa Wong", "Muhammad Ilham Ghozali", "Fajri Koto", "Genta Indra Winata", "Peerat Limkonchotiwat", "Alham Fikri Aji", "Samuel Cahyawijaya"], "title": "Language Surgery in Multilingual Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across tasks and languages, revolutionizing natural language\nprocessing. This paper investigates the naturally emerging representation\nalignment in LLMs, particularly in the middle layers, and its implications for\ndisentangling language-specific and language-agnostic information. We\nempirically confirm the existence of this alignment, analyze its behavior in\ncomparison to explicitly designed alignment models, and demonstrate its\npotential for language-specific manipulation without semantic degradation.\nBuilding on these findings, we propose Inference-Time Language Control (ITLC),\na novel method that leverages latent injection to enable precise cross-lingual\nlanguage control and mitigate language confusion in LLMs. Our experiments\nhighlight ITLC's strong cross-lingual control capabilities while preserving\nsemantic integrity in target languages. Furthermore, we demonstrate its\neffectiveness in alleviating the cross-lingual language confusion problem,\nwhich persists even in current large-scale LLMs, leading to inconsistent\nlanguage generation. This work advances our understanding of representation\nalignment in LLMs and introduces a practical solution for enhancing their\ncross-lingual performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12737", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.12737", "abs": "https://arxiv.org/abs/2506.12737", "authors": ["Changsheng Gao", "Shan Liu", "Feng Wu", "Weisi Lin"], "title": "Cross-architecture universal feature coding via distribution alignment", "comment": null, "summary": "Feature coding has become increasingly important in scenarios where semantic\nrepresentations rather than raw pixels are transmitted and stored. However,\nmost existing methods are architecture-specific, targeting either CNNs or\nTransformers. This design limits their applicability in real-world scenarios\nwhere features from both architectures coexist. To address this gap, we\nintroduce a new research problem: cross-architecture universal feature coding\n(CAUFC), which seeks to build a unified codec that can effectively compress\nfeatures from heterogeneous architectures. To tackle this challenge, we propose\na two-step distribution alignment method. First, we design the format alignment\nmethod that unifies CNN and Transformer features into a consistent 2D token\nformat. Second, we propose the feature value alignment method that harmonizes\nstatistical distributions via truncation and normalization. As a first attempt\nto study CAUFC, we evaluate our method on the image classification task.\nExperimental results demonstrate that our method achieves superior\nrate-accuracy trade-offs compared to the architecture-specific baseline. This\nwork marks an initial step toward universal feature compression across\nheterogeneous model architectures.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12849", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.12849", "abs": "https://arxiv.org/abs/2506.12849", "authors": ["Songtao Jiang", "Yuan Wang", "Ruizhe Chen", "Yan Zhang", "Ruilin Luo", "Bohan Lei", "Sibo Song", "Yang Feng", "Jimeng Sun", "Jian Wu", "Zuozhu Liu"], "title": "CAPO: Reinforcing Consistent Reasoning in Medical Decision-Making", "comment": null, "summary": "In medical visual question answering (Med-VQA), achieving accurate responses\nrelies on three critical steps: precise perception of medical imaging data,\nlogical reasoning grounded in visual input and textual questions, and coherent\nanswer derivation from the reasoning process. Recent advances in general\nvision-language models (VLMs) show that large-scale reinforcement learning (RL)\ncould significantly enhance both reasoning capabilities and overall model\nperformance. However, their application in medical domains is hindered by two\nfundamental challenges: 1) misalignment between perceptual understanding and\nreasoning stages, and 2) inconsistency between reasoning pathways and answer\ngeneration, both compounded by the scarcity of high-quality medical datasets\nfor effective large-scale RL. In this paper, we first introduce Med-Zero-17K, a\ncurated dataset for pure RL-based training, encompassing over 30 medical image\nmodalities and 24 clinical tasks. Moreover, we propose a novel large-scale RL\nframework for Med-VLMs, Consistency-Aware Preference Optimization (CAPO), which\nintegrates rewards to ensure fidelity between perception and reasoning,\nconsistency in reasoning-to-answer derivation, and rule-based accuracy for\nfinal responses. Extensive experiments on both in-domain and out-of-domain\nscenarios demonstrate the superiority of our method over strong VLM baselines,\nshowcasing strong generalization capability to 3D Med-VQA benchmarks and\nR1-like training paradigms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy", "question answering"], "score": 4}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13351", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13351", "abs": "https://arxiv.org/abs/2506.13351", "authors": ["Yifei Xu", "Tusher Chakraborty", "Srinagesh Sharma", "Leonardo Nunes", "Emre Kıcıman", "Songwu Lu", "Ranveer Chandra"], "title": "Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have showcased impressive\nreasoning abilities in structured tasks like mathematics and programming,\nlargely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which\nuses outcome-based signals that are scalable, effective, and robust against\nreward hacking. However, applying similar techniques to open-ended long-form\nreasoning tasks remains challenging due to the absence of generic, verifiable\nreward signals. To address this, we propose Direct Reasoning Optimization\n(DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended,\nparticularly long-form, reasoning tasks, guided by a new reward signal: the\nReasoning Reflection Reward (R3). At its core, R3 selectively identifies and\nemphasizes key tokens in the reference outcome that reflect the influence of\nthe model's preceding chain-of-thought reasoning, thereby capturing the\nconsistency between reasoning and reference outcome at a fine-grained level.\nCrucially, R3 is computed internally using the same model being optimized,\nenabling a fully self-contained training setup. Additionally, we introduce a\ndynamic data filtering strategy based on R3 for open-ended reasoning tasks,\nreducing cost while improving downstream performance. We evaluate DRO on two\ndiverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a\nmath-oriented QA benchmark -- and show that it consistently outperforms strong\nbaselines while remaining broadly applicable across both open-ended and\nstructured domains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "reward hacking"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13307", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13307", "abs": "https://arxiv.org/abs/2506.13307", "authors": ["Solène Debuysère", "Nicolas Trouvé", "Nathan Letheule", "Olivier Lévêque", "Elise Colin"], "title": "Quantitative Comparison of Fine-Tuning Techniques for Pretrained Latent Diffusion Models in the Generation of Unseen SAR Image Concepts", "comment": null, "summary": "This work investigates the adaptation of large pre-trained latent diffusion\nmodels to a radically new imaging domain: Synthetic Aperture Radar (SAR). While\nthese generative models, originally trained on natural images, demonstrate\nimpressive capabilities in text-to-image synthesis, they are not natively\nadapted to represent SAR data, which involves different physics, statistical\ndistributions, and visual characteristics. Using a sizeable SAR dataset (on the\norder of 100,000 to 1 million images), we address the fundamental question of\nfine-tuning such models for this unseen modality. We explore and compare\nmultiple fine-tuning strategies, including full model fine-tuning and\nparameter-efficient approaches like Low-Rank Adaptation (LoRA), focusing\nseparately on the UNet diffusion backbone and the text encoder components. To\nevaluate generative quality, we combine several metrics: statistical distance\nfrom real SAR distributions, textural similarity via GLCM descriptors, and\nsemantic alignment assessed with a CLIP model fine-tuned on SAR data. Our\nresults show that a hybrid tuning strategy yields the best performance: full\nfine-tuning of the UNet is better at capturing low-level SAR-specific patterns,\nwhile LoRA-based partial tuning of the text encoder, combined with embedding\nlearning of the <SAR> token, suffices to preserve prompt alignment. This work\nprovides a methodical strategy for adapting foundation models to unconventional\nimaging modalities beyond natural image domains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13594", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13594", "abs": "https://arxiv.org/abs/2506.13594", "authors": ["Weimin Bai", "Yubo Li", "Wenzheng Chen", "Weijian Luo", "He Sun"], "title": "Dive3D: Diverse Distillation-based Text-to-3D Generation via Score Implicit Matching", "comment": null, "summary": "Distilling pre-trained 2D diffusion models into 3D assets has driven\nremarkable advances in text-to-3D synthesis. However, existing methods\ntypically rely on Score Distillation Sampling (SDS) loss, which involves\nasymmetric KL divergence--a formulation that inherently favors mode-seeking\nbehavior and limits generation diversity. In this paper, we introduce Dive3D, a\nnovel text-to-3D generation framework that replaces KL-based objectives with\nScore Implicit Matching (SIM) loss, a score-based objective that effectively\nmitigates mode collapse. Furthermore, Dive3D integrates both diffusion\ndistillation and reward-guided optimization under a unified divergence\nperspective. Such reformulation, together with SIM loss, yields significantly\nmore diverse 3D outputs while improving text alignment, human preference, and\noverall visual fidelity. We validate Dive3D across various 2D-to-3D prompts and\nfind that it consistently outperforms prior methods in qualitative assessments,\nincluding diversity, photorealism, and aesthetic appeal. We further evaluate\nits performance on the GPTEval3D benchmark, comparing against nine\nstate-of-the-art baselines. Dive3D also achieves strong results on quantitative\nmetrics, including text-asset alignment, 3D plausibility, text-geometry\nconsistency, texture quality, and geometric detail.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "human preference", "consistency"], "score": 3}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12066", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12066", "abs": "https://arxiv.org/abs/2506.12066", "authors": ["Gérôme Meyer", "Philip Breuer"], "title": "Focusing on Students, not Machines: Grounded Question Generation and Automated Answer Grading", "comment": null, "summary": "Digital technologies are increasingly used in education to reduce the\nworkload of teachers and students. However, creating open-ended study or\nexamination questions and grading their answers is still a tedious task. This\nthesis presents the foundation for a system that generates questions grounded\nin class materials and automatically grades student answers. It introduces a\nsophisticated method for chunking documents with a visual layout, specifically\ntargeting PDF documents. This method enhances the accuracy of downstream tasks,\nincluding Retrieval Augmented Generation (RAG). Our thesis demonstrates that\nhigh-quality questions and reference answers can be generated from study\nmaterial. Further, it introduces a new benchmark for automated grading of short\nanswers to facilitate comparison of automated grading systems. An evaluation of\nvarious grading systems is conducted and indicates that Large Language Models\n(LLMs) can generalise to the task of automated grading of short answers from\ntheir pre-training tasks. As with other tasks, increasing the parameter size of\nthe LLMs leads to greater performance. Currently, available systems still need\nhuman oversight, especially in examination scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13756", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13756", "abs": "https://arxiv.org/abs/2506.13756", "authors": ["Jingwei Ma", "Vivek Jayaram", "Brian Curless", "Ira Kemelmacher-Shlizerman", "Steven M. Seitz"], "title": "UltraZoom: Generating Gigapixel Images from Regular Photos", "comment": "Project page: https://ultra-zoom.github.io/", "summary": "We present UltraZoom, a system for generating gigapixel-resolution images of\nobjects from casually captured inputs, such as handheld phone photos. Given a\nfull-shot image (global, low-detail) and one or more close-ups (local,\nhigh-detail), UltraZoom upscales the full image to match the fine detail and\nscale of the close-up examples. To achieve this, we construct a per-instance\npaired dataset from the close-ups and adapt a pretrained generative model to\nlearn object-specific low-to-high resolution mappings. At inference, we apply\nthe model in a sliding window fashion over the full image. Constructing these\npairs is non-trivial: it requires registering the close-ups within the full\nimage for scale estimation and degradation alignment. We introduce a simple,\nrobust method for getting registration on arbitrary materials in casual,\nin-the-wild captures. Together, these components form a system that enables\nseamless pan and zoom across the entire object, producing consistent,\nphotorealistic gigapixel imagery from minimal input.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12097", "categories": ["cs.CL", "cs.CR", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.12097", "abs": "https://arxiv.org/abs/2506.12097", "authors": ["Vinith M. Suriyakumar", "Ayush Sekhari", "Ashia Wilson"], "title": "UCD: Unlearning in LLMs via Contrastive Decoding", "comment": null, "summary": "Machine unlearning aims to remove specific information, e.g. sensitive or\nundesirable content, from large language models (LLMs) while preserving overall\nperformance. We propose an inference-time unlearning algorithm that uses\ncontrastive decoding, leveraging two auxiliary smaller models, one trained\nwithout the forget set and one trained with it, to guide the outputs of the\noriginal model using their difference during inference. Our strategy\nsubstantially improves the tradeoff between unlearning effectiveness and model\nutility. We evaluate our approach on two unlearning benchmarks, TOFU and MUSE.\nResults show notable gains in both forget quality and retained performance in\ncomparison to prior approaches, suggesting that incorporating contrastive\ndecoding can offer an efficient, practical avenue for unlearning concepts in\nlarge-scale models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12109", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12109", "abs": "https://arxiv.org/abs/2506.12109", "authors": ["Hyungjune Bu", "Chanjoo Jung", "Minjae Kang", "Jaehyung Kim"], "title": "Personalized LLM Decoding via Contrasting Personal Preference", "comment": null, "summary": "As large language models (LLMs) are progressively deployed in various\nreal-world applications, personalization of LLMs has become increasingly\nimportant. While various approaches to LLM personalization such as prompt-based\nand training-based methods have been actively explored, the development of\neffective decoding-time algorithms remains largely overlooked, despite their\ndemonstrated potential. In this paper, we propose CoPe (Contrasting Personal\nPreference), a novel decoding-time approach applied after performing\nparameter-efficient fine-tuning (PEFT) on user-specific data. Our core idea is\nto leverage reward-guided decoding specifically for personalization by\nmaximizing each user's implicit reward signal. We evaluate CoPe across five\nopen-ended personalized text generation tasks. Our empirical results\ndemonstrate that CoPe achieves strong performance, improving personalization by\nan average of 10.57% in ROUGE-L, without relying on external reward models or\nadditional training procedures.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12115", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12115", "abs": "https://arxiv.org/abs/2506.12115", "authors": ["Brown Ebouky", "Andrea Bartezzaghi", "Mattia Rigotti"], "title": "Eliciting Reasoning in Language Models with Cognitive Tools", "comment": "22 pages, 2 figures", "summary": "The recent advent of reasoning models like OpenAI's o1 was met with excited\nspeculation by the AI community about the mechanisms underlying these\ncapabilities in closed models, followed by a rush of replication efforts,\nparticularly from the open source community. These speculations were largely\nsettled by the demonstration from DeepSeek-R1 that chains-of-thought and\nreinforcement learning (RL) can effectively replicate reasoning on top of base\nLLMs. However, it remains valuable to explore alternative methods for\ntheoretically eliciting reasoning that could help elucidate the underlying\nmechanisms, as well as providing additional methods that may offer\ncomplementary benefits.\n  Here, we build on the long-standing literature in cognitive psychology and\ncognitive architectures, which postulates that reasoning arises from the\norchestrated, sequential execution of a set of modular, predetermined cognitive\noperations. Crucially, we implement this key idea within a modern agentic\ntool-calling framework. In particular, we endow an LLM with a small set of\n\"cognitive tools\" encapsulating specific reasoning operations, each executed by\nthe LLM itself. Surprisingly, this simple strategy results in considerable\ngains in performance on standard mathematical reasoning benchmarks compared to\nbase LLMs, for both closed and open-weight models. For instance, providing our\n\"cognitive tools\" to GPT-4.1 increases its pass@1 performance on AIME2024 from\n26.7% to 43.3%, bringing it very close to the performance of o1-preview.\n  In addition to its practical implications, this demonstration contributes to\nthe debate regarding the role of post-training methods in eliciting reasoning\nin LLMs versus the role of inherent capabilities acquired during pre-training,\nand whether post-training merely uncovers these latent abilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12158", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12158", "abs": "https://arxiv.org/abs/2506.12158", "authors": ["Tatiana Ankinina", "Jan Cegin", "Jakub Simko", "Simon Ostermann"], "title": "A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages", "comment": "21 pages", "summary": "Large Language Models (LLMs) are increasingly used to generate synthetic\ntextual data for training smaller specialized models. However, a comparison of\nvarious generation strategies for low-resource language settings is lacking.\nWhile various prompting strategies have been proposed, such as demonstrations,\nlabel-based summaries, and self-revision, their comparative effectiveness\nremains unclear, especially for low-resource languages. In this paper, we\nsystematically evaluate the performance of these generation strategies and\ntheir combinations across 11 typologically diverse languages, including several\nextremely low-resource ones. Using three NLP tasks and four open-source LLMs,\nwe assess downstream model performance on generated versus gold-standard data.\nOur results show that strategic combinations of generation methods,\nparticularly target-language demonstrations with LLM-based revisions, yield\nstrong performance, narrowing the gap with real data to as little as 5% in some\nsettings. We also find that smart prompting techniques can reduce the advantage\nof larger LLMs, highlighting efficient generation strategies for synthetic data\ngeneration in low-resource scenarios with smaller models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12198", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.12198", "abs": "https://arxiv.org/abs/2506.12198", "authors": ["Sibo Dong", "Ismail Shaheen", "Maggie Shen", "Rupayan Mallick", "Sarah Adel Bargal"], "title": "ViSTA: Visual Storytelling using Multi-modal Adapters for Text-to-Image Diffusion Models", "comment": null, "summary": "Text-to-image diffusion models have achieved remarkable success, yet\ngenerating coherent image sequences for visual storytelling remains\nchallenging. A key challenge is effectively leveraging all previous text-image\npairs, referred to as history text-image pairs, which provide contextual\ninformation for maintaining consistency across frames. Existing auto-regressive\nmethods condition on all past image-text pairs but require extensive training,\nwhile training-free subject-specific approaches ensure consistency but lack\nadaptability to narrative prompts. To address these limitations, we propose a\nmulti-modal history adapter for text-to-image diffusion models, \\textbf{ViSTA}.\nIt consists of (1) a multi-modal history fusion module to extract relevant\nhistory features and (2) a history adapter to condition the generation on the\nextracted relevant features. We also introduce a salient history selection\nstrategy during inference, where the most salient history text-image pair is\nselected, improving the quality of the conditioning. Furthermore, we propose to\nemploy a Visual Question Answering-based metric TIFA to assess text-image\nalignment in visual storytelling, providing a more targeted and interpretable\nassessment of generated images. Evaluated on the StorySalon and FlintStonesSV\ndataset, our proposed ViSTA model is not only consistent across different\nframes, but also well-aligned with the narrative text descriptions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "question answering"], "score": 3}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12189", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12189", "abs": "https://arxiv.org/abs/2506.12189", "authors": ["Pranav Agarwal", "Ioana Ciucă"], "title": "Supernova Event Dataset: Interpreting Large Language Model's Personality through Critical Event Analysis", "comment": "Project Page - https://www.supernova-event.ai/", "summary": "Large Language Models (LLMs) are increasingly integrated into everyday\napplications. As their influence grows, understanding their decision making and\nunderlying personality becomes essential. In this work, we interpret model\npersonality using our proposed Supernova Event Dataset, a novel dataset with\ndiverse articles spanning biographies, historical events, news, and scientific\ndiscoveries. We use this dataset to benchmark LLMs on extracting and ranking\nkey events from text, a subjective and complex challenge that requires\nreasoning over long-range context and modeling causal chains. We evaluate small\nmodels like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as\nClaude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another\nLLM acts as a judge to infer each model's personality based on its selection\nand classification of events. Our analysis shows distinct personality traits:\nfor instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal\ndynamics, while Qwen 2.5 displays a more strategic, analytical style. When\nanalyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual\nframing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors\nstep-by-step causal reasoning. This analysis improves model interpretability,\nmaking them user-friendly for a wide range of diverse applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12266", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12266", "abs": "https://arxiv.org/abs/2506.12266", "authors": ["Avinash Baidya", "Kamalika Das", "Xiang Gao"], "title": "The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs", "comment": "ACL 2025; 18 pages, 8 figures", "summary": "Large Language Model (LLM)-based agents have significantly impacted\nTask-Oriented Dialog Systems (TODS) but continue to face notable performance\nchallenges, especially in zero-shot scenarios. While prior work has noted this\nperformance gap, the behavioral factors driving the performance gap remain\nunder-explored. This study proposes a comprehensive evaluation framework to\nquantify the behavior gap between AI agents and human experts, focusing on\ndiscrepancies in dialog acts, tool usage, and knowledge utilization. Our\nfindings reveal that this behavior gap is a critical factor negatively\nimpacting the performance of LLM agents. Notably, as task complexity increases,\nthe behavior gap widens (correlation: 0.963), leading to a degradation of agent\nperformance on complex task-oriented dialogs. For the most complex task in our\nstudy, even the GPT-4o-based agent exhibits low alignment with human behavior,\nwith low F1 scores for dialog acts (0.464), excessive and often misaligned tool\nusage with a F1 score of 0.139, and ineffective usage of external knowledge.\nReducing such behavior gaps leads to significant performance improvement (24.3%\non average). This study highlights the importance of comprehensive behavioral\nevaluations and improved alignment strategies to enhance the effectiveness of\nLLM-based TODS in handling complex tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12258", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.12258", "abs": "https://arxiv.org/abs/2506.12258", "authors": ["Yijiang Li", "Genpei Zhang", "Jiacheng Cheng", "Yi Li", "Xiaojun Shan", "Dashan Gao", "Jiancheng Lyu", "Yuan Li", "Ning Bi", "Nuno Vasconcelos"], "title": "EgoPrivacy: What Your First-Person Camera Says About You?", "comment": "ICML 2025", "summary": "While the rapid proliferation of wearable cameras has raised significant\nconcerns about egocentric video privacy, prior work has largely overlooked the\nunique privacy threats posed to the camera wearer. This work investigates the\ncore question: How much privacy information about the camera wearer can be\ninferred from their first-person view videos? We introduce EgoPrivacy, the\nfirst large-scale benchmark for the comprehensive evaluation of privacy risks\nin egocentric vision. EgoPrivacy covers three types of privacy (demographic,\nindividual, and situational), defining seven tasks that aim to recover private\ninformation ranging from fine-grained (e.g., wearer's identity) to\ncoarse-grained (e.g., age group). To further emphasize the privacy threats\ninherent to egocentric vision, we propose Retrieval-Augmented Attack, a novel\nattack strategy that leverages ego-to-exo retrieval from an external pool of\nexocentric videos to boost the effectiveness of demographic privacy attacks. An\nextensive comparison of the different attacks possible under all threat models\nis presented, showing that private information of the wearer is highly\nsusceptible to leakage. For instance, our findings indicate that foundation\nmodels can effectively compromise wearer privacy even in zero-shot settings by\nrecovering attributes such as identity, scene, gender, and race with 70-80%\naccuracy. Our code and data are available at\nhttps://github.com/williamium3000/ego-privacy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12307", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12307", "abs": "https://arxiv.org/abs/2506.12307", "authors": ["Xiaotian Zhang", "Yuan Wang", "Zhaopeng Feng", "Ruizhe Chen", "Zhijie Zhou", "Yan Zhang", "Hongxia Xu", "Jian Wu", "Zuozhu Liu"], "title": "Med-U1: Incentivizing Unified Medical Reasoning in LLMs via Large-scale Reinforcement Learning", "comment": null, "summary": "Medical Question-Answering (QA) encompasses a broad spectrum of tasks,\nincluding multiple choice questions (MCQ), open-ended text generation, and\ncomplex computational reasoning. Despite this variety, a unified framework for\ndelivering high-quality medical QA has yet to emerge. Although recent progress\nin reasoning-augmented large language models (LLMs) has shown promise, their\nability to achieve comprehensive medical understanding is still largely\nunexplored. In this paper, we present Med-U1, a unified framework for robust\nreasoning across medical QA tasks with diverse output formats, ranging from\nMCQs to complex generation and computation tasks. Med-U1 employs pure\nlarge-scale reinforcement learning with mixed rule-based binary reward\nfunctions, incorporating a length penalty to manage output verbosity. With\nmulti-objective reward optimization, Med-U1 directs LLMs to produce concise and\nverifiable reasoning chains. Empirical results reveal that Med-U1 significantly\nimproves performance across multiple challenging Med-QA benchmarks, surpassing\neven larger specialized and proprietary models. Furthermore, Med-U1\ndemonstrates robust generalization to out-of-distribution (OOD) tasks.\nExtensive analysis presents insights into training strategies, reasoning chain\nlength control, and reward design for medical LLMs. The code will be released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.12336", "abs": "https://arxiv.org/abs/2506.12336", "authors": ["Youze Wang", "Zijun Chen", "Ruoyu Chen", "Shishen Gu", "Yinpeng Dong", "Hang Su", "Jun Zhu", "Meng Wang", "Richang Hong", "Wenbo Hu"], "title": "Understanding and Benchmarking the Trustworthiness in Multimodal LLMs for Video Understanding", "comment": null, "summary": "Recent advancements in multimodal large language models for video\nunderstanding (videoLLMs) have improved their ability to process dynamic\nmultimodal data. However, trustworthiness challenges factual inaccuracies,\nharmful content, biases, hallucinations, and privacy risks, undermine\nreliability due to video data's spatiotemporal complexities. This study\nintroduces Trust-videoLLMs, a comprehensive benchmark evaluating videoLLMs\nacross five dimensions: truthfulness, safety, robustness, fairness, and\nprivacy. Comprising 30 tasks with adapted, synthetic, and annotated videos, the\nframework assesses dynamic visual scenarios, cross-modal interactions, and\nreal-world safety concerns. Our evaluation of 23 state-of-the-art videoLLMs (5\ncommercial,18 open-source) reveals significant limitations in dynamic visual\nscene understanding and cross-modal perturbation resilience. Open-source\nvideoLLMs show occasional truthfulness advantages but inferior overall\ncredibility compared to commercial models, with data diversity outperforming\nscale effects. These findings highlight the need for advanced safety alignment\nto enhance capabilities. Trust-videoLLMs provides a publicly available,\nextensible toolbox for standardized trustworthiness assessments, bridging the\ngap between accuracy-focused benchmarks and critical demands for robustness,\nsafety, fairness, and privacy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "truthfulness", "safety", "reliability", "accuracy"], "score": 6}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12433", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12433", "abs": "https://arxiv.org/abs/2506.12433", "authors": ["Hadi Mohammadi", "Efthymia Papadopoulou", "Yasmeen F. S. S. Meijer", "Ayoub Bagheri"], "title": "Exploring Cultural Variations in Moral Judgments with Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have shown strong performance across many tasks,\nbut their ability to capture culturally diverse moral values remains unclear.\nIn this paper, we examine whether LLMs can mirror variations in moral attitudes\nreported by two major cross-cultural surveys: the World Values Survey and the\nPEW Research Center's Global Attitudes Survey. We compare smaller, monolingual,\nand multilingual models (GPT-2, OPT, BLOOMZ, and Qwen) with more recent\ninstruction-tuned models (GPT-4o, GPT-4o-mini, Gemma-2-9b-it, and\nLlama-3.3-70B-Instruct). Using log-probability-based moral justifiability\nscores, we correlate each model's outputs with survey data covering a broad set\nof ethical topics. Our results show that many earlier or smaller models often\nproduce near-zero or negative correlations with human judgments. In contrast,\nadvanced instruction-tuned models (including GPT-4o and GPT-4o-mini) achieve\nsubstantially higher positive correlations, suggesting they better reflect\nreal-world moral attitudes. While scaling up model size and using instruction\ntuning can improve alignment with cross-cultural moral norms, challenges remain\nfor certain topics and regions. We discuss these findings in relation to bias\nanalysis, training data diversity, and strategies for improving the cultural\nsensitivity of LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12537", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12537", "abs": "https://arxiv.org/abs/2506.12537", "authors": ["Xiaoran Fan", "Zhichao Sun", "Yangfan Gao", "Jingfei Xiong", "Hang Yan", "Yifei Cao", "Jiajun Sun", "Shuo Li", "Zhihao Zhang", "Zhiheng Xi", "Yuhao Zhou", "Senjie Jin", "Changhao Jiang", "Junjie Ye", "Ming Zhang", "Rui Zheng", "Zhenhua Han", "Yunke Zhang", "Demei Yan", "Shaokang Dong", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Speech-Language Models with Decoupled Tokenizers and Multi-Token Prediction", "comment": null, "summary": "Speech-language models (SLMs) offer a promising path toward unifying speech\nand text understanding and generation. However, challenges remain in achieving\neffective cross-modal alignment and high-quality speech generation. In this\nwork, we systematically investigate the impact of key components (i.e., speech\ntokenizers, speech heads, and speaker modeling) on the performance of\nLLM-centric SLMs. We compare coupled, semi-decoupled, and fully decoupled\nspeech tokenizers under a fair SLM framework and find that decoupled\ntokenization significantly improves alignment and synthesis quality. To address\nthe information density mismatch between speech and text, we introduce\nmulti-token prediction (MTP) into SLMs, enabling each hidden state to decode\nmultiple speech tokens. This leads to up to 12$\\times$ faster decoding and a\nsubstantial drop in word error rate (from 6.07 to 3.01). Furthermore, we\npropose a speaker-aware generation paradigm and introduce RoleTriviaQA, a\nlarge-scale role-playing knowledge QA benchmark with diverse speaker\nidentities. Experiments demonstrate that our methods enhance both knowledge\nunderstanding and speaker consistency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12571", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.12571", "abs": "https://arxiv.org/abs/2506.12571", "authors": ["Saksorn Ruangtanusak", "Natthapath Rungseesiripak", "Peerawat Rojratchadakorn", "Monthol Charattrakool", "Natapong Nitarach"], "title": "DoTA-RAG: Dynamic of Thought Aggregation RAG", "comment": "SIGIR LiveRAG 2025 (oral presentation)", "summary": "In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a\nretrieval-augmented generation system optimized for high-throughput,\nlarge-scale web knowledge indexes. Traditional RAG pipelines often suffer from\nhigh latency and limited accuracy over massive, diverse datasets. DoTA-RAG\naddresses these challenges with a three-stage pipeline: query rewriting,\ndynamic routing to specialized sub-indexes, and multi-stage retrieval and\nranking. We further enhance retrieval by evaluating and selecting a superior\nembedding model, re-embedding the large FineWeb-10BT corpus. Moreover, we\ncreate a diverse Q&A dataset of 500 questions generated via the DataMorgana\nsetup across a broad range of WebOrganizer topics and formats. DoTA-RAG\nimproves the answer correctness score from 0.752 (baseline, using LiveRAG\npre-built vector store) to 1.478 while maintaining low latency, and it achieves\na 0.929 correctness score on the Live Challenge Day. These results highlight\nDoTA-RAG's potential for practical deployment in domains requiring fast,\nreliable access to large and evolving knowledge sources.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12576", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12576", "abs": "https://arxiv.org/abs/2506.12576", "authors": ["Ananya Joshi", "Celia Cintas", "Skyler Speakman"], "title": "Enabling Precise Topic Alignment in Large Language Models Via Sparse Autoencoders", "comment": null, "summary": "Recent work shows that Sparse Autoencoders (SAE) applied to large language\nmodel (LLM) layers have neurons corresponding to interpretable concepts. These\nSAE neurons can be modified to align generated outputs, but only towards\npre-identified topics and with some parameter tuning. Our approach leverages\nthe observational and modification properties of SAEs to enable alignment for\nany topic. This method 1) scores each SAE neuron by its semantic similarity to\nan alignment text and uses them to 2) modify SAE-layer-level outputs by\nemphasizing topic-aligned neurons. We assess the alignment capabilities of this\napproach on diverse public topic datasets including Amazon reviews, Medicine,\nand Sycophancy, across the currently available open-source LLMs and SAE pairs\n(GPT2 and Gemma) with multiple SAEs configurations. Experiments aligning to\nmedical prompts reveal several benefits over fine-tuning, including increased\naverage language acceptability (0.25 vs. 0.5), reduced training time across\nmultiple alignment topics (333.6s vs. 62s), and acceptable inference time for\nmany applications (+0.00092s/token). Our open-source code is available at\ngithub.com/IBM/sae-steering.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12568", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12568", "abs": "https://arxiv.org/abs/2506.12568", "authors": ["Chunjiang Wang", "Kun Zhang", "Yandong Liu", "Zhiyang He", "Xiaodong Tao", "S. Kevin Zhou"], "title": "MVP-CBM:Multi-layer Visual Preference-enhanced Concept Bottleneck Model for Explainable Medical Image Classification", "comment": "7 pages, 6 figures,", "summary": "The concept bottleneck model (CBM), as a technique improving interpretability\nvia linking predictions to human-understandable concepts, makes high-risk and\nlife-critical medical image classification credible. Typically, existing CBM\nmethods associate the final layer of visual encoders with concepts to explain\nthe model's predictions. However, we empirically discover the phenomenon of\nconcept preference variation, that is, the concepts are preferably associated\nwith the features at different layers than those only at the final layer; yet a\nblind last-layer-based association neglects such a preference variation and\nthus weakens the accurate correspondences between features and concepts,\nimpairing model interpretability. To address this issue, we propose a novel\nMulti-layer Visual Preference-enhanced Concept Bottleneck Model (MVP-CBM),\nwhich comprises two key novel modules: (1) intra-layer concept preference\nmodeling, which captures the preferred association of different concepts with\nfeatures at various visual layers, and (2) multi-layer concept sparse\nactivation fusion, which sparsely aggregates concept activations from multiple\nlayers to enhance performance. Thus, by explicitly modeling concept\npreferences, MVP-CBM can comprehensively leverage multi-layer visual\ninformation to provide a more nuanced and accurate explanation of model\ndecisions. Extensive experiments on several public medical classification\nbenchmarks demonstrate that MVP-CBM achieves state-of-the-art accuracy and\ninteroperability, verifying its superiority. Code is available at\nhttps://github.com/wcj6/MVP-CBM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12585", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.1"], "pdf": "https://arxiv.org/pdf/2506.12585", "abs": "https://arxiv.org/abs/2506.12585", "authors": ["Darryl Ho", "Samuel Madden"], "title": "DejaVid: Encoder-Agnostic Learned Temporal Matching for Video Classification", "comment": "Accepted to CVPR 2025 (IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition), main conference, poster presentation", "summary": "In recent years, large transformer-based video encoder models have greatly\nadvanced state-of-the-art performance on video classification tasks. However,\nthese large models typically process videos by averaging embedding outputs from\nmultiple clips over time to produce fixed-length representations. This approach\nfails to account for a variety of time-related features, such as variable video\ndurations, chronological order of events, and temporal variance in feature\nsignificance. While methods for temporal modeling do exist, they often require\nsignificant architectural changes and expensive retraining, making them\nimpractical for off-the-shelf, fine-tuned large encoders. To overcome these\nlimitations, we propose DejaVid, an encoder-agnostic method that enhances model\nperformance without the need for retraining or altering the architecture. Our\nframework converts a video into a variable-length temporal sequence of\nembeddings, which we call a multivariate time series (MTS). An MTS naturally\npreserves temporal order and accommodates variable video durations. We then\nlearn per-timestep, per-feature weights over the encoded MTS frames, allowing\nus to account for variations in feature importance over time. We introduce a\nnew neural network architecture inspired by traditional time series alignment\nalgorithms for this learning task. Our evaluation demonstrates that DejaVid\nsubstantially improves the performance of a state-of-the-art large encoder,\nachieving leading Top-1 accuracy of 77.2% on Something-Something V2, 89.1% on\nKinetics-400, and 88.6% on HMDB51, while adding fewer than 1.8% additional\nlearnable parameters and requiring less than 3 hours of training time. Our code\nis available at https://github.com/darrylho/DejaVid.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.12609", "abs": "https://arxiv.org/abs/2506.12609", "authors": ["Lexiang Tang", "Xianwei Zhuang", "Bang Yang", "Zhiyuan Hu", "Hongxiang Li", "Lu Ma", "Jinghan Ru", "Yuexian Zou"], "title": "Not All Tokens and Heads Are Equally Important: Dual-Level Attention Intervention for Hallucination Mitigation", "comment": null, "summary": "Large vision-language models (LVLMs) have shown remarkable capabilities\nacross a wide range of multimodal tasks. However, they remain prone to visual\nhallucination (VH), often producing confident but incorrect descriptions of\nvisual content. We present VisFlow, an efficient and training-free framework\ndesigned to mitigate VH by directly manipulating attention patterns during\ninference. Through systematic analysis, we identify three key pathological\nattention behaviors in LVLMs: (1) weak visual grounding, where attention to\nvisual tokens is insufficient or misallocated, over-focusing on uninformative\nregions; (2) language prior dominance, where excessive attention to prior\nresponse tokens reinforces autoregressive patterns and impairs multimodal\nalignment; (3) prompt redundancy, where many attention heads fixate on system\nprompt tokens, disrupting the integration of image, instruction, and response\ncontent. To address these issues, we introduce two inference-time\ninterventions: token-level attention intervention (TAI), which enhances focus\non salient visual content, and head-level attention intervention (HAI), which\nsuppresses over-attention to prompt and nearby text tokens. VisFlow operates\nwithout additional training or model modifications. Extensive experiments\nacross models and benchmarks show that VisFlow effectively reduces\nhallucinations and improves visual factuality, with negligible computational\ncost.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12704", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12704", "abs": "https://arxiv.org/abs/2506.12704", "authors": ["Wenhong Zhu", "Ruobing Xie", "Weinan Zhang", "Rui Wang"], "title": "Flexible Realignment of Language Models", "comment": null, "summary": "Realignment becomes necessary when a language model (LM) fails to meet\nexpected performance. We propose a flexible realignment framework that supports\nquantitative control of alignment degree during training and inference. This\nframework incorporates Training-time Realignment (TrRa), which efficiently\nrealigns the reference model by leveraging the controllable fusion of logits\nfrom both the reference and already aligned models. For example, TrRa reduces\ntoken usage by 54.63% on DeepSeek-R1-Distill-Qwen-1.5B without any performance\ndegradation, outperforming DeepScaleR-1.5B's 33.86%. To complement TrRa during\ninference, we introduce a layer adapter that enables smooth Inference-time\nRealignment (InRa). This adapter is initialized to perform an identity\ntransformation at the bottom layer and is inserted preceding the original\nlayers. During inference, input embeddings are simultaneously processed by the\nadapter and the original layer, followed by the remaining layers, and then\ncontrollably interpolated at the logit level. We upgraded\nDeepSeek-R1-Distill-Qwen-7B from a slow-thinking model to one that supports\nboth fast and slow thinking, allowing flexible alignment control even during\ninference. By encouraging deeper reasoning, it even surpassed its original\nperformance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12758", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12758", "abs": "https://arxiv.org/abs/2506.12758", "authors": ["David Guzman Piedrahita", "Irene Strauss", "Bernhard Schölkopf", "Rada Mihalcea", "Zhijing Jin"], "title": "Democratic or Authoritarian? Probing a New Dimension of Political Biases in Large Language Models", "comment": null, "summary": "As Large Language Models (LLMs) become increasingly integrated into everyday\nlife and information ecosystems, concerns about their implicit biases continue\nto persist. While prior work has primarily examined socio-demographic and\nleft--right political dimensions, little attention has been paid to how LLMs\nalign with broader geopolitical value systems, particularly the\ndemocracy--authoritarianism spectrum. In this paper, we propose a novel\nmethodology to assess such alignment, combining (1) the F-scale, a psychometric\ntool for measuring authoritarian tendencies, (2) FavScore, a newly introduced\nmetric for evaluating model favorability toward world leaders, and (3)\nrole-model probing to assess which figures are cited as general role-models by\nLLMs. We find that LLMs generally favor democratic values and leaders, but\nexhibit increases favorability toward authoritarian figures when prompted in\nMandarin. Further, models are found to often cite authoritarian figures as role\nmodels, even outside explicit political contexts. These results shed light on\nways LLMs may reflect and potentially reinforce global political ideologies,\nhighlighting the importance of evaluating bias beyond conventional\nsocio-political axes. Our code is available at:\nhttps://github.com/irenestrauss/Democratic-Authoritarian-Bias-LLMs", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12886", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12886", "abs": "https://arxiv.org/abs/2506.12886", "authors": ["Adrián Cuadrón", "Aimar Sagasti", "Maitane Urruela", "Iker De la Iglesia", "Ane G Domingo-Aldama", "Aitziber Atutxa", "Josu Goikoetxea", "Ander Barrena"], "title": "ArgHiTZ at ArchEHR-QA 2025: A Two-Step Divide and Conquer Approach to Patient Question Answering for Top Factuality", "comment": "This paper has been accepted for publication in Proceedings of the\n  24th Workshop on Biomedical Natural Language Processing (BioNLP) at ACL 2025", "summary": "This work presents three different approaches to address the ArchEHR-QA 2025\nShared Task on automated patient question answering. We introduce an end-to-end\nprompt-based baseline and two two-step methods to divide the task, without\nutilizing any external knowledge. Both two step approaches first extract\nessential sentences from the clinical text, by prompt or similarity ranking,\nand then generate the final answer from these notes. Results indicate that the\nre-ranker based two-step system performs best, highlighting the importance of\nselecting the right approach for each subtask. Our best run achieved an overall\nscore of 0.44, ranking 8th out of 30 on the leaderboard, securing the top\nposition in overall factuality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "question answering"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12935", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12935", "abs": "https://arxiv.org/abs/2506.12935", "authors": ["Xingjian Diao", "Chunhui Zhang", "Keyi Kong", "Weiyi Wu", "Chiyu Ma", "Zhongyu Ouyang", "Peijun Qing", "Soroush Vosoughi", "Jiang Gui"], "title": "SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models", "comment": null, "summary": "While large language models have shown reasoning capabilities, their\napplication to the audio modality, particularly in large audio-language models\n(ALMs), remains significantly underdeveloped. Addressing this gap requires a\nsystematic approach, involving a capable base model, high-quality\nreasoning-oriented audio data, and effective training algorithms. In this\nstudy, we present a comprehensive solution: we introduce the Audio Logical\nReasoning (ALR) dataset, consisting of 6,446 text-audio annotated samples\nspecifically designed for complex reasoning tasks. Building on this resource,\nwe propose SoundMind, a rule-based reinforcement learning (RL) algorithm\ntailored to endow ALMs with deep bimodal reasoning abilities. By training\nQwen2.5-Omni-7B on the ALR dataset using SoundMind, our approach achieves\nstate-of-the-art performance in audio logical reasoning. This work highlights\nthe impact of combining high-quality, reasoning-focused datasets with\nspecialized RL techniques, advancing the frontier of auditory intelligence in\nlanguage models. Our code and the proposed dataset are available at\nhttps://github.com/xid32/SoundMind.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13020", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13020", "abs": "https://arxiv.org/abs/2506.13020", "authors": ["Ikeoluwa Abioye", "Jiani Ge"], "title": "Edeflip: Supervised Word Translation between English and Yoruba", "comment": null, "summary": "In recent years, embedding alignment has become the state-of-the-art machine\ntranslation approach, as it can yield high-quality translation without training\non parallel corpora. However, existing research and application of embedding\nalignment mostly focus on high-resource languages with high-quality monolingual\nembeddings. It is unclear if and how low-resource languages may be similarly\nbenefited. In this study, we implement an established supervised embedding\nalignment method for word translation from English to Yoruba, the latter a\nlow-resource language. We found that higher embedding quality and normalizing\nembeddings increase word translation precision, with, additionally, an\ninteraction effect between the two. Our results demonstrate the limitations of\nthe state-of-the-art supervised embedding alignment when it comes to\nlow-resource languages, for which there are additional factors that need to be\ntaken into consideration, such as the importance of curating high-quality\nmonolingual embeddings. We hope our work will be a starting point for further\nmachine translation research that takes into account the challenges that\nlow-resource languages face.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13066", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13066", "abs": "https://arxiv.org/abs/2506.13066", "authors": ["Kai Lan", "Jiayong Zhu", "Jiangtong Li", "Dawei Cheng", "Guang Chen", "Changjun Jiang"], "title": "FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design", "comment": "26 pages, 16 figures", "summary": "Large Multimodal Models (LMMs) demonstrate significant cross-modal reasoning\ncapabilities. However, financial applications face challenges due to the lack\nof high-quality multimodal reasoning datasets and the inefficiency of existing\ntraining paradigms for reasoning enhancement. To address these issues, we\npropose an integrated framework, FinLMM-R1, combining an automated and scalable\npipeline for data construction with enhanced training strategies to improve the\nmultimodal reasoning of LMM. The Automated and Scalable Pipeline (ASP) resolves\ntextual-visual misalignment in financial reports through a separate paradigm of\nquestion-answer generation and image-question alignment, ensuring data\nintegrity and extraction efficiency. Through ASP, we collect 89,378 aligned\nimage-question pairs from 23,397 financial reports, covering tasks such as\narithmetic reasoning, statistics reasoning, financial explanation, and\nfinancial knowledge. Moreover, we introduce the Thinking with Adversarial\nReward in LMM (TAR-LMM), extending the prior two-stage training framework [1]\nwith additional reward mechanisms. In the first stage, we focus on text-only\ntasks with format and accuracy rewards to guide the model in generating\nwell-structured thinking contents. In the second stage, we construct\nmulti-image contrastive samples with additional reward components including\nimage selection, thinking content length, and adversarial reward to jointly\noptimize the LMM across visual perception, reasoning efficiency, and logical\ncoherence. Extensive experiments on 7 benchmarks show ASP-derived dataset and\ntraining framework significantly improve answer accuracy and reasoning depth\nover existing reasoning LMMs in both general and financial multimodal contexts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.12871", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.12871", "abs": "https://arxiv.org/abs/2506.12871", "authors": ["Rongxuan Peng", "Shunquan Tan", "Xianbo Mo", "Alex C. Kot", "Jiwu Huang"], "title": "Active Adversarial Noise Suppression for Image Forgery Localization", "comment": null, "summary": "Recent advances in deep learning have significantly propelled the development\nof image forgery localization. However, existing models remain highly\nvulnerable to adversarial attacks: imperceptible noise added to forged images\ncan severely mislead these models. In this paper, we address this challenge\nwith an Adversarial Noise Suppression Module (ANSM) that generate a defensive\nperturbation to suppress the attack effect of adversarial noise. We observe\nthat forgery-relevant features extracted from adversarial and original forged\nimages exhibit distinct distributions. To bridge this gap, we introduce\nForgery-relevant Features Alignment (FFA) as a first-stage training strategy,\nwhich reduces distributional discrepancies by minimizing the channel-wise\nKullback-Leibler divergence between these features. To further refine the\ndefensive perturbation, we design a second-stage training strategy, termed\nMask-guided Refinement (MgR), which incorporates a dual-mask constraint. MgR\nensures that the perturbation remains effective for both adversarial and\noriginal forged images, recovering forgery localization accuracy to their\noriginal level. Extensive experiments across various attack algorithms\ndemonstrate that our method significantly restores the forgery localization\nmodel's performance on adversarial images. Notably, when ANSM is applied to\noriginal forged images, the performance remains nearly unaffected. To our best\nknowledge, this is the first report of adversarial defense in image forgery\nlocalization tasks. We have released the source code and anti-forensics\ndataset.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13181", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13181", "abs": "https://arxiv.org/abs/2506.13181", "authors": ["Philipp Spohn", "Leander Girrbach", "Jessica Bader", "Zeynep Akata"], "title": "Align-then-Unlearn: Embedding Alignment for LLM Unlearning", "comment": "Accepted at ICML 2025 Workshop on Machine Unlearning for Generative\n  AI", "summary": "As large language models (LLMs) are trained on massive datasets, they have\nraised significant privacy and ethical concerns due to their potential to\ninadvertently retain sensitive information. Unlearning seeks to selectively\nremove specific data from trained models, such as personal information or\ncopyrighted content. Current approaches targeting specific output sequences at\nthe token level often fail to achieve complete forgetting and remain\nsusceptible to prompt rephrasing. We propose Align-then-Unlearn, a novel\nframework that performs unlearning in the semantic embedding space rather than\ndirectly on output tokens. Align-then-Unlearn first augments the LLM with an\nembedding prediction module trained to anticipate future context\nrepresentations. Unlearning is then achieved by fine-tuning the model to\nminimize the similarity between these predicted embeddings and a target\nembedding that represents the concept to be removed. Initial results show that\nAlign-then-Unlearn effectively removes targeted knowledge with minimal\ndegradation in overall model utility. These findings suggest that\nembedding-based unlearning offers a promising and robust approach to removing\nconceptual knowledge. Our code is available at\nhttps://github.com/ExplainableML/align-then-unlearn.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13199", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.13199", "abs": "https://arxiv.org/abs/2506.13199", "authors": ["Yongjae Kim", "Seongchan Park"], "title": "Do Music Preferences Reflect Cultural Values? A Cross-National Analysis Using Music Embedding and World Values Survey", "comment": null, "summary": "This study explores the extent to which national music preferences reflect\nunderlying cultural values. We collected long-term popular music data from\nYouTube Music Charts across 62 countries, encompassing both Western and\nnon-Western regions, and extracted audio embeddings using the CLAP model. To\ncomplement these quantitative representations, we generated semantic captions\nfor each track using LP-MusicCaps and GPT-based summarization. Countries were\nclustered based on contrastive embeddings that highlight deviations from global\nmusical norms. The resulting clusters were projected into a two-dimensional\nspace via t-SNE for visualization and evaluated against cultural zones defined\nby the World Values Survey (WVS). Statistical analyses, including MANOVA and\nchi-squared tests, confirmed that music-based clusters exhibit significant\nalignment with established cultural groupings. Furthermore, residual analysis\nrevealed consistent patterns of overrepresentation, suggesting non-random\nassociations between specific clusters and cultural zones. These findings\nindicate that national-level music preferences encode meaningful cultural\nsignals and can serve as a proxy for understanding global cultural boundaries.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13216", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13216", "abs": "https://arxiv.org/abs/2506.13216", "authors": ["Qiming Ge", "Shuhao Xing", "Songyang Gao", "Yunhua Zhou", "Yicheng Zou", "Songyang Zhang", "Zhi Chen", "Hang Yan", "Qi Zhang", "Qipeng Guo", "Kai Chen"], "title": "Capability Salience Vector: Fine-grained Alignment of Loss and Capabilities for Downstream Task Scaling Law", "comment": "9 pages, 9 figures, ACL2025", "summary": "Scaling law builds the relationship between training computation and\nvalidation loss, enabling researchers to effectively predict the loss trending\nof models across different levels of computation. However, a gap still remains\nbetween validation loss and the model's downstream capabilities, making it\nuntrivial to apply scaling law to direct performance prediction for downstream\ntasks. The loss typically represents a cumulative penalty for predicted tokens,\nwhich are implicitly considered to have equal importance. Nevertheless, our\nstudies have shown evidence that when considering different training data\ndistributions, we cannot directly model the relationship between downstream\ncapability and computation or token loss. To bridge the gap between validation\nloss and downstream task capabilities, in this work, we introduce Capability\nSalience Vector, which decomposes the overall loss and assigns different\nimportance weights to tokens to assess a specific meta-capability, aligning the\nvalidation loss with downstream task performance in terms of the model's\ncapabilities. Experiments on various popular benchmarks demonstrate that our\nproposed Capability Salience Vector could significantly improve the\npredictability of language model performance on downstream tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scaling law"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13229", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13229", "abs": "https://arxiv.org/abs/2506.13229", "authors": ["Zijie Lin", "Yang Zhang", "Xiaoyan Zhao", "Fengbin Zhu", "Fuli Feng", "Tat-Seng Chua"], "title": "IGD: Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation", "comment": null, "summary": "Large Language Models (LLMs) have shown strong potential for recommendation\nby framing item prediction as a token-by-token language generation task.\nHowever, existing methods treat all item tokens equally, simply pursuing\nlikelihood maximization during both optimization and decoding. This overlooks\ncrucial token-level differences in decisiveness-many tokens contribute little\nto item discrimination yet can dominate optimization or decoding. To quantify\ntoken decisiveness, we propose a novel perspective that models item generation\nas a decision process, measuring token decisiveness by the Information Gain\n(IG) each token provides in reducing uncertainty about the generated item. Our\nempirical analysis reveals that most tokens have low IG but often correspond to\nhigh logits, disproportionately influencing training loss and decoding, which\nmay impair model performance. Building on these insights, we introduce an\nInformation Gain-based Decisiveness-aware Token handling (IGD) strategy that\nintegrates token decisiveness into both tuning and decoding. Specifically, IGD\ndownweights low-IG tokens during tuning and rebalances decoding to emphasize\ntokens with high IG. In this way, IGD moves beyond pure likelihood\nmaximization, effectively prioritizing high-decisiveness tokens. Extensive\nexperiments on four benchmark datasets with two LLM backbones demonstrate that\nIGD consistently improves recommendation accuracy, achieving significant gains\non widely used ranking metrics compared to strong baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13284", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13284", "abs": "https://arxiv.org/abs/2506.13284", "authors": ["Zihan Liu", "Zhuolin Yang", "Yang Chen", "Chankyu Lee", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "title": "AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy", "comment": "The AceReason-Nemotron collection:\n  https://huggingface.co/collections/nvidia/acereason-682f4e1261dc22f697fd1485", "summary": "In this work, we investigate the synergy between supervised fine-tuning (SFT)\nand reinforcement learning (RL) in developing strong reasoning models. We begin\nby curating the SFT training data through two scaling strategies: increasing\nthe number of collected prompts and the number of generated responses per\nprompt. Both approaches yield notable improvements in reasoning performance,\nwith scaling the number of prompts resulting in more substantial gains. We then\nexplore the following questions regarding the synergy between SFT and RL: (i)\nDoes a stronger SFT model consistently lead to better final performance after\nlarge-scale RL training? (ii) How can we determine an appropriate sampling\ntemperature during RL training to effectively balance exploration and\nexploitation for a given SFT initialization? Our findings suggest that (i)\nholds true, provided effective RL training is conducted, particularly when the\nsampling temperature is carefully chosen to maintain the temperature-adjusted\nentropy around 0.3, a setting that strikes a good balance between exploration\nand exploitation. Notably, the performance gap between initial SFT models\nnarrows significantly throughout the RL process. Leveraging a strong SFT\nfoundation and insights into the synergistic interplay between SFT and RL, our\nAceReason-Nemotron-1.1 7B model significantly outperforms\nAceReason-Nemotron-1.0 and achieves new state-of-the-art performance among\nQwen2.5-7B-based reasoning models on challenging math and code benchmarks,\nthereby demonstrating the effectiveness of our post-training recipe. We release\nthe model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13285", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13285", "abs": "https://arxiv.org/abs/2506.13285", "authors": ["Houcheng Jiang", "Zetong Zhao", "Junfeng Fang", "Haokai Ma", "Ruipeng Wang", "Yang Deng", "Xiang Wang", "Xiangnan He"], "title": "Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs", "comment": null, "summary": "Large language models (LLMs) have shown strong performance across natural\nlanguage tasks, but remain vulnerable to backdoor attacks. Recent model\nediting-based approaches enable efficient backdoor injection by directly\nmodifying parameters to map specific triggers to attacker-desired responses.\nHowever, these methods often suffer from safety fallback, where the model\ninitially responds affirmatively but later reverts to refusals due to safety\nalignment. In this work, we propose DualEdit, a dual-objective model editing\nframework that jointly promotes affirmative outputs and suppresses refusal\nresponses. To address two key challenges -- balancing the trade-off between\naffirmative promotion and refusal suppression, and handling the diversity of\nrefusal expressions -- DualEdit introduces two complementary techniques. (1)\nDynamic loss weighting calibrates the objective scale based on the pre-edited\nmodel to stabilize optimization. (2) Refusal value anchoring compresses the\nsuppression target space by clustering representative refusal value vectors,\nreducing optimization conflict from overly diverse token sets. Experiments on\nsafety-aligned LLMs show that DualEdit improves attack success by 9.98\\% and\nreduces safety fallback rate by 10.88\\% over baselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13300", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.13300", "abs": "https://arxiv.org/abs/2506.13300", "authors": ["Bo Li", "Chengben Xu", "Wufeng Zhang"], "title": "Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning Language Models", "comment": null, "summary": "This paper presents Seewo's systems for both tracks of the Multilingual\nConversational Speech Language Model Challenge (MLC-SLM), addressing automatic\nspeech recognition (ASR) and speaker diarization with ASR (SD-ASR). We\nintroduce a multi-stage training pipeline that explicitly enhances reasoning\nand self-correction in speech language models for ASR. Our approach combines\ncurriculum learning for progressive capability acquisition, Chain-of-Thought\ndata augmentation to foster intermediate reflection, and Reinforcement Learning\nwith Verifiable Rewards (RLVR) to further refine self-correction through\nreward-driven optimization. This approach achieves substantial improvements\nover the official challenge baselines. On the evaluation set, our best system\nattains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track\n2. Comprehensive ablation studies demonstrate the effectiveness of each\ncomponent under challenge constraints.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13038", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.13038", "abs": "https://arxiv.org/abs/2506.13038", "authors": ["Zijian Zhang", "Xuecheng Wu", "Danlei Huang", "Siyu Yan", "Chong Peng", "Xuezhi Cao"], "title": "HKD4VLM: A Progressive Hybrid Knowledge Distillation Framework for Robust Multimodal Hallucination and Factuality Detection in VLMs", "comment": null, "summary": "Driven by the rapid progress in vision-language models (VLMs), the\nresponsible behavior of large-scale multimodal models has become a prominent\nresearch area, particularly focusing on hallucination detection and factuality\nchecking. In this paper, we present the solution for the two tracks of\nResponsible AI challenge. Inspirations from the general domain demonstrate that\na smaller distilled VLM can often outperform a larger VLM that is directly\ntuned on downstream tasks, while achieving higher efficiency. We thus jointly\ntackle two tasks from the perspective of knowledge distillation and propose a\nprogressive hybrid knowledge distillation framework termed HKD4VLM.\nSpecifically, the overall framework can be decomposed into Pyramid-like\nProgressive Online Distillation and Ternary-Coupled Refinement Distillation,\nhierarchically moving from coarse-grained knowledge alignment to fine-grained\nrefinement. Besides, we further introduce the mapping shift-enhanced inference\nand diverse augmentation strategies to enhance model performance and\nrobustness. Extensive experimental results demonstrate the effectiveness of our\nHKD4VLM. Ablation studies provide insights into the critical design choices\ndriving performance gains.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "fine-grained"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13328", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13328", "abs": "https://arxiv.org/abs/2506.13328", "authors": ["Chaoxu Pang", "Yixuan Cao", "Ganbin Zhou", "Hongwei Li", "Ping Luo"], "title": "Document-Level Tabular Numerical Cross-Checking: A Coarse-to-Fine Approach", "comment": "Submitted to IEEE TKDE", "summary": "Numerical consistency across tables in disclosure documents is critical for\nensuring accuracy, maintaining credibility, and avoiding reputational and\neconomic risks. Automated tabular numerical cross-checking presents two\nsignificant challenges: (C1) managing the combinatorial explosion of candidate\ninstances at the document level and (C2) comprehending multi-faceted numerical\nsemantics. Previous research typically depends on heuristic-based filtering or\nsimplified context extraction, often struggling to balance performance and\nefficiency. Recently, large language models (LLMs) have demonstrated remarkable\ncontextual understanding capabilities that helps address C2 at the instance\nlevel, yet they remain hampered by computational inefficiency (C1) and limited\ndomain expertise. This paper introduces CoFiTCheck, a novel LLM-based\ncoarse-to-fine framework that addresses these challenges through two sequential\nstages: embedding-based filtering and discriminative classification. The\nembedding-based filtering stage introduces an instructional parallel encoding\nmethod to efficiently represent all numerical mentions in a table with LLMs, as\nwell as a decoupled InfoNCE objective to mitigate the isolated mention problem.\nThe discriminative classification stage employs a specialized LLM for\nfine-grained analysis of the remaining candidate pairs. This stage is further\nenhanced by our crosstable numerical alignment pretraining paradigm, which\nleverages weak supervision from cross-table numerical equality relationships to\nenrich task-specific priors without requiring manual annotation. Comprehensive\nevaluation across three types of real-world disclosure documents demonstrates\nthat CoFiTCheck significantly outperforms previous methods while maintaining\npractical efficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "consistency", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13039", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13039", "abs": "https://arxiv.org/abs/2506.13039", "authors": ["Amran Bhuiyan", "Mizanur Rahman", "Md Tahmid Rahman Laskar", "Aijun An", "Jimmy Xiangji Huang"], "title": "Evolution of ReID: From Early Methods to LLM Integration", "comment": null, "summary": "Person re-identification (ReID) has evolved from handcrafted feature-based\nmethods to deep learning approaches and, more recently, to models incorporating\nlarge language models (LLMs). Early methods struggled with variations in\nlighting, pose, and viewpoint, but deep learning addressed these issues by\nlearning robust visual features. Building on this, LLMs now enable ReID systems\nto integrate semantic and contextual information through natural language. This\nsurvey traces that full evolution and offers one of the first comprehensive\nreviews of ReID approaches that leverage LLMs, where textual descriptions are\nused as privileged information to improve visual matching. A key contribution\nis the use of dynamic, identity-specific prompts generated by GPT-4o, which\nenhance the alignment between images and text in vision-language ReID systems.\nExperimental results show that these descriptions improve accuracy, especially\nin complex or ambiguous cases. To support further research, we release a large\nset of GPT-4o-generated descriptions for standard ReID datasets. By bridging\ncomputer vision and natural language processing, this survey offers a unified\nperspective on the field's development and outlines key future directions such\nas better prompt design, cross-modal transfer learning, and real-world\nadaptability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13329", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13329", "abs": "https://arxiv.org/abs/2506.13329", "authors": ["Zhongqian Fu", "Ning Ding", "Kai Han", "Xianzhi Yu", "Xiaosong Li", "Xinghao Chen", "Yehui Tang", "Yunhe Wang"], "title": "EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization", "comment": null, "summary": "Mixture-of-Experts (MoE) models have emerged as a cornerstone of large-scale\ndeep learning by efficiently distributing computation and enhancing\nperformance. However, their unique architecture-characterized by sparse expert\nactivation and dynamic routing mechanisms-introduces inherent complexities that\nchallenge conventional quantization techniques. Existing post-training\nquantization (PTQ) methods struggle to address activation outliers, router\nconsistency and sparse expert calibration, leading to significant performance\ndegradation. To bridge this gap, we propose EAQuant, a novel PTQ framework\ntailored for MoE architectures. Our method systematically tackles these\nchallenges through three key innovations: (1) expert-aware smoothing\naggregation to suppress activation outliers and stabilize quantization, (2)\nrouter logits distribution alignment to preserve expert selection consistency\npost-quantization, and (3) expert-level calibration data balance to optimize\nsparsely activated experts. Extensive experiments across W4A4 and extreme W3A4\nquantization configurations demonstrate that EAQuant significantly outperforms\nexisting methods, achieving average score improvements of 1.15 - 2.28% across\nthree diverse MoE architectures, with particularly pronounced gains in\nreasoning tasks and robust performance retention under aggressive quantization.\nBy integrating these innovations, EAQuant establishes a new state-of-the-art\nfor high-precision, efficient MoE model compression. Our code is available at\nhttps://github.com/darren-fzq/EAQuant.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13363", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13363", "abs": "https://arxiv.org/abs/2506.13363", "authors": ["Lijun Liu", "Ruiyang Li", "Zhaocheng Liu", "Chenglin Zhu", "Chong Li", "Jiehan Cheng", "Qiang Ju", "Jian Xie"], "title": "Efficient Medical VIE via Reinforcement Learning", "comment": null, "summary": "Visual Information Extraction (VIE) converts unstructured document images\ninto structured formats like JSON, critical for medical applications such as\nreport analysis and online consultations. Traditional methods rely on OCR and\nlanguage models, while end-to-end multimodal models offer direct JSON\ngeneration. However, domain-specific schemas and high annotation costs limit\ntheir effectiveness in medical VIE. We base our approach on the Reinforcement\nLearning with Verifiable Rewards (RLVR) framework to address these challenges\nusing only 100 annotated samples. Our approach ensures dataset diversity, a\nbalanced precision-recall reward mechanism to reduce hallucinations and improve\nfield coverage, and innovative sampling strategies to enhance reasoning\ncapabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve\nstate-of-the-art performance on medical VIE tasks, significantly improving F1,\nprecision, and recall. While our models excel on tasks similar to medical\ndatasets, performance drops on dissimilar tasks, highlighting the need for\ndomain-specific optimization. Case studies further demonstrate the value of\nreasoning during training and inference for VIE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13067", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13067", "abs": "https://arxiv.org/abs/2506.13067", "authors": ["Xuhui Zhu", "Jing Xu", "Bingjie Wang", "Huikang Dai", "Hao Lu"], "title": "Video Individual Counting With Implicit One-to-Many Matching", "comment": null, "summary": "Video Individual Counting (VIC) is a recently introduced task that aims to\nestimate pedestrian flux from a video. It extends conventional Video Crowd\nCounting (VCC) beyond the per-frame pedestrian count. In contrast to VCC that\nonly learns to count repeated pedestrian patterns across frames, the key\nproblem of VIC is how to identify co-existent pedestrians between frames, which\nturns out to be a correspondence problem. Existing VIC approaches, however,\nmainly follow a one-to-one (O2O) matching strategy where the same pedestrian\nmust be exactly matched between frames, leading to sensitivity to appearance\nvariations or missing detections. In this work, we show that the O2O matching\ncould be relaxed to a one-to-many (O2M) matching problem, which better fits the\nproblem nature of VIC and can leverage the social grouping behavior of walking\npedestrians. We therefore introduce OMAN, a simple but effective VIC model with\nimplicit One-to-Many mAtchiNg, featuring an implicit context generator and a\none-to-many pairwise matcher. Experiments on the SenseCrowd and CroHD\nbenchmarks show that OMAN achieves the state-of-the-art performance. Code is\navailable at \\href{https://github.com/tiny-smart/OMAN}{OMAN}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13073", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13073", "abs": "https://arxiv.org/abs/2506.13073", "authors": ["Bingxi Liu", "Pengju Zhang", "Li He", "Hao Chen", "Shiyi Guo", "Yihong Wu", "Jinqiang Cui", "Hong Zhang"], "title": "SuperPlace: The Renaissance of Classical Feature Aggregation for Visual Place Recognition in the Era of Foundation Models", "comment": "11 pages", "summary": "Recent visual place recognition (VPR) approaches have leveraged foundation\nmodels (FM) and introduced novel aggregation techniques. However, these methods\nhave failed to fully exploit key concepts of FM, such as the effective\nutilization of extensive training sets, and they have overlooked the potential\nof classical aggregation methods, such as GeM and NetVLAD. Building on these\ninsights, we revive classical feature aggregation methods and develop more\nfundamental VPR models, collectively termed SuperPlace. First, we introduce a\nsupervised label alignment method that enables training across various VPR\ndatasets within a unified framework. Second, we propose G$^2$M, a compact\nfeature aggregation method utilizing two GeMs, where one GeM learns the\nprincipal components of feature maps along the channel dimension and calibrates\nthe output of the other. Third, we propose the secondary fine-tuning (FT$^2$)\nstrategy for NetVLAD-Linear (NVL). NetVLAD first learns feature vectors in a\nhigh-dimensional space and then compresses them into a lower-dimensional space\nvia a single linear layer. Extensive experiments highlight our contributions\nand demonstrate the superiority of SuperPlace. Specifically, G$^2$M achieves\npromising results with only one-tenth of the feature dimensions compared to\nrecent methods. Moreover, NVL-FT$^2$ ranks first on the MSLS leaderboard.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13468", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13468", "abs": "https://arxiv.org/abs/2506.13468", "authors": ["Marine Carpuat", "Omri Asscher", "Kalika Bali", "Luisa Bentivogli", "Frédéric Blain", "Lynne Bowker", "Monojit Choudhury", "Hal Daumé III", "Kevin Duh", "Ge Gao", "Alvin Grissom II", "Marzena Karpinska", "Elaine C. Khoong", "William D. Lewis", "André F. T. Martins", "Mary Nurminen", "Douglas W. Oard", "Maja Popovic", "Michel Simard", "François Yvon"], "title": "An Interdisciplinary Approach to Human-Centered Machine Translation", "comment": "20 pages", "summary": "Machine Translation (MT) tools are widely used today, often in contexts where\nprofessional translators are not present. Despite progress in MT technology, a\ngap persists between system development and real-world usage, particularly for\nnon-expert users who may struggle to assess translation reliability. This paper\nadvocates for a human-centered approach to MT, emphasizing the alignment of\nsystem design with diverse communicative goals and contexts of use. We survey\nthe literature in Translation Studies and Human-Computer Interaction to\nrecontextualize MT evaluation and design to address the diverse real-world\nscenarios in which MT is used today.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13133", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13133", "abs": "https://arxiv.org/abs/2506.13133", "authors": ["Bingxi Liu", "Hao Chen", "Shiyi Guo", "Yihong Wu", "Jinqiang Cui", "Hong Zhang"], "title": "EmbodiedPlace: Learning Mixture-of-Features with Embodied Constraints for Visual Place Recognition", "comment": "17 Pages", "summary": "Visual Place Recognition (VPR) is a scene-oriented image retrieval problem in\ncomputer vision in which re-ranking based on local features is commonly\nemployed to improve performance. In robotics, VPR is also referred to as Loop\nClosure Detection, which emphasizes spatial-temporal verification within a\nsequence. However, designing local features specifically for VPR is\nimpractical, and relying on motion sequences imposes limitations. Inspired by\nthese observations, we propose a novel, simple re-ranking method that refines\nglobal features through a Mixture-of-Features (MoF) approach under embodied\nconstraints. First, we analyze the practical feasibility of embodied\nconstraints in VPR and categorize them according to existing datasets, which\ninclude GPS tags, sequential timestamps, local feature matching, and\nself-similarity matrices. We then propose a learning-based MoF\nweight-computation approach, utilizing a multi-metric loss function.\nExperiments demonstrate that our method improves the state-of-the-art (SOTA)\nperformance on public datasets with minimal additional computational overhead.\nFor instance, with only 25 KB of additional parameters and a processing time of\n10 microseconds per frame, our method achieves a 0.9\\% improvement over a\nDINOv2-based baseline performance on the Pitts-30k test set.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13474", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13474", "abs": "https://arxiv.org/abs/2506.13474", "authors": ["David Bani-Harouni", "Chantal Pellegrini", "Ege Özsoy", "Matthias Keicher", "Nassir Navab"], "title": "Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning", "comment": null, "summary": "Clinical decision-making is a dynamic, interactive, and cyclic process where\ndoctors have to repeatedly decide on which clinical action to perform and\nconsider newly uncovered information for diagnosis and treatment. Large\nLanguage Models (LLMs) have the potential to support clinicians in this\nprocess, however, most applications of LLMs in clinical decision support suffer\nfrom one of two limitations: Either they assume the unrealistic scenario of\nimmediate availability of all patient information and do not model the\ninteractive and iterative investigation process, or they restrict themselves to\nthe limited \"out-of-the-box\" capabilities of large pre-trained models without\nperforming task-specific training. In contrast to this, we propose to model\nclinical decision-making for diagnosis with a hypothesis-driven\nuncertainty-aware language agent, LA-CDM, that converges towards a diagnosis\nvia repeatedly requesting and interpreting relevant tests. Using a hybrid\ntraining paradigm combining supervised and reinforcement learning, we train\nLA-CDM with three objectives targeting critical aspects of clinical\ndecision-making: accurate hypothesis generation, hypothesis uncertainty\nestimation, and efficient decision-making. We evaluate our methodology on\nMIMIC-CDM, a real-world dataset covering four abdominal diseases containing\nvarious clinical tests and show the benefit of explicitly training clinical\ndecision-making for increasing diagnostic performance and efficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13559", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13559", "abs": "https://arxiv.org/abs/2506.13559", "authors": ["Settaluri Lakshmi Sravanthi", "Kishan Maharaj", "Sravani Gunnu", "Abhijit Mishra", "Pushpak Bhattacharyya"], "title": "Understand the Implication: Learning to Think for Pragmatic Understanding", "comment": "SS and KM contributed equally to this work", "summary": "Pragmatics, the ability to infer meaning beyond literal interpretation, is\ncrucial for social cognition and communication. While LLMs have been\nbenchmarked for their pragmatic understanding, improving their performance\nremains underexplored. Existing methods rely on annotated labels but overlook\nthe reasoning process humans naturally use to interpret implicit meaning. To\nbridge this gap, we introduce a novel pragmatic dataset,\nImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both\ncorrect and incorrect interpretations. Through preference-tuning and supervised\nfine-tuning, we demonstrate that thought-based learning significantly enhances\nLLMs' pragmatic understanding, improving accuracy by 11.12% across model\nfamilies. We further discuss a transfer-learning study where we evaluate the\nperformance of thought-based training for the other tasks of pragmatics\n(presupposition, deixis) that are not seen during the training time and observe\nan improvement of 16.10% compared to label-trained models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13585", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13585", "abs": "https://arxiv.org/abs/2506.13585", "authors": ["MiniMax", ":", "Aili Chen", "Aonian Li", "Bangwei Gong", "Binyang Jiang", "Bo Fei", "Bo Yang", "Boji Shan", "Changqing Yu", "Chao Wang", "Cheng Zhu", "Chengjun Xiao", "Chengyu Du", "Chi Zhang", "Chu Qiao", "Chunhao Zhang", "Chunhui Du", "Congchao Guo", "Da Chen", "Deming Ding", "Dianjun Sun", "Dong Li", "Enwei Jiao", "Haigang Zhou", "Haimo Zhang", "Han Ding", "Haohai Sun", "Haoyu Feng", "Huaiguang Cai", "Haichao Zhu", "Jian Sun", "Jiaqi Zhuang", "Jiaren Cai", "Jiayuan Song", "Jin Zhu", "Jingyang Li", "Jinhao Tian", "Jinli Liu", "Junhao Xu", "Junjie Yan", "Junteng Liu", "Junxian He", "Kaiyi Feng", "Ke Yang", "Kecheng Xiao", "Le Han", "Leyang Wang", "Lianfei Yu", "Liheng Feng", "Lin Li", "Lin Zheng", "Linge Du", "Lingyu Yang", "Lunbin Zeng", "Minghui Yu", "Mingliang Tao", "Mingyuan Chi", "Mozhi Zhang", "Mujie Lin", "Nan Hu", "Nongyu Di", "Peng Gao", "Pengfei Li", "Pengyu Zhao", "Qibing Ren", "Qidi Xu", "Qile Li", "Qin Wang", "Rong Tian", "Ruitao Leng", "Shaoxiang Chen", "Shaoyu Chen", "Shengmin Shi", "Shitong Weng", "Shuchang Guan", "Shuqi Yu", "Sichen Li", "Songquan Zhu", "Tengfei Li", "Tianchi Cai", "Tianrun Liang", "Weiyu Cheng", "Weize Kong", "Wenkai Li", "Xiancai Chen", "Xiangjun Song", "Xiao Luo", "Xiao Su", "Xiaobo Li", "Xiaodong Han", "Xinzhu Hou", "Xuan Lu", "Xun Zou", "Xuyang Shen", "Yan Gong", "Yan Ma", "Yang Wang", "Yiqi Shi", "Yiran Zhong", "Yonghong Duan", "Yongxiang Fu", "Yongyi Hu", "Yu Gao", "Yuanxiang Fan", "Yufeng Yang", "Yuhao Li", "Yulin Hu", "Yunan Huang", "Yunji Li", "Yunzhi Xu", "Yuxin Mao", "Yuxuan Shi", "Yuze Wenren", "Zehan Li", "Zelin Li", "Zhanxu Tian", "Zhengmao Zhu", "Zhenhua Fan", "Zhenzhen Wu", "Zhichao Xu", "Zhihang Yu", "Zhiheng Lyu", "Zhuo Jiang", "Zibo Gao", "Zijia Wu", "Zijian Song", "Zijun Sun"], "title": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention", "comment": "A technical report from MiniMax. The authors are listed in\n  alphabetical order. We open-source our MiniMax-M1 at\n  https://github.com/MiniMax-AI/MiniMax-M1", "summary": "We introduce MiniMax-M1, the world's first open-weight, large-scale\nhybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid\nMixture-of-Experts (MoE) architecture combined with a lightning attention\nmechanism. The model is developed based on our previous MiniMax-Text-01 model,\nwhich contains a total of 456 billion parameters with 45.9 billion parameters\nactivated per token. The M1 model natively supports a context length of 1\nmillion tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning\nattention mechanism in MiniMax-M1 enables efficient scaling of test-time\ncompute. These properties make M1 particularly suitable for complex tasks that\nrequire processing long inputs and thinking extensively. MiniMax-M1 is trained\nusing large-scale reinforcement learning (RL) on diverse problems including\nsandbox-based, real-world software engineering environments. In addition to\nM1's inherent efficiency advantage for RL training, we propose CISPO, a novel\nRL algorithm to further enhance RL efficiency. CISPO clips importance sampling\nweights rather than token updates, outperforming other competitive RL variants.\nCombining hybrid-attention and CISPO enables MiniMax-M1's full RL training on\n512 H800 GPUs to complete in only three weeks, with a rental cost of just\n$534,700. We release two versions of MiniMax-M1 models with 40K and 80K\nthinking budgets respectively, where the 40K model represents an intermediate\nphase of the 80K training. Experiments on standard benchmarks show that our\nmodels are comparable or superior to strong open-weight models such as the\noriginal DeepSeek-R1 and Qwen3-235B, with particular strengths in complex\nsoftware engineering, tool utilization, and long-context tasks. We publicly\nrelease MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale", "test-time compute", "reasoning model"], "score": 5}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13639", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13639", "abs": "https://arxiv.org/abs/2506.13639", "authors": ["Yusuke Yamauchi", "Taro Yano", "Masafumi Oyamada"], "title": "An Empirical Study of LLM-as-a-Judge: How Design Choices Impact Evaluation Reliability", "comment": null, "summary": "As large language models (LLMs) continue to advance, reliable evaluation\nmethods are essential particularly for open-ended, instruction-following tasks.\nLLM-as-a-Judge enables automatic evaluation using LLMs as evaluators, but its\nreliability remains uncertain. In this work, we analyze key factors affecting\nits trustworthiness, focusing on alignment with human judgments and evaluation\nconsistency. Using BIGGENBench and EvalBiasBench, we study the effects of\nevaluation design, decoding strategies, and Chain-of-Tought (CoT) reasoning in\nevaluation. Our results show that evaluation criteria are critical for\nreliability, non-deterministic sampling improves alignment with human\npreferences over deterministic evaluation, and CoT reasoning offers minimal\ngains when clear evaluation criteria are present.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "reliability", "criteria"], "score": 4}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13322", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13322", "abs": "https://arxiv.org/abs/2506.13322", "authors": ["Weijia Feng", "Yichen Zhu", "Ruojia Zhang", "Chenyang Wang", "Fei Ma", "Xiaobao Wang", "Xiaobai Li"], "title": "Active Multimodal Distillation for Few-shot Action Recognition", "comment": "IJCAI 2025, the 34th International Joint Conference on Artificial\n  Intelligence", "summary": "Owing to its rapid progress and broad application prospects, few-shot action\nrecognition has attracted considerable interest. However, current methods are\npredominantly based on limited single-modal data, which does not fully exploit\nthe potential of multimodal information. This paper presents a novel framework\nthat actively identifies reliable modalities for each sample using\ntask-specific contextual cues, thus significantly improving recognition\nperformance. Our framework integrates an Active Sample Inference (ASI) module,\nwhich utilizes active inference to predict reliable modalities based on\nposterior distributions and subsequently organizes them accordingly. Unlike\nreinforcement learning, active inference replaces rewards with evidence-based\npreferences, making more stable predictions. Additionally, we introduce an\nactive mutual distillation module that enhances the representation learning of\nless reliable modalities by transferring knowledge from more reliable ones.\nAdaptive multimodal inference is employed during the meta-test to assign higher\nweights to reliable modalities. Extensive experiments across multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13326", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.13326", "abs": "https://arxiv.org/abs/2506.13326", "authors": ["Bo Pan", "Yixiao Fu", "Ke Wang", "Junyu Lu", "Lunke Pan", "Ziyang Qian", "Yuhan Chen", "Guoliang Wang", "Yitao Zhou", "Li Zheng", "Yinghao Tang", "Zhen Wen", "Yuchen Wu", "Junhua Lu", "Biao Zhu", "Minfeng Zhu", "Bo Zhang", "Wei Chen"], "title": "VIS-Shepherd: Constructing Critic for LLM-based Data Visualization Generation", "comment": null, "summary": "Data visualization generation using Large Language Models (LLMs) has shown\npromising results but often produces suboptimal visualizations that require\nhuman intervention for improvement. In this work, we introduce VIS-Shepherd, a\nspecialized Multimodal Large Language Model (MLLM)-based critic to evaluate and\nprovide feedback for LLM-generated data visualizations. At the core of our\napproach is a framework to construct a high-quality visualization critique\ndataset, where we collect human-created visualization instances, synthesize\ncorresponding LLM-generated instances, and construct high-quality critiques. We\nconduct both model-based automatic evaluation and human preference studies to\nevaluate the effectiveness of our approach. Our experiments show that even\nsmall (7B parameters) open-source MLLM models achieve substantial performance\ngains by leveraging our high-quality visualization critique dataset, reaching\nlevels comparable to much larger open-source or even proprietary models. Our\nwork demonstrates significant potential for MLLM-based automated visualization\ncritique and indicates promising directions for enhancing LLM-based data\nvisualization generation. Our project page:\nhttps://github.com/bopan3/VIS-Shepherd.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "human preference"], "score": 3}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13335", "abs": "https://arxiv.org/abs/2506.13335", "authors": ["Gabriel A. Carneiro", "Thierry J. Aubry", "António Cunha", "Petia Radeva", "Joaquim Sousa"], "title": "Advancing Image-Based Grapevine Variety Classification with a New Benchmark and Evaluation of Masked Autoencoders", "comment": null, "summary": "Grapevine varieties are essential for the economies of many wine-producing\ncountries, influencing the production of wine, juice, and the consumption of\nfruits and leaves. Traditional identification methods, such as ampelography and\nmolecular analysis, have limitations: ampelography depends on expert knowledge\nand is inherently subjective, while molecular methods are costly and\ntime-intensive. To address these limitations, recent studies have applied deep\nlearning (DL) models to classify grapevine varieties using image data. However,\ndue to the small dataset sizes, these methods often depend on transfer learning\nfrom datasets from other domains, e.g., ImageNet1K (IN1K), which can lead to\nperformance degradation due to domain shift and supervision collapse. In this\ncontext, self-supervised learning (SSL) methods can be a good tool to avoid\nthis performance degradation, since they can learn directly from data, without\nexternal labels. This study presents an evaluation of Masked Autoencoders\n(MAEs) for identifying grapevine varieties based on field-acquired images. The\nmain contributions of this study include two benchmarks comprising 43 grapevine\nvarieties collected across different seasons, an analysis of MAE's application\nin the agricultural context, and a performance comparison of trained models\nacross seasons. Our results show that a ViT-B/16 model pre-trained with MAE and\nthe unlabeled dataset achieved an F1 score of 0.7956, outperforming all other\nmodels. Additionally, we observed that pre-trained models benefit from long\npre-training, perform well under low-data training regime, and that simple data\naugmentation methods are more effective than complex ones. The study also found\nthat the mask ratio in MAE impacts performance only marginally.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13743", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.13743", "abs": "https://arxiv.org/abs/2506.13743", "authors": ["To Eun Kim", "Fernando Diaz"], "title": "LTRR: Learning To Rank Retrievers for LLMs", "comment": "SIGIR 2025 LiveRAG Spotlight", "summary": "Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed\nretriever, despite growing evidence that no single retriever performs optimally\nacross all query types. In this paper, we explore a query routing approach that\ndynamically selects from a pool of retrievers based on the query, using both\ntrain-free heuristics and learned routing models. We frame routing as a\nlearning-to-rank (LTR) problem and introduce LTRR, a framework that learns to\nrank retrievers by their expected utility gain to downstream LLM performance.\nOur experiments, conducted on synthetic QA data with controlled query type\nvariations, show that routing-based RAG systems can outperform the best\nsingle-retriever-based systems. Performance gains are especially pronounced in\nmodels trained with the Answer Correctness (AC) metric and with pairwise\nlearning approaches, especially with XGBoost. We also observe improvements in\ngeneralization to out-of-distribution queries. As part of the SIGIR 2025\nLiveRAG challenge, our submitted system demonstrated the practical viability of\nour approach, achieving competitive performance in both answer correctness and\nfaithfulness. These findings highlight the importance of both training\nmethodology and metric selection in query routing for RAG systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13558", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13558", "abs": "https://arxiv.org/abs/2506.13558", "authors": ["Yu Yang", "Alan Liang", "Jianbiao Mei", "Yukai Ma", "Yong Liu", "Gim Hee Lee"], "title": "X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability", "comment": "28 pages, 9 figures, Project page at https://x-scene.github.io/", "summary": "Diffusion models are advancing autonomous driving by enabling realistic data\nsynthesis, predictive end-to-end planning, and closed-loop simulation, with a\nprimary focus on temporally consistent generation. However, the generation of\nlarge-scale 3D scenes that require spatial coherence remains underexplored. In\nthis paper, we propose X-Scene, a novel framework for large-scale driving scene\ngeneration that achieves both geometric intricacy and appearance fidelity,\nwhile offering flexible controllability. Specifically, X-Scene supports\nmulti-granular control, including low-level conditions such as user-provided or\ntext-driven layout for detailed scene composition and high-level semantic\nguidance such as user-intent and LLM-enriched text prompts for efficient\ncustomization. To enhance geometrical and visual fidelity, we introduce a\nunified pipeline that sequentially generates 3D semantic occupancy and the\ncorresponding multiview images, while ensuring alignment between modalities.\nAdditionally, we extend the generated local region into a large-scale scene\nthrough consistency-aware scene outpainting, which extrapolates new occupancy\nand images conditioned on the previously generated area, enhancing spatial\ncontinuity and preserving visual coherence. The resulting scenes are lifted\ninto high-quality 3DGS representations, supporting diverse applications such as\nscene exploration. Comprehensive experiments demonstrate that X-Scene\nsignificantly advances controllability and fidelity for large-scale driving\nscene generation, empowering data generation and simulation for autonomous\ndriving.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13654", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13654", "abs": "https://arxiv.org/abs/2506.13654", "authors": ["Shulin Tian", "Ruiqi Wang", "Hongming Guo", "Penghao Wu", "Yuhao Dong", "Xiuying Wang", "Jingkang Yang", "Hao Zhang", "Hongyuan Zhu", "Ziwei Liu"], "title": "Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning", "comment": "Project page: https://egolife-ai.github.io/Ego-R1/", "summary": "We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e.,\nin days and weeks) egocentric videos, which leverages a structured\nChain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained\nvia reinforcement learning (RL). Inspired by human problem-solving strategies,\nCoTT decomposes complex reasoning into modular steps, with the RL agent\ninvoking specific tools, one per step, to iteratively and collaboratively\nanswer sub-questions tackling such tasks as temporal retrieval and multi-modal\nunderstanding. We design a two-stage training paradigm involving supervised\nfinetuning (SFT) of a pretrained language model using CoTT data and RL to\nenable our agent to dynamically propose step-by-step tools for long-range\nreasoning. To facilitate training, we construct a dataset called Ego-R1 Data,\nwhich consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our\nEgo-R1 agent is evaluated on a newly curated week-long video QA benchmark,\nEgo-R1 Bench, which contains human-verified QA pairs from hybrid sources.\nExtensive results demonstrate that the dynamic, tool-augmented chain-of-thought\nreasoning by our Ego-R1 Agent can effectively tackle the unique challenges of\nunderstanding ultra-long egocentric videos, significantly extending the time\ncoverage from few hours to a week.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13723", "abs": "https://arxiv.org/abs/2506.13723", "authors": ["Qiyu Xu", "Wenyang Chen", "Zhanxuan Hu", "Huafeng Li", "Yonghang Tai"], "title": "OTFusion: Bridging Vision-only and Vision-Language Models via Optimal Transport for Transductive Zero-Shot Learning", "comment": null, "summary": "Transductive zero-shot learning (ZSL) aims to classify unseen categories by\nleveraging both semantic class descriptions and the distribution of unlabeled\ntest data. While Vision-Language Models (VLMs) such as CLIP excel at aligning\nvisual inputs with textual semantics, they often rely too heavily on\nclass-level priors and fail to capture fine-grained visual cues. In contrast,\nVision-only Foundation Models (VFMs) like DINOv2 provide rich perceptual\nfeatures but lack semantic alignment. To exploit the complementary strengths of\nthese models, we propose OTFusion, a simple yet effective training-free\nframework that bridges VLMs and VFMs via Optimal Transport. Specifically,\nOTFusion aims to learn a shared probabilistic representation that aligns visual\nand semantic information by minimizing the transport cost between their\nrespective distributions. This unified distribution enables coherent class\npredictions that are both semantically meaningful and visually grounded.\nExtensive experiments on 11 benchmark datasets demonstrate that OTFusion\nconsistently outperforms the original CLIP model, achieving an average accuracy\nimprovement of nearly $10\\%$, all without any fine-tuning or additional\nannotations. The code will be publicly released after the paper is accepted.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13750", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13750", "abs": "https://arxiv.org/abs/2506.13750", "authors": ["Yuheng Yuan", "Qiuhong Shen", "Shizun Wang", "Xingyi Yang", "Xinchao Wang"], "title": "Test3R: Learning to Reconstruct 3D at Test Time", "comment": null, "summary": "Dense matching methods like DUSt3R regress pairwise pointmaps for 3D\nreconstruction. However, the reliance on pairwise prediction and the limited\ngeneralization capability inherently restrict the global geometric consistency.\nIn this work, we introduce Test3R, a surprisingly simple test-time learning\ntechnique that significantly boosts geometric accuracy. Using image triplets\n($I_1,I_2,I_3$), Test3R generates reconstructions from pairs ($I_1,I_2$) and\n($I_1,I_3$). The core idea is to optimize the network at test time via a\nself-supervised objective: maximizing the geometric consistency between these\ntwo reconstructions relative to the common image $I_1$. This ensures the model\nproduces cross-pair consistent outputs, regardless of the inputs. Extensive\nexperiments demonstrate that our technique significantly outperforms previous\nstate-of-the-art methods on the 3D reconstruction and multi-view depth\nestimation tasks. Moreover, it is universally applicable and nearly cost-free,\nmaking it easily applied to other models and implemented with minimal test-time\ntraining overhead and parameter footprint. Code is available at\nhttps://github.com/nopQAQ/Test3R.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13757", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13757", "abs": "https://arxiv.org/abs/2506.13757", "authors": ["Zewei Zhou", "Tianhui Cai", "Seth Z. Zhao", "Yun Zhang", "Zhiyu Huang", "Bolei Zhou", "Jiaqi Ma"], "title": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "comment": "Website link:https://autovla.github.io/", "summary": "Recent advancements in Vision-Language-Action (VLA) models have shown promise\nfor end-to-end autonomous driving by leveraging world knowledge and reasoning\ncapabilities. However, current VLA models often struggle with physically\ninfeasible action outputs, complex model structures, or unnecessarily long\nreasoning. In this paper, we propose AutoVLA, a novel VLA model that unifies\nreasoning and action generation within a single autoregressive generation model\nfor end-to-end autonomous driving. AutoVLA performs semantic reasoning and\ntrajectory planning directly from raw visual inputs and language instructions.\nWe tokenize continuous trajectories into discrete, feasible actions, enabling\ndirect integration into the language model. For training, we employ supervised\nfine-tuning to equip the model with dual thinking modes: fast thinking\n(trajectory-only) and slow thinking (enhanced with chain-of-thought reasoning).\nTo further enhance planning performance and efficiency, we introduce a\nreinforcement fine-tuning method based on Group Relative Policy Optimization\n(GRPO), reducing unnecessary reasoning in straightforward scenarios. Extensive\nexperiments across real-world and simulated datasets and benchmarks, including\nnuPlan, nuScenes, Waymo, and CARLA, demonstrate the competitive performance of\nAutoVLA in both open-loop and closed-loop settings. Qualitative results\nshowcase the adaptive reasoning and accurate planning capabilities of AutoVLA\nin diverse scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
{"id": "2506.13756", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13756", "abs": "https://arxiv.org/abs/2506.13756", "authors": ["Jingwei Ma", "Vivek Jayaram", "Brian Curless", "Ira Kemelmacher-Shlizerman", "Steven M. Seitz"], "title": "UltraZoom: Generating Gigapixel Images from Regular Photos", "comment": "Project page: https://ultra-zoom.github.io/", "summary": "We present UltraZoom, a system for generating gigapixel-resolution images of\nobjects from casually captured inputs, such as handheld phone photos. Given a\nfull-shot image (global, low-detail) and one or more close-ups (local,\nhigh-detail), UltraZoom upscales the full image to match the fine detail and\nscale of the close-up examples. To achieve this, we construct a per-instance\npaired dataset from the close-ups and adapt a pretrained generative model to\nlearn object-specific low-to-high resolution mappings. At inference, we apply\nthe model in a sliding window fashion over the full image. Constructing these\npairs is non-trivial: it requires registering the close-ups within the full\nimage for scale estimation and degradation alignment. We introduce a simple,\nrobust method for getting registration on arbitrary materials in casual,\nin-the-wild captures. Together, these components form a system that enables\nseamless pan and zoom across the entire object, producing consistent,\nphotorealistic gigapixel imagery from minimal input.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-17.jsonl"}
