{"id": "2507.01368", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01368", "abs": "https://arxiv.org/abs/2507.01368", "authors": ["Tianning Chai", "Chancharik Mitra", "Brandon Huang", "Gautam Rajendrakumar Gare", "Zhiqiu Lin", "Assaf Arbelle", "Leonid Karlinsky", "Rogerio Feris", "Trevor Darrell", "Deva Ramanan", "Roei Herzig"], "title": "Activation Reward Models for Few-Shot Model Alignment", "comment": null, "summary": "Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to\nhuman preferences is a central challenge in improving the quality of the\nmodels' generative outputs for real-world applications. A common approach is to\nuse reward modeling to encode preferences, enabling alignment via post-training\nusing reinforcement learning. However, traditional reward modeling is not\neasily adaptable to new preferences because it requires a separate reward\nmodel, commonly trained on large preference datasets. To address this, we\nintroduce Activation Reward Models (Activation RMs) -- a novel few-shot reward\nmodeling method that leverages activation steering to construct well-aligned\nreward signals using minimal supervision and no additional model finetuning.\nActivation RMs outperform existing few-shot reward modeling approaches such as\nLLM-as-a-judge with in-context learning, voting-based scoring, and token\nprobability scoring on standard reward modeling benchmarks. Furthermore, we\ndemonstrate the effectiveness of Activation RMs in mitigating reward hacking\nbehaviors, highlighting their utility for safety-critical applications. Toward\nthis end, we propose PreferenceHack, a novel few-shot setting benchmark, the\nfirst to test reward models on reward hacking in a paired preference format.\nFinally, we show that Activation RM achieves state-of-the-art performance on\nthis benchmark, surpassing even GPT-4o.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning", "preference", "alignment", "reward hacking"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01368", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01368", "abs": "https://arxiv.org/abs/2507.01368", "authors": ["Tianning Chai", "Chancharik Mitra", "Brandon Huang", "Gautam Rajendrakumar Gare", "Zhiqiu Lin", "Assaf Arbelle", "Leonid Karlinsky", "Rogerio Feris", "Trevor Darrell", "Deva Ramanan", "Roei Herzig"], "title": "Activation Reward Models for Few-Shot Model Alignment", "comment": null, "summary": "Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to\nhuman preferences is a central challenge in improving the quality of the\nmodels' generative outputs for real-world applications. A common approach is to\nuse reward modeling to encode preferences, enabling alignment via post-training\nusing reinforcement learning. However, traditional reward modeling is not\neasily adaptable to new preferences because it requires a separate reward\nmodel, commonly trained on large preference datasets. To address this, we\nintroduce Activation Reward Models (Activation RMs) -- a novel few-shot reward\nmodeling method that leverages activation steering to construct well-aligned\nreward signals using minimal supervision and no additional model finetuning.\nActivation RMs outperform existing few-shot reward modeling approaches such as\nLLM-as-a-judge with in-context learning, voting-based scoring, and token\nprobability scoring on standard reward modeling benchmarks. Furthermore, we\ndemonstrate the effectiveness of Activation RMs in mitigating reward hacking\nbehaviors, highlighting their utility for safety-critical applications. Toward\nthis end, we propose PreferenceHack, a novel few-shot setting benchmark, the\nfirst to test reward models on reward hacking in a paired preference format.\nFinally, we show that Activation RM achieves state-of-the-art performance on\nthis benchmark, surpassing even GPT-4o.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning", "preference", "alignment", "reward hacking"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01039", "abs": "https://arxiv.org/abs/2507.01039", "authors": ["Kaaustaaub Shankar", "Wilhelm Louw", "Kelly Cohen"], "title": "On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization", "comment": "Submitted to NAFIPS 2025", "summary": "We propose a reinforcement learning (RL) approach for training neuro-fuzzy\ncontrollers using Proximal Policy Optimization (PPO). Building on prior work\nthat applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),\nour method replaces the off-policy value-based framework with a stable\non-policy actor-critic loop. We evaluate this approach in the CartPole-v1\nenvironment using multiple random seeds and compare its learning performance\nagainst ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained\nfuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000\nupdates, showcasing less variance than prior DQN-based methods during training\nand overall faster convergence. These findings suggest that PPO offers a\npromising pathway for training explainable neuro-fuzzy controllers in\nreinforcement learning tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "proximal policy optimization", "reinforcement learning", "policy optimization"], "score": 4}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01492", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01492", "abs": "https://arxiv.org/abs/2507.01492", "authors": ["Jiyang Tang", "Hengyi Li", "Yifan Du", "Wayne Xin Zhao"], "title": "AVC-DPO: Aligned Video Captioning via Direct Preference Optimization", "comment": null, "summary": "Although video multimodal large language models (video MLLMs) have achieved\nsubstantial progress in video captioning tasks, it remains challenging to\nadjust the focal emphasis of video captions according to human preferences. To\naddress this limitation, we propose Aligned Video Captioning via Direct\nPreference Optimization (AVC-DPO), a post-training framework designed to\nenhance captioning capabilities in video MLLMs through preference alignment.\nOur approach designs enhanced prompts that specifically target temporal\ndynamics and spatial information-two key factors that humans care about when\nwatching a video-thereby incorporating human-centric preferences. AVC-DPO\nleverages the same foundation model's caption generation responses under varied\nprompt conditions to conduct preference-aware training and caption alignment.\nUsing this framework, we have achieved exceptional performance in the\nLOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving\nfirst place on the Video Detailed Captioning (VDC) benchmark according to the\nVDCSCORE evaluation metric.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO", "direct preference optimization"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01243", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01243", "abs": "https://arxiv.org/abs/2507.01243", "authors": ["Ziang Zheng", "Guojian Zhan", "Shiqi Liu", "Yao Lyu", "Tao Zhang", "Shengbo Eben Li"], "title": "Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion", "comment": null, "summary": "Reinforcement learning (RL) has shown great potential in enabling quadruped\nrobots to perform agile locomotion. However, directly training policies to\nsimultaneously handle dual extreme challenges, i.e., extreme underactuation and\nextreme terrains, as in monopedal hopping tasks, remains highly challenging due\nto unstable early-stage interactions and unreliable reward feedback. To address\nthis, we propose JumpER (jump-start reinforcement learning via self-evolving\npriors), an RL training framework that structures policy learning into multiple\nstages of increasing complexity. By dynamically generating self-evolving priors\nthrough iterative bootstrapping of previously learned policies, JumpER\nprogressively refines and enhances guidance, thereby stabilizing exploration\nand policy optimization without relying on external expert priors or\nhandcrafted reward shaping. Specifically, when integrated with a structured\nthree-stage curriculum that incrementally evolves action modality, observation\nspace, and task objective, JumpER enables quadruped robots to achieve robust\nmonopedal hopping on unpredictable terrains for the first time. Remarkably, the\nresulting policy effectively handles challenging scenarios that traditional\nmethods struggle to conquer, including wide gaps up to 60 cm, irregularly\nspaced stairs, and stepping stones with distances varying from 15 cm to 35 cm.\nJumpER thus provides a principled and scalable approach for addressing\nlocomotion tasks under the dual challenges of extreme underactuation and\nextreme terrains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01050", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01050", "abs": "https://arxiv.org/abs/2507.01050", "authors": ["Jing Yu", "Yibo Zhao", "Jiapeng Zhu", "Wenming Shao", "Bo Pang", "Zhao Zhang", "Xiang Li"], "title": "Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization", "comment": null, "summary": "The widespread dissemination of toxic content on social media poses a serious\nthreat to both online environments and public discourse, highlighting the\nurgent need for detoxification methods that effectively remove toxicity while\npreserving the original semantics. However, existing approaches often struggle\nto simultaneously achieve strong detoxification performance, semantic\npreservation, and robustness to out-of-distribution data. Moreover, they\ntypically rely on costly, manually annotated parallel corpora while showing\npoor data efficiency. To address these challenges, we propose a two-stage\ntraining framework that jointly optimizes for data efficiency, semantic\npreservation, and model generalization. We first perform supervised fine-tuning\non a small set of high-quality, filtered parallel data to establish a strong\ninitialization. Then, we leverage unlabeled toxic inputs and a custom-designed\nreward model to train the LLM using Group Relative Policy Optimization.\nExperimental results demonstrate that our method effectively mitigates the\ntrade-offs faced by previous work, achieving state-of-the-art performance with\nimproved generalization and significantly reduced dependence on annotated data.\nOur code is available at:\nhttps://anonymous.4open.science/r/Detoxification-of-Text-725F/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "policy optimization"], "score": 2}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01496", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01496", "abs": "https://arxiv.org/abs/2507.01496", "authors": ["Jimyeong Kim", "Jungwon Park", "Yeji Song", "Nojun Kwak", "Wonjong Rhee"], "title": "ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation", "comment": "Published at ICCV 2025. Project page:\n  https://wlaud1001.github.io/ReFlex/", "summary": "Rectified Flow text-to-image models surpass diffusion models in image quality\nand text alignment, but adapting ReFlow for real-image editing remains\nchallenging. We propose a new real-image editing method for ReFlow by analyzing\nthe intermediate representations of multimodal transformer blocks and\nidentifying three key features. To extract these features from real images with\nsufficient structural preservation, we leverage mid-step latent, which is\ninverted only up to the mid-step. We then adapt attention during injection to\nimprove editability and enhance alignment to the target text. Our method is\ntraining-free, requires no user-provided mask, and can be applied even without\na source prompt. Extensive experiments on two benchmarks with nine baselines\ndemonstrate its superior performance over prior methods, further validated by\nhuman evaluations confirming a strong user preference for our approach.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01551", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01551", "abs": "https://arxiv.org/abs/2507.01551", "authors": ["Wu Fei", "Hao Kong", "Shuxian Liang", "Yang Lin", "Yibo Yang", "Jing Tang", "Lei Chen", "Xiansheng Hua"], "title": "Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning", "comment": null, "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "reward hacking"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01926", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01926", "abs": "https://arxiv.org/abs/2507.01926", "authors": ["Yaowei Li", "Xiaoyu Li", "Zhaoyang Zhang", "Yuxuan Bian", "Gan Liu", "Xinyuan Li", "Jiale Xu", "Wenbo Hu", "Yating Liu", "Lingen Li", "Jing Cai", "Yuexian Zou", "Yancheng He", "Ying Shan"], "title": "IC-Custom: Diverse Image Customization via In-Context Learning", "comment": "Project page: https://liyaowei-stu.github.io/project/IC_Custom", "summary": "Image customization, a crucial technique for industrial media production,\naims to generate content that is consistent with reference images. However,\ncurrent approaches conventionally separate image customization into\nposition-aware and position-free customization paradigms and lack a universal\nframework for diverse customization, limiting their applications across various\nscenarios. To overcome these limitations, we propose IC-Custom, a unified\nframework that seamlessly integrates position-aware and position-free image\ncustomization through in-context learning. IC-Custom concatenates reference\nimages with target images to a polyptych, leveraging DiT's multi-modal\nattention mechanism for fine-grained token-level interactions. We introduce the\nIn-context Multi-Modal Attention (ICMA) mechanism with learnable task-oriented\nregister tokens and boundary-aware positional embeddings to enable the model to\ncorrectly handle different task types and distinguish various inputs in\npolyptych configurations. To bridge the data gap, we carefully curated a\nhigh-quality dataset of 12k identity-consistent samples with 8k from real-world\nsources and 4k from high-quality synthetic data, avoiding the overly glossy and\nover-saturated synthetic appearance. IC-Custom supports various industrial\napplications, including try-on, accessory placement, furniture arrangement, and\ncreative IP customization. Extensive evaluations on our proposed ProductBench\nand the publicly available DreamBench demonstrate that IC-Custom significantly\noutperforms community workflows, closed-source models, and state-of-the-art\nopen-source approaches. IC-Custom achieves approximately 73% higher human\npreference across identity consistency, harmonicity, and text alignment\nmetrics, while training only 0.4% of the original model parameters. Project\npage: https://liyaowei-stu.github.io/project/IC_Custom", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01949", "abs": "https://arxiv.org/abs/2507.01949", "authors": ["Kwai Keye Team", "Biao Yang", "Bin Wen", "Changyi Liu", "Chenglong Chu", "Chengru Song", "Chongling Rao", "Chuan Yi", "Da Li", "Dunju Zang", "Fan Yang", "Guorui Zhou", "Hao Peng", "Haojie Ding", "Jiaming Huang", "Jiangxia Cao", "Jiankang Chen", "Jingyun Hua", "Jin Ouyang", "Kaibing Chen", "Kaiyu Jiang", "Kaiyu Tang", "Kun Gai", "Shengnan Zhang", "Siyang Mao", "Sui Huang", "Tianke Zhang", "Tingting Gao", "Wei Chen", "Wei Yuan", "Xiangyu Wu", "Xiao Hu", "Xingyu Lu", "Yang Zhou", "Yi-Fan Zhang", "Yiping Yang", "Yulong Chen", "Zhenhua Wu", "Zhenyu Li", "Zhixin Ling", "Ziming Li", "Dehua Ma", "Di Xu", "Haixuan Gao", "Hang Li", "Jiawei Guo", "Jing Wang", "Lejian Ren", "Muhao Wei", "Qianqian Wang", "Qigen Hu", "Shiyao Wang", "Tao Yu", "Xinchen Luo", "Yan Li", "Yiming Liang", "Yuhang Hu", "Zeyi Lu", "Zhuoran Yang", "Zixing Zhang"], "title": "Kwai Keye-VL Technical Report", "comment": "Technical Report: https://github.com/Kwai-Keye/Keye", "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce \\textbf{Kwai Keye-VL}, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the \\textbf{KC-MMBench}, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01243", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01243", "abs": "https://arxiv.org/abs/2507.01243", "authors": ["Ziang Zheng", "Guojian Zhan", "Shiqi Liu", "Yao Lyu", "Tao Zhang", "Shengbo Eben Li"], "title": "Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion", "comment": null, "summary": "Reinforcement learning (RL) has shown great potential in enabling quadruped\nrobots to perform agile locomotion. However, directly training policies to\nsimultaneously handle dual extreme challenges, i.e., extreme underactuation and\nextreme terrains, as in monopedal hopping tasks, remains highly challenging due\nto unstable early-stage interactions and unreliable reward feedback. To address\nthis, we propose JumpER (jump-start reinforcement learning via self-evolving\npriors), an RL training framework that structures policy learning into multiple\nstages of increasing complexity. By dynamically generating self-evolving priors\nthrough iterative bootstrapping of previously learned policies, JumpER\nprogressively refines and enhances guidance, thereby stabilizing exploration\nand policy optimization without relying on external expert priors or\nhandcrafted reward shaping. Specifically, when integrated with a structured\nthree-stage curriculum that incrementally evolves action modality, observation\nspace, and task objective, JumpER enables quadruped robots to achieve robust\nmonopedal hopping on unpredictable terrains for the first time. Remarkably, the\nresulting policy effectively handles challenging scenarios that traditional\nmethods struggle to conquer, including wide gaps up to 60 cm, irregularly\nspaced stairs, and stepping stones with distances varying from 15 cm to 35 cm.\nJumpER thus provides a principled and scalable approach for addressing\nlocomotion tasks under the dual challenges of extreme underactuation and\nextreme terrains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01099", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01099", "abs": "https://arxiv.org/abs/2507.01099", "authors": ["Zeyi Liu", "Shuang Li", "Eric Cousineau", "Siyuan Feng", "Benjamin Burchfiel", "Shuran Song"], "title": "Geometry-aware 4D Video Generation for Robot Manipulation", "comment": "Project website: https://robot4dgen.github.io", "summary": "Understanding and predicting the dynamics of the physical world can enhance a\nrobot's ability to plan and interact effectively in complex environments. While\nrecent video generation models have shown strong potential in modeling dynamic\nscenes, generating videos that are both temporally coherent and geometrically\nconsistent across camera views remains a significant challenge. To address\nthis, we propose a 4D video generation model that enforces multi-view 3D\nconsistency of videos by supervising the model with cross-view pointmap\nalignment during training. This geometric supervision enables the model to\nlearn a shared 3D representation of the scene, allowing it to predict future\nvideo sequences from novel viewpoints based solely on the given RGB-D\nobservations, without requiring camera poses as inputs. Compared to existing\nbaselines, our method produces more visually stable and spatially aligned\npredictions across multiple simulated and real-world robotic datasets. We\nfurther show that the predicted 4D videos can be used to recover robot\nend-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting\nrobust robot manipulation and generalization to novel camera viewpoints.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01161", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.01161", "abs": "https://arxiv.org/abs/2507.01161", "authors": ["Zhizhuo Zhang", "Hao Peng", "Xiaoli Bai"], "title": "Imitation Learning for Satellite Attitude Control under Unknown Perturbations", "comment": "2025 AAS/AIAA Astrodynamics Specialist Conference", "summary": "This paper presents a novel satellite attitude control framework that\nintegrates Soft Actor-Critic (SAC) reinforcement learning with Generative\nAdversarial Imitation Learning (GAIL) to achieve robust performance under\nvarious unknown perturbations. Traditional control techniques often rely on\nprecise system models and are sensitive to parameter uncertainties and external\nperturbations. To overcome these limitations, we first develop a SAC-based\nexpert controller that demonstrates improved resilience against actuator\nfailures, sensor noise, and attitude misalignments, outperforming our previous\nresults in several challenging scenarios. We then use GAIL to train a learner\npolicy that imitates the expert's trajectories, thereby reducing training costs\nand improving generalization through expert demonstrations. Preliminary\nexperiments under single and combined perturbations show that the SAC expert\ncan rotate the antenna to a specified direction and keep the antenna\norientation reliably stable in most of the listed perturbations. Additionally,\nthe GAIL learner can imitate most of the features from the trajectories\ngenerated by the SAC expert. Comparative evaluations and ablation studies\nconfirm the effectiveness of the SAC algorithm and reward shaping. The\nintegration of GAIL further reduces sample complexity and demonstrates\npromising imitation capabilities, paving the way for more intelligent and\nautonomous spacecraft control systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01255", "abs": "https://arxiv.org/abs/2507.01255", "authors": ["Xiao Liu", "Jiawei Zhang"], "title": "AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation", "comment": "Working in Progress", "summary": "The rapid advancement of AI-generated video models has created a pressing\nneed for robust and interpretable evaluation frameworks. Existing metrics are\nlimited to producing numerical scores without explanatory comments, resulting\nin low interpretability and human evaluation alignment. To address those\nchallenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video\nEvaluation(AIGVE), which can provide not only numerical scores but also\nmulti-aspect language comment feedback in evaluating these generated videos.\nCentral to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising\n2,500 AI-generated videos and 22,500 human-annotated detailed comments and\nnumerical scores across nine critical evaluation aspects. Leveraging\nAIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a\nnovel token-wise weighted loss and a dynamic frame sampling strategy to better\nalign with human evaluators. Comprehensive experiments across supervised and\nzero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art\nperformance in both scoring correlation and comment quality, significantly\noutperforming prior baselines including GPT-4o and VideoScore. In addition, we\nfurther showcase a multi-agent refinement framework where feedback from\nAIGVE-MACS drives iterative improvements in video generation, leading to 53.5%\nquality enhancement. This work establishes a new paradigm for comprehensive,\nhuman-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2\nand AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "correlation"], "score": 3}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01152", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01152", "abs": "https://arxiv.org/abs/2507.01152", "authors": ["Yunke Ao", "Masoud Moghani", "Mayank Mittal", "Manish Prajapat", "Luohong Wu", "Frederic Giraud", "Fabio Carrillo", "Andreas Krause", "Philipp Fürnstahl"], "title": "SonoGym: High Performance Simulation for Challenging Surgical Tasks with Robotic Ultrasound", "comment": "21 pages, 15 figures", "summary": "Ultrasound (US) is a widely used medical imaging modality due to its\nreal-time capabilities, non-invasive nature, and cost-effectiveness. Robotic\nultrasound can further enhance its utility by reducing operator dependence and\nimproving access to complex anatomical regions. For this, while deep\nreinforcement learning (DRL) and imitation learning (IL) have shown potential\nfor autonomous navigation, their use in complex surgical tasks such as anatomy\nreconstruction and surgical guidance remains limited -- largely due to the lack\nof realistic and efficient simulation environments tailored to these tasks. We\nintroduce SonoGym, a scalable simulation platform for complex robotic\nultrasound tasks that enables parallel simulation across tens to hundreds of\nenvironments. Our framework supports realistic and real-time simulation of US\ndata from CT-derived 3D models of the anatomy through both a physics-based and\na generative modeling approach. Sonogym enables the training of DRL and recent\nIL agents (vision transformers and diffusion policies) for relevant tasks in\nrobotic orthopedic surgery by integrating common robotic platforms and\northopedic end effectors. We further incorporate submodular DRL -- a recent\nmethod that handles history-dependent rewards -- for anatomy reconstruction and\nsafe reinforcement learning for surgery. Our results demonstrate successful\npolicy learning across a range of scenarios, while also highlighting the\nlimitations of current methods in clinically relevant environments. We believe\nour simulation can facilitate research in robot learning approaches for such\nchallenging robotic surgery applications. Dataset, codes, and videos are\npublicly available at https://sonogym.github.io/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01209", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01209", "abs": "https://arxiv.org/abs/2507.01209", "authors": ["Paul C. Parsons", "Arran Ridley"], "title": "Judgment as Coordination: A Joint Systems View of Visualization Design Practice", "comment": "IEEE VIS 2025 (conditional acceptance)", "summary": "Professional visualization design has become an increasingly important area\nof inquiry, yet much of the field's discourse remains anchored in\nresearcher-centered contexts. Studies of design practice often focus on\nindividual designers' decisions and reflections, offering limited insight into\nthe collaborative and systemic dimensions of professional work. In this paper,\nwe propose a systems-level reframing of design judgment grounded in the\ncoordination and adaptation that sustain progress amid uncertainty, constraint,\nand misalignment. Drawing on sustained engagement across multiple empirical\nstudies--including ethnographic observation of design teams and qualitative\nstudies of individual practitioners--we identify recurring episodes in which\ncoherence was preserved not by selecting an optimal option, but by repairing\nalignment, adjusting plans, and reframing goals. We interpret these dynamics\nthrough the lens of Joint Cognitive Systems, which provide tools for analyzing\nhow judgment emerges as a distributed capacity within sociotechnical activity.\nThis perspective surfaces often-invisible work in visualization design and\noffers researchers a new conceptual vocabulary for studying how design activity\nis sustained in practice.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01035", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.01035", "abs": "https://arxiv.org/abs/2507.01035", "authors": ["Yushang Zhao", "Haotian Lyu", "Yike Peng", "Aijia Sun", "Feng Jiang", "Xinyue Han"], "title": "Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems", "comment": null, "summary": "The incessant advent of online services demands high speed and efficient\nrecommender systems (ReS) that can maintain real-time performance along with\nprocessing very complex user-item interactions. The present study, therefore,\nconsiders computational bottlenecks involved in hybrid Graph Neural Network\n(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their\ninference latency and training efficiency. An extensive methodology was used:\nhybrid GNN-LLM integrated architecture-optimization strategies(quantization,\nLoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.\nExperimental improvements were significant, with the optimal Hybrid + FPGA +\nDeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms\nof latency, while LoRA brought down training time by 66% (3.8 hours) in\ncomparison to the non-optimized baseline. Irrespective of domain, such as\naccuracy or efficiency, it can be established that hardware-software co-design\nand parameter-efficient tuning permit hybrid models to outperform GNN or LLM\napproaches implemented independently. It recommends the use of FPGA as well as\nLoRA for real-time deployment. Future work should involve federated learning\nalong with advanced fusion architectures for better scalability and privacy\npreservation. Thus, this research marks the fundamental groundwork concerning\nnext-generation ReS balancing low-latency response with cutting-edge\npersonalization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01574", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.01574", "abs": "https://arxiv.org/abs/2507.01574", "authors": ["Yulan Gao", "Ziqiang Ye", "Zhonghao Lyu", "Ming Xiao", "Yue Xiao", "Ping Yang", "Agata Manolova"], "title": "Vision-Aided ISAC in Low-Altitude Economy Networks via De-Diffused Visual Priors", "comment": null, "summary": "Emerging low-altitude economy networks (LAENets) require agile and\nprivacy-preserving resource control under dynamic agent mobility and limited\ninfrastructure support. To meet these challenges, we propose a vision-aided\nintegrated sensing and communication (ISAC) framework for UAV-assisted access\nsystems, where onboard masked De-Diffusion models extract compact semantic\ntokens, including agent type, activity class, and heading orientation, while\nexplicitly suppressing sensitive visual content. These tokens are fused with\nmmWave radar measurements to construct a semantic risk heatmap reflecting\nmotion density, occlusion, and scene complexity, which guides access technology\nselection and resource scheduling. We formulate a multi-objective optimization\nproblem to jointly maximize weighted energy and perception efficiency via radio\naccess technology (RAT) assignment, power control, and beamforming, subject to\nagent-specific QoS constraints. To solve this, we develop De-Diffusion-driven\nvision-aided risk-aware resource optimization algorithm DeDiff-VARARO, a novel\ntwo-stage cross-modal control algorithm: the first stage reconstructs visual\nscenes from tokens via De-Diffusion model for semantic parsing, while the\nsecond stage employs a deep deterministic policy gradient (DDPG)-based policy\nto adapt RAT selection, power control, and beam assignment based on fused\nradar-visual states. Simulation results show that DeDiff-VARARO consistently\noutperforms baselines in reward convergence, link robustness, and semantic\nfidelity, achieving within $4\\%$ of the performance of a raw-image upper bound\nwhile preserving user privacy and scalability in dense environments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy gradient"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01522", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.01522", "abs": "https://arxiv.org/abs/2507.01522", "authors": ["Koen Ponse", "Jan Felix Kleuker", "Aske Plaat", "Thomas Moerland"], "title": "Chargax: A JAX Accelerated EV Charging Simulator", "comment": "Accepted at RLC 2025", "summary": "Deep Reinforcement Learning can play a key role in addressing sustainable\nenergy challenges. For instance, many grid systems are heavily congested,\nhighlighting the urgent need to enhance operational efficiency. However,\nreinforcement learning approaches have traditionally been slow due to the high\nsample complexity and expensive simulation requirements. While recent works\nhave effectively used GPUs to accelerate data generation by converting\nenvironments to JAX, these works have largely focussed on classical toy\nproblems. This paper introduces Chargax, a JAX-based environment for realistic\nsimulation of electric vehicle charging stations designed for accelerated\ntraining of RL agents. We validate our environment in a variety of scenarios\nbased on real data, comparing reinforcement learning agents against baselines.\nChargax delivers substantial computational performance improvements of over\n100x-1000x over existing environments. Additionally, Chargax' modular\narchitecture enables the representation of diverse real-world charging station\nconfigurations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01409", "abs": "https://arxiv.org/abs/2507.01409", "authors": ["Kuniaki Saito", "Donghyun Kim", "Kwanyong Park", "Atsushi Hashimoto", "Yoshitaka Ushiku"], "title": "CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning", "comment": "Accepted to ICCV2025", "summary": "An image captioning model flexibly switching its language pattern, e.g.,\ndescriptiveness and length, should be useful since it can be applied to diverse\napplications. However, despite the dramatic improvement in generative\nvision-language models, fine-grained control over the properties of generated\ncaptions is not easy due to two reasons: (i) existing models are not given the\nproperties as a condition during training and (ii) existing models cannot\nsmoothly transition its language pattern from one state to the other. Given\nthis challenge, we propose a new approach, CaptionSmiths, to acquire a single\ncaptioning model that can handle diverse language patterns. First, our approach\nquantifies three properties of each caption, length, descriptiveness, and\nuniqueness of a word, as continuous scalar values, without human annotation.\nGiven the values, we represent the conditioning via interpolation between two\nendpoint vectors corresponding to the extreme states, e.g., one for a very\nshort caption and one for a very long caption. Empirical results demonstrate\nthat the resulting model can smoothly change the properties of the output\ncaptions and show higher lexical alignment than baselines. For instance,\nCaptionSmiths reduces the error in controlling caption length by 506\\% despite\nbetter lexical alignment. Code will be available on\nhttps://github.com/omron-sinicx/captionsmiths.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "fine-grained"], "score": 2}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01054", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01054", "abs": "https://arxiv.org/abs/2507.01054", "authors": ["Jithendaraa Subramanian", "Linda Hung", "Daniel Schweigert", "Santosh Suram", "Weike Ye"], "title": "XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science", "comment": "10 pages, 6 figures", "summary": "Recent advances in materials discovery have been driven by structure-based\nmodels, particularly those using crystal graphs. While effective for\ncomputational datasets, these models are impractical for real-world\napplications where atomic structures are often unknown or difficult to obtain.\nWe propose a scalable multimodal framework that learns directly from elemental\ncomposition and X-ray diffraction (XRD) -- two of the more available modalities\nin experimental workflows without requiring crystal structure input. Our\narchitecture integrates modality-specific encoders with a cross-attention\nfusion module and is trained on the 5-million-sample Alexandria dataset. We\npresent masked XRD modeling (MXM), and apply MXM and contrastive alignment as\nself-supervised pretraining strategies. Pretraining yields faster convergence\n(up to 4.2x speedup) and improves both accuracy and representation quality. We\nfurther demonstrate that multimodal performance scales more favorably with\ndataset size than unimodal baselines, with gains compounding at larger data\nregimes. Our results establish a path toward structure-free, experimentally\ngrounded foundation models for materials science.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01073", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.01073", "abs": "https://arxiv.org/abs/2507.01073", "authors": ["Dian Jin"], "title": "Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs", "comment": null, "summary": "Graph neural networks (GNNs) have achieved remarkable success in molecular\nproperty prediction. However, traditional graph representations struggle to\neffectively encode the inherent 3D spatial structures of molecules, as\nmolecular orientations in 3D space introduce significant variability, severely\nlimiting model generalization and robustness. Existing approaches primarily\nfocus on rotation-invariant and rotation-equivariant methods. Invariant methods\noften rely heavily on prior knowledge and lack sufficient generalizability,\nwhile equivariant methods suffer from high computational costs. To address\nthese limitations, this paper proposes a novel plug-and-play 3D encoding module\nleveraging rotational sampling. By computing the expectation over the SO(3)\nrotational group, the method naturally achieves approximate rotational\ninvariance. Furthermore, by introducing a carefully designed post-alignment\nstrategy, strict invariance can be achieved without compromising performance.\nExperimental evaluations on the QM9 and C10 Datasets demonstrate superior\npredictive accuracy, robustness, and generalization performance compared to\nexisting methods. Moreover, the proposed approach maintains low computational\ncomplexity and enhanced interpretability, providing a promising direction for\nefficient and effective handling of 3D molecular information in drug discovery\nand material design.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01467", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01467", "abs": "https://arxiv.org/abs/2507.01467", "authors": ["Ge Wu", "Shen Zhang", "Ruijing Shi", "Shanghua Gao", "Zhenyuan Chen", "Lei Wang", "Zhaowei Chen", "Hongcheng Gao", "Yao Tang", "Jian Yang", "Ming-Ming Cheng", "Xiang Li"], "title": "Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think", "comment": null, "summary": "REPA and its variants effectively mitigate training challenges in diffusion\nmodels by incorporating external visual representations from pretrained models,\nthrough alignment between the noisy hidden projections of denoising networks\nand foundational clean image representations. We argue that the external\nalignment, which is absent during the entire denoising inference process, falls\nshort of fully harnessing the potential of discriminative representations. In\nthis work, we propose a straightforward method called Representation\nEntanglement for Generation (REG), which entangles low-level image latents with\na single high-level class token from pretrained foundation models for\ndenoising. REG acquires the capability to produce coherent image-class pairs\ndirectly from pure noise, substantially improving both generation quality and\ntraining efficiency. This is accomplished with negligible additional inference\noverhead, requiring only one single additional token for denoising (<0.5\\%\nincrease in FLOPs and latency). The inference process concurrently reconstructs\nboth image latents and their corresponding global semantics, where the acquired\nsemantic knowledge actively guides and enhances the image generation process.\nOn ImageNet 256$\\times$256, SiT-XL/2 + REG demonstrates remarkable convergence\nacceleration, achieving $\\textbf{63}\\times$ and $\\textbf{23}\\times$ faster\ntraining than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,\nSiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA\ntrained for 4M iterations ($\\textbf{10}\\times$ longer). Code is available at:\nhttps://github.com/Martinser/REG.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01099", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01099", "abs": "https://arxiv.org/abs/2507.01099", "authors": ["Zeyi Liu", "Shuang Li", "Eric Cousineau", "Siyuan Feng", "Benjamin Burchfiel", "Shuran Song"], "title": "Geometry-aware 4D Video Generation for Robot Manipulation", "comment": "Project website: https://robot4dgen.github.io", "summary": "Understanding and predicting the dynamics of the physical world can enhance a\nrobot's ability to plan and interact effectively in complex environments. While\nrecent video generation models have shown strong potential in modeling dynamic\nscenes, generating videos that are both temporally coherent and geometrically\nconsistent across camera views remains a significant challenge. To address\nthis, we propose a 4D video generation model that enforces multi-view 3D\nconsistency of videos by supervising the model with cross-view pointmap\nalignment during training. This geometric supervision enables the model to\nlearn a shared 3D representation of the scene, allowing it to predict future\nvideo sequences from novel viewpoints based solely on the given RGB-D\nobservations, without requiring camera poses as inputs. Compared to existing\nbaselines, our method produces more visually stable and spatially aligned\npredictions across multiple simulated and real-world robotic datasets. We\nfurther show that the predicted 4D videos can be used to recover robot\nend-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting\nrobust robot manipulation and generalization to novel camera viewpoints.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01161", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.01161", "abs": "https://arxiv.org/abs/2507.01161", "authors": ["Zhizhuo Zhang", "Hao Peng", "Xiaoli Bai"], "title": "Imitation Learning for Satellite Attitude Control under Unknown Perturbations", "comment": "2025 AAS/AIAA Astrodynamics Specialist Conference", "summary": "This paper presents a novel satellite attitude control framework that\nintegrates Soft Actor-Critic (SAC) reinforcement learning with Generative\nAdversarial Imitation Learning (GAIL) to achieve robust performance under\nvarious unknown perturbations. Traditional control techniques often rely on\nprecise system models and are sensitive to parameter uncertainties and external\nperturbations. To overcome these limitations, we first develop a SAC-based\nexpert controller that demonstrates improved resilience against actuator\nfailures, sensor noise, and attitude misalignments, outperforming our previous\nresults in several challenging scenarios. We then use GAIL to train a learner\npolicy that imitates the expert's trajectories, thereby reducing training costs\nand improving generalization through expert demonstrations. Preliminary\nexperiments under single and combined perturbations show that the SAC expert\ncan rotate the antenna to a specified direction and keep the antenna\norientation reliably stable in most of the listed perturbations. Additionally,\nthe GAIL learner can imitate most of the features from the trajectories\ngenerated by the SAC expert. Comparative evaluations and ablation studies\nconfirm the effectiveness of the SAC algorithm and reward shaping. The\nintegration of GAIL further reduces sample complexity and demonstrates\npromising imitation capabilities, paving the way for more intelligent and\nautonomous spacecraft control systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01823", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01823", "abs": "https://arxiv.org/abs/2507.01823", "authors": ["Dmytro Kuzmenko", "Nadiya Shvai"], "title": "TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents", "comment": "Preprint of a manuscript submitted for peer review", "summary": "We present a novel approach to knowledge transfer in model-based\nreinforcement learning, addressing the critical challenge of deploying large\nworld models in resource-constrained environments. Our method efficiently\ndistills a high-capacity multi-task agent (317M parameters) into a compact\nmodel (1M parameters) on the MT30 benchmark, significantly improving\nperformance across diverse tasks. Our distilled model achieves a\nstate-of-the-art normalized score of 28.45, surpassing the original 1M\nparameter model score of 18.93. This improvement demonstrates the ability of\nour distillation technique to capture and consolidate complex multi-task\nknowledge. We further optimize the distilled model through FP16 post-training\nquantization, reducing its size by $\\sim$50\\%. Our approach addresses practical\ndeployment limitations and offers insights into knowledge representation in\nlarge world models, paving the way for more efficient and accessible multi-task\nreinforcement learning systems in robotics and other resource-constrained\napplications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01201", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01201", "abs": "https://arxiv.org/abs/2507.01201", "authors": ["Hyoseo", "Yoon", "Yisong Yue", "Been Kim"], "title": "Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models", "comment": null, "summary": "Independently trained vision and language models inhabit disjoint\nrepresentational spaces, shaped by their respective modalities, objectives, and\narchitectures. Yet an emerging hypothesis - the Platonic Representation\nHypothesis - suggests that such models may nonetheless converge toward a shared\nstatistical model of reality. This compatibility, if it exists, raises a\nfundamental question: can we move beyond post-hoc statistical detection of\nalignment and explicitly optimize for it between such disjoint representations?\nWe cast this Platonic alignment problem as a multi-objective optimization task\n- preserve each modality's native structure while aligning for mutual\ncoherence. We introduce the Joint Autoencoder Modulator (JAM) framework that\njointly trains modality-specific autoencoders on the latent representations of\npre-trained single modality models, encouraging alignment through both\nreconstruction and cross-modal objectives. By analogy, this framework serves as\na method to escape Plato's Cave, enabling the emergence of shared structure\nfrom disjoint inputs. We evaluate this framework across three critical design\naxes: (i) the alignment objective - comparing contrastive loss (Con), its\nhard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at\nwhich alignment is most effective, and (iii) the impact of foundation model\nscale on representational convergence. Our findings show that our lightweight\nPareto-efficient framework reliably induces alignment, even across frozen,\nindependently trained representations, offering both theoretical insight and\npractical pathways for transforming generalist unimodal foundations into\nspecialist multimodal models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01603", "abs": "https://arxiv.org/abs/2507.01603", "authors": ["Yue-Jiang Dong", "Wang Zhao", "Jiale Xu", "Ying Shan", "Song-Hai Zhang"], "title": "DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation", "comment": "Accepted by ICCV 2025", "summary": "Diffusion-based video depth estimation methods have achieved remarkable\nsuccess with strong generalization ability. However, predicting depth for long\nvideos remains challenging. Existing methods typically split videos into\noverlapping sliding windows, leading to accumulated scale discrepancies across\ndifferent windows, particularly as the number of windows increases.\nAdditionally, these methods rely solely on 2D diffusion priors, overlooking the\ninherent 3D geometric structure of video depths, which results in geometrically\ninconsistent predictions. In this paper, we propose DepthSync, a novel,\ntraining-free framework using diffusion guidance to achieve scale- and\ngeometry-consistent depth predictions for long videos. Specifically, we\nintroduce scale guidance to synchronize the depth scale across windows and\ngeometry guidance to enforce geometric alignment within windows based on the\ninherent 3D constraints in video depths. These two terms work synergistically,\nsteering the denoising process toward consistent depth predictions. Experiments\non various datasets validate the effectiveness of our method in producing depth\nestimates with improved scale and geometry consistency, particularly for long\nvideos.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01321", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.01321", "abs": "https://arxiv.org/abs/2507.01321", "authors": ["Zhiyao Ren", "Siyuan Liang", "Aishan Liu", "Dacheng Tao"], "title": "ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks", "comment": "ICML 2025", "summary": "In-context learning (ICL) has demonstrated remarkable success in large\nlanguage models (LLMs) due to its adaptability and parameter-free nature.\nHowever, it also introduces a critical vulnerability to backdoor attacks, where\nadversaries can manipulate LLM behaviors by simply poisoning a few ICL\ndemonstrations. In this paper, we propose, for the first time, the\ndual-learning hypothesis, which posits that LLMs simultaneously learn both the\ntask-relevant latent concepts and backdoor latent concepts within poisoned\ndemonstrations, jointly influencing the probability of model outputs. Through\ntheoretical analysis, we derive an upper bound for ICL backdoor effects,\nrevealing that the vulnerability is dominated by the concept preference ratio\nbetween the task and the backdoor. Motivated by these findings, we propose\nICLShield, a defense mechanism that dynamically adjusts the concept preference\nratio. Our method encourages LLMs to select clean demonstrations during the ICL\nphase by leveraging confidence and similarity scores, effectively mitigating\nsusceptibility to backdoor attacks. Extensive experiments across multiple LLMs\nand tasks demonstrate that our method achieves state-of-the-art defense\neffectiveness, significantly outperforming existing approaches (+26.02% on\naverage). Furthermore, our method exhibits exceptional adaptability and\ndefensive performance even for closed-source models (e.g., GPT-4).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01327", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01327", "abs": "https://arxiv.org/abs/2507.01327", "authors": ["Xiaoyun Zhang", "Jingqing Ruan", "Xing Ma", "Yawen Zhu", "Jiansong Chen", "Ke Zeng", "Xunliang Cai"], "title": "Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy", "comment": "15 pages, 6 figures, submitted to EMNLP", "summary": "Detecting abnormal events in real-world customer service dialogues is highly\nchallenging due to the complexity of business data and the dynamic nature of\ncustomer interactions. Moreover, models must demonstrate strong out-of-domain\n(OOD) generalization to enable rapid adaptation across different business\nscenarios and maximize commercial value. In this work, we propose a novel\nAdaptive Perplexity-Aware Reinforcement Learning (APARL) framework that\nleverages the advanced reasoning capabilities of large language models for\nabnormal event detection. APARL introduces a dual-loop dynamic curriculum\nlearning architecture, enabling the model to progressively focus on more\nchallenging samples as its proficiency increases. This design effectively\naddresses performance bottlenecks and significantly enhances OOD\ntransferability. Extensive evaluations on food delivery dialogue tasks show\nthat our model achieves significantly enhanced adaptability and robustness,\nattaining the highest F1 score with an average improvement of 17.19\\%, and an\naverage improvement of 9.59\\% in OOD transfer tests. This method provides a\nsuperior solution for industrial deployment of anomaly detection models,\ncontributing to improved operational efficiency and commercial benefits.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01381", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01381", "abs": "https://arxiv.org/abs/2507.01381", "authors": ["Tong Liu", "Yinuo Wang", "Xujie Song", "Wenjun Zou", "Liangfa Chen", "Likun Wang", "Bin Shuai", "Jingliang Duan", "Shengbo Eben Li"], "title": "Distributional Soft Actor-Critic with Diffusion Policy", "comment": "Accepted IEEE ITSC 2025", "summary": "Reinforcement learning has been proven to be highly effective in handling\ncomplex control tasks. Traditional methods typically use unimodal\ndistributions, such as Gaussian distributions, to model the output of value\ndistributions. However, unimodal distribution often and easily causes bias in\nvalue function estimation, leading to poor algorithm performance. This paper\nproposes a distributional reinforcement learning algorithm called DSAC-D\n(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges\nof estimating bias in value functions and obtaining multimodal policy\nrepresentations. A multimodal distributional policy iteration framework that\ncan converge to the optimal policy was established by introducing policy\nentropy and value distribution function. A diffusion value network that can\naccurately characterize the distribution of multi peaks was constructed by\ngenerating a set of reward samples through reverse sampling using a diffusion\nmodel. Based on this, a distributional reinforcement learning algorithm with\ndual diffusion of the value network and the policy network was derived. MuJoCo\ntesting tasks demonstrate that the proposed algorithm not only learns\nmultimodal policy, but also achieves state-of-the-art (SOTA) performance in all\n9 control tasks, with significant suppression of estimation bias and total\naverage return improvement of over 10\\% compared to existing mainstream\nalgorithms. The results of real vehicle testing show that DSAC-D can accurately\ncharacterize the multimodal distribution of different driving styles, and the\ndiffusion policy network can characterize multimodal trajectories.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01643", "abs": "https://arxiv.org/abs/2507.01643", "authors": ["Weijie Yin", "Dingkang Yang", "Hongyuan Dong", "Zijian Kang", "Jiacong Wang", "Xiao Liang", "Chao Feng", "Jiao Ran"], "title": "SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement", "comment": "We release SAILViT, a series of versatile vision foundation models", "summary": "Vision Transformers (ViTs) are essential as foundation backbones in\nestablishing the visual comprehension capabilities of Multimodal Large Language\nModels (MLLMs). Although most ViTs achieve impressive performance through\nimage-text pair-based contrastive learning or self-supervised mechanisms, they\nstruggle to engage in connector-based co-training directly with LLMs due to\npotential parameter initialization conflicts and modality semantic gaps. To\naddress the above challenges, this paper proposes SAILViT, a gradual feature\nlearning-enhanced ViT for facilitating MLLMs to break through performance\nbottlenecks in complex multimodal interactions. SAILViT achieves\ncoarse-to-fine-grained feature alignment and world knowledge infusion with\ngradual feature refinement, which better serves target training demands. We\nperform thorough empirical analyses to confirm the powerful robustness and\ngeneralizability of SAILViT across different dimensions, including parameter\nsizes, model architectures, training strategies, and data scales. Equipped with\nSAILViT, existing MLLMs show significant and consistent performance\nimprovements on the OpenCompass benchmark across extensive downstream tasks.\nSAILViT series models are released at\nhttps://huggingface.co/BytedanceDouyinContent.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01653", "abs": "https://arxiv.org/abs/2507.01653", "authors": ["Yuran Wang", "Yingping Liang", "Yutao Hu", "Ying Fu"], "title": "RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather", "comment": "accepted by ICCV25", "summary": "Learning-based stereo matching models struggle in adverse weather conditions\ndue to the scarcity of corresponding training data and the challenges in\nextracting discriminative features from degraded images. These limitations\nsignificantly hinder zero-shot generalization to out-of-distribution weather\nconditions. In this paper, we propose \\textbf{RobuSTereo}, a novel framework\nthat enhances the zero-shot generalization of stereo matching models under\nadverse weather by addressing both data scarcity and feature extraction\nchallenges. First, we introduce a diffusion-based simulation pipeline with a\nstereo consistency module, which generates high-quality stereo data tailored\nfor adverse conditions. By training stereo matching models on our synthetic\ndatasets, we reduce the domain gap between clean and degraded images,\nsignificantly improving the models' robustness to unseen weather conditions.\nThe stereo consistency module ensures structural alignment across synthesized\nimage pairs, preserving geometric integrity and enhancing depth estimation\naccuracy. Second, we design a robust feature encoder that combines a\nspecialized ConvNet with a denoising transformer to extract stable and reliable\nfeatures from degraded images. The ConvNet captures fine-grained local\nstructures, while the denoising transformer refines global representations,\neffectively mitigating the impact of noise, low visibility, and weather-induced\ndistortions. This enables more accurate disparity estimation even under\nchallenging visual conditions. Extensive experiments demonstrate that\n\\textbf{RobuSTereo} significantly improves the robustness and generalization of\nstereo matching models across diverse adverse weather scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01470", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01470", "abs": "https://arxiv.org/abs/2507.01470", "authors": ["Yannick Molinghen", "Tom Lenaerts"], "title": "Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals", "comment": "Accepted at \"Finding the Frame 2025\", workshop at RLC", "summary": "This work re-examines the commonly held assumption that the frequency of\nrewards is a reliable measure of task difficulty in reinforcement learning. We\nidentify and formalize a structural challenge that undermines the effectiveness\nof current policy learning methods: when essential subgoals do not directly\nyield rewards. We characterize such settings as exhibiting zero-incentive\ndynamics, where transitions critical to success remain unrewarded. We show that\nstate-of-the-art deep subgoal-based algorithms fail to leverage these dynamics\nand that learning performance is highly sensitive to the temporal proximity\nbetween subgoal completion and eventual reward. These findings reveal a\nfundamental limitation in current approaches and point to the need for\nmechanisms that can infer latent task structure without relying on immediate\nincentives.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01522", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.01522", "abs": "https://arxiv.org/abs/2507.01522", "authors": ["Koen Ponse", "Jan Felix Kleuker", "Aske Plaat", "Thomas Moerland"], "title": "Chargax: A JAX Accelerated EV Charging Simulator", "comment": "Accepted at RLC 2025", "summary": "Deep Reinforcement Learning can play a key role in addressing sustainable\nenergy challenges. For instance, many grid systems are heavily congested,\nhighlighting the urgent need to enhance operational efficiency. However,\nreinforcement learning approaches have traditionally been slow due to the high\nsample complexity and expensive simulation requirements. While recent works\nhave effectively used GPUs to accelerate data generation by converting\nenvironments to JAX, these works have largely focussed on classical toy\nproblems. This paper introduces Chargax, a JAX-based environment for realistic\nsimulation of electric vehicle charging stations designed for accelerated\ntraining of RL agents. We validate our environment in a variety of scenarios\nbased on real data, comparing reinforcement learning agents against baselines.\nChargax delivers substantial computational performance improvements of over\n100x-1000x over existing environments. Additionally, Chargax' modular\narchitecture enables the representation of diverse real-world charging station\nconfigurations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01712", "categories": ["cs.CV", "eess.IV", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.01712", "abs": "https://arxiv.org/abs/2507.01712", "authors": ["Xinle Tian", "Matthew Nunes", "Emiko Dupont", "Shaunagh Downing", "Freddie Lichtenstein", "Matt Burns"], "title": "Using Wavelet Domain Fingerprints to Improve Source Camera Identification", "comment": null, "summary": "Camera fingerprint detection plays a crucial role in source identification\nand image forensics, with wavelet denoising approaches proving to be\nparticularly effective in extracting sensor pattern noise (SPN). In this\narticle, we propose a modification to wavelet-based SPN extraction. Rather than\nconstructing the fingerprint as an image, we introduce the notion of a wavelet\ndomain fingerprint. This avoids the final inversion step of the denoising\nalgorithm and allows fingerprint comparisons to be made directly in the wavelet\ndomain. As such, our modification streamlines the extraction and comparison\nprocess. Experimental results on real-world datasets demonstrate that our\nmethod not only achieves higher detection accuracy but can also significantly\nimprove processing speed.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01722", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01722", "abs": "https://arxiv.org/abs/2507.01722", "authors": ["Enrico Cassano", "Riccardo Renzulli", "Andrea Bragagnolo", "Marco Grangetto"], "title": "When Does Pruning Benefit Vision Representations?", "comment": null, "summary": "Pruning is widely used to reduce the complexity of deep learning models, but\nits effects on interpretability and representation learning remain poorly\nunderstood. This paper investigates how pruning influences vision models across\nthree key dimensions: (i) interpretability, (ii) unsupervised object discovery,\nand (iii) alignment with human perception. We first analyze different vision\nnetwork architectures to examine how varying sparsity levels affect feature\nattribution interpretability methods. Additionally, we explore whether pruning\npromotes more succinct and structured representations, potentially improving\nunsupervised object discovery by discarding redundant information while\npreserving essential features. Finally, we assess whether pruning enhances the\nalignment between model representations and human perception, investigating\nwhether sparser models focus on more discriminative features similarly to\nhumans. Our findings also reveal the presence of sweet spots, where sparse\nmodels exhibit higher interpretability, downstream generalization and human\nalignment. However, these spots highly depend on the network architectures and\ntheir size in terms of trainable parameters. Our results suggest a complex\ninterplay between these three dimensions, highlighting the importance of\ninvestigating when and how pruning benefits vision representations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01663", "abs": "https://arxiv.org/abs/2507.01663", "authors": ["Zhenyu Han", "Ansheng You", "Haibo Wang", "Kui Luo", "Guang Yang", "Wenqi Shi", "Menglong Chen", "Sicheng Zhang", "Zeshun Lan", "Chunshi Deng", "Huazhong Ji", "Wenjie Liu", "Yu Huang", "Yixiang Zhang", "Chenyi Pan", "Jing Wang", "Xin Huang", "Chunsheng Li", "Jianping Wu"], "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training", "comment": null, "summary": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01792", "abs": "https://arxiv.org/abs/2507.01792", "authors": ["Peng Zheng", "Ye Wang", "Rui Ma", "Zuxuan Wu"], "title": "FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization", "comment": null, "summary": "Subject-driven image generation plays a crucial role in applications such as\nvirtual try-on and poster design. Existing approaches typically fine-tune\npretrained generative models or apply LoRA-based adaptations for individual\nsubjects. However, these methods struggle with multi-subject personalization,\nas combining independently adapted modules often requires complex re-tuning or\njoint optimization. We present FreeLoRA, a simple and generalizable framework\nthat enables training-free fusion of subject-specific LoRA modules for\nmulti-subject personalization. Each LoRA module is adapted on a few images of a\nspecific subject using a Full Token Tuning strategy, where it is applied across\nall tokens in the prompt to encourage weakly supervised token-content\nalignment. At inference, we adopt Subject-Aware Inference, activating each\nmodule only on its corresponding subject tokens. This enables training-free\nfusion of multiple personalized subjects within a single image, while\nmitigating overfitting and mutual interference between subjects. Extensive\nexperiments show that FreeLoRA achieves strong performance in both subject\nfidelity and prompt consistency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01912", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01912", "abs": "https://arxiv.org/abs/2507.01912", "authors": ["Ranjan Sapkota", "Zhichao Meng", "Martin Churuvija", "Xiaoqiang Du", "Zenghong Ma", "Manoj Karkee"], "title": "3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP", "comment": "17 pages, 4 tables, 11 figures", "summary": "In orchard automation, dense foliage during the canopy season severely\noccludes tree structures, minimizing visibility to various canopy parts such as\ntrunks and branches, which limits the ability of a machine vision system.\nHowever, canopy structure is more open and visible during the dormant season\nwhen trees are defoliated. In this work, we present an information fusion\nframework that integrates multi-seasonal structural data to support robotic and\nautomated crop load management during the entire growing season. The framework\ncombines high-resolution RGB-D imagery from both dormant and canopy periods\nusing YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D\nreconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for\nmodel alignment. Segmentation outputs from YOLOv9-Seg were used to extract\ndepth-informed masks, which enabled accurate 3D point cloud reconstruction via\nKinect Fusion; these reconstructed models from each season were subsequently\naligned using Fast GICP to achieve spatially coherent multi-season fusion. The\nYOLOv9-Seg model, trained on manually annotated images, achieved a mean squared\nerror (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in\ndormant season dataset. Kinect Fusion enabled accurate reconstruction of tree\ngeometry, validated with field measurements resulting in root mean square\nerrors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and\n13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal\nregistration with a minimum fitness score of 0.00197, allowing integrated,\ncomprehensive tree structure modeling despite heavy occlusions during the\ngrowing season. This fused structural representation enables robotic systems to\naccess otherwise obscured architectural information, improving the precision of\npruning, thinning, and other automated orchard operations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01823", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01823", "abs": "https://arxiv.org/abs/2507.01823", "authors": ["Dmytro Kuzmenko", "Nadiya Shvai"], "title": "TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents", "comment": "Preprint of a manuscript submitted for peer review", "summary": "We present a novel approach to knowledge transfer in model-based\nreinforcement learning, addressing the critical challenge of deploying large\nworld models in resource-constrained environments. Our method efficiently\ndistills a high-capacity multi-task agent (317M parameters) into a compact\nmodel (1M parameters) on the MT30 benchmark, significantly improving\nperformance across diverse tasks. Our distilled model achieves a\nstate-of-the-art normalized score of 28.45, surpassing the original 1M\nparameter model score of 18.93. This improvement demonstrates the ability of\nour distillation technique to capture and consolidate complex multi-task\nknowledge. We further optimize the distilled model through FP16 post-training\nquantization, reducing its size by $\\sim$50\\%. Our approach addresses practical\ndeployment limitations and offers insights into knowledge representation in\nlarge world models, paving the way for more efficient and accessible multi-task\nreinforcement learning systems in robotics and other resource-constrained\napplications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01951", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01951", "abs": "https://arxiv.org/abs/2507.01951", "authors": ["Zixiao Wang", "Yuxin Wang", "Xiaorui Wang", "Mengting Xing", "Jie Gao", "Jianjun Xu", "Guangcan Liu", "Chenhui Jin", "Zhuo Wang", "Shengzhuo Zhang", "Hongtao Xie"], "title": "Test-Time Scaling with Reflective Generative Model", "comment": null, "summary": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time", "scaling", "scaling law"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01201", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01201", "abs": "https://arxiv.org/abs/2507.01201", "authors": ["Hyoseo", "Yoon", "Yisong Yue", "Been Kim"], "title": "Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models", "comment": null, "summary": "Independently trained vision and language models inhabit disjoint\nrepresentational spaces, shaped by their respective modalities, objectives, and\narchitectures. Yet an emerging hypothesis - the Platonic Representation\nHypothesis - suggests that such models may nonetheless converge toward a shared\nstatistical model of reality. This compatibility, if it exists, raises a\nfundamental question: can we move beyond post-hoc statistical detection of\nalignment and explicitly optimize for it between such disjoint representations?\nWe cast this Platonic alignment problem as a multi-objective optimization task\n- preserve each modality's native structure while aligning for mutual\ncoherence. We introduce the Joint Autoencoder Modulator (JAM) framework that\njointly trains modality-specific autoencoders on the latent representations of\npre-trained single modality models, encouraging alignment through both\nreconstruction and cross-modal objectives. By analogy, this framework serves as\na method to escape Plato's Cave, enabling the emergence of shared structure\nfrom disjoint inputs. We evaluate this framework across three critical design\naxes: (i) the alignment objective - comparing contrastive loss (Con), its\nhard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at\nwhich alignment is most effective, and (iii) the impact of foundation model\nscale on representational convergence. Our findings show that our lightweight\nPareto-efficient framework reliably induces alignment, even across frozen,\nindependently trained representations, offering both theoretical insight and\npractical pathways for transforming generalist unimodal foundations into\nspecialist multimodal models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
{"id": "2507.01099", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01099", "abs": "https://arxiv.org/abs/2507.01099", "authors": ["Zeyi Liu", "Shuang Li", "Eric Cousineau", "Siyuan Feng", "Benjamin Burchfiel", "Shuran Song"], "title": "Geometry-aware 4D Video Generation for Robot Manipulation", "comment": "Project website: https://robot4dgen.github.io", "summary": "Understanding and predicting the dynamics of the physical world can enhance a\nrobot's ability to plan and interact effectively in complex environments. While\nrecent video generation models have shown strong potential in modeling dynamic\nscenes, generating videos that are both temporally coherent and geometrically\nconsistent across camera views remains a significant challenge. To address\nthis, we propose a 4D video generation model that enforces multi-view 3D\nconsistency of videos by supervising the model with cross-view pointmap\nalignment during training. This geometric supervision enables the model to\nlearn a shared 3D representation of the scene, allowing it to predict future\nvideo sequences from novel viewpoints based solely on the given RGB-D\nobservations, without requiring camera poses as inputs. Compared to existing\nbaselines, our method produces more visually stable and spatially aligned\npredictions across multiple simulated and real-world robotic datasets. We\nfurther show that the predicted 4D videos can be used to recover robot\nend-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting\nrobust robot manipulation and generalization to novel camera viewpoints.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-07-03.jsonl"}
