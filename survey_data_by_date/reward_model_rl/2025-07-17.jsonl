{"id": "2507.11554", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11554", "abs": "https://arxiv.org/abs/2507.11554", "authors": ["Zejian Li", "Yize Li", "Chenye Meng", "Zhongni Liu", "Yang Ling", "Shengyuan Zhang", "Guang Yang", "Changyuan Yang", "Zhiyuan Yang", "Lingyun Sun"], "title": "Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models", "comment": null, "summary": "Recent advancements in diffusion models (DMs) have been propelled by\nalignment methods that post-train models to better conform to human\npreferences. However, these approaches typically require computation-intensive\ntraining of a base model and a reward model, which not only incurs substantial\ncomputational overhead but may also compromise model accuracy and training\nefficiency. To address these limitations, we propose Inversion-DPO, a novel\nalignment framework that circumvents reward modeling by reformulating Direct\nPreference Optimization (DPO) with DDIM inversion for DMs. Our method conducts\nintractable posterior sampling in Diffusion-DPO with the deterministic\ninversion from winning and losing samples to noise and thus derive a new\npost-training paradigm. This paradigm eliminates the need for auxiliary reward\nmodels or inaccurate appromixation, significantly enhancing both precision and\nefficiency of training. We apply Inversion-DPO to a basic task of text-to-image\ngeneration and a challenging task of compositional image generation. Extensive\nexperiments show substantial performance improvements achieved by Inversion-DPO\ncompared to existing post-training methods and highlight the ability of the\ntrained generative models to generate high-fidelity compositionally coherent\nimages. For the post-training of compostitional image geneation, we curate a\npaired dataset consisting of 11,140 images with complex structural annotations\nand comprehensive scores, designed to enhance the compositional capabilities of\ngenerative models. Inversion-DPO explores a new avenue for efficient,\nhigh-precision alignment in diffusion models, advancing their applicability to\ncomplex realistic generation tasks. Our code is available at\nhttps://github.com/MIGHTYEZ/Inversion-DPO", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward modeling", "preference", "alignment", "DPO"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.11554", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11554", "abs": "https://arxiv.org/abs/2507.11554", "authors": ["Zejian Li", "Yize Li", "Chenye Meng", "Zhongni Liu", "Yang Ling", "Shengyuan Zhang", "Guang Yang", "Changyuan Yang", "Zhiyuan Yang", "Lingyun Sun"], "title": "Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models", "comment": null, "summary": "Recent advancements in diffusion models (DMs) have been propelled by\nalignment methods that post-train models to better conform to human\npreferences. However, these approaches typically require computation-intensive\ntraining of a base model and a reward model, which not only incurs substantial\ncomputational overhead but may also compromise model accuracy and training\nefficiency. To address these limitations, we propose Inversion-DPO, a novel\nalignment framework that circumvents reward modeling by reformulating Direct\nPreference Optimization (DPO) with DDIM inversion for DMs. Our method conducts\nintractable posterior sampling in Diffusion-DPO with the deterministic\ninversion from winning and losing samples to noise and thus derive a new\npost-training paradigm. This paradigm eliminates the need for auxiliary reward\nmodels or inaccurate appromixation, significantly enhancing both precision and\nefficiency of training. We apply Inversion-DPO to a basic task of text-to-image\ngeneration and a challenging task of compositional image generation. Extensive\nexperiments show substantial performance improvements achieved by Inversion-DPO\ncompared to existing post-training methods and highlight the ability of the\ntrained generative models to generate high-fidelity compositionally coherent\nimages. For the post-training of compostitional image geneation, we curate a\npaired dataset consisting of 11,140 images with complex structural annotations\nand comprehensive scores, designed to enhance the compositional capabilities of\ngenerative models. Inversion-DPO explores a new avenue for efficient,\nhigh-precision alignment in diffusion models, advancing their applicability to\ncomplex realistic generation tasks. Our code is available at\nhttps://github.com/MIGHTYEZ/Inversion-DPO", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward modeling", "preference", "alignment", "DPO"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12416", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12416", "abs": "https://arxiv.org/abs/2507.12416", "authors": ["Jaehyun Kwak", "Ramahdani Muhammad Izaaz Inhar", "Se-Young Yun", "Sung-Ju Lee"], "title": "QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval", "comment": "Accepted to ICML 2025", "summary": "Composed Image Retrieval (CIR) retrieves relevant images based on a reference\nimage and accompanying text describing desired modifications. However, existing\nCIR methods only focus on retrieving the target image and disregard the\nrelevance of other images. This limitation arises because most methods\nemploying contrastive learning-which treats the target image as positive and\nall other images in the batch as negatives-can inadvertently include false\nnegatives. This may result in retrieving irrelevant images, reducing user\nsatisfaction even when the target image is retrieved. To address this issue, we\npropose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which\noptimizes a reward model objective to reduce false negatives. Additionally, we\nintroduce a hard negative sampling strategy that selects images positioned\nbetween two steep drops in relevance scores following the target image, to\neffectively filter false negatives. In order to evaluate CIR models on their\nalignment with human satisfaction, we create Human-Preference FashionIQ\n(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond\ntarget retrieval. Extensive experiments demonstrate that QuRe achieves\nstate-of-the-art performance on FashionIQ and CIRR datasets while exhibiting\nthe strongest alignment with human preferences on the HP-FashionIQ dataset. The\nsource code is available at https://github.com/jackwaky/QuRe.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12416", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12416", "abs": "https://arxiv.org/abs/2507.12416", "authors": ["Jaehyun Kwak", "Ramahdani Muhammad Izaaz Inhar", "Se-Young Yun", "Sung-Ju Lee"], "title": "QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval", "comment": "Accepted to ICML 2025", "summary": "Composed Image Retrieval (CIR) retrieves relevant images based on a reference\nimage and accompanying text describing desired modifications. However, existing\nCIR methods only focus on retrieving the target image and disregard the\nrelevance of other images. This limitation arises because most methods\nemploying contrastive learning-which treats the target image as positive and\nall other images in the batch as negatives-can inadvertently include false\nnegatives. This may result in retrieving irrelevant images, reducing user\nsatisfaction even when the target image is retrieved. To address this issue, we\npropose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which\noptimizes a reward model objective to reduce false negatives. Additionally, we\nintroduce a hard negative sampling strategy that selects images positioned\nbetween two steep drops in relevance scores following the target image, to\neffectively filter false negatives. In order to evaluate CIR models on their\nalignment with human satisfaction, we create Human-Preference FashionIQ\n(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond\ntarget retrieval. Extensive experiments demonstrate that QuRe achieves\nstate-of-the-art performance on FashionIQ and CIRR datasets while exhibiting\nthe strongest alignment with human preferences on the HP-FashionIQ dataset. The\nsource code is available at https://github.com/jackwaky/QuRe.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.11662", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11662", "abs": "https://arxiv.org/abs/2507.11662", "authors": ["Moises Andrade", "Joonhyuk Cha", "Brandon Ho", "Vriksha Srihari", "Karmesh Yadav", "Zsolt Kira"], "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification", "comment": "Our code and data are publicly available at\n  https://github.com/mshalimay/mllm-verifiers-abias-sgv", "summary": "Verifiers -- functions assigning rewards to agent behavior -- have been key\nfor AI progress in domains like math and board games. However, extending these\ngains to domains without clear-cut success criteria (e.g.,computer use) remains\na challenge: while humans can recognize suitable outcomes, translating this\nintuition into scalable rules is non-trivial. Multimodal Large Language\nModels(MLLMs) emerge as a promising solution, given their world knowledge,\nhuman-preference alignment, and reasoning skills. We evaluate MLLMs as\nverifiers of agent trajectories across web navigation, computer use, and\nrobotic manipulation, and identify a critical limitation: agreement bias, a\nstrong tendency for MLLMs to favor information in their context window, often\ngenerating chains of thought to rationalize flawed behavior. This bias is\npervasive across models, resilient to test-time scaling, and can impact several\nmethods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs\ndespite MLLMs showing strong, human-aligned priors on desired behavior. To\naddress this, we propose Self-Grounded Verification (SGV), a lightweight method\nthat enables more effective use of MLLMs' knowledge and reasoning by harnessing\ntheir own sampling mechanisms via unconditional and conditional generation. SGV\noperates in two steps: first, the MLLM is elicited to retrieve broad priors\nabout task completion, independent of the data under evaluation. Then,\nconditioned on self-generated priors, it reasons over and evaluates a candidate\ntrajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in\naccuracy and failure detection rates, and can perform real-time supervision of\nheterogeneous agents, boosting task completion of a GUI specialist in OSWorld,\na diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting\na new state of the art on the benchmark, surpassing the previous best by 48%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "agreement", "accuracy", "criteria"], "score": 5}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12215", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12215", "abs": "https://arxiv.org/abs/2507.12215", "authors": ["Yuhao Chen", "Shuochen Liu", "Yuanjie Lyu", "Chao Zhang", "Jiayao Shi", "Tong Xu"], "title": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning", "comment": "10 pages, 7 figures", "summary": "Game playing has long served as a fundamental benchmark for evaluating\nArtificial General Intelligence (AGI). While Large Language Models (LLMs) have\ndemonstrated impressive capabilities in general reasoning, their effectiveness\nin spatial strategic reasoning, which is critical for complex and fully\nobservable board games, remains insufficiently explored. In this work, we adopt\nChinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate\nrules and spatial complexity. To advance LLMs' strategic competence in such\nenvironments, we propose a training framework tailored to Xiangqi, built upon a\nlarge-scale dataset of five million board-move pairs enhanced with expert\nannotations and engine evaluations. Building on this foundation, we introduce\nXiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning\nfor legal move prediction to capture basic spatial rules, (2) incorporating\nstrategic annotations to improve decision-making, and (3) applying\nreinforcement learning via Group Relative Policy Optimization (GRPO) with\nmulti-dimensional reward signals to enhance reasoning stability. Our\nExperimental results indicate that, despite their size and power,\ngeneral-purpose LLMs struggle to achieve satisfactory performance in these\ntasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an\n18% rise in move legality and a 22% boost in analysis accuracy. Our results\npoint to a promising path for creating general strategic intelligence in\nspatially complex areas.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "testbed", "accuracy", "multi-dimensional"], "score": 5}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.11981", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11981", "abs": "https://arxiv.org/abs/2507.11981", "authors": ["Lukas Ellinger", "Miriam Ansch√ºtz", "Georg Groh"], "title": "Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions", "comment": "Accepted by RANLP 2025", "summary": "Large Language Models (LLMs) can provide accurate word definitions and\nexplanations for any context. However, the scope of the definition changes for\ndifferent target groups, like children or language learners. This is especially\nrelevant for homonyms, words with multiple meanings, where oversimplification\nmight risk information loss by omitting key senses, potentially misleading\nusers who trust LLM outputs. We investigate how simplification impacts homonym\ndefinition quality across three target groups: Normal, Simple, and ELI5. Using\ntwo novel evaluation datasets spanning multiple languages, we test DeepSeek v3,\nLlama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge\nand human annotations. Our results show that simplification drastically\ndegrades definition completeness by neglecting polysemy, increasing the risk of\nmisunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization\nsubstantially improves homonym response quality across all prompt types. These\nfindings highlight the need to balance simplicity and completeness in\neducational NLP to ensure reliable, context-aware definitions for all learners.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.11662", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11662", "abs": "https://arxiv.org/abs/2507.11662", "authors": ["Moises Andrade", "Joonhyuk Cha", "Brandon Ho", "Vriksha Srihari", "Karmesh Yadav", "Zsolt Kira"], "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification", "comment": "Our code and data are publicly available at\n  https://github.com/mshalimay/mllm-verifiers-abias-sgv", "summary": "Verifiers -- functions assigning rewards to agent behavior -- have been key\nfor AI progress in domains like math and board games. However, extending these\ngains to domains without clear-cut success criteria (e.g.,computer use) remains\na challenge: while humans can recognize suitable outcomes, translating this\nintuition into scalable rules is non-trivial. Multimodal Large Language\nModels(MLLMs) emerge as a promising solution, given their world knowledge,\nhuman-preference alignment, and reasoning skills. We evaluate MLLMs as\nverifiers of agent trajectories across web navigation, computer use, and\nrobotic manipulation, and identify a critical limitation: agreement bias, a\nstrong tendency for MLLMs to favor information in their context window, often\ngenerating chains of thought to rationalize flawed behavior. This bias is\npervasive across models, resilient to test-time scaling, and can impact several\nmethods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs\ndespite MLLMs showing strong, human-aligned priors on desired behavior. To\naddress this, we propose Self-Grounded Verification (SGV), a lightweight method\nthat enables more effective use of MLLMs' knowledge and reasoning by harnessing\ntheir own sampling mechanisms via unconditional and conditional generation. SGV\noperates in two steps: first, the MLLM is elicited to retrieve broad priors\nabout task completion, independent of the data under evaluation. Then,\nconditioned on self-generated priors, it reasons over and evaluates a candidate\ntrajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in\naccuracy and failure detection rates, and can perform real-time supervision of\nheterogeneous agents, boosting task completion of a GUI specialist in OSWorld,\na diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting\na new state of the art on the benchmark, surpassing the previous best by 48%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "agreement", "accuracy", "criteria"], "score": 5}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12455", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12455", "abs": "https://arxiv.org/abs/2507.12455", "authors": ["Shangpin Peng", "Senqiao Yang", "Li Jiang", "Zhuotao Tian"], "title": "Mitigating Object Hallucinations via Sentence-Level Early Intervention", "comment": null, "summary": "Multimodal large language models (MLLMs) have revolutionized cross-modal\nunderstanding but continue to struggle with hallucinations - fabricated content\ncontradicting visual inputs. Existing hallucination mitigation methods either\nincur prohibitive computational costs or introduce distribution mismatches\nbetween training data and model outputs. We identify a critical insight:\nhallucinations predominantly emerge at the early stages of text generation and\npropagate through subsequent outputs. To address this, we propose **SENTINEL**\n(**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain\npr**E**ference **L**earning), a framework that eliminates dependency on human\nannotations. Specifically, we first bootstrap high-quality in-domain preference\npairs by iteratively sampling model outputs, validating object existence\nthrough cross-checking with two open-vocabulary detectors, and classifying\nsentences into hallucinated/non-hallucinated categories. Subsequently, we use\ncontext-coherent positive samples and hallucinated negative samples to build\ncontext-aware preference data iteratively. Finally, we train models using a\ncontext-aware preference loss (C-DPO) that emphasizes discriminative learning\nat the sentence level where hallucinations initially manifest. Experimental\nresults show that SENTINEL can reduce hallucinations by over 90\\% compared to\nthe original model and outperforms the previous state-of-the-art method on both\nhallucination benchmarks and general capabilities benchmarks, demonstrating its\nsuperiority and generalization ability. The models, datasets, and code are\navailable at https://github.com/pspdada/SENTINEL.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO"], "score": 2}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.11582", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11582", "abs": "https://arxiv.org/abs/2507.11582", "authors": ["Kazuyoshi Otsuka"], "title": "Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance", "comment": "38 pages. Manuscript submitted for review to the Journal of\n  Computational Literary Studies (JCLS)", "summary": "This study positions large language models (LLMs) as \"subjective literary\ncritics\" to explore aesthetic preferences and evaluation patterns in literary\nassessment. Ten Japanese science fiction short stories were translated into\nEnglish and evaluated by six state-of-the-art LLMs across seven independent\nsessions. Principal component analysis and clustering techniques revealed\nsignificant variations in evaluation consistency ({\\alpha} ranging from 1.00 to\n0.35) and five distinct evaluation patterns. Additionally, evaluation variance\nacross stories differed by up to 4.5-fold, with TF-IDF analysis confirming\ndistinctive evaluation vocabularies for each model. Our seven-session\nwithin-day protocol using an original Science Fiction corpus strategically\nminimizes external biases, allowing us to observe implicit value systems shaped\nby RLHF and their influence on literary judgment. These findings suggest that\nLLMs may possess individual evaluation characteristics similar to human\ncritical schools, rather than functioning as neutral benchmarkers.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.11595", "categories": ["cs.AI", "cs.CY", "I.4.8; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.11595", "abs": "https://arxiv.org/abs/2507.11595", "authors": ["Hengyue Zhao"], "title": "A Study on the Application of Artificial Intelligence in Ecological Design", "comment": null, "summary": "This paper asks whether our relationship with nature can move from human\ndominance to genuine interdependence, and whether artificial intelligence (AI)\ncan mediate that shift. We examine a new ecological-design paradigm in which AI\ninteracts with non-human life forms. Through case studies we show how artists\nand designers apply AI for data analysis, image recognition, and ecological\nrestoration, producing results that differ from conventional media. We argue\nthat AI not only expands creative methods but also reframes the theory and\npractice of ecological design. Building on the author's prototype for\nAI-assisted water remediation, the study proposes design pathways that couple\nreinforcement learning with plant-based phytoremediation. The findings\nhighlight AI's potential to link scientific insight, artistic practice, and\nenvironmental stewardship, offering a roadmap for future research on\nsustainable, technology-enabled ecosystems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.11653", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11653", "abs": "https://arxiv.org/abs/2507.11653", "authors": ["Hannah Shafferman", "Annika Thomas", "Jouko Kinnari", "Michael Ricard", "Jose Nino", "Jonathan How"], "title": "VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization", "comment": "9 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Global localization is critical for autonomous navigation, particularly in\nscenarios where an agent must localize within a map generated in a different\nsession or by another agent, as agents often have no prior knowledge about the\ncorrelation between reference frames. However, this task remains challenging in\nunstructured environments due to appearance changes induced by viewpoint\nvariation, seasonal changes, spatial aliasing, and occlusions -- known failure\nmodes for traditional place recognition methods. To address these challenges,\nwe propose VISTA (View-Invariant Segmentation-Based Tracking for Frame\nAlignment), a novel open-set, monocular global localization framework that\ncombines: 1) a front-end, object-based, segmentation and tracking pipeline,\nfollowed by 2) a submap correspondence search, which exploits geometric\nconsistencies between environment maps to align vehicle reference frames. VISTA\nenables consistent localization across diverse camera viewpoints and seasonal\nchanges, without requiring any domain-specific training or finetuning. We\nevaluate VISTA on seasonal and oblique-angle aerial datasets, achieving up to a\n69% improvement in recall over baseline methods. Furthermore, we maintain a\ncompact object-based map that is only 0.6% the size of the most\nmemory-conservative baseline, making our approach capable of real-time\nimplementation on resource-constrained platforms.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.11992", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11992", "abs": "https://arxiv.org/abs/2507.11992", "authors": ["Pranav Rajbhandari", "Abhi Veda", "Matthew Garratt", "Mandayam Srinivasan", "Sridhar Ravi"], "title": "Understanding visual attention beehind bee-inspired UAV navigation", "comment": null, "summary": "Bio-inspired design is often used in autonomous UAV navigation due to the\ncapacity of biological systems for flight and obstacle avoidance despite\nlimited sensory and computational capabilities. In particular, honeybees mainly\nuse the sensory input of optic flow, the apparent motion of objects in their\nvisual field, to navigate cluttered environments. In our work, we train a\nReinforcement Learning agent to navigate a tunnel with obstacles using only\noptic flow as sensory input. We inspect the attention patterns of trained\nagents to determine the regions of optic flow on which they primarily base\ntheir motor decisions. We find that agents trained in this way pay most\nattention to regions of discontinuity in optic flow, as well as regions with\nlarge optic flow magnitude. The trained agents appear to navigate a cluttered\ntunnel by avoiding the obstacles that produce large optic flow, while\nmaintaining a centered position in their environment, which resembles the\nbehavior seen in flying insects. This pattern persists across independently\ntrained agents, which suggests that this could be a good strategy for\ndeveloping a simple explicit control law for physical UAVs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12110", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12110", "abs": "https://arxiv.org/abs/2507.12110", "authors": ["Ye Han", "Lijun Zhang", "Dejian Meng", "Zhuang Zhang"], "title": "Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs", "comment": "16 pages, 16 figures", "summary": "The exploration-exploitation trade-off constitutes one of the fundamental\nchallenges in reinforcement learning (RL), which is exacerbated in multi-agent\nreinforcement learning (MARL) due to the exponential growth of joint\nstate-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)\nmethod for optimizing cooperative decision-making of connected and autonomous\nvehicles (CAVs) in mixed traffic. This work presents two primary contributions:\nFirst, we construct a game topology tensor for dynamic traffic flow,\neffectively compressing high-dimensional traffic state information and decrease\nthe search space for MARL algorithms. Second, building upon the designed game\ntopology tensor and using QMIX as the backbone RL algorithm, we establish a\ntopology-enhanced MARL framework incorporating visit counts and agent mutual\ninformation. Extensive simulations across varying traffic densities and CAV\npenetration rates demonstrate the effectiveness of TPE-MARL. Evaluations\nencompassing training dynamics, exploration patterns, macroscopic traffic\nperformance metrics, and microscopic vehicle behaviors reveal that TPE-MARL\nsuccessfully balances exploration and exploitation. Consequently, it exhibits\nsuperior performance in terms of traffic efficiency, safety, decision\nsmoothness, and task completion. Furthermore, the algorithm demonstrates\ndecision-making rationality comparable to or exceeding that of human drivers in\nboth mixed-autonomy and fully autonomous traffic scenarios. Code of our work is\navailable at\n\\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.11845", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11845", "abs": "https://arxiv.org/abs/2507.11845", "authors": ["Kexuan Shi", "Zhuang Qi", "Jingjing Zhu", "Lei Meng", "Yaochen Zhang", "Haibei Huang", "Xiangxu Meng"], "title": "ProtoConNet: Prototypical Augmentation and Alignment for Open-Set Few-Shot Image Classification", "comment": "Accepted in ChinaMM and recommended to Displays", "summary": "Open-set few-shot image classification aims to train models using a small\namount of labeled data, enabling them to achieve good generalization when\nconfronted with unknown environments. Existing methods mainly use visual\ninformation from a single image to learn class representations to distinguish\nknown from unknown categories. However, these methods often overlook the\nbenefits of integrating rich contextual information. To address this issue,\nthis paper proposes a prototypical augmentation and alignment method, termed\nProtoConNet, which incorporates background information from different samples\nto enhance the diversity of the feature space, breaking the spurious\nassociations between context and image subjects in few-shot scenarios.\nSpecifically, it consists of three main modules: the clustering-based data\nselection (CDS) module mines diverse data patterns while preserving core\nfeatures; the contextual-enhanced semantic refinement (CSR) module builds a\ncontext dictionary to integrate into image representations, which boosts the\nmodel's robustness in various scenarios; and the prototypical alignment (PA)\nmodule reduces the gap between image representations and class prototypes,\namplifying feature distances for known and unknown classes. Experimental\nresults from two datasets verified that ProtoConNet enhances the effectiveness\nof representation learning in few-shot scenarios and identifies open-set\nsamples, making it superior to existing methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.11892", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11892", "abs": "https://arxiv.org/abs/2507.11892", "authors": ["Yu Liu", "Leyuan Qu", "Hanlei Shi", "Di Gao", "Yuhua Zheng", "Taihao Li"], "title": "From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition", "comment": null, "summary": "Dynamic Facial Expression Recognition (DFER) aims to identify human emotions\nfrom temporally evolving facial movements and plays a critical role in\naffective computing. While recent vision-language approaches have introduced\nsemantic textual descriptions to guide expression recognition, existing methods\nstill face two key limitations: they often underutilize the subtle emotional\ncues embedded in generated text, and they have yet to incorporate sufficiently\neffective mechanisms for filtering out facial dynamics that are irrelevant to\nemotional expression. To address these gaps, We propose GRACE, Granular\nRepresentation Alignment for Cross-modal Emotion recognition that integrates\ndynamic motion modeling, semantic text refinement, and token-level cross-modal\nalignment to facilitate the precise localization of emotionally salient\nspatiotemporal features. Our method constructs emotion-aware textual\ndescriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and\nhighlights expression-relevant facial motion through a motion-difference\nweighting mechanism. These refined semantic and visual signals are aligned at\nthe token level using entropy-regularized optimal transport. Experiments on\nthree benchmark datasets demonstrate that our method significantly improves\nrecognition performance, particularly in challenging settings with ambiguous or\nimbalanced emotion classes, establishing new state-of-the-art (SOTA) results in\nterms of both UAR and WAR.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.11875", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11875", "abs": "https://arxiv.org/abs/2507.11875", "authors": ["Tianyou Huang", "Xinglu Chen", "Jingshen Zhang", "Xinying Qiu", "Ruiying Niu"], "title": "DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation", "comment": "Accepted to CCL 2025", "summary": "This paper introduces DualReward, a novel reinforcement learning framework\nfor automatic distractor generation in cloze tests. Unlike conventional\napproaches that rely primarily on supervised learning or static generative\nmodels, our method employs a dual reward structure with adaptive scaling that\ndifferentiates between human-created gold standard distractors and\nmodel-generated candidates. The framework dynamically adjusts reward signal\nintensity based on model performance and confidence. We evaluate our approach\non both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,\ndemonstrating consistent improvements over state-of-the-art baselines.\nExperimental results show that our adaptive reward scaling mechanism provides\nmodest but consistent benefits on homogeneous datasets (CLOTH-F) and more\nsubstantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data\n(MCQ), suggesting its particular effectiveness for handling varied question\ntypes and domains. Our work offers a flexible framework that effectively\nbalances learning from reliable human examples while exploring novel,\nhigh-quality distractors for automated test generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.11955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11955", "abs": "https://arxiv.org/abs/2507.11955", "authors": ["Yuhang Zhang", "Zhengyu Zhang", "Muxin Liao", "Shishun Tian", "Wenbin Zou", "Lu Zhang", "Chen Xu"], "title": "Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation", "comment": "This paper was accepted by IEEE Transactions on Intelligent\n  Transportation Systems", "summary": "Generalizable semantic segmentation aims to perform well on unseen target\ndomains, a critical challenge due to real-world applications requiring high\ngeneralizability. Class-wise prototypes, representing class centroids, serve as\ndomain-invariant cues that benefit generalization due to their stability and\nsemantic consistency. However, this approach faces three challenges. First,\nexisting methods often adopt coarse prototypical alignment strategies, which\nmay hinder performance. Second, naive prototypes computed by averaging source\nbatch features are prone to overfitting and may be negatively affected by\nunrelated source data. Third, most methods treat all source samples equally,\nignoring the fact that different features have varying adaptation difficulties.\nTo address these limitations, we propose a novel framework for generalizable\nsemantic segmentation: Prototypical Progressive Alignment and Reweighting\n(PPAR), leveraging the strong generalization ability of the CLIP model.\nSpecifically, we define two prototypes: the Original Text Prototype (OTP) and\nVisual Text Prototype (VTP), generated via CLIP to serve as a solid base for\nalignment. We then introduce a progressive alignment strategy that aligns\nfeatures in an easy-to-difficult manner, reducing domain gaps gradually.\nFurthermore, we propose a prototypical reweighting mechanism that estimates the\nreliability of source data and adjusts its contribution, mitigating the effect\nof irrelevant or harmful features (i.e., reducing negative transfer). We also\nprovide a theoretical analysis showing the alignment between our method and\ndomain generalization theory. Extensive experiments across multiple benchmarks\ndemonstrate that PPAR achieves state-of-the-art performance, validating its\neffectiveness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "reliability"], "score": 2}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.11967", "categories": ["cs.CV", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11967", "abs": "https://arxiv.org/abs/2507.11967", "authors": ["Yuchi Ishikawa", "Shota Nakada", "Hokuto Munakata", "Kazuhiro Saito", "Tatsuya Komatsu", "Yoshimitsu Aoki"], "title": "Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos", "comment": "Interspeech 2025", "summary": "In this paper, we propose Language-Guided Contrastive Audio-Visual Masked\nAutoencoders (LG-CAV-MAE) to improve audio-visual representation learning.\nLG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual\nmasked autoencoders, enabling the model to learn across audio, visual and text\nmodalities. To train LG-CAV-MAE, we introduce an automatic method to generate\naudio-visual-text triplets from unlabeled videos. We first generate frame-level\ncaptions using an image captioning model and then apply CLAP-based filtering to\nensure strong alignment between audio and captions. This approach yields\nhigh-quality audio-visual-text triplets without requiring manual annotations.\nWe evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an\naudio-visual classification task. Our method significantly outperforms existing\napproaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks\nand a 3.2% improvement for the classification task.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.11892", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11892", "abs": "https://arxiv.org/abs/2507.11892", "authors": ["Yu Liu", "Leyuan Qu", "Hanlei Shi", "Di Gao", "Yuhua Zheng", "Taihao Li"], "title": "From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition", "comment": null, "summary": "Dynamic Facial Expression Recognition (DFER) aims to identify human emotions\nfrom temporally evolving facial movements and plays a critical role in\naffective computing. While recent vision-language approaches have introduced\nsemantic textual descriptions to guide expression recognition, existing methods\nstill face two key limitations: they often underutilize the subtle emotional\ncues embedded in generated text, and they have yet to incorporate sufficiently\neffective mechanisms for filtering out facial dynamics that are irrelevant to\nemotional expression. To address these gaps, We propose GRACE, Granular\nRepresentation Alignment for Cross-modal Emotion recognition that integrates\ndynamic motion modeling, semantic text refinement, and token-level cross-modal\nalignment to facilitate the precise localization of emotionally salient\nspatiotemporal features. Our method constructs emotion-aware textual\ndescriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and\nhighlights expression-relevant facial motion through a motion-difference\nweighting mechanism. These refined semantic and visual signals are aligned at\nthe token level using entropy-regularized optimal transport. Experiments on\nthree benchmark datasets demonstrate that our method significantly improves\nrecognition performance, particularly in challenging settings with ambiguous or\nimbalanced emotion classes, establishing new state-of-the-art (SOTA) results in\nterms of both UAR and WAR.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12260", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12260", "abs": "https://arxiv.org/abs/2507.12260", "authors": ["Yikang Liu", "Wanyang Zhang", "Yiming Wang", "Jialong Tang", "Pei Zhang", "Baosong Yang", "Fei Huang", "Rui Wang", "Hai Hu"], "title": "Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese", "comment": null, "summary": "In this paper, we propose the first quantitative measure for translationese\n-- the translationese-index (T-index) for graded and generalizable measurement\nof translationese, computed from the likelihood ratios of two contrastively\nfine-tuned language models (LMs). We use a synthesized dataset and a dataset\nwith translations in the wild to evaluate T-index's generalizability in\ncross-domain settings and its validity against human judgments. Our results\nshow that T-index is both robust and efficient. T-index scored by two 0.5B LMs\nfine-tuned on only 1-5k pairs of synthetic data can well capture translationese\nin the wild. We find that the relative differences in T-indices between\ntranslations can well predict pairwise translationese annotations obtained from\nhuman annotators; and the absolute values of T-indices correlate well with\nhuman ratings of degrees of translationese (Pearson's $r = 0.568$).\nAdditionally, the correlation between T-index and existing machine translation\n(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting\nthat T-index is not covered by these metrics and can serve as a complementary\nmetric in MT QE.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation"], "score": 2}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12029", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12029", "abs": "https://arxiv.org/abs/2507.12029", "authors": ["Xinhang Wan", "Jiyuan Liu", "Qian Qu", "Suyuan Liu", "Chuyu Zhang", "Fangdi Wang", "Xinwang Liu", "En Zhu", "Kunlun He"], "title": "Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery", "comment": null, "summary": "In this paper, we address the problem of novel class discovery (NCD), which\naims to cluster novel classes by leveraging knowledge from disjoint known\nclasses. While recent advances have made significant progress in this area,\nexisting NCD methods face two major limitations. First, they primarily focus on\nsingle-view data (e.g., images), overlooking the increasingly common multi-view\ndata, such as multi-omics datasets used in disease diagnosis. Second, their\nreliance on pseudo-labels to supervise novel class clustering often results in\nunstable performance, as pseudo-label quality is highly sensitive to factors\nsuch as data noise and feature dimensionality. To address these challenges, we\npropose a novel framework named Intra-view and Inter-view Correlation Guided\nMulti-view Novel Class Discovery (IICMVNCD), which is the first attempt to\nexplore NCD in multi-view setting so far. Specifically, at the intra-view\nlevel, leveraging the distributional similarity between known and novel\nclasses, we employ matrix factorization to decompose features into\nview-specific shared base matrices and factor matrices. The base matrices\ncapture distributional consistency among the two datasets, while the factor\nmatrices model pairwise relationships between samples. At the inter-view level,\nwe utilize view relationships among known classes to guide the clustering of\nnovel classes. This includes generating predicted labels through the weighted\nfusion of factor matrices and dynamically adjusting view weights of known\nclasses based on the supervision loss, which are then transferred to novel\nclass learning. Experimental results validate the effectiveness of our proposed\napproach.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "consistency"], "score": 2}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12295", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12295", "abs": "https://arxiv.org/abs/2507.12295", "authors": ["Feng Xiao", "Jicong Fan"], "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding", "comment": null, "summary": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12029", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12029", "abs": "https://arxiv.org/abs/2507.12029", "authors": ["Xinhang Wan", "Jiyuan Liu", "Qian Qu", "Suyuan Liu", "Chuyu Zhang", "Fangdi Wang", "Xinwang Liu", "En Zhu", "Kunlun He"], "title": "Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery", "comment": null, "summary": "In this paper, we address the problem of novel class discovery (NCD), which\naims to cluster novel classes by leveraging knowledge from disjoint known\nclasses. While recent advances have made significant progress in this area,\nexisting NCD methods face two major limitations. First, they primarily focus on\nsingle-view data (e.g., images), overlooking the increasingly common multi-view\ndata, such as multi-omics datasets used in disease diagnosis. Second, their\nreliance on pseudo-labels to supervise novel class clustering often results in\nunstable performance, as pseudo-label quality is highly sensitive to factors\nsuch as data noise and feature dimensionality. To address these challenges, we\npropose a novel framework named Intra-view and Inter-view Correlation Guided\nMulti-view Novel Class Discovery (IICMVNCD), which is the first attempt to\nexplore NCD in multi-view setting so far. Specifically, at the intra-view\nlevel, leveraging the distributional similarity between known and novel\nclasses, we employ matrix factorization to decompose features into\nview-specific shared base matrices and factor matrices. The base matrices\ncapture distributional consistency among the two datasets, while the factor\nmatrices model pairwise relationships between samples. At the inter-view level,\nwe utilize view relationships among known classes to guide the clustering of\nnovel classes. This includes generating predicted labels through the weighted\nfusion of factor matrices and dynamically adjusting view weights of known\nclasses based on the supervision loss, which are then transferred to novel\nclass learning. Experimental results validate the effectiveness of our proposed\napproach.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "consistency"], "score": 2}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12083", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12083", "abs": "https://arxiv.org/abs/2507.12083", "authors": ["Muleilan Pei", "Shaoshuai Shi", "Xuesong Chen", "Xu Liu", "Shaojie Shen"], "title": "Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics", "comment": "Accepted by ICCV 2025", "summary": "Motion forecasting for on-road traffic agents presents both a significant\nchallenge and a critical necessity for ensuring safety in autonomous driving\nsystems. In contrast to most existing data-driven approaches that directly\npredict future trajectories, we rethink this task from a planning perspective,\nadvocating a \"First Reasoning, Then Forecasting\" strategy that explicitly\nincorporates behavior intentions as spatial guidance for trajectory prediction.\nTo achieve this, we introduce an interpretable, reward-driven intention\nreasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL)\nscheme. Our method first encodes traffic agents and scene elements into a\nunified vectorized representation, then aggregates contextual features through\na query-centric paradigm. This enables the derivation of a reward distribution,\na compact yet informative representation of the target agent's behavior within\nthe given scene context via IRL. Guided by this reward heuristic, we perform\npolicy rollouts to reason about multiple plausible intentions, providing\nvaluable priors for subsequent trajectory generation. Finally, we develop a\nhierarchical DETR-like decoder integrated with bidirectional selective state\nspace models to produce accurate future trajectories along with their\nassociated probabilities. Extensive experiments on the large-scale Argoverse\nand nuScenes motion forecasting datasets demonstrate that our approach\nsignificantly enhances trajectory prediction confidence, achieving highly\ncompetitive performance relative to state-of-the-art methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12428", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12428", "abs": "https://arxiv.org/abs/2507.12428", "authors": ["Yik Siu Chan", "Zheng-Xin Yong", "Stephen H. Bach"], "title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models", "comment": null, "summary": "Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12466", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12466", "abs": "https://arxiv.org/abs/2507.12466", "authors": ["David Mizrahi", "Anders Boesen Lindbo Larsen", "Jesse Allardice", "Suzie Petryk", "Yuri Gorokhov", "Jeffrey Li", "Alex Fang", "Josh Gardner", "Tom Gunter", "Afshin Dehghan"], "title": "Language Models Improve When Pretraining Data Matches Target Tasks", "comment": "44 pages, 25 figures, 13 tables", "summary": "Every data selection method inherently has a target. In practice, these\ntargets often emerge implicitly through benchmark-driven iteration: researchers\ndevelop selection strategies, train models, measure benchmark performance, then\nrefine accordingly. This raises a natural question: what happens when we make\nthis optimization explicit? To explore this, we propose benchmark-targeted\nranking (BETR), a simple method that selects pretraining documents based on\nsimilarity to benchmark training examples. BETR embeds benchmark examples and a\nsample of pretraining documents in a shared space, scores this sample by\nsimilarity to benchmarks, then trains a lightweight classifier to predict these\nscores for the full corpus. We compare data selection methods by training over\n500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to\nthem. From this, we find that simply aligning pretraining data to evaluation\nbenchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline\n(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks\nacross all scales. BETR also generalizes well: when targeting a diverse set of\nbenchmarks disjoint from our evaluation suite, it still matches or outperforms\nbaselines. Our scaling analysis further reveals a clear trend: larger models\nrequire less aggressive filtering. Overall, our findings show that directly\nmatching pretraining data to target tasks precisely shapes model capabilities\nand highlight that optimal selection strategies must adapt to model scale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12295", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12295", "abs": "https://arxiv.org/abs/2507.12295", "authors": ["Feng Xiao", "Jicong Fan"], "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding", "comment": null, "summary": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12428", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12428", "abs": "https://arxiv.org/abs/2507.12428", "authors": ["Yik Siu Chan", "Zheng-Xin Yong", "Stephen H. Bach"], "title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models", "comment": null, "summary": "Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12382", "abs": "https://arxiv.org/abs/2507.12382", "authors": ["Kaiwen Huang", "Yi Zhou", "Huazhu Fu", "Yizhe Zhang", "Chen Gong", "Tao Zhou"], "title": "Text-driven Multiplanar Visual Interaction for Semi-supervised Medical Image Segmentation", "comment": "10 pages; 2 figures; Have been accepted by MICCAI 2025", "summary": "Semi-supervised medical image segmentation is a crucial technique for\nalleviating the high cost of data annotation. When labeled data is limited,\ntextual information can provide additional context to enhance visual semantic\nunderstanding. However, research exploring the use of textual data to enhance\nvisual semantic embeddings in 3D medical imaging tasks remains scarce. In this\npaper, we propose a novel text-driven multiplanar visual interaction framework\nfor semi-supervised medical image segmentation (termed Text-SemiSeg), which\nconsists of three main modules: Text-enhanced Multiplanar Representation (TMR),\nCategory-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation\n(DCA). Specifically, TMR facilitates text-visual interaction through planar\nmapping, thereby enhancing the category awareness of visual features. CSA\nperforms cross-modal semantic alignment between the text features with\nintroduced learnable variables and the intermediate layer of visual features.\nDCA reduces the distribution discrepancy between labeled and unlabeled data\nthrough their interaction, thus improving the model's robustness. Finally,\nexperiments on three public datasets demonstrate that our model effectively\nenhances visual features with textual information and outperforms other\nmethods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-07-17.jsonl"}
{"id": "2507.12441", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12441", "abs": "https://arxiv.org/abs/2507.12441", "authors": ["Yen-Linh Vu", "Dinh-Thang Duong", "Truong-Binh Duong", "Anh-Khoi Nguyen", "Thanh-Huy Nguyen", "Le Thien Phuc Nguyen", "Jianhua Xing", "Xingjian Li", "Tianyang Wang", "Ulas Bagci", "Min Xu"], "title": "Describe Anything Model for Visual Question Answering on Text-rich Images", "comment": "11 pages, 5 figures. Accepted to VisionDocs @ ICCV 2025", "summary": "Recent progress has been made in region-aware vision-language modeling,\nparticularly with the emergence of the Describe Anything Model (DAM). DAM is\ncapable of generating detailed descriptions of any specific image areas or\nobjects without the need for additional localized image-text alignment\nsupervision. We hypothesize that such region-level descriptive capability is\nbeneficial for the task of Visual Question Answering (VQA), especially in\nchallenging scenarios involving images with dense text. In such settings, the\nfine-grained extraction of textual information is crucial to producing correct\nanswers. Motivated by this, we introduce DAM-QA, a framework with a tailored\nevaluation protocol, developed to investigate and harness the region-aware\ncapabilities from DAM for the text-rich VQA problem that requires reasoning\nover text-based information within images. DAM-QA incorporates a mechanism that\naggregates answers from multiple regional views of image content, enabling more\neffective identification of evidence that may be tied to text-related elements.\nExperiments on six VQA benchmarks show that our approach consistently\noutperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA\nalso achieves the best overall performance among region-aware models with fewer\nparameters, significantly narrowing the gap with strong generalist VLMs. These\nresults highlight the potential of DAM-like models for text-rich and broader\nVQA tasks when paired with efficient usage and integration strategies. Our code\nis publicly available at https://github.com/Linvyl/DAM-QA.git.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "question answering", "fine-grained"], "score": 3}}, "source_file": "2025-07-17.jsonl"}
