{"id": "2507.21391", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21391", "abs": "https://arxiv.org/abs/2507.21391", "authors": ["Shijie Zhou", "Ruiyi Zhang", "Huaisheng Zhu", "Branislav Kveton", "Yufan Zhou", "Jiuxiang Gu", "Jian Chen", "Changyou Chen"], "title": "Multimodal LLMs as Customized Reward Models for Text-to-Image Generation", "comment": "Accepted at ICCV 2025. Code available at\n  https://github.com/sjz5202/LLaVA-Reward", "summary": "We introduce LLaVA-Reward, an efficient reward model designed to\nautomatically evaluate text-to-image (T2I) generations across multiple\nperspectives, leveraging pretrained multimodal large language models (MLLMs).\nExisting MLLM-based approaches require instruction-following data for\nsupervised fine-tuning and evaluate generation quality on analyzing text\nresponse, which is time-consuming and difficult to train. To address this\nproblem, we propose LLaVA-Reward, which directly utilizes the hidden states of\nMLLMs given text-image pairs. To enhance the bidirectional interaction between\nvisual and textual representations in decoder-only MLLMs, we further propose\nadding a Skip-connection Cross Attention (SkipCA) module. This design enhances\ntext-image correlation reasoning by connecting early-layer visual features with\nlater-layer hidden representations.In addition, LLaVA-Reward supports different\ntypes of preference data for efficient fine-tuning, including paired preference\ndata and unpaired data. We train LLaVA-Reward on four evaluation perspectives:\ntext-image alignment, fidelity/artifact, safety, and overall ranking. Empirical\nresults demonstrate that LLaVA-Reward outperforms conventional and MLLM-based\nmethods in generating human-aligned scores for automatic evaluations and\ninference-time scaling in text-to-image generations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "ranking", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "correlation"], "score": 3}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21391", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21391", "abs": "https://arxiv.org/abs/2507.21391", "authors": ["Shijie Zhou", "Ruiyi Zhang", "Huaisheng Zhu", "Branislav Kveton", "Yufan Zhou", "Jiuxiang Gu", "Jian Chen", "Changyou Chen"], "title": "Multimodal LLMs as Customized Reward Models for Text-to-Image Generation", "comment": "Accepted at ICCV 2025. Code available at\n  https://github.com/sjz5202/LLaVA-Reward", "summary": "We introduce LLaVA-Reward, an efficient reward model designed to\nautomatically evaluate text-to-image (T2I) generations across multiple\nperspectives, leveraging pretrained multimodal large language models (MLLMs).\nExisting MLLM-based approaches require instruction-following data for\nsupervised fine-tuning and evaluate generation quality on analyzing text\nresponse, which is time-consuming and difficult to train. To address this\nproblem, we propose LLaVA-Reward, which directly utilizes the hidden states of\nMLLMs given text-image pairs. To enhance the bidirectional interaction between\nvisual and textual representations in decoder-only MLLMs, we further propose\nadding a Skip-connection Cross Attention (SkipCA) module. This design enhances\ntext-image correlation reasoning by connecting early-layer visual features with\nlater-layer hidden representations.In addition, LLaVA-Reward supports different\ntypes of preference data for efficient fine-tuning, including paired preference\ndata and unpaired data. We train LLaVA-Reward on four evaluation perspectives:\ntext-image alignment, fidelity/artifact, safety, and overall ranking. Empirical\nresults demonstrate that LLaVA-Reward outperforms conventional and MLLM-based\nmethods in generating human-aligned scores for automatic evaluations and\ninference-time scaling in text-to-image generations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "ranking", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "correlation"], "score": 3}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21391", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21391", "abs": "https://arxiv.org/abs/2507.21391", "authors": ["Shijie Zhou", "Ruiyi Zhang", "Huaisheng Zhu", "Branislav Kveton", "Yufan Zhou", "Jiuxiang Gu", "Jian Chen", "Changyou Chen"], "title": "Multimodal LLMs as Customized Reward Models for Text-to-Image Generation", "comment": "Accepted at ICCV 2025. Code available at\n  https://github.com/sjz5202/LLaVA-Reward", "summary": "We introduce LLaVA-Reward, an efficient reward model designed to\nautomatically evaluate text-to-image (T2I) generations across multiple\nperspectives, leveraging pretrained multimodal large language models (MLLMs).\nExisting MLLM-based approaches require instruction-following data for\nsupervised fine-tuning and evaluate generation quality on analyzing text\nresponse, which is time-consuming and difficult to train. To address this\nproblem, we propose LLaVA-Reward, which directly utilizes the hidden states of\nMLLMs given text-image pairs. To enhance the bidirectional interaction between\nvisual and textual representations in decoder-only MLLMs, we further propose\nadding a Skip-connection Cross Attention (SkipCA) module. This design enhances\ntext-image correlation reasoning by connecting early-layer visual features with\nlater-layer hidden representations.In addition, LLaVA-Reward supports different\ntypes of preference data for efficient fine-tuning, including paired preference\ndata and unpaired data. We train LLaVA-Reward on four evaluation perspectives:\ntext-image alignment, fidelity/artifact, safety, and overall ranking. Empirical\nresults demonstrate that LLaVA-Reward outperforms conventional and MLLM-based\nmethods in generating human-aligned scores for automatic evaluations and\ninference-time scaling in text-to-image generations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "ranking", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "correlation"], "score": 3}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21099", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21099", "abs": "https://arxiv.org/abs/2507.21099", "authors": ["Chloe Ho", "Ishneet Sukhvinder Singh", "Diya Sharma", "Tanvi Reddy Anumandla", "Michael Lu", "Vasu Sharma", "Kevin Zhu"], "title": "Rewrite-to-Rank: Optimizing Ad Visibility via Retrieval-Aware Text Rewriting", "comment": null, "summary": "Search algorithms and user query relevance have given LLMs the ability to\nreturn relevant information, but the effect of content phrasing on ad\nvisibility remains underexplored. We investigate how LLM-based rewriting of\nadvertisements can improve their ranking in retrieval systems and inclusion in\ngenerated LLM responses, without modifying the retrieval model itself. We\nintroduce a supervised fine-tuning framework with a custom loss balancing\nsemantic relevance and content fidelity. To evaluate effectiveness, we propose\ntwo metrics: DeltaMRR@K (ranking improvement) and DeltaDIR@K (inclusion\nfrequency improvement). Our approach presents a scalable method to optimize ad\nphrasing, enhancing visibility in retrieval-based LLM workflows. Experiments\nacross both instruction-based and few-shot prompting demonstrate that PPO\ntrained models outperform both prompt engineering and supervised fine-tuning in\nmost cases, achieving up to a 2.79 DeltaDIR@5 and 0.0073 DeltaMRR@5 in\ninstruction-based prompting. These results highlight the importance of how the\nad is written before retrieval and prompt format and reinforcement learning in\neffective ad rewriting for LLM integrated retrieval systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "reinforcement learning", "ranking"], "score": 3}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21584", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21584", "abs": "https://arxiv.org/abs/2507.21584", "authors": ["Kejia Zhang", "Keda Tao", "Zhiming Luo", "Chang Liu", "Jiasheng Tang", "Huan Wang"], "title": "TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs", "comment": null, "summary": "Multimodal large language models (MLLMs) enable vision-language reasoning,\nyet often generate plausible outputs that are factually incorrect or visually\nungrounded, thereby compromising their reliability. Direct preference\noptimization (DPO) is a common strategy for correcting hallucinations by\naligning model outputs with human preferences. Existing DPO strategies\ntypically treat hallucination-related preferences as fixed targets, relying on\nstatic supervision signals during training. This approach tends to overfit to\nsuperficial linguistic cues in preference data, leading to distributional\nrigidity and spurious correlations that impair grounding in causally relevant\nvisual information. To overcome this limitation, we propose TARS, a\ntoken-adaptive preference strategy that reformulates DPO as a min-max\noptimization problem. TARS maximizes token-level distributional shifts under\nsemantic constraints to simulate alignment uncertainty, and simultaneously\nminimizes the expected preference loss under these controlled perturbations.\nThis joint objective preserves causal grounding while mitigating overfitting to\npreference patterns, thereby reducing hallucinations in multimodal reasoning.\nWe evaluate TARS on multiple hallucination benchmarks and find consistently\nstrong performance. Using only 4.8k preference samples and no expert feedback,\nTARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition\nvalue from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on\nseveral key metrics.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21503", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21503", "abs": "https://arxiv.org/abs/2507.21503", "authors": ["Yanxu Zhu", "Shitong Duan", "Xiangxu Zhang", "Jitao Sang", "Peng Zhang", "Tun Lu", "Xiao Zhou", "Jing Yao", "Xiaoyuan Yi", "Xing Xie"], "title": "MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions", "comment": null, "summary": "Recently Multimodal Large Language Models (MLLMs) have achieved considerable\nadvancements in vision-language tasks, yet produce potentially harmful or\nuntrustworthy content. Despite substantial work investigating the\ntrustworthiness of language models, MMLMs' capability to act honestly,\nespecially when faced with visually unanswerable questions, remains largely\nunderexplored. This work presents the first systematic assessment of honesty\nbehaviors across various MLLMs. We ground honesty in models' response behaviors\nto unanswerable visual questions, define four representative types of such\nquestions, and construct MoHoBench, a large-scale MMLM honest benchmark,\nconsisting of 12k+ visual question samples, whose quality is guaranteed by\nmulti-stage filtering and human verification. Using MoHoBench, we benchmarked\nthe honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our\nfindings show that: (1) most models fail to appropriately refuse to answer when\nnecessary, and (2) MMLMs' honesty is not solely a language modeling issue, but\nis deeply influenced by visual information, necessitating the development of\ndedicated methods for multimodal honesty alignment. Therefore, we implemented\ninitial alignment methods using supervised and preference learning to improve\nhonesty behavior, providing a foundation for future work on trustworthy MLLMs.\nOur data and code can be found at https://github.com/DSTTSD/MoHoBench.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "honesty"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21931", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21931", "abs": "https://arxiv.org/abs/2507.21931", "authors": ["Carel van Niekerk", "Renato Vukovic", "Benjamin Matthias Ruppik", "Hsien-chin Lin", "Milica Gašić"], "title": "Post-Training Large Language Models via Reinforcement Learning from Self-Feedback", "comment": null, "summary": "Large Language Models (LLMs) often produce plausible but poorly-calibrated\nanswers, limiting their reliability on reasoning-intensive tasks. We present\nReinforcement Learning from Self-Feedback (RLSF), a post-training stage that\nuses the model's own confidence as an intrinsic reward, mimicking how humans\nlearn in the absence of external feedback. After a frozen LLM generates several\nchain-of-thought solutions, we define and compute the confidence of each final\nanswer span and rank the traces accordingly. These synthetic preferences are\nthen used to fine-tune the policy with standard preference optimization,\nsimilar to RLHF yet requiring no human labels, gold answers, or externally\ncurated rewards.\n  RLSF simultaneously (i) refines the model's probability estimates --\nrestoring well-behaved calibration -- and (ii) strengthens step-by-step\nreasoning, yielding improved performance on arithmetic reasoning and\nmultiple-choice question answering.\n  By turning a model's own uncertainty into useful self-feedback, RLSF affirms\nreinforcement learning on intrinsic model behaviour as a principled and\ndata-efficient component of the LLM post-training pipeline and warrents further\nresearch in intrinsic rewards for LLM post-training.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning", "preference"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "question answering"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21503", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21503", "abs": "https://arxiv.org/abs/2507.21503", "authors": ["Yanxu Zhu", "Shitong Duan", "Xiangxu Zhang", "Jitao Sang", "Peng Zhang", "Tun Lu", "Xiao Zhou", "Jing Yao", "Xiaoyuan Yi", "Xing Xie"], "title": "MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions", "comment": null, "summary": "Recently Multimodal Large Language Models (MLLMs) have achieved considerable\nadvancements in vision-language tasks, yet produce potentially harmful or\nuntrustworthy content. Despite substantial work investigating the\ntrustworthiness of language models, MMLMs' capability to act honestly,\nespecially when faced with visually unanswerable questions, remains largely\nunderexplored. This work presents the first systematic assessment of honesty\nbehaviors across various MLLMs. We ground honesty in models' response behaviors\nto unanswerable visual questions, define four representative types of such\nquestions, and construct MoHoBench, a large-scale MMLM honest benchmark,\nconsisting of 12k+ visual question samples, whose quality is guaranteed by\nmulti-stage filtering and human verification. Using MoHoBench, we benchmarked\nthe honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our\nfindings show that: (1) most models fail to appropriately refuse to answer when\nnecessary, and (2) MMLMs' honesty is not solely a language modeling issue, but\nis deeply influenced by visual information, necessitating the development of\ndedicated methods for multimodal honesty alignment. Therefore, we implemented\ninitial alignment methods using supervised and preference learning to improve\nhonesty behavior, providing a foundation for future work on trustworthy MLLMs.\nOur data and code can be found at https://github.com/DSTTSD/MoHoBench.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "honesty"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21931", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21931", "abs": "https://arxiv.org/abs/2507.21931", "authors": ["Carel van Niekerk", "Renato Vukovic", "Benjamin Matthias Ruppik", "Hsien-chin Lin", "Milica Gašić"], "title": "Post-Training Large Language Models via Reinforcement Learning from Self-Feedback", "comment": null, "summary": "Large Language Models (LLMs) often produce plausible but poorly-calibrated\nanswers, limiting their reliability on reasoning-intensive tasks. We present\nReinforcement Learning from Self-Feedback (RLSF), a post-training stage that\nuses the model's own confidence as an intrinsic reward, mimicking how humans\nlearn in the absence of external feedback. After a frozen LLM generates several\nchain-of-thought solutions, we define and compute the confidence of each final\nanswer span and rank the traces accordingly. These synthetic preferences are\nthen used to fine-tune the policy with standard preference optimization,\nsimilar to RLHF yet requiring no human labels, gold answers, or externally\ncurated rewards.\n  RLSF simultaneously (i) refines the model's probability estimates --\nrestoring well-behaved calibration -- and (ii) strengthens step-by-step\nreasoning, yielding improved performance on arithmetic reasoning and\nmultiple-choice question answering.\n  By turning a model's own uncertainty into useful self-feedback, RLSF affirms\nreinforcement learning on intrinsic model behaviour as a principled and\ndata-efficient component of the LLM post-training pipeline and warrents further\nresearch in intrinsic rewards for LLM post-training.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning", "preference"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "question answering"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21069", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.21069", "abs": "https://arxiv.org/abs/2507.21069", "authors": ["Andreas Spilz", "Heiko Oppel", "Jochen Werner", "Kathrin Stucke-Straub", "Felix Capanni", "Michael Munz"], "title": "GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data", "comment": null, "summary": "Wearable inertial measurement units (IMUs) offer a cost-effective and\nscalable means to assess human movement quality in clinical and everyday\nsettings. However, the development of robust sensor-based classification models\nfor physiotherapeutic exercises and gait analysis requires large, diverse\ndatasets, which are costly and time-consuming to collect. Here, we present a\nmultimodal dataset of physiotherapeutic exercises - including correct and\nclinically relevant variants - and gait-related exercises - including both\nnormal and impaired gait patterns - recorded from 19 participants using\nsynchronized IMUs and marker-based motion capture (MoCap). The dataset includes\nraw data from nine IMUs and thirty-five optical markers capturing full-body\nkinematics. Each IMU is additionally equipped with four optical markers,\nenabling precise comparison between IMU-derived orientation estimates and\nreference values from the MoCap system. To support further analysis, we also\nprovide processed IMU orientations aligned with common segment coordinate\nsystems, subject-specific OpenSim models, inverse kinematics results, and tools\nfor visualizing IMU orientations in the musculoskeletal context. Detailed\nannotations of movement execution quality and time-stamped segmentations\nsupport diverse analysis goals. This dataset supports the development and\nbenchmarking of machine learning models for tasks such as automatic exercise\nevaluation, gait analysis, temporal activity segmentation, and biomechanical\nparameter estimation. To facilitate reproducibility, we provide code for\npostprocessing, sensor-to-segment alignment, inverse kinematics computation,\nand technical validation. This resource is intended to accelerate research in\nmachine learning-driven human movement analysis.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21131", "categories": ["cs.AI", "68T05", "H.5.1; I.2.6; C.4"], "pdf": "https://arxiv.org/pdf/2507.21131", "abs": "https://arxiv.org/abs/2507.21131", "authors": ["Madhava Gaikwad", "Ashwini Ramchandra Doke"], "title": "NPO: Learning Alignment and Meta-Alignment through Structured Human Feedback", "comment": "20 pages", "summary": "We present NPO, an alignment-aware learning framework that operationalizes\nfeedback-driven adaptation in human-in-the-loop decision systems. Unlike prior\napproaches that treat alignment as a static or post-hoc property, NPO\nintroduces a formalization of alignment loss that is measurable, supervisable,\nand reducible under structured feedback. In parallel, we propose meta-alignment\nas the fidelity of the monitoring process that governs retraining or override\ntriggers, and show that it is formally reducible to primary alignment via\nthreshold fidelity. Our implementation spans a scalable operational loop\ninvolving scenario scoring, threshold tuning, policy validation, and structured\nfeedback ingestion, including \"likes\", overrides, and abstentions. We provide\nformal convergence results under stochastic feedback and show that both\nalignment loss and monitoring fidelity converge additively. Empirically, NPO\ndemonstrates measurable value in hyperscale deployment settings. A\nsimulation-based artifact and ablation studies further illustrate the\ntheoretical principles in action. Together, NPO offers a compact, inspectable\narchitecture for continual alignment monitoring, helping bridge theoretical\nalignment guarantees with practical reliability in dynamic environments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21432", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21432", "abs": "https://arxiv.org/abs/2507.21432", "authors": ["Tareq Alsaleh", "Bilal Farooq"], "title": "Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour", "comment": null, "summary": "This study investigates the adoption of open-access, locally deployable\ncausal large language models (LLMs) for travel mode choice prediction and\nintroduces LiTransMC, the first fine-tuned causal LLM developed for this task.\nWe systematically benchmark eleven LLMs (1-12B parameters) across three stated\nand revealed preference datasets, testing 396 configurations and generating\nover 79,000 synthetic commuter predictions. Beyond predictive accuracy, we\nevaluate models generated reasoning using BERTopic for topic modelling and a\nnovel Explanation Strength Index, providing the first structured analysis of\nhow LLMs articulate decision factors in alignment with behavioural theory.\nLiTransMC, fine-tuned using parameter efficient and loss masking strategy,\nachieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of\n0.000245, surpassing both untuned local models and larger proprietary systems,\nincluding GPT-4o with advanced persona inference and embedding-based loading,\nwhile also outperforming classical mode choice methods such as discrete choice\nmodels and machine learning classifiers for the same dataset. This dual\nimprovement, i.e., high instant-level accuracy and near-perfect distributional\ncalibration, demonstrates the feasibility of creating specialist, locally\ndeployable LLMs that integrate prediction and interpretability. Through\ncombining structured behavioural prediction with natural language reasoning,\nthis work unlocks the potential for conversational, multi-task transport models\ncapable of supporting agent-based simulations, policy testing, and behavioural\ninsight generation. These findings establish a pathway for transforming general\npurpose LLMs into specialized, explainable tools for transportation research\nand policy formulation, while maintaining privacy, reducing cost, and\nbroadening access through local deployment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21645", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21645", "abs": "https://arxiv.org/abs/2507.21645", "authors": ["Meng Zhou", "Bei Li", "Jiahao Liu", "Xiaowen Shi", "Yang Bai", "Rongxiang Weng", "Jingang Wang", "Xunliang Cai"], "title": "Libra: Assessing and Improving Reward Model by Learning to Think", "comment": "Work In Progress", "summary": "Reinforcement learning (RL) has significantly improved the reasoning ability\nof large language models. However, current reward models underperform in\nchallenging reasoning scenarios and predominant RL training paradigms rely on\nrule-based or reference-based rewards, which impose two critical limitations:\n1) the dependence on finely annotated reference answer to attain rewards; and\n2) the requirement for constrained output format. These limitations\nfundamentally hinder further RL data scaling and sustained enhancement of model\nreasoning performance. To address these limitations, we propose a comprehensive\nframework for evaluating and improving the performance of reward models in\ncomplex reasoning scenarios. We first present a reasoning-oriented benchmark\n(Libra Bench), systematically constructed from a diverse collection of\nchallenging mathematical problems and advanced reasoning models, to address the\nlimitations of existing reward model benchmarks in reasoning scenarios. We\nfurther introduce a novel approach for improving the generative reward model\nvia learning-to-think methodologies. Based on the proposed approach, we develop\nLibra-RM series, a collection of generative reward models with reasoning\ncapabilities that achieve state-of-the-art results on various benchmarks.\nComprehensive downstream experiments are conducted and the experimental results\ndemonstrate the correlation between our Libra Bench and downstream application,\nand the potential of Libra-RM to further improve reasoning models with\nunlabeled data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21828", "abs": "https://arxiv.org/abs/2507.21828", "authors": ["Anna Golub", "Beate Zywietz", "Annerose Eichel"], "title": "Modelling Adjectival Modification Effects on Semantic Plausibility", "comment": "Accepted at ESSLLI 2025 Student Session", "summary": "While the task of assessing the plausibility of events such as ''news is\nrelevant'' has been addressed by a growing body of work, less attention has\nbeen paid to capturing changes in plausibility as triggered by event\nmodification. Understanding changes in plausibility is relevant for tasks such\nas dialogue generation, commonsense reasoning, and hallucination detection as\nit allows to correctly model, for example, ''gentle sarcasm'' as a sign of\ncloseness rather than unkindness among friends [9]. In this work, we tackle the\nADEPT challenge benchmark [6] consisting of 16K English sentence pairs\ndiffering by exactly one adjectival modifier. Our modeling experiments provide\na conceptually novel method by using sentence transformers, and reveal that\nboth they and transformer-based models struggle with the task at hand, and\nsentence transformers - despite their conceptual alignment with the task - even\nunder-perform in comparison to models like RoBERTa. Furthermore, an in-depth\ncomparison with prior work highlights the importance of a more realistic,\nbalanced evaluation method: imbalances distort model performance and evaluation\nmetrics, and weaken result trustworthiness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dialogue"], "score": 3}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21802", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21802", "abs": "https://arxiv.org/abs/2507.21802", "authors": ["Junzhe Li", "Yutao Cui", "Tao Huang", "Yinping Ma", "Chun Fan", "Miles Yang", "Zhao Zhong"], "title": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE", "comment": null, "summary": "Although GRPO substantially enhances flow matching models in human preference\nalignment of image generation, methods such as FlowGRPO still exhibit\ninefficiency due to the necessity of sampling and optimizing over all denoising\nsteps specified by the Markov Decision Process (MDP). In this paper, we propose\n$\\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed\nsampling strategies through the integration of stochastic differential\nequations (SDE) and ordinary differential equations (ODE). This streamlines the\noptimization process within the MDP to improve efficiency and boost\nperformance. Specifically, MixGRPO introduces a sliding window mechanism, using\nSDE sampling and GRPO-guided optimization only within the window, while\napplying ODE sampling outside. This design confines sampling randomness to the\ntime-steps within the window, thereby reducing the optimization overhead, and\nallowing for more focused gradient updates to accelerate convergence.\nAdditionally, as time-steps beyond the sliding window are not involved in\noptimization, higher-order solvers are supported for sampling. So we present a\nfaster variant, termed $\\textbf{MixGRPO-Flash}$, which further improves\ntraining efficiency while achieving comparable performance. MixGRPO exhibits\nsubstantial gains across multiple dimensions of human preference alignment,\noutperforming DanceGRPO in both effectiveness and efficiency, with nearly 50%\nlower training time. Notably, MixGRPO-Flash further reduces training time by\n71%. Codes and models are available at\n$\\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21848", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21848", "abs": "https://arxiv.org/abs/2507.21848", "authors": ["Xingjian Zhang", "Siwei Wen", "Wenjun Wu", "Lei Huang"], "title": "EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity", "comment": null, "summary": "Large Language Models (LLMs) have made remarkable progress in enhancing\nstep-by-step reasoning through reinforcement learning. However, the Group\nRelative Policy Optimization (GRPO) algorithm, which relies on sparse reward\nrules, often encounters the issue of identical rewards within groups, leading\nto the advantage collapse problem. Existing works typically address this\nchallenge from two perspectives: enforcing model reflection to enhance response\ndiversity, and introducing internal feedback to augment the training signal\n(advantage). In this work, we begin by analyzing the limitations of model\nreflection and investigating the policy entropy of responses at the\nfine-grained sample level. Based on our experimental findings, we propose the\nEDGE-GRPO algorithm, which adopts \\textbf{E}ntropy-\\textbf{D}riven Advantage\nand \\textbf{G}uided \\textbf{E}rror Correction to effectively mitigate the\nproblem of advantage collapse. Extensive experiments on several main reasoning\nbenchmarks demonstrate the effectiveness and superiority of our approach. It is\navailable at https://github.com/ZhangXJ199/EDGE-GRPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.22025", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22025", "abs": "https://arxiv.org/abs/2507.22025", "authors": ["Shuquan Lian", "Yuhang Wu", "Jia Ma", "Zihan Song", "Bingqi Chen", "Xiawu Zheng", "Hui Li"], "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding", "comment": null, "summary": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.22034", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22034", "abs": "https://arxiv.org/abs/2507.22034", "authors": ["Cheng Qian", "Zuxin Liu", "Akshara Prabhakar", "Zhiwei Liu", "Jianguo Zhang", "Haolin Chen", "Heng Ji", "Weiran Yao", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "UserBench: An Interactive Gym Environment for User-Centric Agents", "comment": "25 Pages, 17 Figures, 6 Tables", "summary": "Large Language Models (LLMs)-based agents have made impressive progress in\nreasoning and tool use, enabling them to solve complex tasks. However, their\nability to proactively collaborate with users, especially when goals are vague,\nevolving, or indirectly expressed, remains underexplored. To address this gap,\nwe introduce UserBench, a user-centric benchmark designed to evaluate agents in\nmulti-turn, preference-driven interactions. UserBench features simulated users\nwho start with underspecified goals and reveal preferences incrementally,\nrequiring agents to proactively clarify intent and make grounded decisions with\ntools. Our evaluation of leading open- and closed-source LLMs reveals a\nsignificant disconnect between task completion and user alignment. For\ninstance, models provide answers that fully align with all user intents only\n20% of the time on average, and even the most advanced models uncover fewer\nthan 30% of all user preferences through active interaction. These results\nhighlight the challenges of building agents that are not just capable task\nexecutors, but true collaborative partners. UserBench offers an interactive\nenvironment to measure and advance this critical capability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.22025", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22025", "abs": "https://arxiv.org/abs/2507.22025", "authors": ["Shuquan Lian", "Yuhang Wu", "Jia Ma", "Zihan Song", "Bingqi Chen", "Xiawu Zheng", "Hui Li"], "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding", "comment": null, "summary": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.22034", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22034", "abs": "https://arxiv.org/abs/2507.22034", "authors": ["Cheng Qian", "Zuxin Liu", "Akshara Prabhakar", "Zhiwei Liu", "Jianguo Zhang", "Haolin Chen", "Heng Ji", "Weiran Yao", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "UserBench: An Interactive Gym Environment for User-Centric Agents", "comment": "25 Pages, 17 Figures, 6 Tables", "summary": "Large Language Models (LLMs)-based agents have made impressive progress in\nreasoning and tool use, enabling them to solve complex tasks. However, their\nability to proactively collaborate with users, especially when goals are vague,\nevolving, or indirectly expressed, remains underexplored. To address this gap,\nwe introduce UserBench, a user-centric benchmark designed to evaluate agents in\nmulti-turn, preference-driven interactions. UserBench features simulated users\nwho start with underspecified goals and reveal preferences incrementally,\nrequiring agents to proactively clarify intent and make grounded decisions with\ntools. Our evaluation of leading open- and closed-source LLMs reveals a\nsignificant disconnect between task completion and user alignment. For\ninstance, models provide answers that fully align with all user intents only\n20% of the time on average, and even the most advanced models uncover fewer\nthan 30% of all user preferences through active interaction. These results\nhighlight the challenges of building agents that are not just capable task\nexecutors, but true collaborative partners. UserBench offers an interactive\nenvironment to measure and advance this critical capability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21069", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.21069", "abs": "https://arxiv.org/abs/2507.21069", "authors": ["Andreas Spilz", "Heiko Oppel", "Jochen Werner", "Kathrin Stucke-Straub", "Felix Capanni", "Michael Munz"], "title": "GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data", "comment": null, "summary": "Wearable inertial measurement units (IMUs) offer a cost-effective and\nscalable means to assess human movement quality in clinical and everyday\nsettings. However, the development of robust sensor-based classification models\nfor physiotherapeutic exercises and gait analysis requires large, diverse\ndatasets, which are costly and time-consuming to collect. Here, we present a\nmultimodal dataset of physiotherapeutic exercises - including correct and\nclinically relevant variants - and gait-related exercises - including both\nnormal and impaired gait patterns - recorded from 19 participants using\nsynchronized IMUs and marker-based motion capture (MoCap). The dataset includes\nraw data from nine IMUs and thirty-five optical markers capturing full-body\nkinematics. Each IMU is additionally equipped with four optical markers,\nenabling precise comparison between IMU-derived orientation estimates and\nreference values from the MoCap system. To support further analysis, we also\nprovide processed IMU orientations aligned with common segment coordinate\nsystems, subject-specific OpenSim models, inverse kinematics results, and tools\nfor visualizing IMU orientations in the musculoskeletal context. Detailed\nannotations of movement execution quality and time-stamped segmentations\nsupport diverse analysis goals. This dataset supports the development and\nbenchmarking of machine learning models for tasks such as automatic exercise\nevaluation, gait analysis, temporal activity segmentation, and biomechanical\nparameter estimation. To facilitate reproducibility, we provide code for\npostprocessing, sensor-to-segment alignment, inverse kinematics computation,\nand technical validation. This resource is intended to accelerate research in\nmachine learning-driven human movement analysis.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21432", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21432", "abs": "https://arxiv.org/abs/2507.21432", "authors": ["Tareq Alsaleh", "Bilal Farooq"], "title": "Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour", "comment": null, "summary": "This study investigates the adoption of open-access, locally deployable\ncausal large language models (LLMs) for travel mode choice prediction and\nintroduces LiTransMC, the first fine-tuned causal LLM developed for this task.\nWe systematically benchmark eleven LLMs (1-12B parameters) across three stated\nand revealed preference datasets, testing 396 configurations and generating\nover 79,000 synthetic commuter predictions. Beyond predictive accuracy, we\nevaluate models generated reasoning using BERTopic for topic modelling and a\nnovel Explanation Strength Index, providing the first structured analysis of\nhow LLMs articulate decision factors in alignment with behavioural theory.\nLiTransMC, fine-tuned using parameter efficient and loss masking strategy,\nachieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of\n0.000245, surpassing both untuned local models and larger proprietary systems,\nincluding GPT-4o with advanced persona inference and embedding-based loading,\nwhile also outperforming classical mode choice methods such as discrete choice\nmodels and machine learning classifiers for the same dataset. This dual\nimprovement, i.e., high instant-level accuracy and near-perfect distributional\ncalibration, demonstrates the feasibility of creating specialist, locally\ndeployable LLMs that integrate prediction and interpretability. Through\ncombining structured behavioural prediction with natural language reasoning,\nthis work unlocks the potential for conversational, multi-task transport models\ncapable of supporting agent-based simulations, policy testing, and behavioural\ninsight generation. These findings establish a pathway for transforming general\npurpose LLMs into specialized, explainable tools for transportation research\nand policy formulation, while maintaining privacy, reducing cost, and\nbroadening access through local deployment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21802", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21802", "abs": "https://arxiv.org/abs/2507.21802", "authors": ["Junzhe Li", "Yutao Cui", "Tao Huang", "Yinping Ma", "Chun Fan", "Miles Yang", "Zhao Zhong"], "title": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE", "comment": null, "summary": "Although GRPO substantially enhances flow matching models in human preference\nalignment of image generation, methods such as FlowGRPO still exhibit\ninefficiency due to the necessity of sampling and optimizing over all denoising\nsteps specified by the Markov Decision Process (MDP). In this paper, we propose\n$\\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed\nsampling strategies through the integration of stochastic differential\nequations (SDE) and ordinary differential equations (ODE). This streamlines the\noptimization process within the MDP to improve efficiency and boost\nperformance. Specifically, MixGRPO introduces a sliding window mechanism, using\nSDE sampling and GRPO-guided optimization only within the window, while\napplying ODE sampling outside. This design confines sampling randomness to the\ntime-steps within the window, thereby reducing the optimization overhead, and\nallowing for more focused gradient updates to accelerate convergence.\nAdditionally, as time-steps beyond the sliding window are not involved in\noptimization, higher-order solvers are supported for sampling. So we present a\nfaster variant, termed $\\textbf{MixGRPO-Flash}$, which further improves\ntraining efficiency while achieving comparable performance. MixGRPO exhibits\nsubstantial gains across multiple dimensions of human preference alignment,\noutperforming DanceGRPO in both effectiveness and efficiency, with nearly 50%\nlower training time. Notably, MixGRPO-Flash further reduces training time by\n71%. Codes and models are available at\n$\\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.22025", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22025", "abs": "https://arxiv.org/abs/2507.22025", "authors": ["Shuquan Lian", "Yuhang Wu", "Jia Ma", "Zihan Song", "Bingqi Chen", "Xiawu Zheng", "Hui Li"], "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding", "comment": null, "summary": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21083", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21083", "abs": "https://arxiv.org/abs/2507.21083", "authors": ["Franck Bardol"], "title": "ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs", "comment": null, "summary": "Large Language Models like GPT-4 adjust their responses not only based on the\nquestion asked, but also on how it is emotionally phrased. We systematically\nvary the emotional tone of 156 prompts - spanning controversial and everyday\ntopics - and analyze how it affects model responses. Our findings show that\nGPT-4 is three times less likely to respond negatively to a negatively framed\nquestion than to a neutral one. This suggests a \"rebound\" bias where the model\novercorrects, often shifting toward neutrality or positivity. On sensitive\ntopics (e.g., justice or politics), this effect is even more pronounced:\ntone-based variation is suppressed, suggesting an alignment override. We\nintroduce concepts like the \"tone floor\" - a lower bound in response negativity\n- and use tone-valence transition matrices to quantify behavior. Visualizations\nbased on 1536-dimensional embeddings confirm semantic drift based on tone. Our\nwork highlights an underexplored class of biases driven by emotional framing in\nprompts, with implications for AI alignment and trust. Code and data are\navailable at: https://github.com/bardolfranck/llm-responses-viewer", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21132", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21132", "abs": "https://arxiv.org/abs/2507.21132", "authors": ["Joshua Adrian Cahyono", "Saran Subramanian"], "title": "Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses", "comment": null, "summary": "Large Language Models (LLMs) are increasingly consulted for high-stakes life\nadvice, yet they lack standard safeguards against providing confident but\nmisguided responses. This creates risks of sycophancy and over-confidence. This\npaper investigates these failure modes through three experiments: (1) a\nmultiple-choice evaluation to measure model stability against user pressure;\n(2) a free-response analysis using a novel safety typology and an LLM Judge;\nand (3) a mechanistic interpretability experiment to steer model behavior by\nmanipulating a \"high-stakes\" activation vector. Our results show that while\nsome models exhibit sycophancy, others like o4-mini remain robust.\nTop-performing models achieve high safety scores by frequently asking\nclarifying questions, a key feature of a safe, inquisitive approach, rather\nthan issuing prescriptive advice. Furthermore, we demonstrate that a model's\ncautiousness can be directly controlled via activation steering, suggesting a\nnew path for safety alignment. These findings underscore the need for nuanced,\nmulti-faceted benchmarks to ensure LLMs can be trusted with life-changing\ndecisions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21159", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.21159", "abs": "https://arxiv.org/abs/2507.21159", "authors": ["Zhihao Peng", "Liuxin Bao", "Shengyuan Liu", "Yixuan Yuan"], "title": "Adaptive Cluster Collaborativeness Boosts LLMs Medical Decision Support Capacity", "comment": null, "summary": "The collaborativeness of large language models (LLMs) has proven effective in\nnatural language processing systems, holding considerable promise for\nhealthcare development. However, it lacks explicit component selection rules,\nnecessitating human intervention or clinical-specific validation. Moreover,\nexisting architectures heavily rely on a predefined LLM cluster, where partial\nLLMs underperform in medical decision support scenarios, invalidating the\ncollaborativeness of LLMs. To this end, we propose an adaptive cluster\ncollaborativeness methodology involving self-diversity and cross-consistency\nmaximization mechanisms to boost LLMs medical decision support capacity. For\nthe self-diversity, we calculate the fuzzy matching value of pairwise outputs\nwithin an LLM as its self-diversity value, subsequently prioritizing LLMs with\nhigh self-diversity values as cluster components in a training-free manner. For\nthe cross-consistency, we first measure cross-consistency values between the\nLLM with the highest self-diversity value and others, and then gradually mask\nout the LLM having the lowest cross-consistency value to eliminate the\npotential inconsistent output during the collaborative propagation. Extensive\nexperiments on two specialized medical datasets, NEJMQA and MMLU-Pro-health,\ndemonstrate the effectiveness of our method across physician-oriented\nspecialties. For example, on NEJMQA, our method achieves the accuracy rate up\nto the publicly official passing score across all disciplines, especially\nachieving ACC of 65.47\\% compared to the 56.12\\% achieved by GPT-4 on the\nObstetrics and Gynecology discipline.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21107", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21107", "abs": "https://arxiv.org/abs/2507.21107", "authors": ["Rob Manson"], "title": "Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams", "comment": "29 pages, 22 figures", "summary": "We propose Curved Inference - a geometric Interpretability framework that\ntracks how the residual stream trajectory of a large language model bends in\nresponse to shifts in semantic concern. Across 20 matched prompts spanning\nemotional, moral, perspective, logical, identity, environmental, and nonsense\ndomains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics,\nwith a primary focus on curvature (\\k{appa}_i) and salience (S(t)). These\nmetrics are computed under a pullback semantic metric derived from the\nunembedding matrix, ensuring that all measurements reflect token-aligned\ngeometry rather than raw coordinate structure. We find that concern-shifted\nprompts reliably alter internal activation trajectories in both models - with\nLLaMA exhibiting consistent, statistically significant scaling in both\ncurvature and salience as concern intensity increases. Gemma also responds to\nconcern but shows weaker differentiation between moderate and strong variants.\nOur results support a two-layer view of LLM geometry - a latent conceptual\nstructure encoded in the embedding space, and a contextual trajectory shaped by\nprompt-specific inference. Curved Inference reveals how models navigate,\nreorient, or reinforce semantic meaning over depth, offering a principled\nmethod for diagnosing alignment, abstraction, and emergent inference dynamics.\nThese findings offer fresh insight into semantic abstraction and model\nalignment through the lens of Curved Inference.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21367", "abs": "https://arxiv.org/abs/2507.21367", "authors": ["I-Hsiang Chen", "Hua-En Chang", "Wei-Ting Chen", "Jenq-Neng Hwang", "Sy-Yen Kuo"], "title": "Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation", "comment": "Accepted by ICCV2025", "summary": "Domain Generalized Semantic Segmentation (DGSS) is a critical yet challenging\ntask, as domain shifts in unseen environments can severely compromise model\nperformance. While recent studies enhance feature alignment by projecting\nfeatures into the source domain, they often neglect intrinsic latent domain\npriors, leading to suboptimal results. In this paper, we introduce PDAF, a\nProbabilistic Diffusion Alignment Framework that enhances the generalization of\nexisting segmentation networks through probabilistic diffusion modeling. PDAF\nintroduces a Latent Domain Prior (LDP) to capture domain shifts and uses this\nprior as a conditioning factor to align both source and unseen target domains.\nTo achieve this, PDAF integrates into a pre-trained segmentation model and\nutilizes paired source and pseudo-target images to simulate latent domain\nshifts, enabling LDP modeling. The framework comprises three modules: the\nLatent Prior Extractor (LPE) predicts the LDP by supervising domain shifts; the\nDomain Compensation Module (DCM) adjusts feature representations to mitigate\ndomain shifts; and the Diffusion Prior Estimator (DPE) leverages a diffusion\nprocess to estimate the LDP without requiring paired samples. This design\nenables PDAF to iteratively model domain shifts, progressively refining feature\nrepresentations to enhance generalization under complex target conditions.\nExtensive experiments validate the effectiveness of PDAF across diverse and\nchallenging urban scenes.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21138", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.21138", "abs": "https://arxiv.org/abs/2507.21138", "authors": ["Oleg Atamanenko", "Anna Chalova", "Joseph Coombes", "Nikki Cope", "Phillip Dang", "Zhifeng Deng", "Jimmy Du", "Michael Ermolenko", "Feifan Fan", "Yufei Feng", "Cheryl Fichter", "Pavel Filimonov", "Louis Fischer", "Kylan Gibbs", "Valeria Gusarova", "Pavel Karpik", "Andreas Assad Kottner", "Ian Lee", "Oliver Louie", "Jasmine Mai", "Mikhail Mamontov", "Suri Mao", "Nurullah Morshed", "Igor Poletaev", "Florin Radu", "Dmytro Semernia", "Evgenii Shingarev", "Vikram Sivaraja", "Peter Skirko", "Rinat Takhautdinov", "Robert Villahermosa", "Jean Wang"], "title": "TTS-1 Technical Report", "comment": "20 pages, 10 figures. For associated modeling and training code, see\n  https://github.com/inworld-ai/tts", "summary": "We introduce Inworld TTS-1, a set of two Transformer-based autoregressive\ntext-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters\nand is designed for utmost quality and expressiveness in demanding\napplications. TTS-1 is our most efficient model, with 1.6B parameters, built\nfor real-time speech synthesis and on-device use cases. By scaling train-time\ncompute and applying a sequential process of pre-training, fine-tuning, and\nRL-alignment of the speech-language model (SpeechLM) component, both models\nachieve state-of-the-art performance on a variety of benchmarks, demonstrating\nexceptional quality relying purely on in-context learning of the speaker's\nvoice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech\nwith low latency, and support 11 languages with fine-grained emotional control\nand non-verbal vocalizations through audio markups. We additionally open-source\nour training and modeling code under an MIT license.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21450", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21450", "abs": "https://arxiv.org/abs/2507.21450", "authors": ["Bolei Chen", "Jiaxu Kang", "Yifei Wang", "Ping Zhong", "Qi Wu", "Jianxin Wang"], "title": "Recursive Visual Imagination and Adaptive Linguistic Grounding for Vision Language Navigation", "comment": "Submitted to AAAI 2026", "summary": "Vision Language Navigation (VLN) typically requires agents to navigate to\nspecified objects or remote regions in unknown scenes by obeying linguistic\ncommands. Such tasks require organizing historical visual observations for\nlinguistic grounding, which is critical for long-sequence navigational\ndecisions. However, current agents suffer from overly detailed scene\nrepresentation and ambiguous vision-language alignment, which weaken their\ncomprehension of navigation-friendly high-level scene priors and easily lead to\nbehaviors that violate linguistic commands. To tackle these issues, we propose\na navigation policy by recursively summarizing along-the-way visual\nperceptions, which are adaptively aligned with commands to enhance linguistic\ngrounding. In particular, by structurally modeling historical trajectories as\ncompact neural grids, several Recursive Visual Imagination (RVI) techniques are\nproposed to motivate agents to focus on the regularity of visual transitions\nand semantic scene layouts, instead of dealing with misleading geometric\ndetails. Then, an Adaptive Linguistic Grounding (ALG) technique is proposed to\nalign the learned situational memories with different linguistic components\npurposefully. Such fine-grained semantic matching facilitates the accurate\nanticipation of navigation actions and progress. Our navigation policy\noutperforms the state-of-the-art methods on the challenging VLN-CE and\nObjectNav tasks, showing the superiority of our RVI and ALG techniques for VLN.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21319", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21319", "abs": "https://arxiv.org/abs/2507.21319", "authors": ["Hadi Mohammadi", "Yasmeen F. S. S. Meijer", "Efthymia Papadopoulou", "Ayoub Bagheri"], "title": "Do Large Language Models Understand Morality Across Cultures?", "comment": null, "summary": "Recent advancements in large language models (LLMs) have established them as\npowerful tools across numerous domains. However, persistent concerns about\nembedded biases, such as gender, racial, and cultural biases arising from their\ntraining data, raise significant questions about the ethical use and societal\nconsequences of these technologies. This study investigates the extent to which\nLLMs capture cross-cultural differences and similarities in moral perspectives.\nSpecifically, we examine whether LLM outputs align with patterns observed in\ninternational survey data on moral attitudes. To this end, we employ three\ncomplementary methods: (1) comparing variances in moral scores produced by\nmodels versus those reported in surveys, (2) conducting cluster alignment\nanalyses to assess correspondence between country groupings derived from LLM\noutputs and survey data, and (3) directly probing models with comparative\nprompts using systematically chosen token pairs. Our results reveal that\ncurrent LLMs often fail to reproduce the full spectrum of cross-cultural moral\nvariation, tending to compress differences and exhibit low alignment with\nempirical survey patterns. These findings highlight a pressing need for more\nrobust approaches to mitigate biases and improve cultural representativeness in\nLLMs. We conclude by discussing the implications for the responsible\ndevelopment and global deployment of LLMs, emphasizing fairness and ethical\nalignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21383", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21383", "abs": "https://arxiv.org/abs/2507.21383", "authors": ["Chunan Tong"], "title": "Optimizing Multi-Tier Supply Chain Ordering with LNN+XGBoost: Mitigating the Bullwhip Effect", "comment": null, "summary": "Supply chain management faces significant challenges, including demand\nfluctuations, inventory imbalances, and amplified upstream order variability\ndue to the bullwhip effect. Traditional methods, such as simple moving\naverages, struggle to address dynamic market conditions. Emerging machine\nlearning techniques, including LSTM, reinforcement learning, and XGBoost, offer\npotential solutions but are limited by computational complexity, training\ninefficiencies, or constraints in time-series modeling. Liquid Neural Networks,\ninspired by dynamic biological systems, present a promising alternative due to\ntheir adaptability, low computational cost, and robustness to noise, making\nthem suitable for real-time decision-making and edge computing. Despite their\nsuccess in applications like autonomous vehicles and medical monitoring, their\npotential in supply chain optimization remains underexplored. This study\nintroduces a hybrid LNN and XGBoost model to optimize ordering strategies in\nmulti-tier supply chains. By leveraging LNN's dynamic feature extraction and\nXGBoost's global optimization capabilities, the model aims to mitigate the\nbullwhip effect and enhance cumulative profitability. The research investigates\nhow local and global synergies within the hybrid framework address the dual\ndemands of adaptability and efficiency in SCM. The proposed approach fills a\ncritical gap in existing methodologies, offering an innovative solution for\ndynamic and efficient supply chain management.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21340", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.21340", "abs": "https://arxiv.org/abs/2507.21340", "authors": ["Satyananda Kashyap", "Sola Shirai", "Nandana Mihindukulasooriya", "Horst Samulowitz"], "title": "StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation", "comment": "Data available:\n  https://huggingface.co/datasets/ibm-research/struct-text and code available\n  at: https://github.com/ibm/struct-text", "summary": "Extracting structured information from text, such as key-value pairs that\ncould augment tabular data, is quite useful in many enterprise use cases.\nAlthough large language models (LLMs) have enabled numerous automated pipelines\nfor converting natural language into structured formats, there is still a lack\nof benchmarks for evaluating their extraction quality, especially in specific\ndomains or focused documents specific to a given organization. Building such\nbenchmarks by manual annotations is labour-intensive and limits the size and\nscalability of the benchmarks. In this work, we present StructText, an\nend-to-end framework for automatically generating high-fidelity benchmarks for\nkey-value extraction from text using existing tabular data. It uses available\ntabular data as structured ground truth, and follows a two-stage\n``plan-then-execute'' pipeline to synthetically generate corresponding\nnatural-language text. To ensure alignment between text and structured source,\nwe introduce a multi-dimensional evaluation strategy that combines (a)\nLLM-based judgments on factuality, hallucination, and coherence and (b)\nobjective extraction metrics measuring numeric and temporal accuracy. We\nevaluated the proposed method on 71,539 examples across 49 datasets. Results\nreveal that while LLMs achieve strong factual accuracy and avoid hallucination,\nthey struggle with narrative coherence in producing extractable text. Notably,\nmodels presume numerical and temporal information with high fidelity yet this\ninformation becomes embedded in narratives that resist automated extraction. We\nrelease a framework, including datasets, evaluation tools, and baseline\nextraction systems, to support continued research.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "factuality", "accuracy", "multi-dimensional"], "score": 5}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21406", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21406", "abs": "https://arxiv.org/abs/2507.21406", "authors": ["Meilin Zhu", "Gaojie Jin", "Xiaowei Huang", "Lijun Zhang"], "title": "Shapley Uncertainty in Natural Language Generation", "comment": null, "summary": "In question-answering tasks, determining when to trust the outputs is crucial\nto the alignment of large language models (LLMs). Kuhn et al. (2023) introduces\nsemantic entropy as a measure of uncertainty, by incorporating linguistic\ninvariances from the same meaning. It primarily relies on setting threshold to\nmeasure the level of semantic equivalence relation. We propose a more nuanced\nframework that extends beyond such thresholding by developing a Shapley-based\nuncertainty metric that captures the continuous nature of semantic\nrelationships. We establish three fundamental properties that characterize\nvalid uncertainty metrics and prove that our Shapley uncertainty satisfies\nthese criteria. Through extensive experiments, we demonstrate that our Shapley\nuncertainty more accurately predicts LLM performance in question-answering and\nother datasets, compared to similar baseline measures.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21488", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21488", "abs": "https://arxiv.org/abs/2507.21488", "authors": ["Zhenwei Tang", "Difan Jiao", "Eric Xue", "Reid McIlroy-Young", "Jon Kleinberg", "Siddhartha Sen", "Ashton Anderson"], "title": "Learning to Imitate with Less: Efficient Individual Behavior Modeling in Chess", "comment": null, "summary": "As humans seek to collaborate with, learn from, and better understand\nartificial intelligence systems, developing AIs that can accurately emulate\nindividual decision-making becomes increasingly important. Chess, a\nlong-standing AI benchmark with precise skill measurement, offers an ideal\ntestbed for human-AI alignment. However, existing approaches to modeling human\nbehavior require prohibitively large amounts of data from each individual,\nmaking them impractical for new or sparsely represented users. In this work, we\nintroduce Maia4All, a framework designed to learn and adapt to individual\ndecision-making styles efficiently, even with limited data. Maia4All achieves\nthis through a two-stage optimization process: (1) an enrichment step, which\nbridges population and individual-level human behavior modeling with a\nprototype-enriched model, and (2) a democratization step, which leverages\nability levels or user prototypes to initialize and refine individual\nembeddings with minimal data. Our experimental results show that Maia4All can\naccurately predict individual moves and profile behavioral patterns with high\nfidelity, establishing a new standard for personalized human-like AI behavior\nmodeling in chess. Maia4All achieves individual human behavior modeling in\nchess with only 20 games, compared to the 5,000 games required previously,\nrepresenting a significant improvement in data efficiency. Our work provides an\nexample of how population AI systems can flexibly adapt to individual users\nusing a prototype-enriched model as a bridge. This approach extends beyond\nchess, as shown in our case study on idiosyncratic LLMs, highlighting its\npotential for broader applications in personalized AI adaptation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "testbed"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21619", "abs": "https://arxiv.org/abs/2507.21619", "authors": ["Wei Guan", "Jun Lan", "Jian Cao", "Hao Tan", "Huijia Zhu", "Weiqiang Wang"], "title": "EMIT: Enhancing MLLMs for Industrial Anomaly Detection via Difficulty-Aware GRPO", "comment": null, "summary": "Industrial anomaly detection (IAD) plays a crucial role in maintaining the\nsafety and reliability of manufacturing systems. While multimodal large\nlanguage models (MLLMs) show strong vision-language reasoning abilities, their\neffectiveness in IAD remains limited without domain-specific adaptation. In\nthis work, we propose EMIT, a unified framework that enhances MLLMs for IAD via\ndifficulty-aware group relative policy optimization (GRPO). EMIT constructs a\nmulti-task IAD dataset and utilizes GPT-generated object text descriptions to\ncompensate for missing defective images. For few-shot anomaly detection, it\nintegrates a soft prompt and heatmap-guided contrastive embeddings derived from\npatch-level comparisons. To better handle difficult data samples, i.e., cases\nwhere the MLLM struggles to generate correct answers, we propose a\ndifficulty-aware GRPO that extends the original GRPO by incorporating a\nresponse resampling strategy to ensure the inclusion of correct answers in the\nsampled responses, as well as an advantage reweighting mechanism to strengthen\nlearning from such difficult data samples. Extensive experiments on the MMAD\nbenchmark demonstrate that EMIT significantly enhances the IAD performance of\nMLLMs, achieving an average improvement of 7.77\\% over the base model\n(InternVL3-8B) across seven tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety", "reliability"], "score": 4}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21609", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21609", "abs": "https://arxiv.org/abs/2507.21609", "authors": ["Jens-Joris Decorte", "Matthias De Lange", "Jeroen Van Hautte"], "title": "Multilingual JobBERT for Cross-Lingual Job Title Matching", "comment": "Accepted to the TalentCLEF 2025 Workshop as part of CLEF 2025", "summary": "We introduce JobBERT-V3, a contrastive learning-based model for cross-lingual\njob title matching. Building on the state-of-the-art monolingual JobBERT-V2,\nour approach extends support to English, German, Spanish, and Chinese by\nleveraging synthetic translations and a balanced multilingual dataset of over\n21 million job titles. The model retains the efficiency-focused architecture of\nits predecessor while enabling robust alignment across languages without\nrequiring task-specific supervision. Extensive evaluations on the TalentCLEF\n2025 benchmark demonstrate that JobBERT-V3 outperforms strong multilingual\nbaselines and achieves consistent performance across both monolingual and\ncross-lingual settings. While not the primary focus, we also show that the\nmodel can be effectively used to rank relevant skills for a given job title,\ndemonstrating its broader applicability in multilingual labor market\nintelligence. The model is publicly available:\nhttps://huggingface.co/TechWolf/JobBERT-v3.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21652", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21652", "abs": "https://arxiv.org/abs/2507.21652", "authors": ["Raj Vardhan Tomar", "Preslav Nakov", "Yuxia Wang"], "title": "UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases", "comment": null, "summary": "As large reasoning models (LRMs) grow more capable, chain-of-thought (CoT)\nreasoning introduces new safety challenges. Existing SFT-based safety alignment\nstudies dominantly focused on filtering prompts with safe, high-quality\nresponses, while overlooking hard prompts that always elicit harmful outputs.\nTo fill this gap, we introduce UnsafeChain, a safety alignment dataset\nconstructed from hard prompts with diverse sources, where unsafe completions\nare identified and explicitly corrected into safe responses. By exposing models\nto unsafe behaviors and guiding their correction, UnsafeChain enhances safety\nwhile preserving general reasoning ability. We fine-tune three LRMs on\nUnsafeChain and compare them against recent SafeChain and STAR-1 across six\nout-of-distribution and five in-distribution benchmarks. UnsafeChain\nconsistently outperforms prior datasets, with even a 1K subset matching or\nsurpassing baseline performance, demonstrating the effectiveness and\ngeneralizability of correction-based supervision. We release our dataset and\ncode at https://github.com/mbzuai-nlp/UnsafeChain", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21636", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21636", "abs": "https://arxiv.org/abs/2507.21636", "authors": ["Alessio Maritan"], "title": "StaffPro: an LLM Agent for Joint Staffing and Profiling", "comment": null, "summary": "Large language model (LLM) agents integrate pre-trained LLMs with modular\nalgorithmic components and have shown remarkable reasoning and decision-making\nabilities. In this work, we investigate their use for two tightly intertwined\nchallenges in workforce management: staffing, i.e., the assignment and\nscheduling of tasks to workers, which may require team formation; and\nprofiling, i.e., the continuous estimation of workers' skills, preferences, and\nother latent attributes from unstructured data. We cast these problems in a\nformal mathematical framework that links scheduling decisions to latent feature\nestimation, and we introduce StaffPro, an LLM agent that addresses staffing and\nprofiling jointly. Differently from existing staffing solutions, StaffPro\nallows expressing optimization objectives using natural language, accepts\ntextual task descriptions and provides high flexibility. StaffPro interacts\ndirectly with humans by establishing a continuous human-agent feedback loop,\nensuring natural and intuitive use. By analyzing human feedback, our agent\ncontinuously estimates the latent features of workers, realizing life-long\nworker profiling and ensuring optimal staffing performance over time. A\nconsulting firm simulation example demonstrates that StaffPro successfully\nestimates workers' attributes and generates high quality schedules. With its\ninnovative design, StaffPro offers a robust, interpretable, and human-centric\nsolution for automated personnel management.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21637", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21637", "abs": "https://arxiv.org/abs/2507.21637", "authors": ["Wanying Wang", "Zeyu Ma", "Han Zheng", "Xin Tan", "Mingang Chen"], "title": "Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models", "comment": "Accepted by ACM Multimedia 2025", "summary": "Large vision-language models (LVLMs) are vulnerable to harmful input compared\nto their language-only backbones. We investigated this vulnerability by\nexploring LVLMs internal dynamics, framing their inherent safety understanding\nin terms of three key capabilities. Specifically, we define these capabilities\nas safety perception, semantic understanding, and alignment for linguistic\nexpression, and experimentally pinpointed their primary locations within the\nmodel architecture. The results indicate that safety perception often emerges\nbefore comprehensive semantic understanding, leading to the reduction in\nsafety. Motivated by these findings, we propose \\textbf{Self-Aware Safety\nAugmentation (SASA)}, a technique that projects informative semantic\nrepresentations from intermediate layers onto earlier safety-oriented layers.\nThis approach leverages the model's inherent semantic understanding to enhance\nsafety recognition without fine-tuning. Then, we employ linear probing to\narticulate the model's internal semantic comprehension to detect the risk\nbefore the generation process. Extensive experiments on various datasets and\ntasks demonstrate that SASA significantly improves the safety of LVLMs, with\nminimal impact on the utility.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21638", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21638", "abs": "https://arxiv.org/abs/2507.21638", "authors": ["Leonard Hinckeldey", "Elliot Fosong", "Elle Miller", "Rimvydas Rubavicius", "Trevor McInroe", "Patricia Wollstadt", "Christiane B. Wiebel-Herboth", "Subramanian Ramamoorthy", "Stefano V. Albrecht"], "title": "Assistax: A Hardware-Accelerated Reinforcement Learning Benchmark for Assistive Robotics", "comment": "Accepted for the Coordination and Cooperation in Multi-Agent\n  Reinforcement Learning Workshop at the Reinforcement Learning Conference 2025", "summary": "The development of reinforcement learning (RL) algorithms has been largely\ndriven by ambitious challenge tasks and benchmarks. Games have dominated RL\nbenchmarks because they present relevant challenges, are inexpensive to run and\neasy to understand. While games such as Go and Atari have led to many\nbreakthroughs, they often do not directly translate to real-world embodied\napplications. In recognising the need to diversify RL benchmarks and addressing\ncomplexities that arise in embodied interaction scenarios, we introduce\nAssistax: an open-source benchmark designed to address challenges arising in\nassistive robotics tasks. Assistax uses JAX's hardware acceleration for\nsignificant speed-ups for learning in physics-based simulations. In terms of\nopen-loop wall-clock time, Assistax runs up to $370\\times$ faster when\nvectorising training runs compared to CPU-based alternatives. Assistax\nconceptualises the interaction between an assistive robot and an active human\npatient using multi-agent RL to train a population of diverse partner agents\nagainst which an embodied robotic agent's zero-shot coordination capabilities\ncan be tested. Extensive evaluation and hyperparameter tuning for popular\ncontinuous control RL and MARL algorithms provide reliable baselines and\nestablish Assistax as a practical benchmark for advancing RL research for\nassistive robotics. The code is available at:\nhttps://github.com/assistive-autonomy/assistax.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21741", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.21741", "abs": "https://arxiv.org/abs/2507.21741", "authors": ["Shaojun E", "Yuchen Yang", "Jiaheng Wu", "Yan Zhang", "Tiejun Zhao", "Ziyan Chen"], "title": "MAGE: Multimodal Alignment and Generation Enhancement via Bridging Visual and Semantic Spaces", "comment": "9 pages", "summary": "In the latest advancements in multimodal learning, effectively addressing the\nspatial and semantic losses of visual data after encoding remains a critical\nchallenge. This is because the performance of large multimodal models is\npositively correlated with the coupling between visual encoders and large\nlanguage models. Existing approaches often face issues such as vector gaps or\nsemantic disparities, resulting in information loss during the propagation\nprocess. To address these issues, we propose MAGE (Multimodal Alignment and\nGeneration Enhancement), a novel framework that bridges the semantic spaces of\nvision and text through an innovative alignment mechanism. By introducing the\nIntelligent Alignment Network (IAN), MAGE achieves dimensional and semantic\nalignment. To reduce the gap between synonymous heterogeneous data, we employ a\ntraining strategy that combines cross-entropy and mean squared error,\nsignificantly enhancing the alignment effect. Moreover, to enhance MAGE's\n\"Any-to-Any\" capability, we developed a fine-tuning dataset for multimodal\ntool-calling instructions to expand the model's output capability boundaries.\nFinally, our proposed multimodal large model architecture, MAGE, achieved\nsignificantly better performance compared to similar works across various\nevaluation benchmarks, including MME, MMBench, and SEED. Complete code and\nappendix are available at: https://github.com/GTCOM-NLP/MAGE.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21745", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21745", "abs": "https://arxiv.org/abs/2507.21745", "authors": ["Aybora Koksal", "A. Aydin Alatan"], "title": "Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable Rewards", "comment": "ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL). 10\n  pages, 3 figures, 6 tables. Our model, training code and dataset will be at\n  https://github.com/aybora/FewShotReasoning", "summary": "Recent advances in large language and vision-language models have enabled\nstrong reasoning capabilities, yet they remain impractical for specialized\ndomains like remote sensing, where annotated data is scarce and expensive. We\npresent the first few-shot reinforcement learning with verifiable reward (RLVR)\nframework for satellite imagery that eliminates the need for caption\nsupervision--relying solely on lightweight, rule-based binary or IoU-based\nrewards. Adapting the \"1-shot RLVR\" paradigm from language models to\nvision-language models, we employ policy-gradient optimization with as few as\none curated example to align model outputs for satellite reasoning tasks.\nComprehensive experiments across multiple remote sensing benchmarks--including\nclassification, visual question answering, and grounding--show that even a\nsingle example yields substantial improvements over the base model. Scaling to\n128 examples matches or exceeds models trained on thousands of annotated\nsamples. While the extreme one-shot setting can induce mild, task-specific\noverfitting, our approach consistently demonstrates robust generalization and\nefficiency across diverse tasks. Further, we find that prompt design and loss\nweighting significantly influence training stability and final accuracy. Our\nmethod enables cost-effective and data-efficient development of\ndomain-specialist vision-language reasoning models, offering a pragmatic recipe\nfor data-scarce fields: start from a compact VLM, curate a handful of\nreward-checkable cases, and train via RLVR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21836", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21836", "abs": "https://arxiv.org/abs/2507.21836", "authors": ["Yifan Wei", "Xiaoyan Yu", "Yixuan Weng", "Tengfei Pan", "Angsheng Li", "Li Du"], "title": "AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs), when enhanced through reasoning-oriented\npost-training, evolve into powerful Large Reasoning Models (LRMs).\nTool-Integrated Reasoning (TIR) further extends their capabilities by\nincorporating external tools, but existing methods often rely on rigid,\npredefined tool-use patterns that risk degrading core language competence.\nInspired by the human ability to adaptively select tools, we introduce AutoTIR,\na reinforcement learning framework that enables LLMs to autonomously decide\nwhether and which tool to invoke during the reasoning process, rather than\nfollowing static tool-use strategies. AutoTIR leverages a hybrid reward\nmechanism that jointly optimizes for task-specific answer correctness,\nstructured output adherence, and penalization of incorrect tool usage, thereby\nencouraging both precise reasoning and efficient tool integration. Extensive\nevaluations across diverse knowledge-intensive, mathematical, and general\nlanguage modeling tasks demonstrate that AutoTIR achieves superior overall\nperformance, significantly outperforming baselines and exhibits superior\ngeneralization in tool-use behavior. These results highlight the promise of\nreinforcement learning in building truly generalizable and scalable TIR\ncapabilities in LLMs. The code and data are available at\nhttps://github.com/weiyifan1023/AutoTIR.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21892", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21892", "abs": "https://arxiv.org/abs/2507.21892", "authors": ["Haoran Luo", "Haihong E", "Guanting Chen", "Qika Lin", "Yikai Guo", "Fangzhi Xu", "Zemin Kuang", "Meina Song", "Xiaobao Wu", "Yifan Zhu", "Luu Anh Tuan"], "title": "Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning", "comment": "Preprint", "summary": "Retrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by\nincorporating external knowledge, but relies on chunk-based retrieval that\nlacks structural semantics. GraphRAG methods improve RAG by modeling knowledge\nas entity-relation graphs, but still face challenges in high construction cost,\nfixed one-time retrieval, and reliance on long-context reasoning and prompt\ndesign. To address these challenges, we propose Graph-R1, an agentic GraphRAG\nframework via end-to-end reinforcement learning (RL). It introduces lightweight\nknowledge hypergraph construction, models retrieval as a multi-turn\nagent-environment interaction, and optimizes the agent process via an\nend-to-end reward mechanism. Experiments on standard RAG datasets show that\nGraph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in\nreasoning accuracy, retrieval efficiency, and generation quality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21830", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21830", "abs": "https://arxiv.org/abs/2507.21830", "authors": ["Kuiye Ding", "Fanda Fan", "Yao Wang", "Ruijie jian", "Xiaorui Wang", "Luqi Gong", "Yishan Jiang", "Chunjie Luo an Jianfeng Zhan"], "title": "DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series Forecasting Framework", "comment": "This paper has been accepted by ACM Multimedia 2025 (ACM MM 2025)", "summary": "Multivariate Time Series Forecasting plays a key role in many applications.\nRecent works have explored using Large Language Models for MTSF to take\nadvantage of their reasoning abilities. However, many methods treat LLMs as\nend-to-end forecasters, which often leads to a loss of numerical precision and\nforces LLMs to handle patterns beyond their intended design. Alternatively,\nmethods that attempt to align textual and time series modalities within latent\nspace frequently encounter alignment difficulty. In this paper, we propose to\ntreat LLMs not as standalone forecasters, but as semantic guidance modules\nwithin a dual-stream framework. We propose DualSG, a dual-stream framework that\nprovides explicit semantic guidance, where LLMs act as Semantic Guides to\nrefine rather than replace traditional predictions. As part of DualSG, we\nintroduce Time Series Caption, an explicit prompt format that summarizes trend\npatterns in natural language and provides interpretable context for LLMs,\nrather than relying on implicit alignment between text and time series in the\nlatent space. We also design a caption-guided fusion module that explicitly\nmodels inter-variable relationships while reducing noise and computation.\nExperiments on real-world datasets from diverse domains show that DualSG\nconsistently outperforms 15 state-of-the-art baselines, demonstrating the value\nof explicitly combining numerical forecasting with semantic guidance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21820", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21820", "abs": "https://arxiv.org/abs/2507.21820", "authors": ["Ahmed B Mustafa", "Zihan Ye", "Yang Lu", "Michael P Pound", "Shreyank N Gowda"], "title": "Anyone Can Jailbreak: Prompt-Based Attacks on LLMs and T2Is", "comment": null, "summary": "Despite significant advancements in alignment and content moderation, large\nlanguage models (LLMs) and text-to-image (T2I) systems remain vulnerable to\nprompt-based attacks known as jailbreaks. Unlike traditional adversarial\nexamples requiring expert knowledge, many of today's jailbreaks are low-effort,\nhigh-impact crafted by everyday users with nothing more than cleverly worded\nprompts. This paper presents a systems-style investigation into how non-experts\nreliably circumvent safety mechanisms through techniques such as multi-turn\nnarrative escalation, lexical camouflage, implication chaining, fictional\nimpersonation, and subtle semantic edits. We propose a unified taxonomy of\nprompt-level jailbreak strategies spanning both text-output and T2I models,\ngrounded in empirical case studies across popular APIs. Our analysis reveals\nthat every stage of the moderation pipeline, from input filtering to output\nvalidation, can be bypassed with accessible strategies. We conclude by\nhighlighting the urgent need for context-aware defenses that reflect the ease\nwith which these jailbreaks can be reproduced in real-world settings.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21974", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.21974", "abs": "https://arxiv.org/abs/2507.21974", "authors": ["Mohamed Sana", "Nicola Piovesan", "Antonio De Domenico", "Yibin Kang", "Haozhe Zhang", "Merouane Debbah", "Fadhel Ayed"], "title": "Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks", "comment": null, "summary": "Root Cause Analysis (RCA) in mobile networks remains a challenging task due\nto the need for interpretability, domain expertise, and causal reasoning. In\nthis work, we propose a lightweight framework that leverages Large Language\nModels (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of\nannotated troubleshooting problems designed to benchmark RCA capabilities. Our\nevaluation reveals that existing open-source reasoning LLMs struggle with these\nproblems, underscoring the need for domain-specific adaptation. To address this\nissue, we propose a two-stage training methodology that combines supervised\nfine-tuning with reinforcement learning to improve the accuracy and reasoning\nquality of LLMs. The proposed approach fine-tunes a series of RCA models to\nintegrate domain knowledge and generate structured, multi-step diagnostic\nexplanations, improving both interpretability and effectiveness. Extensive\nexperiments across multiple LLM sizes show significant performance gains over\nstate-of-the-art reasoning and non-reasoning models, including strong\ngeneralization to randomized test variants. These results demonstrate the\npromise of domain-adapted, reasoning-enhanced LLMs for practical and\nexplainable RCA in network operation and management.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21945", "abs": "https://arxiv.org/abs/2507.21945", "authors": ["Xin Wang", "Peng-Jie Li", "Yuan-Yuan Shen"], "title": "Attention-Driven Multimodal Alignment for Long-term Action Quality Assessment", "comment": "Accepted to Applied Soft Computing", "summary": "Long-term action quality assessment (AQA) focuses on evaluating the quality\nof human activities in videos lasting up to several minutes. This task plays an\nimportant role in the automated evaluation of artistic sports such as rhythmic\ngymnastics and figure skating, where both accurate motion execution and\ntemporal synchronization with background music are essential for performance\nassessment. However, existing methods predominantly fall into two categories:\nunimodal approaches that rely solely on visual features, which are inadequate\nfor modeling multimodal cues like music; and multimodal approaches that\ntypically employ simple feature-level contrastive fusion, overlooking deep\ncross-modal collaboration and temporal dynamics. As a result, they struggle to\ncapture complex interactions between modalities and fail to accurately track\ncritical performance changes throughout extended sequences. To address these\nchallenges, we propose the Long-term Multimodal Attention Consistency Network\n(LMAC-Net). LMAC-Net introduces a multimodal attention consistency mechanism to\nexplicitly align multimodal features, enabling stable integration of visual and\naudio information and enhancing feature representations. Specifically, we\nintroduce a multimodal local query encoder module to capture temporal semantics\nand cross-modal relations, and use a two-level score evaluation for\ninterpretable results. In addition, attention-based and regression-based losses\nare applied to jointly optimize multimodal alignment and score fusion.\nExperiments conducted on the RG and Fis-V datasets demonstrate that LMAC-Net\nsignificantly outperforms existing methods, validating the effectiveness of our\nproposed approach.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21083", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21083", "abs": "https://arxiv.org/abs/2507.21083", "authors": ["Franck Bardol"], "title": "ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs", "comment": null, "summary": "Large Language Models like GPT-4 adjust their responses not only based on the\nquestion asked, but also on how it is emotionally phrased. We systematically\nvary the emotional tone of 156 prompts - spanning controversial and everyday\ntopics - and analyze how it affects model responses. Our findings show that\nGPT-4 is three times less likely to respond negatively to a negatively framed\nquestion than to a neutral one. This suggests a \"rebound\" bias where the model\novercorrects, often shifting toward neutrality or positivity. On sensitive\ntopics (e.g., justice or politics), this effect is even more pronounced:\ntone-based variation is suppressed, suggesting an alignment override. We\nintroduce concepts like the \"tone floor\" - a lower bound in response negativity\n- and use tone-valence transition matrices to quantify behavior. Visualizations\nbased on 1536-dimensional embeddings confirm semantic drift based on tone. Our\nwork highlights an underexplored class of biases driven by emotional framing in\nprompts, with implications for AI alignment and trust. Code and data are\navailable at: https://github.com/bardolfranck/llm-responses-viewer", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21107", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21107", "abs": "https://arxiv.org/abs/2507.21107", "authors": ["Rob Manson"], "title": "Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams", "comment": "29 pages, 22 figures", "summary": "We propose Curved Inference - a geometric Interpretability framework that\ntracks how the residual stream trajectory of a large language model bends in\nresponse to shifts in semantic concern. Across 20 matched prompts spanning\nemotional, moral, perspective, logical, identity, environmental, and nonsense\ndomains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics,\nwith a primary focus on curvature (\\k{appa}_i) and salience (S(t)). These\nmetrics are computed under a pullback semantic metric derived from the\nunembedding matrix, ensuring that all measurements reflect token-aligned\ngeometry rather than raw coordinate structure. We find that concern-shifted\nprompts reliably alter internal activation trajectories in both models - with\nLLaMA exhibiting consistent, statistically significant scaling in both\ncurvature and salience as concern intensity increases. Gemma also responds to\nconcern but shows weaker differentiation between moderate and strong variants.\nOur results support a two-layer view of LLM geometry - a latent conceptual\nstructure encoded in the embedding space, and a contextual trajectory shaped by\nprompt-specific inference. Curved Inference reveals how models navigate,\nreorient, or reinforce semantic meaning over depth, offering a principled\nmethod for diagnosing alignment, abstraction, and emergent inference dynamics.\nThese findings offer fresh insight into semantic abstraction and model\nalignment through the lens of Curved Inference.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.22002", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22002", "abs": "https://arxiv.org/abs/2507.22002", "authors": ["Yida Tao", "Yen-Chia Hsu"], "title": "Bridging Synthetic and Real-World Domains: A Human-in-the-Loop Weakly-Supervised Framework for Industrial Toxic Emission Segmentation", "comment": null, "summary": "Industrial smoke segmentation is critical for air-quality monitoring and\nenvironmental protection but is often hampered by the high cost and scarcity of\npixel-level annotations in real-world settings. We introduce CEDANet, a\nhuman-in-the-loop, class-aware domain adaptation framework that uniquely\nintegrates weak, citizen-provided video-level labels with adversarial feature\nalignment. Specifically, we refine pseudo-labels generated by a source-trained\nsegmentation model using citizen votes, and employ class-specific domain\ndiscriminators to transfer rich source-domain representations to the industrial\ndomain. Comprehensive experiments on SMOKE5K and custom IJmond datasets\ndemonstrate that CEDANet achieves an F1-score of 0.414 and a smoke-class IoU of\n0.261 with citizen feedback, vastly outperforming the baseline model, which\nscored 0.083 and 0.043 respectively. This represents a five-fold increase in\nF1-score and a six-fold increase in smoke-class IoU. Notably, CEDANet with\ncitizen-constrained pseudo-labels achieves performance comparable to the same\narchitecture trained on limited 100 fully annotated images with F1-score of\n0.418 and IoU of 0.264, demonstrating its ability to reach small-sampled fully\nsupervised-level accuracy without target-domain annotations. Our research\nvalidates the scalability and cost-efficiency of combining citizen science with\nweakly supervised domain adaptation, offering a practical solution for complex,\ndata-scarce environmental monitoring applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.22003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22003", "abs": "https://arxiv.org/abs/2507.22003", "authors": ["Ziyun Dai", "Xiaoqiang Li", "Shaohua Zhang", "Yuanchen Wu", "Jide Li"], "title": "See Different, Think Better: Visual Variations Mitigating Hallucinations in LVLMs", "comment": "Accepted by ACM MM25", "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in visual understanding and multimodal reasoning. However, LVLMs\nfrequently exhibit hallucination phenomena, manifesting as the generated\ntextual responses that demonstrate inconsistencies with the provided visual\ncontent. Existing hallucination mitigation methods are predominantly\ntext-centric, the challenges of visual-semantic alignment significantly limit\ntheir effectiveness, especially when confronted with fine-grained visual\nunderstanding scenarios. To this end, this paper presents ViHallu, a\nVision-Centric Hallucination mitigation framework that enhances visual-semantic\nalignment through Visual Variation Image Generation and Visual Instruction\nConstruction. ViHallu introduces \\textbf{\\textit{visual variation images}} with\ncontrollable visual alterations while maintaining the overall image structure.\nThese images, combined with carefully constructed visual instructions, enable\nLVLMs to better understand fine-grained visual content through fine-tuning,\nallowing models to more precisely capture the correspondence between visual\ncontent and text, thereby enhancing visual-semantic alignment. Extensive\nexperiments on multiple benchmarks show that ViHallu effectively enhances\nmodels' fine-grained visual understanding while significantly reducing\nhallucination tendencies. Furthermore, we release ViHallu-Instruction, a visual\ninstruction dataset specifically designed for hallucination mitigation and\nvisual-semantic alignment. Code is available at\nhttps://github.com/oliviadzy/ViHallu.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.22008", "categories": ["cs.CV", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.22008", "abs": "https://arxiv.org/abs/2507.22008", "authors": ["Sajay Raj"], "title": "VeS: Teaching Pixels to Listen Without Supervision", "comment": "6 pages, 1 figure, 1 table. Code and models are released", "summary": "Recent dense audio-visual (AV) models achieve impressive retrieval and\nemergent localization, but almost all evidence comes from English-centric,\ncaption-rich web video. It is unclear whether these objectives survive in\nlow-resource, code-switched, and noisy multilingual settings that typify\ndeveloping regions. We show they do**-**and that the choice of aggregation\nfunction becomes even more critical. Using a multilingual subset of Project\nVaani spanning dozens of Indian languages and dialectal variants, we compare\nthree contrastive objectives: (i) a global mean-pooled loss (CLIP-style), (ii)\na dense max-mean token matcher (DenseAV-style), and (iii) a simple hybrid\n(motivated by frozen-vision alignment strategies). The dense objective delivers\na +59% relative R@1 (Audio Visual) improvement over global pooling and\nsubstantially lower mean/median ranks, while consistently producing sharp\nzero-shot localization heatmaps of spoken objects-despite keeping the vision\nbackbone entirely frozen (no LoRA / partial fine-tuning). Our results\ndemonstrate that dense token routing is not a luxury of high-resource English\ncorpora; it is more decisive when annotations and acoustic cleanliness are\nscarce. We release the codebase and trained models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21138", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.21138", "abs": "https://arxiv.org/abs/2507.21138", "authors": ["Oleg Atamanenko", "Anna Chalova", "Joseph Coombes", "Nikki Cope", "Phillip Dang", "Zhifeng Deng", "Jimmy Du", "Michael Ermolenko", "Feifan Fan", "Yufei Feng", "Cheryl Fichter", "Pavel Filimonov", "Louis Fischer", "Kylan Gibbs", "Valeria Gusarova", "Pavel Karpik", "Andreas Assad Kottner", "Ian Lee", "Oliver Louie", "Jasmine Mai", "Mikhail Mamontov", "Suri Mao", "Nurullah Morshed", "Igor Poletaev", "Florin Radu", "Dmytro Semernia", "Evgenii Shingarev", "Vikram Sivaraja", "Peter Skirko", "Rinat Takhautdinov", "Robert Villahermosa", "Jean Wang"], "title": "TTS-1 Technical Report", "comment": "20 pages, 10 figures. For associated modeling and training code, see\n  https://github.com/inworld-ai/tts", "summary": "We introduce Inworld TTS-1, a set of two Transformer-based autoregressive\ntext-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters\nand is designed for utmost quality and expressiveness in demanding\napplications. TTS-1 is our most efficient model, with 1.6B parameters, built\nfor real-time speech synthesis and on-device use cases. By scaling train-time\ncompute and applying a sequential process of pre-training, fine-tuning, and\nRL-alignment of the speech-language model (SpeechLM) component, both models\nachieve state-of-the-art performance on a variety of benchmarks, demonstrating\nexceptional quality relying purely on in-context learning of the speaker's\nvoice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech\nwith low latency, and support 11 languages with fine-grained emotional control\nand non-verbal vocalizations through audio markups. We additionally open-source\nour training and modeling code under an MIT license.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.22028", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.22028", "abs": "https://arxiv.org/abs/2507.22028", "authors": ["Honglin He", "Yukai Ma", "Wayne Wu", "Bolei Zhou"], "title": "From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning", "comment": null, "summary": "Navigation foundation models trained on massive webscale data enable agents\nto generalize across diverse environments and embodiments. However, these\nmodels trained solely on offline data, often lack the capacity to reason about\nthe consequences of their actions or adapt through counterfactual\nunderstanding. They thus face significant limitations in the real-world urban\nnavigation where interactive and safe behaviors, such as avoiding obstacles and\nmoving pedestrians, are critical. To tackle these challenges, we introduce the\nSeeing-to-Experiencing framework to scale the capability of navigation\nfoundation models with reinforcement learning. S2E combines the strengths of\npre-training on videos and post-training through RL. It maintains the\ngeneralizability acquired from large-scale real-world videos while enhancing\nits interactivity through RL in simulation environments. Specifically, we\nintroduce two innovations: an Anchor-Guided Distribution Matching strategy,\nwhich stabilizes learning and models diverse motion patterns through\nanchor-based supervision; and a Residual-Attention Module, which obtains\nreactive behaviors from simulation environments without erasing the model's\npretrained knowledge. Moreover, we establish a comprehensive end-to-end\nevaluation benchmark, NavBench-GS, built on photorealistic 3DGS reconstructions\nof real-world scenes that incorporate physical interactions. It can\nsystematically assess the generalizability and safety of navigation foundation\nmodels. Extensive experiments show that S2E mitigates the diminishing returns\noften seen when scaling with offline data alone. We perform a thorough analysis\nof the benefits of Reinforcement Learning compared to Supervised Fine-Tuning in\nthe context of post-training for robot learning. Our findings emphasize the\ncrucial role of integrating interactive online experiences to effectively scale\nfoundation models in Robotics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "safety"], "score": 3}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.22052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22052", "abs": "https://arxiv.org/abs/2507.22052", "authors": ["Ziren Gong", "Xiaohan Li", "Fabio Tosi", "Jiawei Han", "Stefano Mattoccia", "Jianfei Cai", "Matteo Poggi"], "title": "Ov3R: Open-Vocabulary Semantic 3D Reconstruction from RGB Videos", "comment": null, "summary": "We present Ov3R, a novel framework for open-vocabulary semantic 3D\nreconstruction from RGB video streams, designed to advance Spatial AI. The\nsystem features two key components: CLIP3R, a CLIP-informed 3D reconstruction\nmodule that predicts dense point maps from overlapping clips while embedding\nobject-level semantics; and 2D-3D OVS, a 2D-3D open-vocabulary semantic module\nthat lifts 2D features into 3D by learning fused descriptors integrating\nspatial, geometric, and semantic cues. Unlike prior methods, Ov3R incorporates\nCLIP semantics directly into the reconstruction process, enabling globally\nconsistent geometry and fine-grained semantic alignment. Our framework achieves\nstate-of-the-art performance in both dense 3D reconstruction and\nopen-vocabulary 3D segmentation, marking a step forward toward real-time,\nsemantics-aware Spatial AI.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.22058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22058", "abs": "https://arxiv.org/abs/2507.22058", "authors": ["Zigang Geng", "Yibing Wang", "Yeyao Ma", "Chen Li", "Yongming Rao", "Shuyang Gu", "Zhao Zhong", "Qinglin Lu", "Han Hu", "Xiaosong Zhang", "Linus", "Di Wang", "Jie Jiang"], "title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again", "comment": null, "summary": "Numerous efforts have been made to extend the ``next token prediction''\nparadigm to visual contents, aiming to create a unified approach for both image\ngeneration and understanding. Nevertheless, attempts to generate images through\nautoregressive modeling with discrete tokens have been plagued by issues such\nas low visual fidelity, distorted outputs, and failure to adhere to complex\ninstructions when rendering intricate details. These shortcomings are likely\nattributed to cumulative errors during autoregressive inference or information\nloss incurred during the discretization process. Probably due to this\nchallenge, recent research has increasingly shifted toward jointly training\nimage generation with diffusion objectives and language generation with\nautoregressive objectives, moving away from unified modeling approaches. In\nthis work, we demonstrate that reinforcement learning can effectively mitigate\nartifacts and largely enhance the generation quality of a discrete\nautoregressive modeling method, thereby enabling seamless integration of image\nand language generation. Our framework comprises a semantic image tokenizer, a\nunified autoregressive model for both language and images, and an offline\ndiffusion decoder for image generation, termed X-Omni. X-Omni achieves\nstate-of-the-art performance in image generation tasks using a 7B language\nmodel, producing images with high aesthetic quality while exhibiting strong\ncapabilities in following instructions and rendering long texts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.21340", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.21340", "abs": "https://arxiv.org/abs/2507.21340", "authors": ["Satyananda Kashyap", "Sola Shirai", "Nandana Mihindukulasooriya", "Horst Samulowitz"], "title": "StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation", "comment": "Data available:\n  https://huggingface.co/datasets/ibm-research/struct-text and code available\n  at: https://github.com/ibm/struct-text", "summary": "Extracting structured information from text, such as key-value pairs that\ncould augment tabular data, is quite useful in many enterprise use cases.\nAlthough large language models (LLMs) have enabled numerous automated pipelines\nfor converting natural language into structured formats, there is still a lack\nof benchmarks for evaluating their extraction quality, especially in specific\ndomains or focused documents specific to a given organization. Building such\nbenchmarks by manual annotations is labour-intensive and limits the size and\nscalability of the benchmarks. In this work, we present StructText, an\nend-to-end framework for automatically generating high-fidelity benchmarks for\nkey-value extraction from text using existing tabular data. It uses available\ntabular data as structured ground truth, and follows a two-stage\n``plan-then-execute'' pipeline to synthetically generate corresponding\nnatural-language text. To ensure alignment between text and structured source,\nwe introduce a multi-dimensional evaluation strategy that combines (a)\nLLM-based judgments on factuality, hallucination, and coherence and (b)\nobjective extraction metrics measuring numeric and temporal accuracy. We\nevaluated the proposed method on 71,539 examples across 49 datasets. Results\nreveal that while LLMs achieve strong factual accuracy and avoid hallucination,\nthey struggle with narrative coherence in producing extractable text. Notably,\nmodels presume numerical and temporal information with high fidelity yet this\ninformation becomes embedded in narratives that resist automated extraction. We\nrelease a framework, including datasets, evaluation tools, and baseline\nextraction systems, to support continued research.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "factuality", "accuracy", "multi-dimensional"], "score": 5}}, "source_file": "2025-07-30.jsonl"}
{"id": "2507.22002", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22002", "abs": "https://arxiv.org/abs/2507.22002", "authors": ["Yida Tao", "Yen-Chia Hsu"], "title": "Bridging Synthetic and Real-World Domains: A Human-in-the-Loop Weakly-Supervised Framework for Industrial Toxic Emission Segmentation", "comment": null, "summary": "Industrial smoke segmentation is critical for air-quality monitoring and\nenvironmental protection but is often hampered by the high cost and scarcity of\npixel-level annotations in real-world settings. We introduce CEDANet, a\nhuman-in-the-loop, class-aware domain adaptation framework that uniquely\nintegrates weak, citizen-provided video-level labels with adversarial feature\nalignment. Specifically, we refine pseudo-labels generated by a source-trained\nsegmentation model using citizen votes, and employ class-specific domain\ndiscriminators to transfer rich source-domain representations to the industrial\ndomain. Comprehensive experiments on SMOKE5K and custom IJmond datasets\ndemonstrate that CEDANet achieves an F1-score of 0.414 and a smoke-class IoU of\n0.261 with citizen feedback, vastly outperforming the baseline model, which\nscored 0.083 and 0.043 respectively. This represents a five-fold increase in\nF1-score and a six-fold increase in smoke-class IoU. Notably, CEDANet with\ncitizen-constrained pseudo-labels achieves performance comparable to the same\narchitecture trained on limited 100 fully annotated images with F1-score of\n0.418 and IoU of 0.264, demonstrating its ability to reach small-sampled fully\nsupervised-level accuracy without target-domain annotations. Our research\nvalidates the scalability and cost-efficiency of combining citizen science with\nweakly supervised domain adaptation, offering a practical solution for complex,\ndata-scarce environmental monitoring applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-30.jsonl"}
