{"id": "2508.02618", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02618", "abs": "https://arxiv.org/abs/2508.02618", "authors": ["Jianxiang Zang", "Meiling Ning", "Shihan Dou", "Jiazheng Zhang", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation", "comment": null, "summary": "The reward model (RM), as the core component of reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs), responsible for\nproviding reward signals to generated responses. However, mainstream preference\nmodeling in RM is inadequate in terms of token-level interaction, making its\njudgment signals vulnerable to being hacked by misallocated attention to\ncontext. This stems from two fundamental limitations: (1) Current preference\nmodeling employs decoder-only architectures, where the unidirectional causal\nattention mechanism leads to forward-decaying intra-sequence attention within\nthe prompt-response sequence. (2) The independent Siamese-encoding paradigm\ninduces the absence of token-level inter-sequence attention between chosen and\nrejected sequences. To address this \"attention hacking\", we propose\n\"Interaction Distillation\", a novel training framework for more adequate\npreference modeling through attention-level optimization. The method introduces\nan interaction-based natural language understanding model as the teacher to\nprovide sophisticated token interaction patterns via comprehensive attention,\nand guides the preference modeling to simulate teacher model's interaction\npattern through an attentional alignment objective. Through extensive\nexperiments, interaction distillation has demonstrated its ability to provide\nmore stable and generalizable reward signals compared to state-of-the-art RM\noptimization methods that target data noise, highlighting the attention hacking\nconstitute a more fundamental limitation in RM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward modeling", "RLHF", "human feedback", "reinforcement learning", "preference", "alignment"], "score": 7}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01930", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.01930", "abs": "https://arxiv.org/abs/2508.01930", "authors": ["Tom S. Juzek", "Zina B. Ward"], "title": "Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback", "comment": "Accepted for publication in the Proceedings of the 5th Workshop on\n  Bias and Fairness in AI (BIAS 2025) at ECML PKDD", "summary": "Large Language Models (LLMs) are known to overuse certain terms like \"delve\"\nand \"intricate.\" The exact reasons for these lexical choices, however, have\nbeen unclear. Using Meta's Llama model, this study investigates the\ncontribution of Learning from Human Feedback (LHF), under which we subsume\nReinforcement Learning from Human Feedback and Direct Preference Optimization.\nWe present a straightforward procedure for detecting the lexical preferences of\nLLMs that are potentially LHF-induced. Next, we more conclusively link LHF to\nlexical overuse by experimentally emulating the LHF procedure and demonstrating\nthat participants systematically prefer text variants that include certain\nwords. This lexical overuse can be seen as a sort of misalignment, though our\nstudy highlights the potential divergence between the lexical expectations of\ndifferent populations -- namely LHF workers versus LLM users. Our work\ncontributes to the growing body of research on explainable artificial\nintelligence and emphasizes the importance of both data and procedural\ntransparency in alignment research.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning from human feedback", "human feedback", "reinforcement learning", "preference", "alignment", "direct preference optimization"], "score": 6}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01930", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.01930", "abs": "https://arxiv.org/abs/2508.01930", "authors": ["Tom S. Juzek", "Zina B. Ward"], "title": "Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback", "comment": "Accepted for publication in the Proceedings of the 5th Workshop on\n  Bias and Fairness in AI (BIAS 2025) at ECML PKDD", "summary": "Large Language Models (LLMs) are known to overuse certain terms like \"delve\"\nand \"intricate.\" The exact reasons for these lexical choices, however, have\nbeen unclear. Using Meta's Llama model, this study investigates the\ncontribution of Learning from Human Feedback (LHF), under which we subsume\nReinforcement Learning from Human Feedback and Direct Preference Optimization.\nWe present a straightforward procedure for detecting the lexical preferences of\nLLMs that are potentially LHF-induced. Next, we more conclusively link LHF to\nlexical overuse by experimentally emulating the LHF procedure and demonstrating\nthat participants systematically prefer text variants that include certain\nwords. This lexical overuse can be seen as a sort of misalignment, though our\nstudy highlights the potential divergence between the lexical expectations of\ndifferent populations -- namely LHF workers versus LLM users. Our work\ncontributes to the growing body of research on explainable artificial\nintelligence and emphasizes the importance of both data and procedural\ntransparency in alignment research.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning from human feedback", "human feedback", "reinforcement learning", "preference", "alignment", "direct preference optimization"], "score": 6}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01543", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01543", "abs": "https://arxiv.org/abs/2508.01543", "authors": ["Derin Cayir", "Renjie Tao", "Rashi Rungta", "Kai Sun", "Sean Chen", "Haidar Khan", "Minseok Kim", "Julia Reinspach", "Yue Liu"], "title": "Refine-n-Judge: Curating High-Quality Preference Chains for LLM-Fine-Tuning", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable progress through\npreference-based fine-tuning, which critically depends on the quality of the\nunderlying training data. While human feedback is essential for improving data\nquality, it is costly and does not scale well. In this paper, we introduce\nRefine-n-Judge, an automated iterative approach that leverages a single LLM as\nboth a refiner and a judge to enhance dataset quality. Unlike existing\niterative refinement methods, Refine-n-Judge employs an LLM to both generate\nrefinements and explicitly evaluate each improvement, ensuring that every\niteration meaningfully enhances the dataset without requiring additional human\nannotation or a separate reward model. At each step, the LLM refines a response\nand judges whether the refinement is an improvement over the previous answer.\nThis process continues until the LLM prefers the initial answer over the\nrefinement, indicating no further improvements. This produces sequences of\nincreasing quality, preference-labeled responses ideal for fine-tuning.\n  We demonstrate the effectiveness of Refine-n-Judge across a range of public\ndatasets spanning five corpora, targeting tasks such as coding, math, and\nconversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on\nRefine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of\ncomparisons against models tuned on the original dataset by GPT-4.\nAdditionally, we report performance gains: +5% on AlpacaEval and AlpacaEval\n2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces\nhigh-quality datasets and scalable model improvements.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "iterative refinement"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "human feedback", "preference"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02374", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02374", "abs": "https://arxiv.org/abs/2508.02374", "authors": ["Shuo Lu", "Yanyin Chen", "Wei Feng", "Jiahao Fan", "Fengheng Li", "Zheng Zhang", "Jingjing Lv", "Junjie Shen", "Ching Law", "Jian Liang"], "title": "Uni-Layout: Integrating Human Feedback in Unified Layout Generation and Evaluation", "comment": "Accepted to ACM MM 2025", "summary": "Layout generation plays a crucial role in enhancing both user experience and\ndesign efficiency. However, current approaches suffer from task-specific\ngeneration capabilities and perceptually misaligned evaluation metrics, leading\nto limited applicability and ineffective measurement. In this paper, we propose\n\\textit{Uni-Layout}, a novel framework that achieves unified generation,\nhuman-mimicking evaluation and alignment between the two. For universal\ngeneration, we incorporate various layout tasks into a single taxonomy and\ndevelop a unified generator that handles background or element contents\nconstrained tasks via natural language prompts. To introduce human feedback for\nthe effective evaluation of layouts, we build \\textit{Layout-HF100k}, the first\nlarge-scale human feedback dataset with 100,000 expertly annotated layouts.\nBased on \\textit{Layout-HF100k}, we introduce a human-mimicking evaluator that\nintegrates visual and geometric information, employing a Chain-of-Thought\nmechanism to conduct qualitative assessments alongside a confidence estimation\nmodule to yield quantitative measurements. For better alignment between the\ngenerator and the evaluator, we integrate them into a cohesive system by\nadopting Dynamic-Margin Preference Optimization (DMPO), which dynamically\nadjusts margins based on preference strength to better align with human\njudgments. Extensive experiments show that \\textit{Uni-Layout} significantly\noutperforms both task-specific and general-purpose methods. Our code is\npublicly available at https://github.com/JD-GenX/Uni-Layout.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02464", "abs": "https://arxiv.org/abs/2508.02464", "authors": ["Yonghuang Wu", "Wenwen Zeng", "Xuan Xie", "Chengqian Zhao", "Guoqing Wu", "Jinhua Yu"], "title": "SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with Vision Foundation Models", "comment": null, "summary": "Foundation models like Segment Anything Model (SAM) excel in promptable\nsegmentation but suffer from an intent gap: they segment only explicitly\nprompted objects, failing to generalize to semantically related instances\nimplicitly desired by users. This limitation is critical in domains with dense\nhomogeneous objects (e.g., biomedical nuclei segmentation), where sparse visual\nprompts typically yield incomplete results, rendering dense annotations\nimpractical due to prohibitive cost. To bridge this gap, we introduce SAMPO\n(Segment Anything Model with Preference Optimization), a novel framework that\nteaches visual foundation models to infer high-level categorical intent from\nsparse visual interactions. Unlike conventional pixel-level fine-tuning, SAMPO\noptimizes models to implicitly capture target-class characteristics through\npreference optimization. This approach, which operates without dependency on\nlanguage models, enables robust multi-object segmentation even under sparse\nprompting and demonstrates superior data efficiency during fine-tuning.\nValidated on three medical segmentation tasks, SAMPO achieves state-of-the-art\nperformance: on challenging tasks like PanNuke-T2, our method, when fine-tuned\nwith only 10% of the training data, significantly outperforms all existing\nmethods trained on the full 100% dataset, achieving an improvement of over 9\npercentage points compared to the best baseline. Our work establishes a new\nparadigm for intent-aware alignment in visual foundation models, removing\ndependencies on auxiliary prompt generators or language-model-assisted\npreference learning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01674", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.01674", "abs": "https://arxiv.org/abs/2508.01674", "authors": ["Tae Soo Kim", "Yoonjoo Lee", "Yoonah Park", "Jiho Kim", "Young-Ho Kim", "Juho Kim"], "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions", "comment": "Accepted to COLM 2025. Project Website: https://cupid.kixlab.org/", "summary": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01682", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01682", "abs": "https://arxiv.org/abs/2508.01682", "authors": ["Lingyin Zhang", "Jun Gao", "Xiaoxue Ren", "Ziqiang Cao"], "title": "The Bidirectional Process Reward Model", "comment": null, "summary": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning quality of Large Language Models (LLMs) by assigning fine-grained\nscores to intermediate reasoning steps within a solution trajectory. However,\nexisting PRMs predominantly adopt a unidirectional left-to-right (L2R)\nevaluation paradigm, which limits their ability to leverage global context,\nmaking it challenging to verify the consistency of earlier steps based on later\nones. In light of these challenges, we propose a novel bidirectional evaluation\nparadigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly\nincorporates a parallel right-to-left (R2L) evaluation stream alongside the\nconventional L2R flow, enabling later reasoning steps to help assess earlier\nones in real time. Notably, the built-in R2L evaluation is implemented solely\nthrough prompt modifications that reverse the original reasoning trajectory,\nwithout any additional parameters or inference latency introduced. This ensures\nBiPRM remains both efficient and broadly compatible with existing PRM studies.\nWe conduct extensive experiments on two mathematical reasoning benchmarks using\nsamples generated by three different policy models. Our method, BiPRM, is\nevaluated across three backbones and three distinct PRM objectives. Across all\nsettings, BiPRM consistently outperforms unidirectional baselines, achieving up\nto a 31.9% improvement in stepwise reward evaluation. Generally, our results\nhighlight BiPRM's effectiveness, robustness, and general applicability,\noffering a promising new direction for process-based reward modeling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward modeling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "mathematical reasoning", "fine-grained"], "score": 4}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02063", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02063", "abs": "https://arxiv.org/abs/2508.02063", "authors": ["Amitava Das", "Vinija Jain", "Aman Chadha"], "title": "TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs", "comment": null, "summary": "Large Language Models (LLMs) fine-tuned to align with human values often\nexhibit alignment drift, producing unsafe or policy-violating completions when\nexposed to adversarial prompts, decoding perturbations, or paraphrased\njailbreaks. While prior work has behaviorally characterized alignment failure,\nlittle is known about the training-time belief sources underlying these\nfailures. We introduce TraceAlign, a unified framework for tracing unsafe\ncompletions back to their root causes in the model's training corpus. Central\nto our approach is the Belief Conflict Index (BCI), which quantifies semantic\ninconsistency between generated spans and aligned policies, based on retrieved\ntraining documents using suffix-array matching. We propose three complementary\ninterventions: (i) TraceShield, an inference-time safety filter that refuses\ncompletions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a\ncontrastive fine-tuning objective penalizing high-BCI continuations during DPO,\nand (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam\nexpansions predicted to yield high-BCI spans. Together, these defenses reduce\nalignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB)\nwhile preserving utility on standard tasks, with delta less than 0.2 and\nimproved refusal quality. We further derive a theoretical upper bound on drift\nlikelihood via suffix-array span statistics, linking memorization frequency and\nlength to adversarial reactivation risk. TraceAlign thus provides the first\nscalable, traceable, and grounded toolkit for understanding and mitigating\nalignment failures at source. To encourage further exploration and development,\nwe open-source our implementation at:\nhttps://anonymous.4open.science/r/tracealign-2DA7", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "DPO"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02087", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02087", "abs": "https://arxiv.org/abs/2508.02087", "authors": ["Jin Li", "Keyu Wang", "Shu Yang", "Zhuoran Zhang", "Di Wang"], "title": "When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing\nwith user-stated opinions even when those contradict factual knowledge. While\nprior work has documented this tendency, the internal mechanisms that enable\nsuch behavior remain poorly understood. In this paper, we provide a mechanistic\naccount of how sycophancy arises within LLMs. We first systematically study how\nuser opinions induce sycophancy across different model families. We find that\nsimple opinion statements reliably induce sycophancy, whereas user expertise\nframing has a negligible impact. Through logit-lens analysis and causal\nactivation patching, we identify a two-stage emergence of sycophancy: (1) a\nlate-layer output preference shift and (2) deeper representational divergence.\nWe also verify that user authority fails to influence behavior because models\ndo not encode it internally. In addition, we examine how grammatical\nperspective affects sycophantic behavior, finding that first-person prompts\n(``I believe...'') consistently induce higher sycophancy rates than\nthird-person framings (``They believe...'') by creating stronger\nrepresentational perturbations in deeper layers. These findings highlight that\nsycophancy is not a surface-level artifact but emerges from a structural\noverride of learned knowledge in deeper layers, with implications for alignment\nand truthful AI systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01533", "abs": "https://arxiv.org/abs/2508.01533", "authors": ["Jiaxin Liu", "Zhaolu Kang"], "title": "ReasonAct: Progressive Training for Fine-Grained Video Reasoning in Small Models", "comment": null, "summary": "While recent multimodal models have shown progress in vision-language tasks,\nsmall-scale variants still struggle with the fine-grained temporal reasoning\nrequired for video understanding. We introduce ReasonAct, a method that\nenhances video reasoning in smaller models through a three-stage training\nprocess: first building a foundation with text-only reasoning, then fine-tuning\non video, and finally refining with temporal-aware reinforcement learning. We\nbuild upon Temporal Group Relative Policy Optimization (T-GRPO) by\nincorporating temporal consistency modeling into policy optimization. We also\npropose a biomechanically-motivated sub-action decomposition mechanism that\nprovides graduated rewards for constituent action phases. Through experiments\non HMDB51, UCF-101, and Kinetics-400, our 3B-parameter model achieves 67.2%,\n94.1%, and 78.9% accuracy respectively, demonstrating improvements of 17.9,\n15.8, and 12.3 points over baselines. Ablation studies validate that our\nprogressive training methodology enables smaller models to achieve competitive\nvideo reasoning performance while maintaining computational efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02644", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02644", "abs": "https://arxiv.org/abs/2508.02644", "authors": ["Guowei Zou", "Weibing Li", "Hejun Wu", "Yukun Qian", "Yuhang Wang", "Haitao Wang"], "title": "D2PPO: Diffusion Policy Policy Optimization with Dispersive Loss", "comment": null, "summary": "Diffusion policies excel at robotic manipulation by naturally modeling\nmultimodal action distributions in high-dimensional spaces. Nevertheless,\ndiffusion policies suffer from diffusion representation collapse: semantically\nsimilar observations are mapped to indistinguishable features, ultimately\nimpairing their ability to handle subtle but critical variations required for\ncomplex robotic manipulation. To address this problem, we propose D2PPO\n(Diffusion Policy Policy Optimization with Dispersive Loss). D2PPO introduces\ndispersive loss regularization that combats representation collapse by treating\nall hidden representations within each batch as negative pairs. D2PPO compels\nthe network to learn discriminative representations of similar observations,\nthereby enabling the policy to identify subtle yet crucial differences\nnecessary for precise manipulation. In evaluation, we find that early-layer\nregularization benefits simple tasks, while late-layer regularization sharply\nenhances performance on complex manipulation tasks. On RoboMimic benchmarks,\nD2PPO achieves an average improvement of 22.7% in pre-training and 26.1% after\nfine-tuning, setting new SOTA results. In comparison with SOTA, results of\nreal-world experiments on a Franka Emika Panda robot show the excitingly high\nsuccess rate of our method. The superiority of our method is especially evident\nin complex tasks. Project page: https://guowei-zou.github.io/d2ppo/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization", "comparison"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01674", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.01674", "abs": "https://arxiv.org/abs/2508.01674", "authors": ["Tae Soo Kim", "Yoonjoo Lee", "Yoonah Park", "Jiho Kim", "Young-Ho Kim", "Juho Kim"], "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions", "comment": "Accepted to COLM 2025. Project Website: https://cupid.kixlab.org/", "summary": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02149", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02149", "abs": "https://arxiv.org/abs/2508.02149", "authors": ["Ziyang Luo", "Nian Liu", "Fahad Shahbaz Khan", "Junwei Han"], "title": "AURORA: Augmented Understanding via Structured Reasoning and Reinforcement Learning for Reference Audio-Visual Segmentation", "comment": null, "summary": "Reference Audio-Visual Segmentation (Ref-AVS) tasks challenge models to\nprecisely locate sounding objects by integrating visual, auditory, and textual\ncues. Existing methods often lack genuine semantic understanding, tending to\nmemorize fixed reasoning patterns. Furthermore, jointly training for reasoning\nand segmentation can compromise pixel-level precision. To address these issues,\nwe introduce AURORA, a novel framework designed to enhance genuine reasoning\nand language comprehension in reference audio-visual segmentation. We employ a\nstructured Chain-of-Thought (CoT) prompting mechanism to guide the model\nthrough a step-by-step reasoning process and introduce a novel segmentation\nfeature distillation loss to effectively integrate these reasoning abilities\nwithout sacrificing segmentation performance. To further cultivate the model's\ngenuine reasoning capabilities, we devise a further two-stage training\nstrategy: first, a ``corrective reflective-style training\" stage utilizes\nself-correction to enhance the quality of reasoning paths, followed by\nreinforcement learning via Group Reward Policy Optimization (GRPO) to bolster\nrobustness in challenging scenarios. Experiments demonstrate that AURORA\nachieves state-of-the-art performance on Ref-AVS benchmarks and generalizes\neffectively to unreferenced segmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02151", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02151", "abs": "https://arxiv.org/abs/2508.02151", "authors": ["Die Chen", "Zhongjie Duan", "Zhiwen Li", "Cen Chen", "Daoyuan Chen", "Yaliang Li", "Yinda Chen"], "title": "AttriCtrl: Fine-Grained Control of Aesthetic Attribute Intensity in Diffusion Models", "comment": null, "summary": "Recent breakthroughs in text-to-image diffusion models have significantly\nenhanced both the visual fidelity and semantic controllability of generated\nimages. However, fine-grained control over aesthetic attributes remains\nchallenging, especially when users require continuous and intensity-specific\nadjustments. Existing approaches often rely on vague textual prompts, which are\ninherently ambiguous in expressing both the aesthetic semantics and the desired\nintensity, or depend on costly human preference data for alignment, limiting\ntheir scalability and practicality. To address these limitations, we propose\nAttriCtrl, a plug-and-play framework for precise and continuous control of\naesthetic attributes. Specifically, we quantify abstract aesthetics by\nleveraging semantic similarity from pre-trained vision-language models, and\nemploy a lightweight value encoder that maps scalar intensities in $[0,1]$ to\nlearnable embeddings within diffusion-based generation. This design enables\nintuitive and customizable aesthetic manipulation, with minimal training\noverhead and seamless integration into existing generation pipelines. Extensive\nexperiments demonstrate that AttriCtrl achieves accurate control over\nindividual attributes as well as flexible multi-attribute composition.\nMoreover, it is fully compatible with popular open-source controllable\ngeneration frameworks, showcasing strong integration capability and practical\nutility across diverse generation scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "fine-grained"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.00945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00945", "abs": "https://arxiv.org/abs/2508.00945", "authors": ["Yifan Wang", "Hongfeng Ai", "Quangao Liu", "Maowei Jiang", "Ruiyuan Kang", "Ruiqi Li", "Jiahua Dong", "Mengting Xiao", "Cheng Jiang", "Chenzhong Li"], "title": "Optimizing Vision-Language Consistency via Cross-Layer Regional Attention Alignment", "comment": "10 pages", "summary": "Vision Language Models (VLMs) face challenges in effectively coordinating\ndiverse attention mechanisms for cross-modal embedding learning, leading to\nmismatched attention and suboptimal performance. We propose Consistent\nCross-layer Regional Alignment (CCRA), which introduces Layer-Patch-wise Cross\nAttention (LPWCA) to capture fine-grained regional-semantic correlations by\njointly weighting patch and layer-wise embedding, and Progressive Attention\nIntegration (PAI) that systematically coordinates LPWCA, layer-wise, and\npatch-wise attention mechanisms in sequence. This progressive design ensures\nconsistency from semantic to regional levels while preventing attention drift\nand maximizing individual attention benefits. Experimental results on ten\ndiverse vision-language benchmarks demonstrate that our CCRA-enhanced\nLLaVA-v1.5-7B model achieves state-of-the-art performance, outperforming all\nbaseline methods with only 3.55M additional parameters, while providing\nenhanced interpretability through more regionally focused and semantically\naligned attention patterns.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01198", "abs": "https://arxiv.org/abs/2508.01198", "authors": ["Yige Li", "Peihai Jiang", "Jun Sun", "Peng Shu", "Tianming Liu", "Zhen Xiang"], "title": "Adaptive Content Restriction for Large Language Models via Suffix Optimization", "comment": "19 pages", "summary": "Large Language Models (LLMs) have demonstrated significant success across\ndiverse applications. However, enforcing content restrictions remains a\nsignificant challenge due to their expansive output space. One aspect of\ncontent restriction is preventing LLMs from generating harmful content via\nmodel alignment approaches such as supervised fine-tuning (SFT). Yet, the need\nfor content restriction may vary significantly across user groups, change\nrapidly over time, and not always align with general definitions of\nharmfulness. Applying SFT to each of these specific use cases is impractical\ndue to the high computational, data, and storage demands. Motivated by this\nneed, we propose a new task called \\textit{Adaptive Content Restriction}\n(AdaCoRe), which focuses on lightweight strategies -- methods without model\nfine-tuning -- to prevent deployed LLMs from generating restricted terms for\nspecific use cases. We propose the first method for AdaCoRe, named\n\\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to\nany prompt to a) prevent a target LLM from generating a set of restricted\nterms, while b) preserving the output quality. To evaluate AdaCoRe approaches,\nincluding our SOP, we create a new \\textit{Content Restriction Benchmark}\n(CoReBench), which contains 400 prompts for 80 restricted terms across 8\ncarefully selected categories. We demonstrate the effectiveness of SOP on\nCoReBench, which outperforms the system-level baselines such as system suffix\nby 15\\%, 17\\%, 10\\%, 9\\%, and 6\\% on average restriction rates for Gemma2-2B,\nMistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also\ndemonstrate that SOP is effective on POE, an online platform hosting various\ncommercial LLMs, highlighting its practicality in real-world scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "Vicuna"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01263", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01263", "abs": "https://arxiv.org/abs/2508.01263", "authors": ["Long S. T. Nguyen", "Khang H. N. Vo", "Thu H. A. Nguyen", "Tuan C. Bui", "Duc Q. Nguyen", "Thanh-Tung Tran", "Anh D. Nguyen", "Minh L. Nguyen", "Fabien Baldacci", "Thang H. Bui", "Emanuel Di Nardo", "Angelo Ciaramella", "Son H. Le", "Ihsan Ullah", "Lorenzo Di Rocco", "Tho T. Quan"], "title": "Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025", "comment": "The XAI Challenge @ TRNS-AI Workshop, IJCNN 2025: Explainable AI for\n  Educational Question Answering. Website:\n  https://sites.google.com/view/trns-ai/challenge/", "summary": "The growing integration of Artificial Intelligence (AI) into education has\nintensified the need for transparency and interpretability. While hackathons\nhave long served as agile environments for rapid AI prototyping, few have\ndirectly addressed eXplainable AI (XAI) in real-world educational contexts.\nThis paper presents a comprehensive analysis of the XAI Challenge 2025, a\nhackathon-style competition jointly organized by Ho Chi Minh City University of\nTechnology (HCMUT) and the International Workshop on Trustworthiness and\nReliability in Neurosymbolic AI (TRNS-AI), held as part of the International\nJoint Conference on Neural Networks (IJCNN 2025). The challenge tasked\nparticipants with building Question-Answering (QA) systems capable of answering\nstudent queries about university policies while generating clear, logic-based\nnatural language explanations. To promote transparency and trustworthiness,\nsolutions were required to use lightweight Large Language Models (LLMs) or\nhybrid LLM-symbolic systems. A high-quality dataset was provided, constructed\nvia logic-based templates with Z3 validation and refined through expert student\nreview to ensure alignment with real-world academic scenarios. We describe the\nchallenge's motivation, structure, dataset construction, and evaluation\nprotocol. Situating the competition within the broader evolution of AI\nhackathons, we argue that it represents a novel effort to bridge LLMs and\nsymbolic reasoning in service of explainability. Our findings offer actionable\ninsights for future XAI-centered educational systems and competitive research\ninitiatives.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "reliability"], "score": 3}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01109", "categories": ["cs.AI", "68T07", "I.2; J.4"], "pdf": "https://arxiv.org/pdf/2508.01109", "abs": "https://arxiv.org/abs/2508.01109", "authors": ["Satiyabooshan Murugaboopathy", "Connor T. Jerzak", "Adel Daoud"], "title": "Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?", "comment": "7 figures", "summary": "We investigate whether socio-economic indicators like household wealth leave\nrecoverable imprints in satellite imagery (capturing physical features) and\nInternet-sourced text (reflecting historical/economic narratives). Using\nDemographic and Health Survey (DHS) data from African neighborhoods, we pair\nLandsat images with LLM-generated textual descriptions conditioned on\nlocation/year and text retrieved by an AI search agent from web sources. We\ndevelop a multimodal framework predicting household wealth (International\nWealth Index) through five pipelines: (i) vision model on satellite images,\n(ii) LLM using only location/year, (iii) AI agent searching/synthesizing web\ntext, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework\nyields three contributions. First, fusing vision and agent/LLM text outperforms\nvision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on\nout-of-sample splits), with LLM-internal knowledge proving more effective than\nagent-retrieved text, improving robustness to out-of-country and out-of-time\ngeneralization. Second, we find partial representational convergence: fused\nembeddings from vision/language modalities correlate moderately (median cosine\nsimilarity of 0.60 after alignment), suggesting a shared latent code of\nmaterial well-being while retaining complementary details, consistent with the\nPlatonic Representation Hypothesis. Although LLM-only text outperforms\nagent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest\ngains from combining agent data in some splits weakly support the notion that\nagent-gathered information introduces unique representational structures not\nfully captured by static LLM knowledge. Third, we release a large-scale\nmultimodal dataset comprising more than 60,000 DHS clusters linked to satellite\nimages, LLM-generated descriptions, and agent-retrieved texts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01302", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01302", "abs": "https://arxiv.org/abs/2508.01302", "authors": ["Chenming Tang", "Yutong Yang", "Yunfang Wu"], "title": "KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference", "comment": "Preprint", "summary": "Knowledge editing aims to modify outdated knowledge in large language models\n(LLMs) efficiently while retaining their powerful capabilities. Most existing\nmethods rely on either parameter-level editing or retrieval-based approaches.\nIn this work, we propose Knowledge Editing alignment with Diverse Augmentation\nand Self-adaptive inference (KEDAS) to better align LLMs with knowledge\nediting. In the alignment phase, LLMs learn to apply in-context edited\nknowledge via low-rank adaptation. During editing, we design a diverse edit\naugmentation technique to improve the recall of edits. After that, a\nself-adaptive post-alignment inference mechanism is proposed, in which a\nfilter-based smart retriever is employed to perform a dynamic selection of\ninference routing. Specifically, irrelevant queries will go through the\noriginal pre-alignment model directly, while relevant ones, together with their\nrelated edits, go through the model with aligned adapters activated. In\nexperiments, KEDAS secures the highest overall performance scores in 35 out of\n36 cases across four datasets with three LLMs on three settings, surpassing its\nstrong knowledge editing alignment counterpart by about 19.8 harmonic mean\nscores of edit success, locality and portability and outperforming both\nparameter editing and retrieval-based baselines significantly. Analysis of\ncomputational cost and performance on general tasks further validates the\nrobustness and efficiency of KEDAS, indicating that it presents an ideal\nparadigm of knowledge editing alignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01370", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.01370", "abs": "https://arxiv.org/abs/2508.01370", "authors": ["Roman Koshkin", "Pengyu Dai", "Nozomi Fujikawa", "Masahito Togami", "Marco Visentini-Scarzanella"], "title": "MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis", "comment": null, "summary": "We present an autonomous framework that leverages Large Language Models\n(LLMs) to automate end-to-end business analysis and market report generation.\nAt its core, the system employs specialized agents - Researcher, Reviewer,\nWriter, and Retriever - that collaborate to analyze data and produce\ncomprehensive reports. These agents learn from real professional consultants'\npresentation materials at Amazon through in-context learning to replicate\nprofessional analytical methodologies. The framework executes a multi-step\nprocess: querying databases, analyzing data, generating insights, creating\nvisualizations, and composing market reports. We also introduce a novel\nLLM-based evaluation system for assessing report quality, which shows alignment\nwith expert human evaluations. Building on these evaluations, we implement an\niterative improvement mechanism that optimizes report quality through automated\nreview cycles. Experimental results show that report quality can be improved by\nboth automated review cycles and consultants' unstructured knowledge. In\nexperimental validation, our framework generates detailed 6-page reports in 7\nminutes at a cost of approximately \\$1. Our work could be an important step to\nautomatically create affordable market insights.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01119", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01119", "abs": "https://arxiv.org/abs/2508.01119", "authors": ["Saba Ahmadi", "Rabiul Awal", "Ankur Sikarwar", "Amirhossein Kazemnejad", "Ge Ya Luo", "Juan A. Rodriguez", "Sai Rajeswar", "Siva Reddy", "Christopher Pal", "Benno Krojer", "Aishwarya Agrawal"], "title": "The Promise of RL for Autoregressive Image Editing", "comment": null, "summary": "We explore three strategies to enhance performance on a wide range of image\nediting tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and\nChain-of-Thought (CoT) reasoning. In order to study all these components in one\nconsistent framework, we adopt an autoregressive multimodal model that\nprocesses textual and visual tokens in a unified manner. We find RL combined\nwith a large multi-modal LLM verifier to be the most effective of these\nstrategies. As a result, we release EARL: Editing with Autoregression and RL, a\nstrong RL-based image editing model that performs competitively on a diverse\nrange of edits compared to strong baselines, despite using much less training\ndata. Thus, EARL pushes the frontier of autoregressive multimodal models on\nimage editing. We release our code, training data, and trained models at\nhttps://github.com/mair-lab/EARL.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01273", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01273", "abs": "https://arxiv.org/abs/2508.01273", "authors": ["Xianda Zheng", "Zijian Huang", "Meng-Fen Chiang", "Michael J. Witbrock", "Kaiqi Zhao"], "title": "KCR: Resolving Long-Context Knowledge Conflicts via Reasoning in LLMs", "comment": null, "summary": "Knowledge conflicts commonly arise across diverse sources, and their\nprevalence has increased with the advent of LLMs. When dealing with conflicts\nbetween multiple contexts, also known as \\emph{inter-context knowledge\nconflicts}, LLMs are often confused by lengthy and conflicting contexts. To\naddress this challenge, we propose the Knowledge Conflict Reasoning (KCR)\nframework, which enhances the ability of LLMs to resolve conflicting knowledge.\nThe key idea of KCR is to train backbone LLMs to establish a correct reasoning\nprocess by rewarding them for selecting and adhering to the context with\nstronger logical consistency when presented with conflicting contexts.\nSpecifically, we first extract reasoning paths, represented by either text or\nlocal knowledge graphs, from the conflicting long contexts. Subsequently, we\nemploy Reinforcement Learning to encourage the model to learn the paradigm of\nreasoning process that follows correct reasoning paths rather than the\nincorrect counterparts. This enables the backbone models to genuinely acquire\nthe capability to resolve inter-context knowledge conflicts within long\ncontexts. Experimental results demonstrate that our framework significantly\nimproves the ability of various backbone models to resolve knowledge conflicts\nin long-context scenarios, yielding substantial performance gains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01137", "abs": "https://arxiv.org/abs/2508.01137", "authors": ["Zeduo Zhang", "Yalda Mohsenzadeh"], "title": "Semi-Supervised Anomaly Detection in Brain MRI Using a Domain-Agnostic Deep Reinforcement Learning Approach", "comment": "34 pages, 6 figures and 4 tables in main text, 17 pages supplementary\n  material with 3 tables and 3 figures; Submitted to Radiology: Artificial\n  Intelligence", "summary": "To develop a domain-agnostic, semi-supervised anomaly detection framework\nthat integrates deep reinforcement learning (DRL) to address challenges such as\nlarge-scale data, overfitting, and class imbalance, focusing on brain MRI\nvolumes. This retrospective study used publicly available brain MRI datasets\ncollected between 2005 and 2021. The IXI dataset provided 581 T1-weighted and\n578 T2-weighted MRI volumes (from healthy subjects) for training, while the\nBraTS 2021 dataset provided 251 volumes for validation and 1000 for testing\n(unhealthy subjects with Glioblastomas). Preprocessing included normalization,\nskull-stripping, and co-registering to a uniform voxel size. Experiments were\nconducted on both T1- and T2-weighted modalities. Additional experiments and\nablation analyses were also carried out on the industrial datasets. The\nproposed method integrates DRL with feature representations to handle label\nscarcity, large-scale data and overfitting. Statistical analysis was based on\nseveral detection and segmentation metrics including AUROC and Dice score. The\nproposed method achieved an AUROC of 88.7% (pixel-level) and 96.7%\n(image-level) on brain MRI datasets, outperforming State-of-The-Art (SOTA)\nmethods. On industrial surface datasets, the model also showed competitive\nperformance (AUROC = 99.8% pixel-level, 99.3% image-level) on MVTec AD dataset,\nindicating strong cross-domain generalization. Studies on anomaly sample size\nshowed a monotonic increase in AUROC as more anomalies were seen, without\nevidence of overfitting or additional computational cost. The domain-agnostic\nsemi-supervised approach using DRL shows significant promise for MRI anomaly\ndetection, achieving strong performance on both medical and industrial\ndatasets. Its robustness, generalizability and efficiency highlight its\npotential for real-world clinical applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01285", "categories": ["cs.AI", "cs.ET", "cs.IR", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.01285", "abs": "https://arxiv.org/abs/2508.01285", "authors": ["Yujing Ke", "Kevin George", "Kathan Pandya", "David Blumenthal", "Maximilian Sprang", "Gerrit Großmann", "Sebastian Vollmer", "David Antony Selby"], "title": "BioDisco: Multi-agent hypothesis generation with dual-mode evidence, iterative feedback and temporal evaluation", "comment": "7 pages main content + 11 pages appendices", "summary": "Identifying novel hypotheses is essential to scientific research, yet this\nprocess risks being overwhelmed by the sheer volume and complexity of available\ninformation. Existing automated methods often struggle to generate novel and\nevidence-grounded hypotheses, lack robust iterative refinement and rarely\nundergo rigorous temporal evaluation for future discovery potential. To address\nthis, we propose BioDisco, a multi-agent framework that draws upon language\nmodel-based reasoning and a dual-mode evidence system (biomedical knowledge\ngraphs and automated literature retrieval) for grounded novelty, integrates an\ninternal scoring and feedback loop for iterative refinement, and validates\nperformance through pioneering temporal and human evaluations and a\nBradley-Terry paired comparison model to provide statistically-grounded\nassessment. Our evaluations demonstrate superior novelty and significance over\nablated configurations representative of existing agentic architectures.\nDesigned for flexibility and modularity, BioDisco allows seamless integration\nof custom language models or knowledge graphs, and can be run with just a few\nlines of code. We anticipate researchers using this practical tool as a\ncatalyst for the discovery of new hypotheses.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01151", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01151", "abs": "https://arxiv.org/abs/2508.01151", "authors": ["Yu Lei", "Jinbin Bai", "Qingyu Shi", "Aosong Feng", "Kaidong Yu"], "title": "Personalized Safety Alignment for Text-to-Image Diffusion Models", "comment": "14 pages, 8 figures, 4 tables", "summary": "Text-to-image diffusion models have revolutionized visual content generation,\nbut current safety mechanisms apply uniform standards that often fail to\naccount for individual user preferences. These models overlook the diverse\nsafety boundaries shaped by factors like age, mental health, and personal\nbeliefs. To address this, we propose Personalized Safety Alignment (PSA), a\nframework that allows user-specific control over safety behaviors in generative\nmodels. PSA integrates personalized user profiles into the diffusion process,\nadjusting the model's behavior to match individual safety preferences while\npreserving image quality. We introduce a new dataset, Sage, which captures\nuser-specific safety preferences and incorporates these profiles through a\ncross-attention mechanism. Experiments show that PSA outperforms existing\nmethods in harmful content suppression and aligns generated content better with\nuser constraints, achieving higher Win Rate and Pass Rate scores. Our code,\ndata, and models are publicly available at\nhttps://torpedo2648.github.io/PSAlign/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01475", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01475", "abs": "https://arxiv.org/abs/2508.01475", "authors": ["Zhen Wu", "Ritam Dutt", "Luke M. Breitfeller", "Armineh Nourbakhsh", "Siddharth Parekh", "Carolyn Rosé"], "title": "$R^2$-CoD: Understanding Text-Graph Complementarity in Relational Reasoning via Knowledge Co-Distillation", "comment": null, "summary": "Relational reasoning lies at the core of many NLP tasks, drawing on\ncomplementary signals from text and graphs. While prior research has\ninvestigated how to leverage this dual complementarity, a detailed and\nsystematic understanding of text-graph interplay and its effect on hybrid\nmodels remains underexplored. We take an analysis-driven approach to\ninvestigate text-graph representation complementarity via a unified\narchitecture that supports knowledge co-distillation (CoD). We explore five\ntasks involving relational reasoning that differ in how text and graph\nstructures encode the information needed to solve that task. By tracking how\nthese dual representations evolve during training, we uncover interpretable\npatterns of alignment and divergence, and provide insights into when and why\ntheir integration is beneficial.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01739", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01739", "abs": "https://arxiv.org/abs/2508.01739", "authors": ["Cheng Wang", "ziru Liu", "Pengcheng Tang", "Mingyu Zhang", "Quanyu Dai", "Yue Zhu"], "title": "Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction", "comment": null, "summary": "Identifying user preferences in dialogue systems is a pivotal aspect of\nproviding satisfying services. Current research shows that using large language\nmodels (LLMs) to fine-tune a task-specific preference extractor yields\nexcellent results in terms of accuracy and generalization. However, the primary\nchallenge stems from the inherent difficulty in obtaining high-quality labeled\nmulti-turn dialogue data. Accurately tracking user preference transitions\nacross turns not only demands intensive domain expertise and contextual\nconsistency maintenance for annotators (termed \\textbf{``Annotating\nDisaster''}) but also complicates model training due to error propagation in\nsequential dependency learning. Inspired by the observation that multi-turn\npreference extraction can be decomposed into iterative executions of one-turn\nextraction processes. We propose a novel dialogue data generation framework\nnamed \\textbf{IterChat}. First, we construct a new data format that categorizes\nthe dialogue data into attributed historical preferences and one-turn\ndialogues. This reduces the probability of annotation errors and improves\nannotation efficiency. Then, to generate a high-quality and diverse dialogue\ndataset, we adopt GPT4 to pre-define the preference slots in the target\npreference extractor task and then randomly sample the subset of the slots and\ntheir corresponding schema values to create the dialogue datasets. Experimental\nresults indicate that fine-tuning or only few-shot prompting with the new\ndialogue format yields superior performance compared to the original multi-turn\ndialogues. Additionally, the new data format improves annotator efficiency with\na win rate of 28.4\\% higher than the original multi-turn dialogues.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "consistency", "accuracy", "dialogue"], "score": 5}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01561", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01561", "abs": "https://arxiv.org/abs/2508.01561", "authors": ["Zijian Guo", "İlker Işık", "H. M. Sabbir Ahmad", "Wenchao Li"], "title": "One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear Temporal Logic Requirements in Multi-Task Reinforcement Learning", "comment": null, "summary": "Generalizing to complex and temporally extended task objectives and safety\nconstraints remains a critical challenge in reinforcement learning (RL). Linear\ntemporal logic (LTL) offers a unified formalism to specify such requirements,\nyet existing methods are limited in their abilities to handle nested\nlong-horizon tasks and safety constraints, and cannot identify situations when\na subgoal is not satisfiable and an alternative should be sought. In this\npaper, we introduce GenZ-LTL, a method that enables zero-shot generalization to\narbitrary LTL specifications. GenZ-LTL leverages the structure of B\\\"uchi\nautomata to decompose an LTL task specification into sequences of reach-avoid\nsubgoals. Contrary to the current state-of-the-art method that conditions on\nsubgoal sequences, we show that it is more effective to achieve zero-shot\ngeneralization by solving these reach-avoid problems \\textit{one subgoal at a\ntime} through proper safe RL formulations. In addition, we introduce a novel\nsubgoal-induced observation reduction technique that can mitigate the\nexponential complexity of subgoal-state combinations under realistic\nassumptions. Empirical results show that GenZ-LTL substantially outperforms\nexisting methods in zero-shot generalization to unseen LTL specifications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01225", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01225", "abs": "https://arxiv.org/abs/2508.01225", "authors": ["Xinyu Chen", "Haotian Zhai", "Can Zhang", "Xiupeng Shi", "Ruirui Li"], "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models", "comment": "Accepted by ICCV 2025", "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01815", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01815", "abs": "https://arxiv.org/abs/2508.01815", "authors": ["Yang Zhao", "Chengxiao Dai", "Wei Zhuo", "Tan Chuan Fu", "Yue Xiu", "Dusit Niyato", "Jonathan Z. Low", "Eugene Ho Hong Zhuang", "Daren Zong Loong Tan"], "title": "AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy", "comment": null, "summary": "Question answering over heterogeneous knowledge graphs (KGQA) involves\nreasoning across diverse schemas, incomplete alignments, and distributed data\nsources. Existing text-to-SPARQL approaches rely on large-scale domain-specific\nfine-tuning or operate within single-graph settings, limiting their\ngeneralizability in low-resource domains and their ability to handle queries\nspanning multiple graphs. These challenges are particularly relevant in domains\nsuch as the circular economy, where information about classifications,\nprocesses, and emissions is distributed across independently curated knowledge\ngraphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes\nKGQA into subtasks managed by specialized agents responsible for retrieval,\nquery generation, and verification. A scheduler assigns subgoals to different\ngraphs using weak-to-strong alignment strategies. A two-stage verifier detects\nstructurally invalid and semantically underspecified queries through symbolic\nvalidation and counterfactual consistency checks. Experiments on real-world\ncircular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy\nby 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing\nthe average prompt length by 46.4%. These results demonstrate the benefits of\nagent-based schema-aware reasoning for scalable KGQA and support\ndecision-making in sustainability domains through robust cross-graph reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01254", "abs": "https://arxiv.org/abs/2508.01254", "authors": ["Zihan Li", "Wei Sun", "Jing Hu", "Jianhua Yin", "Jianlong Wu", "Liqiang Nie"], "title": "Self-Enhanced Image Clustering with Cross-Modal Semantic Consistency", "comment": null, "summary": "While large language-image pre-trained models like CLIP offer powerful\ngeneric features for image clustering, existing methods typically freeze the\nencoder. This creates a fundamental mismatch between the model's task-agnostic\nrepresentations and the demands of a specific clustering task, imposing a\nceiling on performance. To break this ceiling, we propose a self-enhanced\nframework based on cross-modal semantic consistency for efficient image\nclustering. Our framework first builds a strong foundation via Cross-Modal\nSemantic Consistency and then specializes the encoder through Self-Enhancement.\nIn the first stage, we focus on Cross-Modal Semantic Consistency. By mining\nconsistency between generated image-text pairs at the instance, cluster\nassignment, and cluster center levels, we train lightweight clustering heads to\nalign with the rich semantics of the pre-trained model. This alignment process\nis bolstered by a novel method for generating higher-quality cluster centers\nand a dynamic balancing regularizer to ensure well-distributed assignments. In\nthe second stage, we introduce a Self-Enhanced fine-tuning strategy. The\nwell-aligned model from the first stage acts as a reliable pseudo-label\ngenerator. These self-generated supervisory signals are then used to feed back\nthe efficient, joint optimization of the vision encoder and clustering heads,\nunlocking their full potential. Extensive experiments on six mainstream\ndatasets show that our method outperforms existing deep clustering methods by\nsignificant margins. Notably, our ViT-B/32 model already matches or even\nsurpasses the accuracy of state-of-the-art methods built upon the far larger\nViT-L/14.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01275", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01275", "abs": "https://arxiv.org/abs/2508.01275", "authors": ["Chuang-Wei Liu", "Mingjian Sun", "Cairong Zhao", "Hanli Wang", "Alexander Dvorkovich", "Rui Fan"], "title": "Integrating Disparity Confidence Estimation into Relative Depth Prior-Guided Unsupervised Stereo Matching", "comment": "13 pages", "summary": "Unsupervised stereo matching has garnered significant attention for its\nindependence from costly disparity annotations. Typical unsupervised methods\nrely on the multi-view consistency assumption for training networks, which\nsuffer considerably from stereo matching ambiguities, such as repetitive\npatterns and texture-less regions. A feasible solution lies in transferring 3D\ngeometric knowledge from a relative depth map to the stereo matching networks.\nHowever, existing knowledge transfer methods learn depth ranking information\nfrom randomly built sparse correspondences, which makes inefficient utilization\nof 3D geometric knowledge and introduces noise from mistaken disparity\nestimates. This work proposes a novel unsupervised learning framework to\naddress these challenges, which comprises a plug-and-play disparity confidence\nestimation algorithm and two depth prior-guided loss functions. Specifically,\nthe local coherence consistency between neighboring disparities and their\ncorresponding relative depths is first checked to obtain disparity confidence.\nAfterwards, quasi-dense correspondences are built using only confident\ndisparity estimates to facilitate efficient depth ranking learning. Finally, a\ndual disparity smoothness loss is proposed to boost stereo matching performance\nat disparity discontinuities. Experimental results demonstrate that our method\nachieves state-of-the-art stereo matching accuracy on the KITTI Stereo\nbenchmarks among all unsupervised stereo matching methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01293", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01293", "abs": "https://arxiv.org/abs/2508.01293", "authors": ["Ngoc Bui Lam Quang", "Nam Le Nguyen Binh", "Thanh-Huy Nguyen", "Le Thien Phuc Nguyen", "Quan Nguyen", "Ulas Bagci"], "title": "GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification", "comment": null, "summary": "Multiple Instance Learning (MIL) is the leading approach for whole slide\nimage (WSI) classification, enabling efficient analysis of gigapixel pathology\nslides. Recent work has introduced vision-language models (VLMs) into MIL\npipelines to incorporate medical knowledge through text-based class\ndescriptions rather than simple class names. However, when these methods rely\non large language models (LLMs) to generate clinical descriptions or use\nfixed-length prompts to represent complex pathology concepts, the limited token\ncapacity of VLMs often constrains the expressiveness and richness of the\nencoded class information. Additionally, descriptions generated solely by LLMs\nmay lack domain grounding and fine-grained medical specificity, leading to\nsuboptimal alignment with visual features. To address these challenges, we\npropose a vision-language MIL framework with two key contributions: (1) A\ngrounded multi-agent description generation system that leverages curated\npathology textbooks and agent specialization (e.g., morphology, spatial\ncontext) to produce accurate and diverse clinical descriptions; (2) A text\nencoding strategy using a list of descriptions rather than a single prompt,\ncapturing fine-grained and complementary clinical signals for better alignment\nwith visual features. Integrated into a VLM-MIL pipeline, our approach shows\nimproved performance over single-prompt class baselines and achieves results\ncomparable to state-of-the-art models, as demonstrated on renal and lung cancer\ndatasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02076", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.02076", "abs": "https://arxiv.org/abs/2508.02076", "authors": ["Yunhao Liang", "Yuan Qu", "Jingyuan Yang", "Shaochong Lin", "Zuo-Jun Max Shen"], "title": "Everyone Contributes! Incentivizing Strategic Cooperation in Multi-LLM Systems via Sequential Public Goods Games", "comment": null, "summary": "Coordinating multiple large language models (LLMs) to solve complex tasks\ncollaboratively poses a fundamental trade-off between the computation costs and\ncollective performance compared with individual model. We introduce a novel,\ngame-theoretically grounded reinforcement learning (RL) framework, the\nMulti-Agent Cooperation Sequential Public Goods Game (MAC-SPGG), to\nsystematically incentivize cooperation in multi-LLM ensembles. In MAC-SPGG, LLM\nagents move in sequence, observing predecessors' outputs and updating beliefs\nto condition their own contributions. By redesigning the public-goods reward,\neffortful contributions become the unique Subgame Perfect Nash Equilibrium\n(SPNE), which eliminates free-riding under traditional SPGG or PGG. Its\nsequential protocol replaces costly round-based information exchanges with a\nstreamlined decision flow, cutting communication overhead while retaining\nstrategic depth. We prove the existence and uniqueness of the SPNE under\nrealistic parameters, and empirically show that MAC-SPGG-trained ensembles\noutperform single-agent baselines, chain-of-thought prompting, and other\ncooperative methods, even achieving comparable performance to large-scale\nmodels across reasoning, math, code generation, and NLP tasks. Our results\nhighlight the power of structured, incentive-aligned MAC-SPGG cooperation for\nscalable and robust multi-agent language generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["code generation"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02120", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02120", "abs": "https://arxiv.org/abs/2508.02120", "authors": ["Linan Yue", "Yichao Du", "Yizhi Wang", "Weibo Gao", "Fangzhou Yao", "Li Wang", "Ye Liu", "Ziyu Xu", "Qi Liu", "Shimin Di", "Min-Ling Zhang"], "title": "Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models", "comment": null, "summary": "Recently, Large Reasoning Models (LRMs) have gradually become a research\nhotspot due to their outstanding performance in handling complex tasks. Among\nthem, DeepSeek R1 has garnered significant attention for its exceptional\nperformance and open-source nature, driving advancements in the research of\nR1-style LRMs. Unlike traditional Large Language Models (LLMs), these models\nenhance logical deduction and decision-making capabilities during reasoning by\nincorporating mechanisms such as long chain-of-thought and self-reflection\nthrough reinforcement learning. However, with the widespread application of\nthese models, the problem of overthinking has gradually emerged. Specifically,\nwhen generating answers, these models often construct excessively long\nreasoning chains with redundant or repetitive steps, which leads to reduced\nreasoning efficiency and may affect the accuracy of the final answer. To this\nend, various efficient reasoning methods have been proposed, aiming to reduce\nthe length of reasoning paths without compromising model performance and\nreasoning capability. By reviewing the current research advancements in the\nfield of efficient reasoning methods systematically, we categorize existing\nworks into two main directions based on the lens of single-model optimization\nversus model collaboration: (1) Efficient Reasoning with Single Model, which\nfocuses on improving the reasoning efficiency of individual models; and (2)\nEfficient Reasoning with Model Collaboration, which explores optimizing\nreasoning paths through collaboration among multiple models. Besides, we\nmaintain a public GitHub repository that tracks the latest progress in\nefficient reasoning methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02241", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02241", "abs": "https://arxiv.org/abs/2508.02241", "authors": ["Danial Namazifard", "Lukas Galke"], "title": "Isolating Culture Neurons in Multilingual Large Language Models", "comment": "18 pages, 13 figures", "summary": "Language and culture are deeply intertwined, yet it is so far unclear how and\nwhere multilingual large language models encode culture. Here, we extend upon\nan established methodology for identifying language-specific neurons and extend\nit to localize and isolate culture-specific neurons, carefully disentangling\ntheir overlap and interaction with language-specific neurons. To facilitate our\nexperiments, we introduce MUREL, a curated dataset of 85.2 million tokens\nspanning six different cultures. Our localization and intervention experiments\nshow that LLMs encode different cultures in distinct neuron populations,\npredominantly in upper layers, and that these culture neurons can be modulated\nindependently from language-specific neurons or those specific to other\ncultures. These findings suggest that cultural knowledge and propensities in\nmultilingual language models can be selectively isolated and edited - promoting\nfairness, inclusivity, and alignment. Code and data is available at\nhttps://github.com/namazifard/Culture_Neurons .", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02150", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02150", "abs": "https://arxiv.org/abs/2508.02150", "authors": ["Qingyu Ren", "Qianyu He", "Bowei Zhang", "Jie Zeng", "Jiaqing Liang", "Yanghua Xiao", "Weikang Zhou", "Zeye Sun", "Fei Yu"], "title": "Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following", "comment": null, "summary": "Reasoning models excel in complex problem solving but exhibit a concerning\ntrade off between reasoning capabilities and instruction following abilities.\nExisting approaches for improving instruction following rely on stronger\nexternal models, creating methodological bottlenecks and practical limitations\nincluding increased costs and accessibility constraints. We propose a\nself-supervised RL framework that leverages reasoning models' own internal\nsignals to improve instruction following capabilities without external\nsupervision. Extensive experiments demonstrate that our framework significantly\nimproves instruction following capabilities while maintaining reasoning\nperformance, offering a scalable and cost-effective approach to enhance\ninstruction following in reasoning models. The data and code are publicly\navailable at https://github.com/Rainier-rq/verl-if.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02260", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02260", "abs": "https://arxiv.org/abs/2508.02260", "authors": ["Jia Deng", "Jie Chen", "Zhipeng Chen", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning", "comment": "7 pages, 20 figures", "summary": "Recently, reinforcement learning with verifiable rewards (RLVR) has been\nwidely used for enhancing the reasoning abilities of large language models\n(LLMs). A core challenge in RLVR involves managing the exchange between entropy\nand performance of policies. Despite the importance of this exchange, a\nfine-grained understanding of when and how this exchange operates most\neffectively remains limited. To bridge this gap, we conduct a systematic\nempirical analysis of the entropy-performance exchange mechanism of RLVR across\ndifferent levels of granularity. Specifically, we first divide the training\nprocess into two distinct stages based on entropy dynamics, i.e., rising stage\nand plateau stage, and then systematically investigate how this mechanism\nvaries across stage-level, instance-level, and token-level granularitiess. Our\nanalysis reveals that, in the rising stage, entropy reduction in negative\nsamples facilitates the learning of effective reasoning patterns, which in turn\ndrives rapid performance gains. Moreover, in the plateau stage, learning\nefficiency strongly correlates with high-entropy tokens present in\nlow-perplexity samples and those located at the end of sequences. Motivated by\nthese findings, we propose two methods that dynamically adjust the reward\nsignal using perplexity and positional information to focus RL updates on\ntokens that exhibit high learning potential, achieving improvements compared to\nthe baseline methods on various LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02178", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02178", "abs": "https://arxiv.org/abs/2508.02178", "authors": ["Jialiang Hong", "Taihang Zhen", "Kai Chen", "Jiaheng Liu", "Wenpeng Zhu", "Jing Huo", "Yang Gao", "Depeng Wang", "Haitao Wan", "Xi Yang", "Boyan Wang", "Fanyu Meng"], "title": "Reconsidering Overthinking: Penalizing Internal and External Redundancy in CoT Reasoning", "comment": null, "summary": "Large Reasoning Models (LRMs) often produce excessively verbose reasoning\ntraces, a phenomenon known as overthinking, which hampers both efficiency and\ninterpretability. Prior works primarily address this issue by reducing response\nlength, without fully examining the underlying semantic structure of the\nreasoning process. In this paper, we revisit overthinking by decomposing it\ninto two distinct forms: internal redundancy, which consists of\nlow-contribution reasoning steps within the first correct solution (FCS), and\nexternal redundancy, which refers to unnecessary continuation after the FCS. To\nmitigate both forms, we propose a dual-penalty reinforcement learning\nframework. For internal redundancy, we adopt a sliding-window semantic analysis\nto penalize low-gain reasoning steps that contribute little toward reaching the\ncorrect answer. For external redundancy, we penalize its proportion beyond the\nFCS to encourage earlier termination. Our method significantly compresses\nreasoning traces with minimal accuracy loss, and generalizes effectively to\nout-of-domain tasks such as question answering and code generation. Crucially,\nwe find that external redundancy can be safely removed without degrading\nperformance, whereas internal redundancy must be reduced more cautiously to\navoid impairing correctness. These findings suggest that our method not only\nimproves reasoning efficiency but also enables implicit, semantic-aware control\nover Chain-of-Thought length, paving the way for more concise and interpretable\nLRMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering", "code generation"], "score": 3}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02292", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02292", "abs": "https://arxiv.org/abs/2508.02292", "authors": ["Wentao Zhang", "Yilei Zhao", "Chuqiao Zong", "Xinrun Wang", "Bo An"], "title": "FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI Research and Deployment", "comment": null, "summary": "Financial AI holds great promise for transforming modern finance, with the\npotential to support a wide range of tasks such as market forecasting,\nportfolio management, quantitative trading, and automated analysis. However,\nexisting platforms remain limited in task coverage, lack robust multimodal data\nintegration, and offer insufficient support for the training and deployment of\nlarge language models (LLMs). In response to these limitations, we present\nFinWorld, an all-in-one open-source platform that provides end-to-end support\nfor the entire financial AI workflow, from data acquisition to experimentation\nand deployment. FinWorld distinguishes itself through native integration of\nheterogeneous financial data, unified support for diverse AI paradigms, and\nadvanced agent automation, enabling seamless development and deployment.\nLeveraging data from 2 representative markets, 4 stock pools, and over 800\nmillion financial data points, we conduct comprehensive experiments on 4 key\nfinancial AI tasks. These experiments systematically evaluate deep learning and\nreinforcement learning algorithms, with particular emphasis on RL-based\nfinetuning for LLMs and LLM Agents. The empirical results demonstrate that\nFinWorld significantly enhances reproducibility, supports transparent\nbenchmarking, and streamlines deployment, thereby providing a strong foundation\nfor future research and real-world applications. Code is available at\nGithub~\\footnote{https://github.com/DVampire/FinWorld}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02344", "abs": "https://arxiv.org/abs/2508.02344", "authors": ["Xingchen Zou", "Yuhao Yang", "Zheng Chen", "Xixuan Hao", "Yiqi Chen", "Chao Huang", "Yuxuan Liang"], "title": "Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems", "comment": null, "summary": "Traffic signal control (TSC) is vital for mitigating congestion and\nsustaining urban mobility. In this paper, we introduce Traffic-R1, a foundation\nmodel with human-like reasoning for TSC systems. Our model is developed through\nself-exploration and iteration of reinforced large language models (LLMs) with\nexpert guidance in a simulated traffic environment. Compared to traditional\nreinforcement learning (RL) and recent LLM-based methods, Traffic-R1 offers\nthree significant advantages. First, Traffic-R1 delivers zero-shot\ngeneralisation, transferring unchanged to new road networks and\nout-of-distribution incidents by utilizing its internal traffic control\npolicies and human-like reasoning. Second, its 3B-parameter architecture is\nlightweight enough for real-time inference on mobile-class chips, enabling\nlarge-scale edge deployment. Third, Traffic-R1 provides an explainable TSC\nprocess and facilitates multi-intersection communication through its\nself-iteration and a new synchronous communication network. Extensive\nbenchmarks demonstrate that Traffic-R1 sets a new state of the art,\noutperforming strong baselines and training-intensive RL controllers. In\npractice, the model now manages signals for more than 55,000 drivers daily,\nshortening average queues by over 5% and halving operator workload. Our\ncheckpoint is available at https://huggingface.co/Season998/Traffic-R1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01423", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.01423", "abs": "https://arxiv.org/abs/2508.01423", "authors": ["Shitian Yang", "Deyu Li", "Xiaoke Jiang", "Lei Zhang"], "title": "3DRot: 3D Rotation Augmentation for RGB-Based 3D Tasks", "comment": null, "summary": "RGB-based 3D tasks, e.g., 3D detection, depth estimation, 3D keypoint\nestimation, still suffer from scarce, expensive annotations and a thin\naugmentation toolbox, since most image transforms, including resize and\nrotation, disrupt geometric consistency. In this paper, we introduce 3DRot, a\nplug-and-play augmentation that rotates and mirrors images about the camera's\noptical center while synchronously updating RGB images, camera intrinsics,\nobject poses, and 3D annotations to preserve projective geometry-achieving\ngeometry-consistent rotations and reflections without relying on any scene\ndepth. We validate 3DRot with a classical 3D task, monocular 3D detection. On\nSUN RGB-D dataset, 3DRot raises $IoU_{3D}$ from 43.21 to 44.51, cuts rotation\nerror (ROT) from 22.91$^\\circ$ to 20.93$^\\circ$, and boosts $mAP_{0.5}$ from\n35.70 to 38.11. As a comparison, Cube R-CNN adds 3 other datasets together with\nSUN RGB-D for monocular 3D estimation, with a similar mechanism and test\ndataset, increases $IoU_{3D}$ from 36.2 to 37.8, boosts $mAP_{0.5}$ from 34.7\nto 35.4. Because it operates purely through camera-space transforms, 3DRot is\nreadily transferable to other 3D tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01435", "abs": "https://arxiv.org/abs/2508.01435", "authors": ["Zhuoran Peng", "Yiqing Shen"], "title": "Hyperspectral Image Recovery Constrained by Multi-Granularity Non-Local Self-Similarity Priors", "comment": null, "summary": "Hyperspectral image (HSI) recovery, as an upstream image processing task,\n  holds significant importance for downstream tasks such as classification,\n  segmentation, and detection. In recent years, HSI recovery methods based on\n  non-local prior representations have demonstrated outstanding performance.\nHowever,\n  these methods employ a fixed-format factor to represent the non-local\nself-similarity\n  tensor groups, making them unable to adapt to diverse missing scenarios. To\naddress\n  this issue, we introduce the concept of granularity in tensor decomposition\nfor the first\n  time and propose an HSI recovery model constrained by multi-granularity\nnon-local\n  self-similarity priors. Specifically, the proposed model alternately performs\n  coarse-grained decomposition and fine-grained decomposition on the non-local\n  self-similarity tensor groups. Among them, the coarse-grained decomposition\nbuilds\n  upon Tucker tensor decomposition, which extracts global structural\ninformation of the\n  image by performing singular value shrinkage on the mode-unfolded matrices.\nThe\n  fine-grained decomposition employs the FCTN decomposition, capturing local\ndetail\n  information through modeling pairwise correlations among factor tensors. This\n  architectural approach achieves a unified representation of global, local,\nand non-local\n  priors for HSIs. Experimental results demonstrate that the model has strong\n  applicability and exhibits outstanding recovery effects in various types of\nmissing\n  scenes such as pixels and stripes.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01460", "abs": "https://arxiv.org/abs/2508.01460", "authors": ["Sikha O K", "Meritxell Riera-Marín", "Adrian Galdran", "Javier García Lopez", "Julia Rodríguez-Comas", "Gemma Piella", "Miguel A. González Ballester"], "title": "Uncertainty-Aware Segmentation Quality Prediction via Deep Learning Bayesian Modeling: Comprehensive Evaluation and Interpretation on Skin Cancer and Liver Segmentation", "comment": null, "summary": "Image segmentation is a critical step in computational biomedical image\nanalysis, typically evaluated using metrics like the Dice coefficient during\ntraining and validation. However, in clinical settings without manual\nannotations, assessing segmentation quality becomes challenging, and models\nlacking reliability indicators face adoption barriers. To address this gap, we\npropose a novel framework for predicting segmentation quality without requiring\nground truth annotations during test time. Our approach introduces two\ncomplementary frameworks: one leveraging predicted segmentation and uncertainty\nmaps, and another integrating the original input image, uncertainty maps, and\npredicted segmentation maps. We present Bayesian adaptations of two benchmark\nsegmentation models-SwinUNet and Feature Pyramid Network with ResNet50-using\nMonte Carlo Dropout, Ensemble, and Test Time Augmentation to quantify\nuncertainty. We evaluate four uncertainty estimates: confidence map, entropy,\nmutual information, and expected pairwise Kullback-Leibler divergence on 2D\nskin lesion and 3D liver segmentation datasets, analyzing their correlation\nwith segmentation quality metrics. Our framework achieves an R2 score of 93.25\nand Pearson correlation of 96.58 on the HAM10000 dataset, outperforming\nprevious segmentation quality assessment methods. For 3D liver segmentation,\nTest Time Augmentation with entropy achieves an R2 score of 85.03 and a Pearson\ncorrelation of 65.02, demonstrating cross-modality robustness. Additionally, we\npropose an aggregation strategy that combines multiple uncertainty estimates\ninto a single score per image, offering a more robust and comprehensive\nassessment of segmentation quality. Finally, we use Grad-CAM and UMAP-based\nembedding analysis to interpret the model's behavior and reliability,\nhighlighting the impact of uncertainty integration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "correlation", "reliability"], "score": 5}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02555", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.02555", "abs": "https://arxiv.org/abs/2508.02555", "authors": ["Motaz Saad", "David Langlois", "Kamel Smaili"], "title": "Building and Aligning Comparable Corpora", "comment": "27 pages, 11 figures", "summary": "Comparable corpus is a set of topic aligned documents in multiple languages,\nwhich are not necessarily translations of each other. These documents are\nuseful for multilingual natural language processing when there is no parallel\ntext available in some domains or languages. In addition, comparable documents\nare informative because they can tell what is being said about a topic in\ndifferent languages. In this paper, we present a method to build comparable\ncorpora from Wikipedia encyclopedia and EURONEWS website in English, French and\nArabic languages. We further experiment a method to automatically align\ncomparable documents using cross-lingual similarity measures. We investigate\ntwo cross-lingual similarity measures to align comparable documents. The first\nmeasure is based on bilingual dictionary, and the second measure is based on\nLatent Semantic Indexing (LSI). Experiments on several corpora show that the\nCross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure.\nFinally, we collect English and Arabic news documents from the British\nBroadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively.\nThen we use the CL-LSI similarity measure to automatically align comparable\ndocuments of BBC and JSC. The evaluation of the alignment shows that CL-LSI is\nnot only able to align cross-lingual documents at the topic level, but also it\nis able to do this at the event level.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01582", "abs": "https://arxiv.org/abs/2508.01582", "authors": ["Xinhui Li", "Xinyu He", "Qiming Hu", "Xiaojie Guo"], "title": "Set Pivot Learning: Redefining Generalized Segmentation with Vision Foundation Models", "comment": null, "summary": "In this paper, we introduce, for the first time, the concept of Set Pivot\nLearning, a paradigm shift that redefines domain generalization (DG) based on\nVision Foundation Models (VFMs). Traditional DG assumes that the target domain\nis inaccessible during training, but the emergence of VFMs, trained on vast and\ndiverse data, renders this assumption unclear and obsolete. Traditional DG\nassumes that the target domain is inaccessible during training, but the\nemergence of VFMs, which are trained on vast and diverse datasets, renders this\nassumption unclear and obsolete. To address this challenge, we propose Set\nPivot Learning (SPL), a new definition of domain migration task based on VFMs,\nwhich is more suitable for current research and application requirements.\nUnlike conventional DG methods, SPL prioritizes adaptive refinement over rigid\ndomain transfer, ensuring continuous alignment with evolving real-world\nconditions. Specifically, SPL features two key attributes: (i) Dynamic\nadaptation, transitioning from static domain alignment to flexible, task-driven\nfeature optimization, enabling models to evolve with downstream scenarios; (ii)\nVFM-centric tuning, leveraging pretrained knowledge as a pivot to hone\ntask-specific representations while preserving cross-domain robustness.\nBuilding on SPL, we propose a Dynamic Prompt Fine-Tuning method, which combines\na Dynamic Class-aware Prompter with a Prompt-guided Feature Focuser, to elevate\nVFM performance in targeted scenarios. Extensive experiments on benchmark\ndatasets show the effectiveness of our method, highlighting its superiority\nover state-of-the-art methods, particularly in generalized segmentation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02573", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02573", "abs": "https://arxiv.org/abs/2508.02573", "authors": ["Jérémie Dentan", "Davide Buscaldi", "Sonia Vanier"], "title": "Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs", "comment": null, "summary": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted\nphenomenon involving distinct underlying mechanisms. We introduce a novel\nmethod to analyze the different forms of memorization described by the existing\ntaxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the\nattention weights of the LLM and evaluate the alignment between this taxonomy\nand the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect\ndistinct mechanisms within the attention blocks. We propose a new taxonomy that\nmaximizes alignment with the attention weights, consisting of three categories:\nmemorized samples that are guessed using language modeling abilities, memorized\nsamples that are recalled due to high duplication in the training set, and\nnon-memorized samples. Our results reveal that few-shot verbatim memorization\ndoes not correspond to a distinct attention mechanism. We also show that a\nsignificant proportion of extractable samples are in fact guessed by the model\nand should therefore be studied separately. Finally, we develop a custom visual\ninterpretability technique to localize the regions of the attention weights\ninvolved in each form of memorization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01587", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01587", "abs": "https://arxiv.org/abs/2508.01587", "authors": ["Mingyu Wang", "Haojie Liu", "Zhiyong Li", "Wei Jiang"], "title": "Lifelong Person Re-identification via Privacy-Preserving Data Replay", "comment": "15 pages, 11 figures", "summary": "Lifelong person re-identification (LReID) aims to incrementally accumulate\nknowledge across a sequence of tasks under domain shifts. Recently,\nreplay-based methods have demonstrated strong effectiveness in LReID by\nrehearsing past samples stored in an auxiliary memory. However, storing\nhistorical exemplars raises concerns over data privacy. To avoid this,\nexemplar-free approaches attempt to match the distribution of past data without\nstoring raw samples. Despite being privacy-friendly, these methods often suffer\nfrom performance degradation due to the forgetting of specific past knowledge\nrepresentations. To this end, we propose to condense information from\nsequential data into the pixel space in the replay memory, enabling\nPrivacy-Preserving Replay (Pr^2R). More specifically, by distilling the\ntraining characteristics of multiple real images into a single image, the\ncondensed samples undergo pixel-level changes. This not only protects the\nprivacy of the original data but also makes the replay samples more\nrepresentative for sequential tasks. During the style replay phase, we align\nthe current domain to the previous one while simultaneously adapting the replay\nsamples to match the style of the current domain. This dual-alignment strategy\neffectively mitigates both class-incremental challenges and forgetting caused\nby domain shifts. Extensive experiments on multiple benchmarks show that the\nproposed method significantly improves replay effectiveness while preserving\ndata privacy. Specifically, Pr^2R achieves 4% and 6% higher accuracy on\nsequential tasks compared to the current state-of-the-art and other\nreplay-based methods, respectively.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02631", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02631", "abs": "https://arxiv.org/abs/2508.02631", "authors": ["Zixi Li"], "title": "Pointer: Linear-Complexity Long-Range Modeling without Pre-training", "comment": "Submitted to Nordic AI Meet 2025", "summary": "We introduce Pointer, a novel architecture that achieves linear $O(NK)$\ncomplexity for long-range sequence modeling while maintaining superior\nperformance without requiring pre-training. Unlike standard attention\nmechanisms that compute $O(N^2)$ pairwise interactions, our approach uses\nlayer-wise pointer chaining where each layer's pointer selection depends on\nprevious layer's pointer positions, creating explicit long-distance connections\nthrough pointer chains. We demonstrate that this architecture achieves\n$2$--$10\\times$ speedup on long sequences compared to standard transformers,\nmaintains $>95\\%$ accuracy on copy tasks at distances up to 2048 tokens, and\nlearns interpretable pointer patterns that reveal structured dependency\nmodeling. Our experiments on efficiency benchmarks, long-range dependency\ntasks, and interpretability analysis show that Pointer offers a compelling\nalternative to attention mechanisms for scenarios requiring efficient\nlong-range modeling without pre-training dependencies.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01602", "abs": "https://arxiv.org/abs/2508.01602", "authors": ["Lubin Gan", "Jing Zhang", "Linhao Qu", "Yijun Wang", "Siying Wu", "Xiaoyan Sun"], "title": "Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained Patch-Text Alignment", "comment": null, "summary": "The fine-grained classification of brain tumor subtypes from\nhistopathological whole slide images is highly challenging due to subtle\nmorphological variations and the scarcity of annotated data. Although\nvision-language models have enabled promising zero-shot classification, their\nability to capture fine-grained pathological features remains limited,\nresulting in suboptimal subtype discrimination. To address these challenges, we\npropose the Fine-Grained Patch Alignment Network (FG-PAN), a novel zero-shot\nframework tailored for digital pathology. FG-PAN consists of two key modules:\n(1) a local feature refinement module that enhances patch-level visual features\nby modeling spatial relationships among representative patches, and (2) a\nfine-grained text description generation module that leverages large language\nmodels to produce pathology-aware, class-specific semantic prototypes. By\naligning refined visual features with LLM-generated fine-grained descriptions,\nFG-PAN effectively increases class separability in both visual and semantic\nspaces. Extensive experiments on multiple public pathology datasets, including\nEBRAINS and TCGA, demonstrate that FG-PAN achieves state-of-the-art performance\nand robust generalization in zero-shot brain tumor subtype classification.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01151", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01151", "abs": "https://arxiv.org/abs/2508.01151", "authors": ["Yu Lei", "Jinbin Bai", "Qingyu Shi", "Aosong Feng", "Kaidong Yu"], "title": "Personalized Safety Alignment for Text-to-Image Diffusion Models", "comment": "14 pages, 8 figures, 4 tables", "summary": "Text-to-image diffusion models have revolutionized visual content generation,\nbut current safety mechanisms apply uniform standards that often fail to\naccount for individual user preferences. These models overlook the diverse\nsafety boundaries shaped by factors like age, mental health, and personal\nbeliefs. To address this, we propose Personalized Safety Alignment (PSA), a\nframework that allows user-specific control over safety behaviors in generative\nmodels. PSA integrates personalized user profiles into the diffusion process,\nadjusting the model's behavior to match individual safety preferences while\npreserving image quality. We introduce a new dataset, Sage, which captures\nuser-specific safety preferences and incorporates these profiles through a\ncross-attention mechanism. Experiments show that PSA outperforms existing\nmethods in harmful content suppression and aligns generated content better with\nuser constraints, achieving higher Win Rate and Pass Rate scores. Our code,\ndata, and models are publicly available at\nhttps://torpedo2648.github.io/PSAlign/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01617", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01617", "abs": "https://arxiv.org/abs/2508.01617", "authors": ["Xuanzhao Dong", "Wenhui Zhu", "Xiwen Chen", "Zhipeng Wang", "Peijie Qiu", "Shao Tang", "Xin Li", "Yalin Wang"], "title": "LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding", "comment": null, "summary": "Autoregressive models (ARMs) have long dominated the landscape of biomedical\nvision-language models (VLMs). Recently, masked diffusion models such as LLaDA\nhave emerged as promising alternatives, yet their application in the biomedical\ndomain remains largely underexplored. To bridge this gap, we introduce\n\\textbf{LLaDA-MedV}, the first large language diffusion model tailored for\nbiomedical image understanding through vision instruction tuning. LLaDA-MedV\nachieves relative performance gains of 7.855\\% over LLaVA-Med and 1.867\\% over\nLLaDA-V in the open-ended biomedical visual conversation task, and sets new\nstate-of-the-art accuracy on the closed-form subset of three VQA benchmarks:\n84.93\\% on VQA-RAD, 92.31\\% on SLAKE, and 95.15\\% on PathVQA. Furthermore, a\ndetailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of\ngenerating reasonably longer responses by explicitly controlling response\nlength, which can lead to more informative outputs. We also conduct an in-depth\nanalysis of both the training and inference stages, highlighting the critical\nroles of initialization weight selection, fine-tuning strategies, and the\ninterplay between sampling steps and response repetition. The code and model\nweight is released at https://github.com/LLM-VLM-GSL/LLaDA-MedV.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01198", "abs": "https://arxiv.org/abs/2508.01198", "authors": ["Yige Li", "Peihai Jiang", "Jun Sun", "Peng Shu", "Tianming Liu", "Zhen Xiang"], "title": "Adaptive Content Restriction for Large Language Models via Suffix Optimization", "comment": "19 pages", "summary": "Large Language Models (LLMs) have demonstrated significant success across\ndiverse applications. However, enforcing content restrictions remains a\nsignificant challenge due to their expansive output space. One aspect of\ncontent restriction is preventing LLMs from generating harmful content via\nmodel alignment approaches such as supervised fine-tuning (SFT). Yet, the need\nfor content restriction may vary significantly across user groups, change\nrapidly over time, and not always align with general definitions of\nharmfulness. Applying SFT to each of these specific use cases is impractical\ndue to the high computational, data, and storage demands. Motivated by this\nneed, we propose a new task called \\textit{Adaptive Content Restriction}\n(AdaCoRe), which focuses on lightweight strategies -- methods without model\nfine-tuning -- to prevent deployed LLMs from generating restricted terms for\nspecific use cases. We propose the first method for AdaCoRe, named\n\\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to\nany prompt to a) prevent a target LLM from generating a set of restricted\nterms, while b) preserving the output quality. To evaluate AdaCoRe approaches,\nincluding our SOP, we create a new \\textit{Content Restriction Benchmark}\n(CoReBench), which contains 400 prompts for 80 restricted terms across 8\ncarefully selected categories. We demonstrate the effectiveness of SOP on\nCoReBench, which outperforms the system-level baselines such as system suffix\nby 15\\%, 17\\%, 10\\%, 9\\%, and 6\\% on average restriction rates for Gemma2-2B,\nMistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also\ndemonstrate that SOP is effective on POE, an online platform hosting various\ncommercial LLMs, highlighting its practicality in real-world scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "Vicuna"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01225", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01225", "abs": "https://arxiv.org/abs/2508.01225", "authors": ["Xinyu Chen", "Haotian Zhai", "Can Zhang", "Xiupeng Shi", "Ruirui Li"], "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models", "comment": "Accepted by ICCV 2025", "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01791", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01791", "abs": "https://arxiv.org/abs/2508.01791", "authors": ["Fatimah Mohamed Emad Elden"], "title": "CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase", "comment": null, "summary": "The field of Continuous Sign Language Recognition (CSLR) poses substantial\ntechnical challenges, including fluid inter-sign transitions, the absence of\ntemporal boundaries, and co-articulation effects. This paper, developed for the\nMSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of\nsigner-independent recognition to advance the generalization capabilities of\nCSLR systems across diverse signers. A data-centric methodology is proposed,\ncentered on systematic feature engineering, a robust preprocessing pipeline,\nand an optimized model architecture. Key contributions include a principled\nfeature selection process guided by Exploratory Data Analysis (EDA) to isolate\ncommunicative keypoints, a rigorous preprocessing pipeline incorporating\nDBSCAN-based outlier filtering and spatial normalization, and the novel\nCSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer\ndesign of the Conformer model, leveraging its capacity to model local temporal\ndependencies and global sequence context; a characteristic uniquely suited for\nthe spatio-temporal dynamics of sign language. The proposed methodology\nachieved a competitive performance, with a Word Error Rate (WER) of 5.60% on\nthe development set and 12.01% on the test set, a result that secured a 3rd\nplace ranking on the official competition platform. This research validates the\nefficacy of cross-domain architectural adaptation, demonstrating that the\nConformer model, originally conceived for speech recognition, can be\nsuccessfully repurposed to establish a new state-of-the-art performance in\nkeypoint-based CSLR.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01293", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01293", "abs": "https://arxiv.org/abs/2508.01293", "authors": ["Ngoc Bui Lam Quang", "Nam Le Nguyen Binh", "Thanh-Huy Nguyen", "Le Thien Phuc Nguyen", "Quan Nguyen", "Ulas Bagci"], "title": "GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification", "comment": null, "summary": "Multiple Instance Learning (MIL) is the leading approach for whole slide\nimage (WSI) classification, enabling efficient analysis of gigapixel pathology\nslides. Recent work has introduced vision-language models (VLMs) into MIL\npipelines to incorporate medical knowledge through text-based class\ndescriptions rather than simple class names. However, when these methods rely\non large language models (LLMs) to generate clinical descriptions or use\nfixed-length prompts to represent complex pathology concepts, the limited token\ncapacity of VLMs often constrains the expressiveness and richness of the\nencoded class information. Additionally, descriptions generated solely by LLMs\nmay lack domain grounding and fine-grained medical specificity, leading to\nsuboptimal alignment with visual features. To address these challenges, we\npropose a vision-language MIL framework with two key contributions: (1) A\ngrounded multi-agent description generation system that leverages curated\npathology textbooks and agent specialization (e.g., morphology, spatial\ncontext) to produce accurate and diverse clinical descriptions; (2) A text\nencoding strategy using a list of descriptions rather than a single prompt,\ncapturing fine-grained and complementary clinical signals for better alignment\nwith visual features. Integrated into a VLM-MIL pipeline, our approach shows\nimproved performance over single-prompt class baselines and achieves results\ncomparable to state-of-the-art models, as demonstrated on renal and lung cancer\ndatasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02419", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02419", "abs": "https://arxiv.org/abs/2508.02419", "authors": ["Haohan Zheng", "Zhenguo Zhang"], "title": "Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens", "comment": null, "summary": "Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01678", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01678", "abs": "https://arxiv.org/abs/2508.01678", "authors": ["Zhaochen Wang", "Yiwei Wang", "Yujun Cai"], "title": "Cure or Poison? Embedding Instructions Visually Alters Hallucination in Vision-Language Models", "comment": "Work in progress", "summary": "Vision-Language Models (VLMs) often suffer from hallucination, partly due to\nchallenges in aligning multimodal information. We propose Prompt-in-Image, a\nsimple method that embeds textual instructions directly into images. This\nremoves the need for separate text inputs and forces the model to process all\ncontent through the visual channel. We evaluate this method on three popular\nopen-source VLMs: Qwen2.5-VL, LLaVA-1.5, and InstructBLIP. The results reveal\nsharp differences. Prompt-in-Image improves Qwen2.5-VL's performance,\nincreasing POPE accuracy by 4.1 percent (from 80.2 percent to 84.3 percent) and\nalso reducing hallucination rates on MS-COCO. In contrast, LLaVA-1.5 and\nInstructBLIP experience a severe performance drop, with accuracy falling from\naround 84 percent to near-random levels. Through detailed analysis, we found\nthat CLIP-based encoders in LLaVA and InstructBLIP exhibit excessive attention\nbias toward embedded text regions, disrupting visual understanding. In\ncontrast, Qwen's vision encoder handles text-embedded images robustly.\nCrucially, Prompt-in-Image reduces Qwen's modality gap, enhancing cross-modal\nalignment by unifying information processing through a single modality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01698", "abs": "https://arxiv.org/abs/2508.01698", "authors": ["Zuhao Yang", "Jiahui Zhang", "Yingchen Yu", "Shijian Lu", "Song Bai"], "title": "Versatile Transition Generation with Image-to-Video Diffusion", "comment": null, "summary": "Leveraging text, images, structure maps, or motion trajectories as\nconditional guidance, diffusion models have achieved great success in automated\nand high-quality video generation. However, generating smooth and rational\ntransition videos given the first and last video frames as well as descriptive\ntext prompts is far underexplored. We present VTG, a Versatile Transition video\nGeneration framework that can generate smooth, high-fidelity, and semantically\ncoherent video transitions. VTG introduces interpolation-based initialization\nthat helps preserve object identity and handle abrupt content changes\neffectively. In addition, it incorporates dual-directional motion fine-tuning\nand representation alignment regularization to mitigate the limitations of\npre-trained image-to-video diffusion models in motion smoothness and generation\nfidelity, respectively. To evaluate VTG and facilitate future studies on\nunified transition generation, we collected TransitBench, a comprehensive\nbenchmark for transition generation covering two representative transition\ntasks: concept blending and scene transition. Extensive experiments show that\nVTG achieves superior transition performance consistently across all four\ntasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01711", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01711", "abs": "https://arxiv.org/abs/2508.01711", "authors": ["Bowen Yang", "Yun Cao", "Chen He", "Xiaosu Su"], "title": "GAID: Frame-Level Gated Audio-Visual Integration with Directional Perturbation for Text-Video Retrieval", "comment": null, "summary": "Text-to-video retrieval requires precise alignment between language and\ntemporally rich video signals. Existing methods predominantly exploit visual\ncues and often overlook complementary audio semantics or adopt coarse fusion\nstrategies, leading to suboptimal multimodal representations. We present GAID,\na framework that jointly address this gap via two key components: (i) a\nFrame-level Gated Fusion (FGF) that adaptively integrates audio and visual\nfeatures under textual guidance, enabling fine-grained temporal alignment; and\n(ii) a Directional Adaptive Semantic Perturbation (DASP) that injects\nstructure-aware perturbations into text embeddings, enhancing robustness and\ndiscrimination without incurring multi-pass inference. These modules complement\neach other -- fusion reduces modality gaps while perturbation regularizes\ncross-modal matching -- yielding more stable and expressive representations.\nExtensive experiments on MSR-VTT, DiDeMo, LSMDC, and VATEX show consistent\nstate-of-the-art results across all retrieval metrics with notable efficiency\ngains. Our code is available at https://github.com/YangBowenn/GAID.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01728", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01728", "abs": "https://arxiv.org/abs/2508.01728", "authors": ["Dahee Kwon", "Sehyun Lee", "Jaesik Choi"], "title": "Granular Concept Circuits: Toward a Fine-Grained Circuit Discovery for Concept Representations", "comment": "ICCV 2025 accepted paper", "summary": "Deep vision models have achieved remarkable classification performance by\nleveraging a hierarchical architecture in which human-interpretable concepts\nemerge through the composition of individual neurons across layers. Given the\ndistributed nature of representations, pinpointing where specific visual\nconcepts are encoded within a model remains a crucial yet challenging task. In\nthis paper, we introduce an effective circuit discovery method, called Granular\nConcept Circuit (GCC), in which each circuit represents a concept relevant to a\ngiven query. To construct each circuit, our method iteratively assesses\ninter-neuron connectivity, focusing on both functional dependencies and\nsemantic alignment. By automatically discovering multiple circuits, each\ncapturing specific concepts within that query, our approach offers a profound,\nconcept-wise interpretation of models and is the first to identify circuits\ntied to specific visual concepts at a fine-grained level. We validate the\nversatility and effectiveness of GCCs across various deep image classification\nmodels.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01742", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01742", "abs": "https://arxiv.org/abs/2508.01742", "authors": ["Qiaohui Chu", "Haoyu Zhang", "Meng Liu", "Yisen Feng", "Haoxiang Shi", "Liqiang Nie"], "title": "Intention-Guided Cognitive Reasoning for Egocentric Long-Term Action Anticipation", "comment": "Our code will be released upon acceptance", "summary": "Long-term action anticipation from egocentric video is critical for\napplications such as human-computer interaction and assistive technologies,\nwhere anticipating user intent enables proactive and context-aware AI\nassistance. However, existing approaches suffer from three key limitations: 1)\nunderutilization of fine-grained visual cues from hand-object interactions, 2)\nneglect of semantic dependencies between verbs and nouns, and 3) lack of\nexplicit cognitive reasoning, limiting generalization and long-term forecasting\nability. To overcome these challenges, we propose INSIGHT, a unified two-stage\nframework for egocentric action anticipation. In the first stage, INSIGHT\nfocuses on extracting semantically rich features from hand-object interaction\nregions and enhances action representations using a verb-noun co-occurrence\nmatrix. In the second stage, it introduces a reinforcement learning-based\nmodule that simulates explicit cognitive reasoning through a structured\nprocess: visual perception (think) -> intention inference (reason) -> action\nanticipation (answer). Extensive experiments on Ego4D, EPIC-Kitchens-55, and\nEGTEA Gaze+ benchmarks show that INSIGHT achieves state-of-the-art performance,\ndemonstrating its effectiveness and strong generalization capability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01678", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01678", "abs": "https://arxiv.org/abs/2508.01678", "authors": ["Zhaochen Wang", "Yiwei Wang", "Yujun Cai"], "title": "Cure or Poison? Embedding Instructions Visually Alters Hallucination in Vision-Language Models", "comment": "Work in progress", "summary": "Vision-Language Models (VLMs) often suffer from hallucination, partly due to\nchallenges in aligning multimodal information. We propose Prompt-in-Image, a\nsimple method that embeds textual instructions directly into images. This\nremoves the need for separate text inputs and forces the model to process all\ncontent through the visual channel. We evaluate this method on three popular\nopen-source VLMs: Qwen2.5-VL, LLaVA-1.5, and InstructBLIP. The results reveal\nsharp differences. Prompt-in-Image improves Qwen2.5-VL's performance,\nincreasing POPE accuracy by 4.1 percent (from 80.2 percent to 84.3 percent) and\nalso reducing hallucination rates on MS-COCO. In contrast, LLaVA-1.5 and\nInstructBLIP experience a severe performance drop, with accuracy falling from\naround 84 percent to near-random levels. Through detailed analysis, we found\nthat CLIP-based encoders in LLaVA and InstructBLIP exhibit excessive attention\nbias toward embedded text regions, disrupting visual understanding. In\ncontrast, Qwen's vision encoder handles text-embedded images robustly.\nCrucially, Prompt-in-Image reduces Qwen's modality gap, enhancing cross-modal\nalignment by unifying information processing through a single modality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01711", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01711", "abs": "https://arxiv.org/abs/2508.01711", "authors": ["Bowen Yang", "Yun Cao", "Chen He", "Xiaosu Su"], "title": "GAID: Frame-Level Gated Audio-Visual Integration with Directional Perturbation for Text-Video Retrieval", "comment": null, "summary": "Text-to-video retrieval requires precise alignment between language and\ntemporally rich video signals. Existing methods predominantly exploit visual\ncues and often overlook complementary audio semantics or adopt coarse fusion\nstrategies, leading to suboptimal multimodal representations. We present GAID,\na framework that jointly address this gap via two key components: (i) a\nFrame-level Gated Fusion (FGF) that adaptively integrates audio and visual\nfeatures under textual guidance, enabling fine-grained temporal alignment; and\n(ii) a Directional Adaptive Semantic Perturbation (DASP) that injects\nstructure-aware perturbations into text embeddings, enhancing robustness and\ndiscrimination without incurring multi-pass inference. These modules complement\neach other -- fusion reduces modality gaps while perturbation regularizes\ncross-modal matching -- yielding more stable and expressive representations.\nExtensive experiments on MSR-VTT, DiDeMo, LSMDC, and VATEX show consistent\nstate-of-the-art results across all retrieval metrics with notable efficiency\ngains. Our code is available at https://github.com/YangBowenn/GAID.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01728", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01728", "abs": "https://arxiv.org/abs/2508.01728", "authors": ["Dahee Kwon", "Sehyun Lee", "Jaesik Choi"], "title": "Granular Concept Circuits: Toward a Fine-Grained Circuit Discovery for Concept Representations", "comment": "ICCV 2025 accepted paper", "summary": "Deep vision models have achieved remarkable classification performance by\nleveraging a hierarchical architecture in which human-interpretable concepts\nemerge through the composition of individual neurons across layers. Given the\ndistributed nature of representations, pinpointing where specific visual\nconcepts are encoded within a model remains a crucial yet challenging task. In\nthis paper, we introduce an effective circuit discovery method, called Granular\nConcept Circuit (GCC), in which each circuit represents a concept relevant to a\ngiven query. To construct each circuit, our method iteratively assesses\ninter-neuron connectivity, focusing on both functional dependencies and\nsemantic alignment. By automatically discovering multiple circuits, each\ncapturing specific concepts within that query, our approach offers a profound,\nconcept-wise interpretation of models and is the first to identify circuits\ntied to specific visual concepts at a fine-grained level. We validate the\nversatility and effectiveness of GCCs across various deep image classification\nmodels.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01791", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01791", "abs": "https://arxiv.org/abs/2508.01791", "authors": ["Fatimah Mohamed Emad Elden"], "title": "CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase", "comment": null, "summary": "The field of Continuous Sign Language Recognition (CSLR) poses substantial\ntechnical challenges, including fluid inter-sign transitions, the absence of\ntemporal boundaries, and co-articulation effects. This paper, developed for the\nMSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of\nsigner-independent recognition to advance the generalization capabilities of\nCSLR systems across diverse signers. A data-centric methodology is proposed,\ncentered on systematic feature engineering, a robust preprocessing pipeline,\nand an optimized model architecture. Key contributions include a principled\nfeature selection process guided by Exploratory Data Analysis (EDA) to isolate\ncommunicative keypoints, a rigorous preprocessing pipeline incorporating\nDBSCAN-based outlier filtering and spatial normalization, and the novel\nCSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer\ndesign of the Conformer model, leveraging its capacity to model local temporal\ndependencies and global sequence context; a characteristic uniquely suited for\nthe spatio-temporal dynamics of sign language. The proposed methodology\nachieved a competitive performance, with a Word Error Rate (WER) of 5.60% on\nthe development set and 12.01% on the test set, a result that secured a 3rd\nplace ranking on the official competition platform. This research validates the\nefficacy of cross-domain architectural adaptation, demonstrating that the\nConformer model, originally conceived for speech recognition, can be\nsuccessfully repurposed to establish a new state-of-the-art performance in\nkeypoint-based CSLR.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01742", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01742", "abs": "https://arxiv.org/abs/2508.01742", "authors": ["Qiaohui Chu", "Haoyu Zhang", "Meng Liu", "Yisen Feng", "Haoxiang Shi", "Liqiang Nie"], "title": "Intention-Guided Cognitive Reasoning for Egocentric Long-Term Action Anticipation", "comment": "Our code will be released upon acceptance", "summary": "Long-term action anticipation from egocentric video is critical for\napplications such as human-computer interaction and assistive technologies,\nwhere anticipating user intent enables proactive and context-aware AI\nassistance. However, existing approaches suffer from three key limitations: 1)\nunderutilization of fine-grained visual cues from hand-object interactions, 2)\nneglect of semantic dependencies between verbs and nouns, and 3) lack of\nexplicit cognitive reasoning, limiting generalization and long-term forecasting\nability. To overcome these challenges, we propose INSIGHT, a unified two-stage\nframework for egocentric action anticipation. In the first stage, INSIGHT\nfocuses on extracting semantically rich features from hand-object interaction\nregions and enhances action representations using a verb-noun co-occurrence\nmatrix. In the second stage, it introduces a reinforcement learning-based\nmodule that simulates explicit cognitive reasoning through a structured\nprocess: visual perception (think) -> intention inference (reason) -> action\nanticipation (answer). Extensive experiments on Ego4D, EPIC-Kitchens-55, and\nEGTEA Gaze+ benchmarks show that INSIGHT achieves state-of-the-art performance,\ndemonstrating its effectiveness and strong generalization capability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01791", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01791", "abs": "https://arxiv.org/abs/2508.01791", "authors": ["Fatimah Mohamed Emad Elden"], "title": "CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase", "comment": null, "summary": "The field of Continuous Sign Language Recognition (CSLR) poses substantial\ntechnical challenges, including fluid inter-sign transitions, the absence of\ntemporal boundaries, and co-articulation effects. This paper, developed for the\nMSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of\nsigner-independent recognition to advance the generalization capabilities of\nCSLR systems across diverse signers. A data-centric methodology is proposed,\ncentered on systematic feature engineering, a robust preprocessing pipeline,\nand an optimized model architecture. Key contributions include a principled\nfeature selection process guided by Exploratory Data Analysis (EDA) to isolate\ncommunicative keypoints, a rigorous preprocessing pipeline incorporating\nDBSCAN-based outlier filtering and spatial normalization, and the novel\nCSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer\ndesign of the Conformer model, leveraging its capacity to model local temporal\ndependencies and global sequence context; a characteristic uniquely suited for\nthe spatio-temporal dynamics of sign language. The proposed methodology\nachieved a competitive performance, with a Word Error Rate (WER) of 5.60% on\nthe development set and 12.01% on the test set, a result that secured a 3rd\nplace ranking on the official competition platform. This research validates the\nefficacy of cross-domain architectural adaptation, demonstrating that the\nConformer model, originally conceived for speech recognition, can be\nsuccessfully repurposed to establish a new state-of-the-art performance in\nkeypoint-based CSLR.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01815", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01815", "abs": "https://arxiv.org/abs/2508.01815", "authors": ["Yang Zhao", "Chengxiao Dai", "Wei Zhuo", "Tan Chuan Fu", "Yue Xiu", "Dusit Niyato", "Jonathan Z. Low", "Eugene Ho Hong Zhuang", "Daren Zong Loong Tan"], "title": "AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy", "comment": null, "summary": "Question answering over heterogeneous knowledge graphs (KGQA) involves\nreasoning across diverse schemas, incomplete alignments, and distributed data\nsources. Existing text-to-SPARQL approaches rely on large-scale domain-specific\nfine-tuning or operate within single-graph settings, limiting their\ngeneralizability in low-resource domains and their ability to handle queries\nspanning multiple graphs. These challenges are particularly relevant in domains\nsuch as the circular economy, where information about classifications,\nprocesses, and emissions is distributed across independently curated knowledge\ngraphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes\nKGQA into subtasks managed by specialized agents responsible for retrieval,\nquery generation, and verification. A scheduler assigns subgoals to different\ngraphs using weak-to-strong alignment strategies. A two-stage verifier detects\nstructurally invalid and semantically underspecified queries through symbolic\nvalidation and counterfactual consistency checks. Experiments on real-world\ncircular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy\nby 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing\nthe average prompt length by 46.4%. These results demonstrate the benefits of\nagent-based schema-aware reasoning for scalable KGQA and support\ndecision-making in sustainability domains through robust cross-graph reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.01889", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01889", "abs": "https://arxiv.org/abs/2508.01889", "authors": ["Michael W. Rutherford", "Tracy Nolan", "Linmin Pei", "Ulrike Wagner", "Qinyan Pan", "Phillip Farmer", "Kirk Smith", "Benjamin Kopchick", "Laura Opsahl-Ong", "Granger Sutton", "David Clunie", "Keyvan Farahani", "Fred Prior"], "title": "Medical Image De-Identification Resources: Synthetic DICOM Data and Tools for Validation", "comment": null, "summary": "Medical imaging research increasingly depends on large-scale data sharing to\npromote reproducibility and train Artificial Intelligence (AI) models. Ensuring\npatient privacy remains a significant challenge for open-access data sharing.\nDigital Imaging and Communications in Medicine (DICOM), the global standard\ndata format for medical imaging, encodes both essential clinical metadata and\nextensive protected health information (PHI) and personally identifiable\ninformation (PII). Effective de-identification must remove identifiers,\npreserve scientific utility, and maintain DICOM validity. Tools exist to\nperform de-identification, but few assess its effectiveness, and most rely on\nsubjective reviews, limiting reproducibility and regulatory confidence. To\naddress this gap, we developed an openly accessible DICOM dataset infused with\nsynthetic PHI/PII and an evaluation framework for benchmarking image\nde-identification workflows. The Medical Image de-identification (MIDI) dataset\nwas built using publicly available de-identified data from The Cancer Imaging\nArchive (TCIA). It includes 538 subjects (216 for validation, 322 for testing),\n605 studies, 708 series, and 53,581 DICOM image instances. These span multiple\nvendors, imaging modalities, and cancer types. Synthetic PHI and PII were\nembedded into structured data elements, plain text data elements, and pixel\ndata to simulate real-world identity leaks encountered by TCIA curation teams.\nAccompanying evaluation tools include a Python script, answer keys (known\ntruth), and mapping files that enable automated comparison of curated data\nagainst expected transformations. The framework is aligned with the HIPAA\nPrivacy Rule \"Safe Harbor\" method, DICOM PS3.15 Confidentiality Profiles, and\nTCIA best practices. It supports objective, standards-driven evaluation of\nde-identification workflows, promoting safer and more consistent medical image\nsharing.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02004", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02004", "abs": "https://arxiv.org/abs/2508.02004", "authors": ["Kyungmin Jo", "Jooyeol Yun", "Jaegul Choo"], "title": "Devil is in the Detail: Towards Injecting Fine Details of Image Prompt in Image Generation via Conflict-free Guidance and Stratified Attention", "comment": null, "summary": "While large-scale text-to-image diffusion models enable the generation of\nhigh-quality, diverse images from text prompts, these prompts struggle to\ncapture intricate details, such as textures, preventing the user intent from\nbeing reflected. This limitation has led to efforts to generate images\nconditioned on user-provided images, referred to as image prompts. Recent work\nmodifies the self-attention mechanism to impose image conditions in generated\nimages by replacing or concatenating the keys and values from the image prompt.\nThis enables the self-attention layer to work like a cross-attention layer,\ngenerally used to incorporate text prompts. In this paper, we identify two\ncommon issues in existing methods of modifying self-attention to generate\nimages that reflect the details of image prompts. First, existing approaches\nneglect the importance of image prompts in classifier-free guidance.\nSpecifically, current methods use image prompts as both desired and undesired\nconditions in classifier-free guidance, causing conflicting signals. To resolve\nthis, we propose conflict-free guidance by using image prompts only as desired\nconditions, ensuring that the generated image faithfully reflects the image\nprompt. In addition, we observe that the two most common self-attention\nmodifications involve a trade-off between the realism of the generated image\nand alignment with the image prompt. Specifically, selecting more keys and\nvalues from the image prompt improves alignment, while selecting more from the\ngenerated image enhances realism. To balance both, we propose an new\nself-attention modification method, Stratified Attention to jointly use keys\nand values from both images rather than selecting between them. Through\nextensive experiments across three image generation tasks, we show that the\nproposed method outperforms existing image-prompting models in faithfully\nreflecting the image prompt.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02260", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02260", "abs": "https://arxiv.org/abs/2508.02260", "authors": ["Jia Deng", "Jie Chen", "Zhipeng Chen", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning", "comment": "7 pages, 20 figures", "summary": "Recently, reinforcement learning with verifiable rewards (RLVR) has been\nwidely used for enhancing the reasoning abilities of large language models\n(LLMs). A core challenge in RLVR involves managing the exchange between entropy\nand performance of policies. Despite the importance of this exchange, a\nfine-grained understanding of when and how this exchange operates most\neffectively remains limited. To bridge this gap, we conduct a systematic\nempirical analysis of the entropy-performance exchange mechanism of RLVR across\ndifferent levels of granularity. Specifically, we first divide the training\nprocess into two distinct stages based on entropy dynamics, i.e., rising stage\nand plateau stage, and then systematically investigate how this mechanism\nvaries across stage-level, instance-level, and token-level granularitiess. Our\nanalysis reveals that, in the rising stage, entropy reduction in negative\nsamples facilitates the learning of effective reasoning patterns, which in turn\ndrives rapid performance gains. Moreover, in the plateau stage, learning\nefficiency strongly correlates with high-entropy tokens present in\nlow-perplexity samples and those located at the end of sequences. Motivated by\nthese findings, we propose two methods that dynamically adjust the reward\nsignal using perplexity and positional information to focus RL updates on\ntokens that exhibit high learning potential, achieving improvements compared to\nthe baseline methods on various LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02082", "abs": "https://arxiv.org/abs/2508.02082", "authors": ["Yingshu Li", "Yunyi Liu", "Zhanyu Wang", "Xinyu Liang", "Lingqiao Liu", "Lei Wang", "Luping Zhou"], "title": "S-RRG-Bench: Structured Radiology Report Generation with Fine-Grained Evaluation Framework", "comment": null, "summary": "Radiology report generation (RRG) for diagnostic images, such as chest\nX-rays, plays a pivotal role in both clinical practice and AI. Traditional\nfree-text reports suffer from redundancy and inconsistent language,\ncomplicating the extraction of critical clinical details. Structured radiology\nreport generation (S-RRG) offers a promising solution by organizing information\ninto standardized, concise formats. However, existing approaches often rely on\nclassification or visual question answering (VQA) pipelines that require\npredefined label sets and produce only fragmented outputs. Template-based\napproaches, which generate reports by replacing keywords within fixed sentence\npatterns, further compromise expressiveness and often omit clinically important\ndetails. In this work, we present a novel approach to S-RRG that includes\ndataset construction, model training, and the introduction of a new evaluation\nframework. We first create a robust chest X-ray dataset (MIMIC-STRUC) that\nincludes disease names, severity levels, probabilities, and anatomical\nlocations, ensuring that the dataset is both clinically relevant and\nwell-structured. We train an LLM-based model to generate standardized,\nhigh-quality reports. To assess the generated reports, we propose a specialized\nevaluation metric (S-Score) that not only measures disease prediction accuracy\nbut also evaluates the precision of disease-specific details, thus offering a\nclinically meaningful metric for report quality that focuses on elements\ncritical to clinical decision-making and demonstrates a stronger alignment with\nhuman assessments. Our approach highlights the effectiveness of structured\nreports and the importance of a tailored evaluation metric for S-RRG, providing\na more clinically relevant measure of report quality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy", "question answering", "fine-grained"], "score": 5}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02106", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02106", "abs": "https://arxiv.org/abs/2508.02106", "authors": ["Kaiyang Ji", "Ye Shi", "Zichen Jin", "Kangyi Chen", "Lan Xu", "Yuexin Ma", "Jingyi Yu", "Jingya Wang"], "title": "Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis", "comment": "Accepted by ICCV 2025", "summary": "Real-time synthesis of physically plausible human interactions remains a\ncritical challenge for immersive VR/AR systems and humanoid robotics. While\nexisting methods demonstrate progress in kinematic motion generation, they\noften fail to address the fundamental tension between real-time responsiveness,\nphysical feasibility, and safety requirements in dynamic human-machine\ninteractions. We introduce Human-X, a novel framework designed to enable\nimmersive and physically plausible human interactions across diverse entities,\nincluding human-avatar, human-humanoid, and human-robot systems. Unlike\nexisting approaches that focus on post-hoc alignment or simplified physics, our\nmethod jointly predicts actions and reactions in real-time using an\nauto-regressive reaction diffusion planner, ensuring seamless synchronization\nand context-aware responses. To enhance physical realism and safety, we\nintegrate an actor-aware motion tracking policy trained with reinforcement\nlearning, which dynamically adapts to interaction partners' movements while\navoiding artifacts like foot sliding and penetration. Extensive experiments on\nthe Inter-X and InterHuman datasets demonstrate significant improvements in\nmotion quality, interaction continuity, and physical plausibility over\nstate-of-the-art methods. Our framework is validated in real-world\napplications, including virtual reality interface for human-robot interaction,\nshowcasing its potential for advancing human-robot collaboration.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02129", "abs": "https://arxiv.org/abs/2508.02129", "authors": ["Yuru Xiao", "Zihan Lin", "Chao Lu", "Deming Zhai", "Kui Jiang", "Wenbo Zhao", "Wei Zhang", "Junjun Jiang", "Huanran Wang", "Xianming Liu"], "title": "VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic Urban Scenes Modeling", "comment": null, "summary": "Dynamic urban scene modeling is a rapidly evolving area with broad\napplications. While current approaches leveraging neural radiance fields or\nGaussian Splatting have achieved fine-grained reconstruction and high-fidelity\nnovel view synthesis, they still face significant limitations. These often stem\nfrom a dependence on pre-calibrated object tracks or difficulties in accurately\nmodeling fast-moving objects from undersampled capture, particularly due to\nchallenges in handling temporal discontinuities. To overcome these issues, we\npropose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our\nkey insight is to distill robust, temporally consistent priors from a test-time\nadapted video diffusion model. To ensure precise pose alignment and effective\nintegration of this denoised content, we introduce two core innovations: a\njoint timestamp optimization strategy that refines interpolated frame poses,\nand an uncertainty distillation method that adaptively extracts target content\nwhile preserving well-reconstructed regions. Extensive experiments demonstrate\nthat our method significantly enhances dynamic modeling, especially for\nfast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view\nsynthesis over baseline approaches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02140", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02140", "abs": "https://arxiv.org/abs/2508.02140", "authors": ["Daniel Lengerer", "Mathias Pechinger", "Klaus Bogenberger", "Carsten Markgraf"], "title": "AID4AD: Aerial Image Data for Automated Driving Perception", "comment": null, "summary": "This work investigates the integration of spatially aligned aerial imagery\ninto perception tasks for automated vehicles (AVs). As a central contribution,\nwe present AID4AD, a publicly available dataset that augments the nuScenes\ndataset with high-resolution aerial imagery precisely aligned to its local\ncoordinate system. The alignment is performed using SLAM-based point cloud maps\nprovided by nuScenes, establishing a direct link between aerial data and\nnuScenes local coordinate system. To ensure spatial fidelity, we propose an\nalignment workflow that corrects for localization and projection distortions. A\nmanual quality control process further refines the dataset by identifying a set\nof high-quality alignments, which we publish as ground truth to support future\nresearch on automated registration. We demonstrate the practical value of\nAID4AD in two representative tasks: in online map construction, aerial imagery\nserves as a complementary input that improves the mapping process; in motion\nprediction, it functions as a structured environmental representation that\nreplaces high-definition maps. Experiments show that aerial imagery leads to a\n15-23% improvement in map construction accuracy and a 2% gain in trajectory\nprediction performance. These results highlight the potential of aerial imagery\nas a scalable and adaptable source of environmental context in automated\nvehicle systems, particularly in scenarios where high-definition maps are\nunavailable, outdated, or costly to maintain. AID4AD, along with evaluation\ncode and pretrained models, is publicly released to foster further research in\nthis direction: https://github.com/DriverlessMobility/AID4AD.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02238", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02238", "abs": "https://arxiv.org/abs/2508.02238", "authors": ["Xin Dong", "Yiwei Zhang", "Yangjie Cui", "Jinwu Xiang", "Daochun Li", "Zhan Tu"], "title": "An Event-based Fast Intensity Reconstruction Scheme for UAV Real-time Perception", "comment": "A supplementary video is available at https://youtu.be/tLzXjXVRkVg", "summary": "Event cameras offer significant advantages, including a wide dynamic range,\nhigh temporal resolution, and immunity to motion blur, making them highly\npromising for addressing challenging visual conditions. Extracting and\nutilizing effective information from asynchronous event streams is essential\nfor the onboard implementation of event cameras. In this paper, we propose a\nstreamlined event-based intensity reconstruction scheme, event-based single\nintegration (ESI), to address such implementation challenges. This method\nguarantees the portability of conventional frame-based vision methods to\nevent-based scenarios and maintains the intrinsic advantages of event cameras.\nThe ESI approach reconstructs intensity images by performing a single\nintegration of the event streams combined with an enhanced decay algorithm.\nSuch a method enables real-time intensity reconstruction at a high frame rate,\ntypically 100 FPS. Furthermore, the relatively low computation load of ESI fits\nonboard implementation suitably, such as in UAV-based visual tracking\nscenarios. Extensive experiments have been conducted to evaluate the\nperformance comparison of ESI and state-of-the-art algorithms. Compared to\nstate-of-the-art algorithms, ESI demonstrates remarkable runtime efficiency\nimprovements, superior reconstruction quality, and a high frame rate. As a\nresult, ESI enhances UAV onboard perception significantly under visual\nadversary surroundings. In-flight tests, ESI demonstrates effective performance\nfor UAV onboard visual tracking under extremely low illumination\nconditions(2-10lux), whereas other comparative algorithms fail due to\ninsufficient frame rate, poor image quality, or limited real-time performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02258", "abs": "https://arxiv.org/abs/2508.02258", "authors": ["Wenchuan Zhang", "Jingru Guo", "Hengzhe Zhang", "Penghao Zhang", "Jie Chen", "Shuwan Zhang", "Zhang Zhang", "Yuhao Yi", "Hong Bu"], "title": "Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented Generation for Pathology VLMs via Reinforcement Learning", "comment": null, "summary": "Although Vision Language Models (VLMs) have shown strong generalization in\nmedical imaging, pathology presents unique challenges due to ultra-high\nresolution, complex tissue structures, and nuanced clinical semantics. These\nfactors make pathology VLMs prone to hallucinations, i.e., generating outputs\ninconsistent with visual evidence, which undermines clinical trust. Existing\nRAG approaches in this domain largely depend on text-based knowledge bases,\nlimiting their ability to leverage diagnostic visual cues. To address this, we\npropose Patho-AgenticRAG, a multimodal RAG framework with a database built on\npage-level embeddings from authoritative pathology textbooks. Unlike\ntraditional text-only retrieval systems, it supports joint text-image search,\nenabling direct retrieval of textbook pages that contain both the queried text\nand relevant visual cues, thus avoiding the loss of critical image-based\ninformation. Patho-AgenticRAG also supports reasoning, task decomposition, and\nmulti-turn search interactions, improving accuracy in complex diagnostic\nscenarios. Experiments show that Patho-AgenticRAG significantly outperforms\nexisting multimodal models in complex pathology tasks like multiple-choice\ndiagnosis and visual question answering. Our project is available at the\nPatho-AgenticRAG repository:\nhttps://github.com/Wenchuan-Zhang/Patho-AgenticRAG.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02278", "abs": "https://arxiv.org/abs/2508.02278", "authors": ["Xiangzeng Liu", "Chi Wang", "Guanglu Shi", "Xiaodong Zhang", "Qiguang Miao", "Miao Fan"], "title": "SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching", "comment": null, "summary": "Local feature matching remains a fundamental challenge in computer vision.\nRecent Area to Point Matching (A2PM) methods have improved matching accuracy.\nHowever, existing research based on this framework relies on inefficient\npixel-level comparisons and complex graph matching that limit scalability. In\nthis work, we introduce the Semantic and Geometric-aware Descriptor Network\n(SGAD), which fundamentally rethinks area-based matching by generating highly\ndiscriminative area descriptors that enable direct matching without complex\ngraph optimization. This approach significantly improves both accuracy and\nefficiency of area matching. We further improve the performance of area\nmatching through a novel supervision strategy that decomposes the area matching\ntask into classification and ranking subtasks. Finally, we introduce the\nHierarchical Containment Redundancy Filter (HCRF) to eliminate overlapping\nareas by analyzing containment graphs. SGAD demonstrates remarkable performance\ngains, reducing runtime by 60x (0.82s vs. 60.23s) compared to MESA. Extensive\nevaluations show consistent improvements across multiple point matchers:\nSGAD+LoFTR reduces runtime compared to DKM, while achieving higher accuracy\n(0.82s vs. 1.51s, 65.98 vs. 61.11) in outdoor pose estimation, and SGAD+ROMA\ndelivers +7.39% AUC@5{\\deg} in indoor pose estimation, establishing a new\nstate-of-the-art.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02339", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02339", "abs": "https://arxiv.org/abs/2508.02339", "authors": ["Anik Sarker", "Alan T. Asbeck"], "title": "Correspondence-Free Fast and Robust Spherical Point Pattern Registration", "comment": "Accepted at ICCV 2025", "summary": "Existing methods for rotation estimation between two spherical\n($\\mathbb{S}^2$) patterns typically rely on spherical cross-correlation\nmaximization between two spherical function. However, these approaches exhibit\ncomputational complexities greater than cubic $O(n^3)$ with respect to rotation\nspace discretization and lack extensive evaluation under significant outlier\ncontamination. To this end, we propose a rotation estimation algorithm between\ntwo spherical patterns with linear time complexity $O(n)$. Unlike existing\nspherical-function-based methods, we explicitly represent spherical patterns as\ndiscrete 3D point sets on the unit sphere, reformulating rotation estimation as\na spherical point-set alignment (i.e., Wahba problem for 3D unit vectors).\nGiven the geometric nature of our formulation, our spherical pattern alignment\nalgorithm naturally aligns with the Wahba problem framework for 3D unit\nvectors. Specifically, we introduce three novel algorithms: (1) SPMC (Spherical\nPattern Matching by Correlation), (2) FRS (Fast Rotation Search), and (3) a\nhybrid approach (SPMC+FRS) that combines the advantages of the previous two\nmethods. Our experiments demonstrate that in the $\\mathbb{S}^2$ domain and in\ncorrespondence-free settings, our algorithms are over 10x faster and over 10x\nmore accurate than current state-of-the-art methods for the Wahba problem with\noutliers. We validate our approach through extensive simulations on a new\ndataset of spherical patterns, the ``Robust Vector Alignment Dataset.\n\"Furthermore, we adapt our methods to two real-world tasks: (i) Point Cloud\nRegistration (PCR) and (ii) rotation estimation for spherical images.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation"], "score": 3}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02340", "categories": ["cs.CV", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.02340", "abs": "https://arxiv.org/abs/2508.02340", "authors": ["Fan Hu", "Zijie Xin", "Xirong Li"], "title": "Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search", "comment": "Accepted by ACMMM2025", "summary": "Ad-hoc Video Search (AVS) involves using a textual query to search for\nmultiple relevant videos in a large collection of unlabeled short videos. The\nmain challenge of AVS is the visual diversity of relevant videos. A simple\nquery such as \"Find shots of a man and a woman dancing together indoors\" can\nspan a multitude of environments, from brightly lit halls and shadowy bars to\ndance scenes in black-and-white animations. It is therefore essential to\nretrieve relevant videos as comprehensively as possible. Current solutions for\nthe AVS task primarily fuse multiple features into one or more common spaces,\nyet overlook the need for diverse spaces. To fully exploit the expressive\ncapability of individual features, we propose LPD, short for Learning Partially\nDecorrelated common spaces. LPD incorporates two key innovations:\nfeature-specific common space construction and the de-correlation loss.\nSpecifically, LPD learns a separate common space for each video and text\nfeature, and employs de-correlation loss to diversify the ordering of negative\nsamples across different spaces. To enhance the consistency of multi-space\nconvergence, we designed an entropy-based fair multi-space triplet ranking\nloss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify\nthe effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces\nhighlight its ability to enhance result diversity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "consistency"], "score": 2}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02372", "abs": "https://arxiv.org/abs/2508.02372", "authors": ["Emre Gülsoylu", "André Kelm", "Lennart Bengtson", "Matthias Hirsch", "Christian Wilms", "Tim Rolff", "Janick Edinger", "Simone Frintrop"], "title": "TRUDI and TITUS: A Multi-Perspective Dataset and A Three-Stage Recognition System for Transportation Unit Identification", "comment": "13 pages, 2 figures, 6 tables. Author version of the paper. Accepted\n  for publication in The 36th British Machine Vision Conference (BMVC) 2025", "summary": "Identifying transportation units (TUs) is essential for improving the\nefficiency of port logistics. However, progress in this field has been hindered\nby the lack of publicly available benchmark datasets that capture the diversity\nand dynamics of real-world port environments. To address this gap, we present\nthe TRUDI dataset-a comprehensive collection comprising 35,034 annotated\ninstances across five categories: container, tank container, trailer, ID text,\nand logo. The images were captured at operational ports using both ground-based\nand aerial cameras, under a wide variety of lighting and weather conditions.\nFor the identification of TUs-which involves reading the 11-digit alphanumeric\nID typically painted on each unit-we introduce TITUS, a dedicated pipeline that\noperates in three stages: (1) segmenting the TU instances, (2) detecting the\nlocation of the ID text, and (3) recognising and validating the extracted ID.\nUnlike alternative systems, which often require similar scenes, specific camera\nangles or gate setups, our evaluation demonstrates that TITUS reliably\nidentifies TUs from a range of camera perspectives and in varying lighting and\nweather conditions. By making the TRUDI dataset publicly available, we provide\na robust benchmark that enables the development and comparison of new\napproaches. This contribution supports digital transformation efforts in\nmultipurpose ports and helps to increase the efficiency of entire logistics\nchains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02419", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02419", "abs": "https://arxiv.org/abs/2508.02419", "authors": ["Haohan Zheng", "Zhenguo Zhang"], "title": "Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens", "comment": null, "summary": "Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02479", "abs": "https://arxiv.org/abs/2508.02479", "authors": ["Xinquan Yu", "Wei Lu", "Xiangyang Luo"], "title": "Fine-grained Multiple Supervisory Network for Multi-modal Manipulation Detecting and Grounding", "comment": null, "summary": "The task of Detecting and Grounding Multi-Modal Media Manipulation (DGM$^4$)\nis a branch of misinformation detection. Unlike traditional binary\nclassification, it includes complex subtasks such as forgery content\nlocalization and forgery method classification. Consider that existing methods\nare often limited in performance due to neglecting the erroneous interference\ncaused by unreliable unimodal data and failing to establish comprehensive\nforgery supervision for mining fine-grained tampering traces. In this paper, we\npresent a Fine-grained Multiple Supervisory (FMS) network, which incorporates\nmodality reliability supervision, unimodal internal supervision and cross-modal\nsupervision to provide comprehensive guidance for DGM$^4$ detection. For\nmodality reliability supervision, we propose the Multimodal Decision Supervised\nCorrection (MDSC) module. It leverages unimodal weak supervision to correct the\nmulti-modal decision-making process. For unimodal internal supervision, we\npropose the Unimodal Forgery Mining Reinforcement (UFMR) module. It amplifies\nthe disparity between real and fake information within unimodal modality from\nboth feature-level and sample-level perspectives. For cross-modal supervision,\nwe propose the Multimodal Forgery Alignment Reasoning (MFAR) module. It\nutilizes soft-attention interactions to achieve cross-modal feature perception\nfrom both consistency and inconsistency perspectives, where we also design the\ninteraction constraints to ensure the interaction quality. Extensive\nexperiments demonstrate the superior performance of our FMS compared to\nstate-of-the-art methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "reliability", "fine-grained"], "score": 3}}, "source_file": "2025-08-05.jsonl"}
{"id": "2508.02669", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02669", "abs": "https://arxiv.org/abs/2508.02669", "authors": ["Xiaoke Huang", "Juncheng Wu", "Hui Liu", "Xianfeng Tang", "Yuyin Zhou"], "title": "MedVLThinker: Simple Baselines for Multimodal Medical Reasoning", "comment": "Project page and code: https://ucsc-vlaa.github.io/MedVLThinker/", "summary": "Large Reasoning Models (LRMs) have introduced a new paradigm in AI by\nenabling models to ``think before responding\" via chain-of-thought reasoning.\nHowever, the absence of open and reproducible recipes for building\nreasoning-centric medical LMMs hinders community-wide research, analysis, and\ncomparison. In this paper, we present MedVLThinker, a suite of simple yet\nstrong baselines. Our fully open recipe consists of: (1) systematic data\ncuration for both text-only and image-text medical data, filtered according to\nvarying levels of reasoning difficulty, and (2) two training paradigms:\nSupervised Fine-Tuning (SFT) on distilled reasoning traces and Reinforcement\nLearning with Verifiable Rewards (RLVR) based on final answer correctness.\nAcross extensive experiments on the Qwen2.5-VL model family (3B, 7B) and six\nmedical QA benchmarks, we find that RLVR consistently and significantly\noutperforms SFT. Additionally, under the RLVR framework, a key,\ncounter-intuitive finding is that training on our curated text-only reasoning\ndata provides a more substantial performance boost than training on multimodal\nimage-text data. Our best open 7B model, trained using the RLVR recipe on\ntext-only data, establishes a new state-of-the-art on existing public VQA\nbenchmarks, surpassing all previous open-source medical LMMs. Furthermore,\nscaling our model to 32B achieves performance on par with the proprietary\nGPT-4o. We release all curated data, models, and code to provide the community\nwith a strong, open foundation for future research in multimodal medical\nreasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-08-05.jsonl"}
