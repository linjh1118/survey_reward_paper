{"id": "2508.11452", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11452", "abs": "https://arxiv.org/abs/2508.11452", "authors": ["Kangyu Wang", "Hongliang He", "Lin Liu", "Ruiqi Liang", "Zhenzhong Lan", "Jianguo Li"], "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps", "comment": "Our platform is publicly accessible at\n  https://doraemon.alipay.com/model-ranking", "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "comparison", "ranking", "pairwise"], "score": 4}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11452", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11452", "abs": "https://arxiv.org/abs/2508.11452", "authors": ["Kangyu Wang", "Hongliang He", "Lin Liu", "Ruiqi Liang", "Zhenzhong Lan", "Jianguo Li"], "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps", "comment": "Our platform is publicly accessible at\n  https://doraemon.alipay.com/model-ranking", "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "comparison", "ranking", "pairwise"], "score": 4}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11360", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11360", "abs": "https://arxiv.org/abs/2508.11360", "authors": ["Songqin Nong", "Jingxuan Xu", "Sheng Zhou", "Jianfeng Chen", "Xiaoxuan Tang", "Tao Jiang", "Wenhao Xu"], "title": "CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks", "comment": null, "summary": "As autonomous agents become adept at understanding and interacting with\ngraphical user interface (GUI) environments, a new era of automated task\nexecution is emerging. Recent studies have demonstrated that Reinforcement\nLearning (RL) can effectively enhance agents' performance in dynamic\ninteractive GUI environments. However, these methods face two key limitations:\n(1) they overlook the significant variation in difficulty across different GUI\ntasks by treating the entire training data as a uniform set, which hampers the\nagent's ability to adapt its learning process; and (2) most approaches collapse\ntask-specific nuances into a single, coarse reward, leaving the agent with a\nuniform signal that yields inefficient policy updates. To address these\nlimitations, we propose CRAFT-GUI, a curriculum learning framework based on\nGroup Relative Policy Optimization (GRPO) that explicitly accounts for the\nvarying difficulty across trajectories. To enable more fine-grained policy\noptimization, we design a reward function that combines simple rule-based\nsignals with model-judged evaluation, providing richer and more nuanced\nfeedback during training. Experimental results demonstrate that our method\nachieves significant improvements over previous state-of-the-art approaches,\noutperforming them by 5.6% on public benchmarks Android Control and 10.3% on\nour internal online benchmarks, respectively. These findings empirically\nvalidate the effectiveness of integrating reinforcement learning with\ncurriculum learning in GUI interaction tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning", "policy optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11350", "abs": "https://arxiv.org/abs/2508.11350", "authors": ["Zhenhao Zhang", "Hanqing Wang", "Xiangyu Zeng", "Ziyu Cheng", "Jiaxin Liu", "Haoyu Yan", "Zhirui Liu", "Kaiyang Ji", "Tianxiang Gui", "Ke Hu", "Kangyi Chen", "Yahao Fan", "Mokai Pan"], "title": "HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model", "comment": null, "summary": "Understanding and recognizing human-object interaction (HOI) is a pivotal\napplication in AR/VR and robotics. Recent open-vocabulary HOI detection\napproaches depend exclusively on large language models for richer textual\nprompts, neglecting their inherent 3D spatial understanding capabilities. To\naddress this shortcoming, we introduce HOID-R1, the first HOI detection\nframework that integrates chain-of-thought (CoT) guided supervised fine-tuning\n(SFT) with group relative policy optimization (GRPO) within a reinforcement\nlearning (RL) paradigm. Specifically, we initially apply SFT to imbue the model\nwith essential reasoning capabilities, forcing the model to articulate its\nthought process in the output. Subsequently, we integrate GRPO to leverage\nmulti-reward signals for policy optimization, thereby enhancing alignment\nacross diverse modalities. To mitigate hallucinations in the CoT reasoning, we\nintroduce an \"MLLM-as-a-judge\" mechanism that supervises the CoT outputs,\nfurther improving generalization. Extensive experiments show that HOID-R1\nachieves state-of-the-art performance on HOI detection benchmarks and\noutperforms existing methods in open-world generalization to novel scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization", "alignment"], "score": 3}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11538", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11538", "abs": "https://arxiv.org/abs/2508.11538", "authors": ["Sitong Gong", "Lu Zhang", "Yunzhi Zhuge", "Xu Jia", "Pingping Zhang", "Huchuan Lu"], "title": "Reinforcing Video Reasoning Segmentation to Think Before It Segments", "comment": "12 pages", "summary": "Video reasoning segmentation (VRS) endeavors to delineate referred objects in\nvideos guided by implicit instructions that encapsulate human intent and\ntemporal logic. Previous approaches leverage large vision language models\n(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.\nHowever, this paradigm suffers from limited interpretability during inference\nand suboptimal performance due to inadequate spatiotemporal reasoning. Drawing\ninspiration from seminal breakthroughs in reinforcement learning, we introduce\nVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in\nsegmentation. Veason-R1 is trained through Group Relative Policy Optimization\n(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we\ncurate high-quality CoT training data to instill structured reasoning\ntrajectories, bridging video-level semantics and frame-level spatial grounding,\nyielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO\nfine-tuning encourages efficient exploration of the reasoning space by\noptimizing reasoning chains. To this end, we incorporate a holistic reward\nmechanism that synergistically enhances spatial alignment and temporal\nconsistency, bolstering keyframe localization and fine-grained grounding.\nComprehensive empirical evaluations demonstrate that Veason-R1 achieves\nstate-of-the-art performance on multiple benchmarks, surpassing prior art by\nsignificant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),\nwhile exhibiting robustness to hallucinations (+8.8 R). Our code and model\nweights will be available at Veason-R1.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11414", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11414", "abs": "https://arxiv.org/abs/2508.11414", "authors": ["Shangrui Nie", "Florian Mai", "David Kaczér", "Charles Welch", "Zhixue Zhao", "Lucie Flek"], "title": "Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions", "comment": "7 pages 1 figure", "summary": "Large language models implicitly encode preferences over human values, yet\nsteering them often requires large training data. In this work, we investigate\na simple approach: Can we reliably modify a model's value system in downstream\nbehavior by training it to answer value survey questions accordingly? We first\nconstruct value profiles of several open-source LLMs by asking them to rate a\nseries of value-related descriptions spanning 20 distinct human values, which\nwe use as a baseline for subsequent experiments. We then investigate whether\nthe value system of a model can be governed by fine-tuning on the value\nsurveys. We evaluate the effect of finetuning on the model's behavior in two\nways; first, we assess how answers change on in-domain, held-out survey\nquestions. Second, we evaluate whether the model's behavior changes in\nout-of-domain settings (situational scenarios). To this end, we construct a\ncontextualized moral judgment dataset based on Reddit posts and evaluate\nchanges in the model's behavior in text-based adventure games. We demonstrate\nthat our simple approach can not only change the model's answers to in-domain\nsurvey questions, but also produces substantial shifts (value alignment) in\nimplicit downstream task behavior.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11534", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11534", "abs": "https://arxiv.org/abs/2508.11534", "authors": ["Monika Jotautaitė", "Lucius Caviola", "David A. Brewster", "Thilo Hagendorff"], "title": "Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models", "comment": null, "summary": "As large language models (LLMs) become more widely deployed, it is crucial to\nexamine their ethical tendencies. Building on research on fairness and\ndiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --\ndiscrimination based on species membership -- and how they value non-human\nanimals. We systematically examine this issue across three paradigms: (1)\nSpeciesismBench, a 1,003-item benchmark assessing recognition and moral\nevaluation of speciesist statements; (2) established psychological measures\ncomparing model responses with those of human participants; (3) text-generation\ntasks probing elaboration on, or resistance to, speciesist rationalizations. In\nour benchmark, LLMs reliably detected speciesist statements but rarely\ncondemned them, often treating speciesist attitudes as morally acceptable. On\npsychological measures, results were mixed: LLMs expressed slightly lower\nexplicit speciesism than people, yet in direct trade-offs they more often chose\nto save one human over multiple animals. A tentative interpretation is that\nLLMs may weight cognitive capacity rather than species per se: when capacities\nwere equal, they showed no species preference, and when an animal was described\nas more capable, they tended to prioritize it over a less capable human. In\nopen-ended text generation tasks, LLMs frequently normalized or rationalized\nharm toward farmed animals while refusing to do so for non-farmed animals.\nThese findings suggest that while LLMs reflect a mixture of progressive and\nmainstream human views, they nonetheless reproduce entrenched cultural norms\naround animal exploitation. We argue that expanding AI fairness and alignment\nframeworks to explicitly include non-human moral patients is essential for\nreducing these biases and preventing the entrenchment of speciesist attitudes\nin AI systems and the societies they influence.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11196", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11196", "abs": "https://arxiv.org/abs/2508.11196", "authors": ["Jiajin Guan", "Haibo Mei", "Bonan Zhang", "Dan Liu", "Yuanshuang Fu", "Yue Zhang"], "title": "UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning", "comment": null, "summary": "Recent advances in vision-language models (VLMs) have demonstrated strong\ngeneralization in natural image tasks. However, their performance often\ndegrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features\nhigh resolution, complex spatial semantics, and strict real-time constraints.\nThese challenges limit the applicability of general-purpose VLMs to structured\naerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a\nlightweight VLM explicitly designed for aerial visual reasoning. It is trained\nusing a hybrid method that combines supervised fine-tuning (SFT) and\nmulti-stage reinforcement learning (RL). We leverage the group relative policy\noptimization (GRPO) algorithm to promote structured and interpretable reasoning\nthrough rule-guided rewards and intra-group policy alignment. To support model\ntraining and evaluation, we introduce a high-resolution visual question\nanswering dataset named HRVQA-VL, which consists of 50,019 annotated samples\ncovering eight UAV-relevant reasoning tasks, including object counting,\ntransportation recognition, and spatial scene inference. Experimental results\nshow that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the\nQwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which\nis 36x larger, on multiple tasks. Ablation studies reveal that while SFT\nimproves semantic alignment, it may reduce reasoning diversity in mathematical\ntasks. GRPO-based RL compensates for this limitation by enhancing logical\nflexibility and the robustness of inference. Additionally, UAV-VL-R1 requires\nonly 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with\nINT8, supporting real-time deployment on resource-constrained UAV platforms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11255", "abs": "https://arxiv.org/abs/2508.11255", "authors": ["MengChao Wang", "Qiang Wang", "Fan Jiang", "Mu Xu"], "title": "FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation", "comment": "https://fantasy-amap.github.io/fantasy-talking2/", "summary": "Recent advances in audio-driven portrait animation have demonstrated\nimpressive capabilities. However, existing methods struggle to align with\nfine-grained human preferences across multiple dimensions, such as motion\nnaturalness, lip-sync accuracy, and visual quality. This is due to the\ndifficulty of optimizing among competing preference objectives, which often\nconflict with one another, and the scarcity of large-scale, high-quality\ndatasets with multidimensional preference annotations. To address these, we\nfirst introduce Talking-Critic, a multimodal reward model that learns\nhuman-aligned reward functions to quantify how well generated videos satisfy\nmultidimensional expectations. Leveraging this model, we curate Talking-NSQ, a\nlarge-scale multidimensional human preference dataset containing 410K\npreference pairs. Finally, we propose Timestep-Layer adaptive multi-expert\nPreference Optimization (TLPO), a novel framework for aligning diffusion-based\nportrait animation models with fine-grained, multidimensional preferences. TLPO\ndecouples preferences into specialized expert modules, which are then fused\nacross timesteps and network layers, enabling comprehensive, fine-grained\nenhancement across all dimensions without mutual interference. Experiments\ndemonstrate that Talking-Critic significantly outperforms existing methods in\naligning with human preference ratings. Meanwhile, TLPO achieves substantial\nimprovements over baseline models in lip-sync accuracy, motion naturalness, and\nvisual quality, exhibiting superior performance in both qualitative and\nquantitative evaluations. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking2/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "human preference", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11376", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11376", "abs": "https://arxiv.org/abs/2508.11376", "authors": ["Durgesh Mishra", "Rishabh Uikey"], "title": "Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition", "comment": "The paper spans a total of 14 pages, 10 pages for the main content\n  (including references) and 4 pages for the appendix. The main paper contains\n  3 figures and 1 table, while the appendix includes 1 pseudo-code algorithm\n  and 4 tables. The work was recently accepted for publication at IJCB 2025", "summary": "Knowledge Distillation is crucial for optimizing face recognition models for\ndeployment in computationally limited settings, such as edge devices.\nTraditional KD methods, such as Raw L2 Feature Distillation or Feature\nConsistency loss, often fail to capture both fine-grained instance-level\ndetails and complex relational structures, leading to suboptimal performance.\nWe propose a unified approach that integrates two novel loss functions,\nInstance-Level Embedding Distillation and Relation-Based Pairwise Similarity\nDistillation. Instance-Level Embedding Distillation focuses on aligning\nindividual feature embeddings by leveraging a dynamic hard mining strategy,\nthereby enhancing learning from challenging examples. Relation-Based Pairwise\nSimilarity Distillation captures relational information through pairwise\nsimilarity relationships, employing a memory bank mechanism and a sample mining\nstrategy. This unified framework ensures both effective instance-level\nalignment and preservation of geometric relationships between samples, leading\nto a more comprehensive distillation process. Our unified framework outperforms\nstate-of-the-art distillation methods across multiple benchmark face\nrecognition datasets, as demonstrated by extensive experimental evaluations.\nInterestingly, when using strong teacher networks compared to the student, our\nunified KD enables the student to even surpass the teacher's accuracy.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.10925", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10925", "abs": "https://arxiv.org/abs/2508.10925", "authors": ["OpenAI", ":", "Sandhini Agarwal", "Lama Ahmad", "Jason Ai", "Sam Altman", "Andy Applebaum", "Edwin Arbus", "Rahul K. Arora", "Yu Bai", "Bowen Baker", "Haiming Bao", "Boaz Barak", "Ally Bennett", "Tyler Bertao", "Nivedita Brett", "Eugene Brevdo", "Greg Brockman", "Sebastien Bubeck", "Che Chang", "Kai Chen", "Mark Chen", "Enoch Cheung", "Aidan Clark", "Dan Cook", "Marat Dukhan", "Casey Dvorak", "Kevin Fives", "Vlad Fomenko", "Timur Garipov", "Kristian Georgiev", "Mia Glaese", "Tarun Gogineni", "Adam Goucher", "Lukas Gross", "Katia Gil Guzman", "John Hallman", "Jackie Hehir", "Johannes Heidecke", "Alec Helyar", "Haitang Hu", "Romain Huet", "Jacob Huh", "Saachi Jain", "Zach Johnson", "Chris Koch", "Irina Kofman", "Dominik Kundel", "Jason Kwon", "Volodymyr Kyrylov", "Elaine Ya Le", "Guillaume Leclerc", "James Park Lennon", "Scott Lessans", "Mario Lezcano-Casado", "Yuanzhi Li", "Zhuohan Li", "Ji Lin", "Jordan Liss", "Lily", "Liu", "Jiancheng Liu", "Kevin Lu", "Chris Lu", "Zoran Martinovic", "Lindsay McCallum", "Josh McGrath", "Scott McKinney", "Aidan McLaughlin", "Song Mei", "Steve Mostovoy", "Tong Mu", "Gideon Myles", "Alexander Neitz", "Alex Nichol", "Jakub Pachocki", "Alex Paino", "Dana Palmie", "Ashley Pantuliano", "Giambattista Parascandolo", "Jongsoo Park", "Leher Pathak", "Carolina Paz", "Ludovic Peran", "Dmitry Pimenov", "Michelle Pokrass", "Elizabeth Proehl", "Huida Qiu", "Gaby Raila", "Filippo Raso", "Hongyu Ren", "Kimmy Richardson", "David Robinson", "Bob Rotsted", "Hadi Salman", "Suvansh Sanjeev", "Max Schwarzer", "D. Sculley", "Harshit Sikchi", "Kendal Simon", "Karan Singhal", "Yang Song", "Dane Stuckey", "Zhiqing Sun", "Philippe Tillet", "Sam Toizer", "Foivos Tsimpourlas", "Nikhil Vyas", "Eric Wallace", "Xin Wang", "Miles Wang", "Olivia Watkins", "Kevin Weil", "Amy Wendling", "Kevin Whinnery", "Cedric Whitney", "Hannah Wong", "Lin Yang", "Yu Yang", "Michihiro Yasunaga", "Kristen Ying", "Wojciech Zaremba", "Wenting Zhan", "Cyril Zhang", "Brian Zhang", "Eddie Zhang", "Shengjia Zhao"], "title": "gpt-oss-120b & gpt-oss-20b Model Card", "comment": null, "summary": "We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models\nthat push the frontier of accuracy and inference cost. The models use an\nefficient mixture-of-expert transformer architecture and are trained using\nlarge-scale distillation and reinforcement learning. We optimize the models to\nhave strong agentic capabilities (deep research browsing, python tool use, and\nsupport for developer-provided functions), all while using a rendered chat\nformat that enables clear instruction following and role delineation. Both\nmodels achieve strong results on benchmarks ranging from mathematics, coding,\nand safety. We release the model weights, inference implementations, tool\nenvironments, and tokenizers under an Apache 2.0 license to enable broad use\nand further research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11203", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "51-04", "I.3.8; I.4.9"], "pdf": "https://arxiv.org/pdf/2508.11203", "abs": "https://arxiv.org/abs/2508.11203", "authors": ["Seungmi Lee", "Kwan Yun", "Junyong Noh"], "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation", "comment": "Pacific graphics 2025, CGF, 15 pages", "summary": "We introduce StyleMM, a novel framework that can construct a stylized 3D\nMorphable Model (3DMM) based on user-defined text descriptions specifying a\ntarget style. Building upon a pre-trained mesh deformation network and a\ntexture generator for original 3DMM-based realistic human faces, our approach\nfine-tunes these models using stylized facial images generated via text-guided\nimage-to-image (i2i) translation with a diffusion model, which serve as\nstylization targets for the rendered mesh. To prevent undesired changes in\nidentity, facial alignment, or expressions during i2i translation, we introduce\na stylization method that explicitly preserves the facial attributes of the\nsource image. By maintaining these critical attributes during image\nstylization, the proposed approach ensures consistent 3D style transfer across\nthe 3DMM parameter space through image-based training. Once trained, StyleMM\nenables feed-forward generation of stylized face meshes with explicit control\nover shape, expression, and texture parameters, producing meshes with\nconsistent vertex connectivity and animatability. Quantitative and qualitative\nevaluations demonstrate that our approach outperforms state-of-the-art methods\nin terms of identity-level facial diversity and stylization capability. The\ncode and videos are available at\n[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11085", "abs": "https://arxiv.org/abs/2508.11085", "authors": ["Qingqing Wang", "Liqiang Xiao", "Chang Chang"], "title": "Learn to optimize for automatic proton PBS treatment planning for H&N cancers", "comment": "27 pages, 4 figures", "summary": "Proton PBS treatment planning for H&N cancers involves numerous conflicting\nobjectives, requiring significant effort from human planners to balance and\nsatisfy multiple clinical goals during planning. To achieve this,\nexperience-demanding objective parameter adjustment and computationally\nexpensive inverse optimization are performed iteratively. Extensive efforts\nhave been made to automatically adjust objective parameters, but the most\ntime-consuming component, i.e., inverse optimization, still relies heavily on\ntheory-driven approaches. We propose a data-driven inverse optimizer and\nintegrate it into a PPO-based automatic treatment planning framework to\nautomatically generate high-quality plans within a clinical acceptable planning\ntime. The inverse optimizer is a L2O method that predicts update steps by\nlearning from the task-specific data distribution. For the first time, we\nintegrate techniques designed for long-context processing, originally developed\nfor LLMs, into a Transformer-based L2O framework to address the scalability\nissue of existing L2O methods. The PPO framework functions as an outer-loop\nvirtual planner, autonomously adjusting objective parameters through a policy\nnetwork, and the dose predictor is used to initialize objective parameters. The\ninner-loop L2O inverse optimizer computes machine-deliverable MU values based\non objectives refined by the PPO policy network. 97 patients are collected in\nthis study, and compared with L-BFGSB, our L2O-based inverse optimizer improves\nthe effectiveness and efficiency by 22.97% and 36.41%, respectively. In\nconjunction with the PPO-based learned virtual planner, plans generated by our\nframework within an average of 2.55 hours show improved or comparable OAR\nsparing with superior target coverage for patients with different prescription\ndose levels, number of target volumes, beam angles, etc., compared with\nhuman-generated plans.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.10935", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10935", "abs": "https://arxiv.org/abs/2508.10935", "authors": ["Qi Liu", "Yabei Li", "Hongsong Wang", "Lei He"], "title": "HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model", "comment": null, "summary": "Traditional closed-set 3D detection frameworks fail to meet the demands of\nopen-world applications like autonomous driving. Existing open-vocabulary 3D\ndetection methods typically adopt a two-stage pipeline consisting of\npseudo-label generation followed by semantic alignment. While vision-language\nmodels (VLMs) recently have dramatically improved the semantic accuracy of\npseudo-labels, their geometric quality, particularly bounding box precision,\nremains commonly neglected.To address this issue, we propose a High Box Quality\nOpen-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and\nrefine high-quality pseudo-labels for open-vocabulary classes. The framework\ncomprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal\nGenerator that utilizes cross-modality geometric consistency to generate\nhigh-quality initial 3D proposals, and an Annotated-Class Assisted (ACA)\nDenoiser that progressively refines 3D proposals by leveraging geometric priors\nfrom annotated categories through a DDIM-based denoising mechanism.Compared to\nthe state-of-the-art method, training with pseudo-labels generated by our\napproach achieves a 7.37% improvement in mAP on novel classes, demonstrating\nthe superior quality of the pseudo-labels produced by our framework. HQ-OV3D\ncan serve not only as a strong standalone open-vocabulary 3D detector but also\nas a plug-in high-quality pseudo-label generator for existing open-vocabulary\ndetection or annotation pipelines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.10936", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10936", "abs": "https://arxiv.org/abs/2508.10936", "authors": ["Cheng Chen", "Hao Huang", "Saurabh Bagchi"], "title": "Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction", "comment": null, "summary": "Collaborative perception enables connected vehicles to share information,\novercoming occlusions and extending the limited sensing range inherent in\nsingle-agent (non-collaborative) systems. Existing vision-only methods for 3D\nsemantic occupancy prediction commonly rely on dense 3D voxels, which incur\nhigh communication costs, or 2D planar features, which require accurate depth\nestimation or additional supervision, limiting their applicability to\ncollaborative scenarios. To address these challenges, we propose the first\napproach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D\nsemantic occupancy prediction. By sharing and fusing intermediate Gaussian\nprimitives, our method provides three benefits: a neighborhood-based\ncross-agent fusion that removes duplicates and suppresses noisy or inconsistent\nGaussians; a joint encoding of geometry and semantics in each primitive, which\nreduces reliance on depth supervision and allows simple rigid alignment; and\nsparse, object-centric messages that preserve structural information while\nreducing communication volume. Extensive experiments demonstrate that our\napproach outperforms single-agent perception and baseline collaborative methods\nby +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU,\nrespectively. When further reducing the number of transmitted Gaussians, our\nmethod still achieves a +1.9 improvement in mIoU, using only 34.6%\ncommunication volume, highlighting robust performance under limited\ncommunication budgets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.10925", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10925", "abs": "https://arxiv.org/abs/2508.10925", "authors": ["OpenAI", ":", "Sandhini Agarwal", "Lama Ahmad", "Jason Ai", "Sam Altman", "Andy Applebaum", "Edwin Arbus", "Rahul K. Arora", "Yu Bai", "Bowen Baker", "Haiming Bao", "Boaz Barak", "Ally Bennett", "Tyler Bertao", "Nivedita Brett", "Eugene Brevdo", "Greg Brockman", "Sebastien Bubeck", "Che Chang", "Kai Chen", "Mark Chen", "Enoch Cheung", "Aidan Clark", "Dan Cook", "Marat Dukhan", "Casey Dvorak", "Kevin Fives", "Vlad Fomenko", "Timur Garipov", "Kristian Georgiev", "Mia Glaese", "Tarun Gogineni", "Adam Goucher", "Lukas Gross", "Katia Gil Guzman", "John Hallman", "Jackie Hehir", "Johannes Heidecke", "Alec Helyar", "Haitang Hu", "Romain Huet", "Jacob Huh", "Saachi Jain", "Zach Johnson", "Chris Koch", "Irina Kofman", "Dominik Kundel", "Jason Kwon", "Volodymyr Kyrylov", "Elaine Ya Le", "Guillaume Leclerc", "James Park Lennon", "Scott Lessans", "Mario Lezcano-Casado", "Yuanzhi Li", "Zhuohan Li", "Ji Lin", "Jordan Liss", "Lily", "Liu", "Jiancheng Liu", "Kevin Lu", "Chris Lu", "Zoran Martinovic", "Lindsay McCallum", "Josh McGrath", "Scott McKinney", "Aidan McLaughlin", "Song Mei", "Steve Mostovoy", "Tong Mu", "Gideon Myles", "Alexander Neitz", "Alex Nichol", "Jakub Pachocki", "Alex Paino", "Dana Palmie", "Ashley Pantuliano", "Giambattista Parascandolo", "Jongsoo Park", "Leher Pathak", "Carolina Paz", "Ludovic Peran", "Dmitry Pimenov", "Michelle Pokrass", "Elizabeth Proehl", "Huida Qiu", "Gaby Raila", "Filippo Raso", "Hongyu Ren", "Kimmy Richardson", "David Robinson", "Bob Rotsted", "Hadi Salman", "Suvansh Sanjeev", "Max Schwarzer", "D. Sculley", "Harshit Sikchi", "Kendal Simon", "Karan Singhal", "Yang Song", "Dane Stuckey", "Zhiqing Sun", "Philippe Tillet", "Sam Toizer", "Foivos Tsimpourlas", "Nikhil Vyas", "Eric Wallace", "Xin Wang", "Miles Wang", "Olivia Watkins", "Kevin Weil", "Amy Wendling", "Kevin Whinnery", "Cedric Whitney", "Hannah Wong", "Lin Yang", "Yu Yang", "Michihiro Yasunaga", "Kristen Ying", "Wojciech Zaremba", "Wenting Zhan", "Cyril Zhang", "Brian Zhang", "Eddie Zhang", "Shengjia Zhao"], "title": "gpt-oss-120b & gpt-oss-20b Model Card", "comment": null, "summary": "We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models\nthat push the frontier of accuracy and inference cost. The models use an\nefficient mixture-of-expert transformer architecture and are trained using\nlarge-scale distillation and reinforcement learning. We optimize the models to\nhave strong agentic capabilities (deep research browsing, python tool use, and\nsupport for developer-provided functions), all while using a rendered chat\nformat that enables clear instruction following and role delineation. Both\nmodels achieve strong results on benchmarks ranging from mathematics, coding,\nand safety. We release the model weights, inference implementations, tool\nenvironments, and tokenizers under an Apache 2.0 license to enable broad use\nand further research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11197", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.11197", "abs": "https://arxiv.org/abs/2508.11197", "authors": ["Ahmad Mousavi", "Yeganeh Abdollahinejad", "Roberto Corizzo", "Nathalie Japkowicz", "Zois Boukouvalas"], "title": "E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection", "comment": null, "summary": "Detecting multimodal misinformation on social media remains challenging due\nto inconsistencies between modalities, changes in temporal patterns, and\nsubstantial class imbalance. Many existing methods treat posts independently\nand fail to capture the event-level structure that connects them across time\nand modality. We propose E-CaTCH, an interpretable and scalable framework for\nrobustly detecting misinformation. If needed, E-CaTCH clusters posts into\npseudo-events based on textual similarity and temporal proximity, then\nprocesses each event independently. Within each event, textual and visual\nfeatures are extracted using pre-trained BERT and ResNet encoders, refined via\nintra-modal self-attention, and aligned through bidirectional cross-modal\nattention. A soft gating mechanism fuses these representations to form\ncontextualized, content-aware embeddings of each post. To model temporal\nevolution, E-CaTCH segments events into overlapping time windows and uses a\ntrend-aware LSTM, enhanced with semantic shift and momentum signals, to encode\nnarrative progression over time. Classification is performed at the event\nlevel, enabling better alignment with real-world misinformation dynamics. To\naddress class imbalance and promote stable learning, the model integrates\nadaptive class weighting, temporal consistency regularization, and hard-example\nmining. The total loss is aggregated across all events. Extensive experiments\non Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH\nconsistently outperforms state-of-the-art baselines. Cross-dataset evaluations\nfurther demonstrate its robustness, generalizability, and practical\napplicability across diverse misinformation scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11280", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11280", "abs": "https://arxiv.org/abs/2508.11280", "authors": ["Ruiyan Qi", "Congding Wen", "Weibo Zhou", "Shangsong Liang", "Lingbo Li"], "title": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought", "comment": null, "summary": "Evaluating large language models (LLMs) in specific domain like tourism\nremains challenging due to the prohibitive cost of annotated benchmarks and\npersistent issues like hallucinations. We propose $\\textbf{L}$able-Free\n$\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert\n$\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that\nleverages expert-derived reasoning structures-instead of labeled data-to access\nLLMs in tourism. First, we iteratively refine and validate hierarchical ToT\ncomponents through alignment with generic quality dimensions and expert\nfeedback. Results demonstrate the effectiveness of our systematically optimized\nexpert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we\napply LETToT's optimized expert ToT to evaluate models of varying scales\n(32B-671B parameters), revealing: (1) Scaling laws persist in specialized\ndomains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,\nDeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit\nreasoning architectures outperform counterparts in accuracy and conciseness\n($p<0.05$). Our work established a scalable, label-free paradigm for\ndomain-specific LLM evaluation, offering a robust alternative to conventional\nannotated benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.10962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10962", "abs": "https://arxiv.org/abs/2508.10962", "authors": ["Jiarong Li", "Imad Ali Shah", "Diarmaid Geever", "Fiachra Collins", "Enda Ward", "Martin Glavin", "Edward Jones", "Brian Deegan"], "title": "CSNR and JMIM Based Spectral Band Selection for Reducing Metamerism in Urban Driving", "comment": "Under Review at IEEE OJITS, July, 2025", "summary": "Protecting Vulnerable Road Users (VRU) is a critical safety challenge for\nautomotive perception systems, particularly under visual ambiguity caused by\nmetamerism, a phenomenon where distinct materials appear similar in RGB\nimagery. This work investigates hyperspectral imaging (HSI) to overcome this\nlimitation by capturing unique material signatures beyond the visible spectrum,\nespecially in the Near-Infrared (NIR). To manage the inherent\nhigh-dimensionality of HSI data, we propose a band selection strategy that\nintegrates information theory techniques (joint mutual information\nmaximization, correlation analysis) with a novel application of an image\nquality metric (contrast signal-to-noise ratio) to identify the most spectrally\ninformative bands. Using the Hyperspectral City V2 (H-City) dataset, we\nidentify three informative bands (497 nm, 607 nm, and 895 nm, $\\pm$27 nm) and\nreconstruct pseudo-color images for comparison with co-registered RGB.\nQuantitative results demonstrate increased dissimilarity and perceptual\nseparability of VRU from the background. The selected HSI bands yield\nimprovements of 70.24%, 528.46%, 1206.83%, and 246.62% for dissimilarity\n(Euclidean, SAM, $T^2$) and perception (CIE $\\Delta E$) metrics, consistently\noutperforming RGB and confirming a marked reduction in metameric confusion. By\nproviding a spectrally optimized input, our method enhances VRU separability,\nestablishing a robust foundation for downstream perception tasks in Advanced\nDriver Assistance Systems (ADAS) and Autonomous Driving (AD), ultimately\ncontributing to improved road safety.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "correlation"], "score": 3}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11141", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11141", "abs": "https://arxiv.org/abs/2508.11141", "authors": ["Bin Ma", "Yifei Zhang", "Yongjin Xian", "Qi Li", "Linna Zhou", "Gongxun Miao"], "title": "A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations", "comment": null, "summary": "Existing rumor detection methods often neglect the content within images as\nwell as the inherent relationships between contexts and images across different\nvisual scales, thereby resulting in the loss of critical information pertinent\nto rumor identification. To address these issues, this paper presents a novel\ncross-modal rumor detection scheme based on contrastive learning, namely the\nMulti-scale Image and Context Correlation exploration algorithm (MICC).\nSpecifically, we design an SCLIP encoder to generate unified semantic\nembeddings for text and multi-scale image patches through contrastive\npretraining, enabling their relevance to be measured via dot-product\nsimilarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is\nintroduced to identify image regions most relevant to the textual semantics,\nguided by mutual information maximization and the information bottleneck\nprinciple, through a Top-K selection strategy based on a cross-modal relevance\nmatrix constructed between the text and multi-scale image patches. Moreover, a\nscale-aware fusion network is designed to integrate the highly correlated\nmulti-scale image features with global text features by assigning adaptive\nweights to image regions based on their semantic importance and cross-modal\nrelevance. The proposed methodology has been extensively evaluated on two\nreal-world datasets. The experimental results demonstrate that it achieves a\nsubstantial performance improvement over existing state-of-the-art approaches\nin rumor detection, highlighting its effectiveness and potential for practical\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11170", "abs": "https://arxiv.org/abs/2508.11170", "authors": ["Baihong Qian", "Haotian Fan", "Wenjie Liao", "Yunqiu Wang", "Tao Li", "Junhui Cui"], "title": "Better Supervised Fine-tuning for VQA: Integer-Only Loss", "comment": null, "summary": "With the rapid advancement of vision language models(VLM), their ability to\nassess visual content based on specific criteria and dimensions has become\nincreasingly critical for applications such as video-theme consistency\nassessment and visual quality scoring. However, existing methods often suffer\nfrom imprecise results and inefficient loss calculation, which limit the focus\nof the model on key evaluation indicators. To address this, we propose\nIOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to\nenhance their performance in video quality assessment tasks. The key innovation\nof IOVQA lies in its label construction and its targeted loss calculation\nmechanism. Specifically, during dataset curation, we constrain the model's\noutput to integers within the range of [10,50], ensuring numerical stability,\nand convert decimal Overall_MOS to integer before using them as labels. We also\nintroduce a target-mask strategy: when computing the loss, only the first\ntwo-digit-integer of the label is unmasked, forcing the model to learn the\ncritical components of the numerical evaluation. After fine-tuning the\nQwen2.5-VL model using the constructed dataset, experimental results\ndemonstrate that the proposed method significantly improves the model's\naccuracy and consistency in the VQA task, ranking 3rd in VQualA 2025\nGenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work\nhighlights the effectiveness of merely leaving integer labels during\nfine-tuning, providing an effective idea for optimizing VLMs in quantitative\nevaluation scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "accuracy", "criteria"], "score": 5}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11310", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11310", "abs": "https://arxiv.org/abs/2508.11310", "authors": ["Beichen Guo", "Zhiyuan Wen", "Yu Yang", "Peng Gao", "Ruosong Yang", "Jiaxing Shen"], "title": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems", "comment": "Accepted to The 21st International Conference on Advanced Data Mining\n  and Applications (ADMA2025)", "summary": "The growing interest in automatic survey generation (ASG), a task that\ntraditionally required considerable time and effort, has been spurred by recent\nadvances in large language models (LLMs). With advancements in\nretrieval-augmented generation (RAG) and the rising popularity of multi-agent\nsystems (MASs), synthesizing academic surveys using LLMs has become a viable\napproach, thereby elevating the need for robust evaluation methods in this\ndomain. However, existing evaluation methods suffer from several limitations,\nincluding biased metrics, a lack of human preference, and an over-reliance on\nLLMs-as-judges. To address these challenges, we propose SGSimEval, a\ncomprehensive benchmark for Survey Generation with Similarity-Enhanced\nEvaluation that evaluates automatic survey generation systems by integrating\nassessments of the outline, content, and references, and also combines\nLLM-based scoring with quantitative metrics to provide a multifaceted\nevaluation framework. In SGSimEval, we also introduce human preference metrics\nthat emphasize both inherent quality and similarity to humans. Extensive\nexperiments reveal that current ASG systems demonstrate human-comparable\nsuperiority in outline generation, while showing significant room for\nimprovement in content and reference generation, and our evaluation metrics\nmaintain strong consistency with human assessments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "human preference", "consistency"], "score": 4}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11197", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.11197", "abs": "https://arxiv.org/abs/2508.11197", "authors": ["Ahmad Mousavi", "Yeganeh Abdollahinejad", "Roberto Corizzo", "Nathalie Japkowicz", "Zois Boukouvalas"], "title": "E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection", "comment": null, "summary": "Detecting multimodal misinformation on social media remains challenging due\nto inconsistencies between modalities, changes in temporal patterns, and\nsubstantial class imbalance. Many existing methods treat posts independently\nand fail to capture the event-level structure that connects them across time\nand modality. We propose E-CaTCH, an interpretable and scalable framework for\nrobustly detecting misinformation. If needed, E-CaTCH clusters posts into\npseudo-events based on textual similarity and temporal proximity, then\nprocesses each event independently. Within each event, textual and visual\nfeatures are extracted using pre-trained BERT and ResNet encoders, refined via\nintra-modal self-attention, and aligned through bidirectional cross-modal\nattention. A soft gating mechanism fuses these representations to form\ncontextualized, content-aware embeddings of each post. To model temporal\nevolution, E-CaTCH segments events into overlapping time windows and uses a\ntrend-aware LSTM, enhanced with semantic shift and momentum signals, to encode\nnarrative progression over time. Classification is performed at the event\nlevel, enabling better alignment with real-world misinformation dynamics. To\naddress class imbalance and promote stable learning, the model integrates\nadaptive class weighting, temporal consistency regularization, and hard-example\nmining. The total loss is aggregated across all events. Extensive experiments\non Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH\nconsistently outperforms state-of-the-art baselines. Cross-dataset evaluations\nfurther demonstrate its robustness, generalizability, and practical\napplicability across diverse misinformation scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11203", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "51-04", "I.3.8; I.4.9"], "pdf": "https://arxiv.org/pdf/2508.11203", "abs": "https://arxiv.org/abs/2508.11203", "authors": ["Seungmi Lee", "Kwan Yun", "Junyong Noh"], "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation", "comment": "Pacific graphics 2025, CGF, 15 pages", "summary": "We introduce StyleMM, a novel framework that can construct a stylized 3D\nMorphable Model (3DMM) based on user-defined text descriptions specifying a\ntarget style. Building upon a pre-trained mesh deformation network and a\ntexture generator for original 3DMM-based realistic human faces, our approach\nfine-tunes these models using stylized facial images generated via text-guided\nimage-to-image (i2i) translation with a diffusion model, which serve as\nstylization targets for the rendered mesh. To prevent undesired changes in\nidentity, facial alignment, or expressions during i2i translation, we introduce\na stylization method that explicitly preserves the facial attributes of the\nsource image. By maintaining these critical attributes during image\nstylization, the proposed approach ensures consistent 3D style transfer across\nthe 3DMM parameter space through image-based training. Once trained, StyleMM\nenables feed-forward generation of stylized face meshes with explicit control\nover shape, expression, and texture parameters, producing meshes with\nconsistent vertex connectivity and animatability. Quantitative and qualitative\nevaluations demonstrate that our approach outperforms state-of-the-art methods\nin terms of identity-level facial diversity and stylization capability. The\ncode and videos are available at\n[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11364", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11364", "abs": "https://arxiv.org/abs/2508.11364", "authors": ["Sylvio Rüdian", "Yassin Elsir", "Marvin Kretschmer", "Sabine Cayrou", "Niels Pinkwart"], "title": "Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning", "comment": "11 pages, one table", "summary": "Automated feedback generation has the potential to enhance students' learning\nprogress by providing timely and targeted feedback. Moreover, it can assist\nteachers in optimizing their time, allowing them to focus on more strategic and\npersonalized aspects of teaching. To generate high-quality, information-rich\nformative feedback, it is essential first to extract relevant indicators, as\nthese serve as the foundation upon which the feedback is constructed. Teachers\noften employ feedback criteria grids composed of various indicators that they\nevaluate systematically. This study examines the initial phase of extracting\nsuch indicators from students' submissions of a language learning course using\nthe large language model Llama 3.1. Accordingly, the alignment between\nindicators generated by the LLM and human ratings across various feedback\ncriteria is investigated. The findings demonstrate statistically significant\nstrong correlations, even in cases involving unanticipated combinations of\nindicators and criteria. The methodology employed in this paper offers a\npromising foundation for extracting indicators from students' submissions using\nLLMs. Such indicators can potentially be utilized to auto-generate explainable\nand transparent formative feedback in future research.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11058", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.11058", "abs": "https://arxiv.org/abs/2508.11058", "authors": ["Wentao Mo", "Qingchao Chen", "Yuxin Peng", "Siyuan Huang", "Yang Liu"], "title": "Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset", "comment": "Accepeted to ACM MM 25", "summary": "The advancement of 3D vision-language (3D VL) learning is hindered by several\nlimitations in existing 3D VL datasets: they rarely necessitate reasoning\nbeyond a close range of objects in single viewpoint, and annotations often link\ninstructions to single objects, missing richer contextual alignments between\nmultiple objects. This significantly curtails the development of models capable\nof deep, multi-view 3D scene understanding over distant objects. To address\nthese challenges, we introduce MV-ScanQA, a novel 3D question answering dataset\nwhere 68% of questions explicitly require integrating information from multiple\nviews (compared to less than 7% in existing datasets), thereby rigorously\ntesting multi-view compositional reasoning. To facilitate the training of\nmodels for such demanding scenarios, we present TripAlign dataset, a\nlarge-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D\nview, set of 3D objects, text> triplets that explicitly aligns groups of\ncontextually related objects with text, providing richer, view-grounded\nmulti-object multimodal alignment signals than previous single-object\nannotations. We further develop LEGO, a baseline method for the multi-view\nreasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D\nLVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign\nachieves state-of-the-art performance not only on the proposed MV-ScanQA, but\nalso on existing benchmarks for 3D dense captioning and question answering.\nDatasets and code are available at\nhttps://matthewdm0816.github.io/tripalign-mvscanqa.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "question answering"], "score": 3}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11383", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11383", "abs": "https://arxiv.org/abs/2508.11383", "authors": ["Mikhail Seleznyov", "Mikhail Chaichuk", "Gleb Ershov", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs", "comment": null, "summary": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11262", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11262", "abs": "https://arxiv.org/abs/2508.11262", "authors": ["Aiswarya Konavoor", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Vision-Language Models display a strong gender bias", "comment": null, "summary": "Vision-language models (VLM) align images and text in a shared representation\nspace that is useful for retrieval and zero-shot transfer. Yet, this alignment\ncan encode and amplify social stereotypes in subtle ways that are not obvious\nfrom standard accuracy metrics. In this study, we test whether the contrastive\nvision-language encoder exhibits gender-linked associations when it places\nembeddings of face images near embeddings of short phrases that describe\noccupations and activities. We assemble a dataset of 220 face photographs split\nby perceived binary gender and a set of 150 unique statements distributed\nacross six categories covering emotional labor, cognitive labor, domestic\nlabor, technical labor, professional roles, and physical labor. We compute\nunit-norm image embeddings for every face and unit-norm text embeddings for\nevery statement, then define a statement-level association score as the\ndifference between the mean cosine similarity to the male set and the mean\ncosine similarity to the female set, where positive values indicate stronger\nassociation with the male set and negative values indicate stronger association\nwith the female set. We attach bootstrap confidence intervals by resampling\nimages within each gender group, aggregate by category with a separate\nbootstrap over statements, and run a label-swap null model that estimates the\nlevel of mean absolute association we would expect if no gender structure were\npresent. The outcome is a statement-wise and category-wise map of gender\nassociations in a contrastive vision-language space, accompanied by\nuncertainty, simple sanity checks, and a robust gender bias evaluation\nframework.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11272", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11272", "abs": "https://arxiv.org/abs/2508.11272", "authors": ["Jun Li", "Kai Li", "Shaoguo Liu", "Tingting Gao"], "title": "Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering", "comment": null, "summary": "Composed Image Retrieval (CIR) presents a significant challenge as it\nrequires jointly understanding a reference image and a modified textual\ninstruction to find relevant target images. Some existing methods attempt to\nuse a two-stage approach to further refine retrieval results. However, this\noften requires additional training of a ranking model. Despite the success of\nChain-of-Thought (CoT) techniques in reducing training costs for language\nmodels, their application in CIR tasks remains limited -- compressing visual\ninformation into text or relying on elaborate prompt designs. Besides, existing\nworks only utilize it for zero-shot CIR, as it is challenging to achieve\nsatisfactory results in supervised CIR with a well-trained model. In this work,\nwe proposed a framework that includes the Pyramid Matching Model with\nTraining-Free Refinement (PMTFR) to address these challenges. Through a simple\nbut effective module called Pyramid Patcher, we enhanced the Pyramid Matching\nModel's understanding of visual information at different granularities.\nInspired by representation engineering, we extracted representations from COT\ndata and injected them into the LVLMs. This approach allowed us to obtain\nrefined retrieval scores in the Training-Free Refinement paradigm without\nrelying on explicit textual reasoning, further enhancing performance. Extensive\nexperiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art\nmethods in supervised CIR tasks. The code will be made public.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11280", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11280", "abs": "https://arxiv.org/abs/2508.11280", "authors": ["Ruiyan Qi", "Congding Wen", "Weibo Zhou", "Shangsong Liang", "Lingbo Li"], "title": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought", "comment": null, "summary": "Evaluating large language models (LLMs) in specific domain like tourism\nremains challenging due to the prohibitive cost of annotated benchmarks and\npersistent issues like hallucinations. We propose $\\textbf{L}$able-Free\n$\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert\n$\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that\nleverages expert-derived reasoning structures-instead of labeled data-to access\nLLMs in tourism. First, we iteratively refine and validate hierarchical ToT\ncomponents through alignment with generic quality dimensions and expert\nfeedback. Results demonstrate the effectiveness of our systematically optimized\nexpert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we\napply LETToT's optimized expert ToT to evaluate models of varying scales\n(32B-671B parameters), revealing: (1) Scaling laws persist in specialized\ndomains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,\nDeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit\nreasoning architectures outperform counterparts in accuracy and conciseness\n($p<0.05$). Our work established a scalable, label-free paradigm for\ndomain-specific LLM evaluation, offering a robust alternative to conventional\nannotated benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11393", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11393", "abs": "https://arxiv.org/abs/2508.11393", "authors": ["Marc Brinner", "Sina Zarrieß"], "title": "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "comment": null, "summary": "We propose an end-to-end differentiable training paradigm for stable training\nof a rationalized transformer classifier. Our approach results in a single\nmodel that simultaneously classifies a sample and scores input tokens based on\ntheir relevance to the classification. To this end, we build on the widely-used\nthree-player-game for training rationalized models, which typically relies on\ntraining a rationale selector, a classifier and a complement classifier. We\nsimplify this approach by making a single model fulfill all three roles,\nleading to a more efficient training paradigm that is not susceptible to the\ncommon training instabilities that plague existing approaches. Further, we\nextend this paradigm to produce class-wise rationales while incorporating\nrecent advances in parameterizing and regularizing the resulting rationales,\nthus leading to substantially improved and state-of-the-art alignment with\nhuman annotations without any explicit supervision.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11141", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11141", "abs": "https://arxiv.org/abs/2508.11141", "authors": ["Bin Ma", "Yifei Zhang", "Yongjin Xian", "Qi Li", "Linna Zhou", "Gongxun Miao"], "title": "A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations", "comment": null, "summary": "Existing rumor detection methods often neglect the content within images as\nwell as the inherent relationships between contexts and images across different\nvisual scales, thereby resulting in the loss of critical information pertinent\nto rumor identification. To address these issues, this paper presents a novel\ncross-modal rumor detection scheme based on contrastive learning, namely the\nMulti-scale Image and Context Correlation exploration algorithm (MICC).\nSpecifically, we design an SCLIP encoder to generate unified semantic\nembeddings for text and multi-scale image patches through contrastive\npretraining, enabling their relevance to be measured via dot-product\nsimilarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is\nintroduced to identify image regions most relevant to the textual semantics,\nguided by mutual information maximization and the information bottleneck\nprinciple, through a Top-K selection strategy based on a cross-modal relevance\nmatrix constructed between the text and multi-scale image patches. Moreover, a\nscale-aware fusion network is designed to integrate the highly correlated\nmulti-scale image features with global text features by assigning adaptive\nweights to image regions based on their semantic importance and cross-modal\nrelevance. The proposed methodology has been extensively evaluated on two\nreal-world datasets. The experimental results demonstrate that it achieves a\nsubstantial performance improvement over existing state-of-the-art approaches\nin rumor detection, highlighting its effectiveness and potential for practical\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11310", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11310", "abs": "https://arxiv.org/abs/2508.11310", "authors": ["Beichen Guo", "Zhiyuan Wen", "Yu Yang", "Peng Gao", "Ruosong Yang", "Jiaxing Shen"], "title": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems", "comment": "Accepted to The 21st International Conference on Advanced Data Mining\n  and Applications (ADMA2025)", "summary": "The growing interest in automatic survey generation (ASG), a task that\ntraditionally required considerable time and effort, has been spurred by recent\nadvances in large language models (LLMs). With advancements in\nretrieval-augmented generation (RAG) and the rising popularity of multi-agent\nsystems (MASs), synthesizing academic surveys using LLMs has become a viable\napproach, thereby elevating the need for robust evaluation methods in this\ndomain. However, existing evaluation methods suffer from several limitations,\nincluding biased metrics, a lack of human preference, and an over-reliance on\nLLMs-as-judges. To address these challenges, we propose SGSimEval, a\ncomprehensive benchmark for Survey Generation with Similarity-Enhanced\nEvaluation that evaluates automatic survey generation systems by integrating\nassessments of the outline, content, and references, and also combines\nLLM-based scoring with quantitative metrics to provide a multifaceted\nevaluation framework. In SGSimEval, we also introduce human preference metrics\nthat emphasize both inherent quality and similarity to humans. Extensive\nexperiments reveal that current ASG systems demonstrate human-comparable\nsuperiority in outline generation, while showing significant room for\nimprovement in content and reference generation, and our evaluation metrics\nmaintain strong consistency with human assessments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "human preference", "consistency"], "score": 4}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11429", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11429", "abs": "https://arxiv.org/abs/2508.11429", "authors": ["Shivam Dubey"], "title": "HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor", "comment": null, "summary": "Automated humor generation with Large Language Models (LLMs) often yields\njokes that feel generic, repetitive, or tone-deaf because humor is deeply\nsituated and hinges on the listener's cultural background, mindset, and\nimmediate context. We introduce HumorPlanSearch, a modular pipeline that\nexplicitly models context through: (1) Plan-Search for diverse, topic-tailored\nstrategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and\nstylistic reasoning; (3) a Knowledge Graph to retrieve and adapt\nhigh-performing historical strategies; (4) novelty filtering via semantic\nembeddings; and (5) an iterative judge-driven revision loop. To evaluate\ncontext sensitivity and comedic quality, we propose the Humor Generation Score\n(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,\nand topic relevance. In experiments across nine topics with feedback from 13\nhuman judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent\n(p < 0.05) over a strong baseline. By foregrounding context at every stage from\nstrategy planning to multi-signal evaluation, HumorPlanSearch advances\nAI-driven humor toward more coherent, adaptive, and culturally attuned comedy.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11153", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11153", "abs": "https://arxiv.org/abs/2508.11153", "authors": ["Maoquan Zhang", "Bisser Raytchev", "Xiujuan Sun"], "title": "LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction", "comment": "The International Conference on Neural Information Processing\n  (ICONIP) 2025", "summary": "LEARN is a layout-aware diffusion framework designed to generate\npedagogically aligned illustrations for STEM education. It leverages a curated\nBookCover dataset that provides narrative layouts and structured visual cues,\nenabling the model to depict abstract and sequential scientific concepts with\nstrong semantic alignment. Through layout-conditioned generation, contrastive\nvisual-semantic training, and prompt modulation, LEARN produces coherent visual\nsequences that support mid-to-high-level reasoning in line with Bloom's\ntaxonomy while reducing extraneous cognitive load as emphasized by Cognitive\nLoad Theory. By fostering spatially organized and story-driven narratives, the\nframework counters fragmented attention often induced by short-form media and\npromotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates\npotential for integration with multimodal systems and curriculum-linked\nknowledge graphs to create adaptive, exploratory educational content. As the\nfirst generative approach to unify layout-based storytelling, semantic\nstructure learning, and cognitive scaffolding, LEARN represents a novel\ndirection for generative AI in education. The code and dataset will be released\nto facilitate future research and practical deployment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11442", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11442", "abs": "https://arxiv.org/abs/2508.11442", "authors": ["Bowen Zhang", "Zixin Song", "Chunquan Chen", "Qian-Wen Zhang", "Di Yin", "Xing Sun"], "title": "CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity", "comment": null, "summary": "Learning unified text embeddings that excel across diverse downstream tasks\nis a central goal in representation learning, yet negative transfer remains a\npersistent obstacle. This challenge is particularly pronounced when jointly\ntraining a single encoder for Information Retrieval (IR) and Semantic Textual\nSimilarity (STS), two essential but fundamentally disparate tasks for which\nnaive co-training typically yields steep performance trade-offs. We argue that\nresolving this conflict requires systematically decoupling task-specific\nlearning signals throughout the training pipeline. To this end, we introduce\nCoDiEmb, a unified framework that reconciles the divergent requirements of IR\nand STS in a collaborative yet distinct manner. CoDiEmb integrates three key\ninnovations for effective joint optimization: (1) Task-specialized objectives\npaired with a dynamic sampler that forms single-task batches and balances\nper-task updates, thereby preventing gradient interference. For IR, we employ a\ncontrastive loss with multiple positives and hard negatives, augmented by\ncross-device sampling. For STS, we adopt order-aware objectives that directly\noptimize correlation and ranking consistency. (2) A delta-guided model fusion\nstrategy that computes fine-grained merging weights for checkpoints by\nanalyzing each parameter's deviation from its pre-trained initialization,\nproving more effective than traditional Model Soups. (3) An efficient,\nsingle-stage training pipeline that is simple to implement and converges\nstably. Extensive experiments on 15 standard IR and STS benchmarks across three\nbase encoders validate CoDiEmb. Our results and analysis demonstrate that the\nframework not only mitigates cross-task trade-offs but also measurably improves\nthe geometric properties of the embedding space.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11167", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11167", "abs": "https://arxiv.org/abs/2508.11167", "authors": ["Jianhong Han", "Yupei Wang", "Liang Chen"], "title": "VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images", "comment": "Manuscript submitted to IEEE TGRS", "summary": "Unsupervised domain adaptation methods have been widely explored to bridge\ndomain gaps. However, in real-world remote-sensing scenarios, privacy and\ntransmission constraints often preclude access to source domain data, which\nlimits their practical applicability. Recently, Source-Free Object Detection\n(SFOD) has emerged as a promising alternative, aiming at cross-domain\nadaptation without relying on source data, primarily through a self-training\nparadigm. Despite its potential, SFOD frequently suffers from training collapse\ncaused by noisy pseudo-labels, especially in remote sensing imagery with dense\nobjects and complex backgrounds. Considering that limited target domain\nannotations are often feasible in practice, we propose a Vision\nfoundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised\nframework for SFOD in remote sensing images. VG-DETR integrates a Vision\nFoundation Model (VFM) into the training pipeline in a \"free lunch\" manner,\nleveraging a small amount of labeled target data to mitigate pseudo-label noise\nwhile improving the detector's feature-extraction capability. Specifically, we\nintroduce a VFM-guided pseudo-label mining strategy that leverages the VFM's\nsemantic priors to further assess the reliability of the generated\npseudo-labels. By recovering potentially correct predictions from\nlow-confidence outputs, our strategy improves pseudo-label quality and\nquantity. In addition, a dual-level VFM-guided alignment method is proposed,\nwhich aligns detector features with VFM embeddings at both the instance and\nimage levels. Through contrastive learning among fine-grained prototypes and\nsimilarity matching between feature maps, this dual-level alignment further\nenhances the robustness of feature representations against domain gaps.\nExtensive experiments demonstrate that VG-DETR achieves superior performance in\nsource-free remote sensing detection tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "fine-grained"], "score": 2}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11170", "abs": "https://arxiv.org/abs/2508.11170", "authors": ["Baihong Qian", "Haotian Fan", "Wenjie Liao", "Yunqiu Wang", "Tao Li", "Junhui Cui"], "title": "Better Supervised Fine-tuning for VQA: Integer-Only Loss", "comment": null, "summary": "With the rapid advancement of vision language models(VLM), their ability to\nassess visual content based on specific criteria and dimensions has become\nincreasingly critical for applications such as video-theme consistency\nassessment and visual quality scoring. However, existing methods often suffer\nfrom imprecise results and inefficient loss calculation, which limit the focus\nof the model on key evaluation indicators. To address this, we propose\nIOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to\nenhance their performance in video quality assessment tasks. The key innovation\nof IOVQA lies in its label construction and its targeted loss calculation\nmechanism. Specifically, during dataset curation, we constrain the model's\noutput to integers within the range of [10,50], ensuring numerical stability,\nand convert decimal Overall_MOS to integer before using them as labels. We also\nintroduce a target-mask strategy: when computing the loss, only the first\ntwo-digit-integer of the label is unmasked, forcing the model to learn the\ncritical components of the numerical evaluation. After fine-tuning the\nQwen2.5-VL model using the constructed dataset, experimental results\ndemonstrate that the proposed method significantly improves the model's\naccuracy and consistency in the VQA task, ranking 3rd in VQualA 2025\nGenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work\nhighlights the effectiveness of merely leaving integer labels during\nfine-tuning, providing an effective idea for optimizing VLMs in quantitative\nevaluation scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "accuracy", "criteria"], "score": 5}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11383", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11383", "abs": "https://arxiv.org/abs/2508.11383", "authors": ["Mikhail Seleznyov", "Mikhail Chaichuk", "Gleb Ershov", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs", "comment": null, "summary": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11536", "abs": "https://arxiv.org/abs/2508.11536", "authors": ["Maria Ryskina", "Greta Tuckute", "Alexander Fung", "Ashley Malkin", "Evelina Fedorenko"], "title": "Language models align with brain regions that represent concepts across modalities", "comment": "Accepted to COLM 2025. Code and data can be found at\n  https://github.com/ryskina/concepts-brain-llms", "summary": "Cognitive science and neuroscience have long faced the challenge of\ndisentangling representations of language from representations of conceptual\nmeaning. As the same problem arises in today's language models (LMs), we\ninvestigate the relationship between LM--brain alignment and two neural\nmetrics: (1) the level of brain activation during processing of sentences,\ntargeting linguistic processing, and (2) a novel measure of meaning consistency\nacross input modalities, which quantifies how consistently a brain region\nresponds to the same concept across paradigms (sentence, word cloud, image)\nusing an fMRI dataset (Pereira et al., 2018). Our experiments show that both\nlanguage-only and language-vision models predict the signal better in more\nmeaning-consistent areas of the brain, even when these areas are not strongly\nsensitive to language processing, suggesting that LMs might internally\nrepresent cross-modal conceptual meaning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11582", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11582", "abs": "https://arxiv.org/abs/2508.11582", "authors": ["Qiguang Chen", "Dengyun Peng", "Jinhao Liu", "HuiKang Su", "Jiannan Guan", "Libo Qin", "Wanxiang Che"], "title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models", "comment": "Preprint", "summary": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11582", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11582", "abs": "https://arxiv.org/abs/2508.11582", "authors": ["Qiguang Chen", "Dengyun Peng", "Jinhao Liu", "HuiKang Su", "Jiannan Guan", "Libo Qin", "Wanxiang Che"], "title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models", "comment": "Preprint", "summary": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11616", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11616", "abs": "https://arxiv.org/abs/2508.11616", "authors": ["Oscar Mañas", "Pierluca D'Oro", "Koustuv Sinha", "Adriana Romero-Soriano", "Michal Drozdzal", "Aishwarya Agrawal"], "title": "Controlling Multimodal LLMs via Reward-guided Decoding", "comment": "Published at ICCV 2025", "summary": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time compute"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11218", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11218", "abs": "https://arxiv.org/abs/2508.11218", "authors": ["Jialin Li", "Shuqi Wu", "Ning Wang"], "title": "A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving", "comment": null, "summary": "Re-Identification (ReID) is a critical technology in intelligent perception\nsystems, especially within autonomous driving, where onboard cameras must\nidentify pedestrians across views and time in real-time to support safe\nnavigation and trajectory prediction. However, the presence of uncertain or\nmissing input modalities--such as RGB, infrared, sketches, or textual\ndescriptions--poses significant challenges to conventional ReID approaches.\nWhile large-scale pre-trained models offer strong multimodal semantic modeling\ncapabilities, their computational overhead limits practical deployment in\nresource-constrained environments. To address these challenges, we propose a\nlightweight Uncertainty Modal Modeling (UMM) framework, which integrates a\nmultimodal token mapper, synthetic modality augmentation strategy, and\ncross-modal cue interactive learner. Together, these components enable unified\nfeature representation, mitigate the impact of missing modalities, and extract\ncomplementary information across different data types. Additionally, UMM\nleverages CLIP's vision-language alignment ability to fuse multimodal inputs\nefficiently without extensive finetuning. Experimental results demonstrate that\nUMM achieves strong robustness, generalization, and computational efficiency\nunder uncertain modality conditions, offering a scalable and practical solution\nfor pedestrian re-identification in autonomous driving scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11141", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11141", "abs": "https://arxiv.org/abs/2508.11141", "authors": ["Bin Ma", "Yifei Zhang", "Yongjin Xian", "Qi Li", "Linna Zhou", "Gongxun Miao"], "title": "A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations", "comment": null, "summary": "Existing rumor detection methods often neglect the content within images as\nwell as the inherent relationships between contexts and images across different\nvisual scales, thereby resulting in the loss of critical information pertinent\nto rumor identification. To address these issues, this paper presents a novel\ncross-modal rumor detection scheme based on contrastive learning, namely the\nMulti-scale Image and Context Correlation exploration algorithm (MICC).\nSpecifically, we design an SCLIP encoder to generate unified semantic\nembeddings for text and multi-scale image patches through contrastive\npretraining, enabling their relevance to be measured via dot-product\nsimilarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is\nintroduced to identify image regions most relevant to the textual semantics,\nguided by mutual information maximization and the information bottleneck\nprinciple, through a Top-K selection strategy based on a cross-modal relevance\nmatrix constructed between the text and multi-scale image patches. Moreover, a\nscale-aware fusion network is designed to integrate the highly correlated\nmulti-scale image features with global text features by assigning adaptive\nweights to image regions based on their semantic importance and cross-modal\nrelevance. The proposed methodology has been extensively evaluated on two\nreal-world datasets. The experimental results demonstrate that it achieves a\nsubstantial performance improvement over existing state-of-the-art approaches\nin rumor detection, highlighting its effectiveness and potential for practical\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11262", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11262", "abs": "https://arxiv.org/abs/2508.11262", "authors": ["Aiswarya Konavoor", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Vision-Language Models display a strong gender bias", "comment": null, "summary": "Vision-language models (VLM) align images and text in a shared representation\nspace that is useful for retrieval and zero-shot transfer. Yet, this alignment\ncan encode and amplify social stereotypes in subtle ways that are not obvious\nfrom standard accuracy metrics. In this study, we test whether the contrastive\nvision-language encoder exhibits gender-linked associations when it places\nembeddings of face images near embeddings of short phrases that describe\noccupations and activities. We assemble a dataset of 220 face photographs split\nby perceived binary gender and a set of 150 unique statements distributed\nacross six categories covering emotional labor, cognitive labor, domestic\nlabor, technical labor, professional roles, and physical labor. We compute\nunit-norm image embeddings for every face and unit-norm text embeddings for\nevery statement, then define a statement-level association score as the\ndifference between the mean cosine similarity to the male set and the mean\ncosine similarity to the female set, where positive values indicate stronger\nassociation with the male set and negative values indicate stronger association\nwith the female set. We attach bootstrap confidence intervals by resampling\nimages within each gender group, aggregate by category with a separate\nbootstrap over statements, and run a label-swap null model that estimates the\nlevel of mean absolute association we would expect if no gender structure were\npresent. The outcome is a statement-wise and category-wise map of gender\nassociations in a contrastive vision-language space, accompanied by\nuncertainty, simple sanity checks, and a robust gender bias evaluation\nframework.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11265", "abs": "https://arxiv.org/abs/2508.11265", "authors": ["Pei He", "Lingling Li", "Licheng Jiao", "Ronghua Shang", "Fang Liu", "Shuang Wang", "Xu Liu", "Wenping Ma"], "title": "Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds", "comment": "to be published in International Conference on Computer Vision, ICCV\n  2025", "summary": "Domain generalization in 3D segmentation is a critical challenge in deploying\nmodels to unseen environments. Current methods mitigate the domain shift by\naugmenting the data distribution of point clouds. However, the model learns\nglobal geometric patterns in point clouds while ignoring the category-level\ndistribution and alignment. In this paper, a category-level geometry learning\nframework is proposed to explore the domain-invariant geometric features for\ndomain generalized 3D semantic segmentation. Specifically, Category-level\nGeometry Embedding (CGE) is proposed to perceive the fine-grained geometric\nproperties of point cloud features, which constructs the geometric properties\nof each class and couples geometric embedding to semantic learning. Secondly,\nGeometric Consistent Learning (GCL) is proposed to simulate the latent 3D\ndistribution and align the category-level geometric embeddings, allowing the\nmodel to focus on the geometric invariant information to improve\ngeneralization. Experimental results verify the effectiveness of the proposed\nmethod, which has very competitive segmentation accuracy compared with the\nstate-of-the-art domain generalized point cloud methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11616", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11616", "abs": "https://arxiv.org/abs/2508.11616", "authors": ["Oscar Mañas", "Pierluca D'Oro", "Koustuv Sinha", "Adriana Romero-Soriano", "Michal Drozdzal", "Aishwarya Agrawal"], "title": "Controlling Multimodal LLMs via Reward-guided Decoding", "comment": "Published at ICCV 2025", "summary": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time compute"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11272", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11272", "abs": "https://arxiv.org/abs/2508.11272", "authors": ["Jun Li", "Kai Li", "Shaoguo Liu", "Tingting Gao"], "title": "Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering", "comment": null, "summary": "Composed Image Retrieval (CIR) presents a significant challenge as it\nrequires jointly understanding a reference image and a modified textual\ninstruction to find relevant target images. Some existing methods attempt to\nuse a two-stage approach to further refine retrieval results. However, this\noften requires additional training of a ranking model. Despite the success of\nChain-of-Thought (CoT) techniques in reducing training costs for language\nmodels, their application in CIR tasks remains limited -- compressing visual\ninformation into text or relying on elaborate prompt designs. Besides, existing\nworks only utilize it for zero-shot CIR, as it is challenging to achieve\nsatisfactory results in supervised CIR with a well-trained model. In this work,\nwe proposed a framework that includes the Pyramid Matching Model with\nTraining-Free Refinement (PMTFR) to address these challenges. Through a simple\nbut effective module called Pyramid Patcher, we enhanced the Pyramid Matching\nModel's understanding of visual information at different granularities.\nInspired by representation engineering, we extracted representations from COT\ndata and injected them into the LVLMs. This approach allowed us to obtain\nrefined retrieval scores in the Training-Free Refinement paradigm without\nrelying on explicit textual reasoning, further enhancing performance. Extensive\nexperiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art\nmethods in supervised CIR tasks. The code will be made public.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11313", "abs": "https://arxiv.org/abs/2508.11313", "authors": ["Weijia Liu", "Jiuxin Cao", "Bo Miao", "Zhiheng Fu", "Xuelin Zhu", "Jiawei Ge", "Bo Liu", "Mehwish Nasim", "Ajmal Mian"], "title": "Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval", "comment": "Accepted by IJCAI 2025", "summary": "Current text-driven Video Moment Retrieval (VMR) methods encode all video\nclips, including irrelevant ones, disrupting multimodal alignment and hindering\noptimization. To this end, we propose a denoise-then-retrieve paradigm that\nexplicitly filters text-irrelevant clips from videos and then retrieves the\ntarget moment using purified multimodal representations. Following this\nparadigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising\nText-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF)\nmodules. TCD integrates cross-attention and structured state space blocks to\ndynamically identify noisy clips and produce a noise mask to purify multimodal\nvideo representations. TRF further distills a single query embedding from\npurified video representations and aligns it with the text embedding, serving\nas auxiliary supervision for denoising during training. Finally, we perform\nconditional retrieval using text embeddings on purified video representations\nfor accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that\nour approach surpasses state-of-the-art methods on all metrics. Furthermore,\nour denoise-then-retrieve paradigm is adaptable and can be seamlessly\nintegrated into advanced VMR models to boost performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11317", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.11317", "abs": "https://arxiv.org/abs/2508.11317", "authors": ["Yuchen Zhou", "Jiayu Tang", "Shuo Yang", "Xiaoyan Xiao", "Yuqin Dai", "Wenhao Yang", "Chao Gou", "Xiaobo Xia", "Tat-Seng Chua"], "title": "Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs), exemplified by CLIP, have emerged as\nfoundational for multimodal intelligence. However, their capacity for logical\nunderstanding remains significantly underexplored, resulting in critical\n''logical blindspots'' that limit their reliability in practical applications.\nTo systematically diagnose this, we introduce LogicBench, a comprehensive\nbenchmark with over 50,000 vision-language pairs across 9 logical categories\nand 4 diverse scenarios: images, videos, anomaly detection, and medical\ndiagnostics. Our evaluation reveals that existing VLMs, even the\nstate-of-the-art ones, fall at over 40 accuracy points below human performance,\nparticularly in challenging tasks like Causality and Conditionality,\nhighlighting their reliance on surface semantics over critical logical\nstructures. To bridge this gap, we propose LogicCLIP, a novel training\nframework designed to boost VLMs' logical sensitivity through advancements in\nboth data generation and optimization objectives. LogicCLIP utilizes\nlogic-aware data generation and a contrastive learning strategy that combines\ncoarse-grained alignment, a fine-grained multiple-choice objective, and a novel\nlogical structure-aware objective. Extensive experiments demonstrate\nLogicCLIP's substantial improvements in logical comprehension across all\nLogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP\nretains, and often surpasses, competitive performance on general\nvision-language benchmarks, demonstrating that the enhanced logical\nunderstanding does not come at the expense of general alignment. We believe\nthat LogicBench and LogicCLIP will be important resources for advancing VLM\nlogical capabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "reliability", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11433", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11433", "abs": "https://arxiv.org/abs/2508.11433", "authors": ["Qian Liang", "Yujia Wu", "Kuncheng Li", "Jiwei Wei", "Shiyuan He", "Jinyu Guo", "Ning Xie"], "title": "MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) with unified architectures excel\nacross a wide range of vision-language tasks, yet aligning them with\npersonalized image generation remains a significant challenge. Existing methods\nfor MLLMs are frequently subject-specific, demanding a data-intensive\nfine-tuning process for every new subject, which limits their scalability. In\nthis paper, we introduce MM-R1, a framework that integrates a cross-modal\nChain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of\nunified MLLMs for personalized image generation. Specifically, we structure\npersonalization as an integrated visual reasoning and generation process: (1)\ngrounding subject concepts by interpreting and understanding user-provided\nimages and contextual cues, and (2) generating personalized images conditioned\non both the extracted subject representations and user prompts. To further\nenhance the reasoning capability, we adopt Grouped Reward Proximal Policy\nOptimization (GRPO) to explicitly align the generation. Experiments demonstrate\nthat MM-R1 unleashes the personalization capability of unified MLLMs to\ngenerate images with high subject fidelity and strong text alignment in a\nzero-shot manner.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11603", "abs": "https://arxiv.org/abs/2508.11603", "authors": ["Zhe Zhu", "Honghua Chen", "Peng Li", "Mingqiang Wei"], "title": "CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion", "comment": null, "summary": "Text-driven 3D editing seeks to modify 3D scenes according to textual\ndescriptions, and most existing approaches tackle this by adapting pre-trained\n2D image editors to multi-view inputs. However, without explicit control over\nmulti-view information exchange, they often fail to maintain cross-view\nconsistency, leading to insufficient edits and blurry details. We introduce\nCoreEditor, a novel framework for consistent text-to-3D editing. The key\ninnovation is a correspondence-constrained attention mechanism that enforces\nprecise interactions between pixels expected to remain consistent throughout\nthe diffusion denoising process. Beyond relying solely on geometric alignment,\nwe further incorporate semantic similarity estimated during denoising, enabling\nmore reliable correspondence modeling and robust multi-view editing. In\naddition, we design a selective editing pipeline that allows users to choose\npreferred results from multiple candidates, offering greater flexibility and\nuser control. Extensive experiments show that CoreEditor produces high-quality,\n3D-consistent edits with sharper details, significantly outperforming prior\nmethods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11616", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11616", "abs": "https://arxiv.org/abs/2508.11616", "authors": ["Oscar Mañas", "Pierluca D'Oro", "Koustuv Sinha", "Adriana Romero-Soriano", "Michal Drozdzal", "Aishwarya Agrawal"], "title": "Controlling Multimodal LLMs via Reward-guided Decoding", "comment": "Published at ICCV 2025", "summary": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time compute"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11624", "abs": "https://arxiv.org/abs/2508.11624", "authors": ["Niki Foteinopoulou", "Ignas Budvytis", "Stephan Liwicki"], "title": "LoRAtorio: An intrinsic approach to LoRA Skill Composition", "comment": "32 pages, 17 figures", "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted technique in\ntext-to-image diffusion models, enabling the personalisation of visual concepts\nsuch as characters, styles, and objects. However, existing approaches struggle\nto effectively compose multiple LoRA adapters, particularly in open-ended\nsettings where the number and nature of required skills are not known in\nadvance. In this work, we present LoRAtorio, a novel train-free framework for\nmulti-LoRA composition that leverages intrinsic model behaviour. Our method is\nmotivated by two key observations: (1) LoRA adapters trained on narrow domains\nproduce denoised outputs that diverge from the base model, and (2) when\noperating out-of-distribution, LoRA outputs show behaviour closer to the base\nmodel than when conditioned in distribution. The balance between these two\nobservations allows for exceptional performance in the single LoRA scenario,\nwhich nevertheless deteriorates when multiple LoRAs are loaded. Our method\noperates in the latent space by dividing it into spatial patches and computing\ncosine similarity between each patch's predicted noise and that of the base\nmodel. These similarities are used to construct a spatially-aware weight\nmatrix, which guides a weighted aggregation of LoRA outputs. To address domain\ndrift, we further propose a modification to classifier-free guidance that\nincorporates the base model's unconditional score into the composition. We\nextend this formulation to a dynamic module selection setting, enabling\ninference-time selection of relevant LoRA adapters from a large pool. LoRAtorio\nachieves state-of-the-art performance, showing up to a 1.3% improvement in\nClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises\neffectively to multiple latent diffusion models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
{"id": "2508.11203", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "51-04", "I.3.8; I.4.9"], "pdf": "https://arxiv.org/pdf/2508.11203", "abs": "https://arxiv.org/abs/2508.11203", "authors": ["Seungmi Lee", "Kwan Yun", "Junyong Noh"], "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation", "comment": "Pacific graphics 2025, CGF, 15 pages", "summary": "We introduce StyleMM, a novel framework that can construct a stylized 3D\nMorphable Model (3DMM) based on user-defined text descriptions specifying a\ntarget style. Building upon a pre-trained mesh deformation network and a\ntexture generator for original 3DMM-based realistic human faces, our approach\nfine-tunes these models using stylized facial images generated via text-guided\nimage-to-image (i2i) translation with a diffusion model, which serve as\nstylization targets for the rendered mesh. To prevent undesired changes in\nidentity, facial alignment, or expressions during i2i translation, we introduce\na stylization method that explicitly preserves the facial attributes of the\nsource image. By maintaining these critical attributes during image\nstylization, the proposed approach ensures consistent 3D style transfer across\nthe 3DMM parameter space through image-based training. Once trained, StyleMM\nenables feed-forward generation of stylized face meshes with explicit control\nover shape, expression, and texture parameters, producing meshes with\nconsistent vertex connectivity and animatability. Quantitative and qualitative\nevaluations demonstrate that our approach outperforms state-of-the-art methods\nin terms of identity-level facial diversity and stylization capability. The\ncode and videos are available at\n[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-18.jsonl"}
