{"id": "2503.12001", "pdf": "https://arxiv.org/pdf/2503.12001", "abs": "https://arxiv.org/abs/2503.12001", "authors": ["Peizhen Zheng", "Longfei Wei", "Dongjing Jiang", "Jianfei Zhang"], "title": "3D Gaussian Splatting against Moving Objects for High-Fidelity Street Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "The accurate reconstruction of dynamic street scenes is critical for\napplications in autonomous driving, augmented reality, and virtual reality.\nTraditional methods relying on dense point clouds and triangular meshes\nstruggle with moving objects, occlusions, and real-time processing constraints,\nlimiting their effectiveness in complex urban environments. While multi-view\nstereo and neural radiance fields have advanced 3D reconstruction, they face\nchallenges in computational efficiency and handling scene dynamics. This paper\nproposes a novel 3D Gaussian point distribution method for dynamic street scene\nreconstruction. Our approach introduces an adaptive transparency mechanism that\neliminates moving objects while preserving high-fidelity static scene details.\nAdditionally, iterative refinement of Gaussian point distribution enhances\ngeometric accuracy and texture representation. We integrate directional\nencoding with spatial position optimization to optimize storage and rendering\nefficiency, reducing redundancy while maintaining scene integrity. Experimental\nresults demonstrate that our method achieves high reconstruction quality,\nimproved rendering performance, and adaptability in large-scale dynamic\nenvironments. These contributions establish a robust framework for real-time,\nhigh-precision 3D reconstruction, advancing the practicality of dynamic scene\nmodeling across multiple applications. The source code for this work is\navailable to the public at https://github.com/deepcoxcom/3dgs", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "iterative refinement"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12150", "pdf": "https://arxiv.org/pdf/2503.12150", "abs": "https://arxiv.org/abs/2503.12150", "authors": ["Hongyu Sun", "Qiuhong Ke", "Ming Cheng", "Yongcai Wang", "Deying Li", "Chenhui Gou", "Jianfei Cai"], "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables", "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12271", "pdf": "https://arxiv.org/pdf/2503.12271", "abs": "https://arxiv.org/abs/2503.12271", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Akash Gokul", "Arsh Koneru", "Yusuke Kato", "Kazuki Kozuka", "Aditya Grover"], "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection", "categories": ["cs.CV"], "comment": "17 pages, 9 figures", "summary": "The predominant approach to advancing text-to-image generation has been\ntraining-time scaling, where larger models are trained on more data using\ngreater computational resources. While effective, this approach is\ncomputationally expensive, leading to growing interest in inference-time\nscaling to improve performance. Currently, inference-time scaling for\ntext-to-image diffusion models is largely limited to best-of-N sampling, where\nmultiple images are generated per prompt and a selection model chooses the best\noutput. Inspired by the recent success of reasoning models like DeepSeek-R1 in\nthe language domain, we introduce an alternative to naive best-of-N sampling by\nequipping text-to-image Diffusion Transformers with in-context reflection\ncapabilities. We propose Reflect-DiT, a method that enables Diffusion\nTransformers to refine their generations using in-context examples of\npreviously generated images alongside textual feedback describing necessary\nimprovements. Instead of passively relying on random sampling and hoping for a\nbetter result in a future generation, Reflect-DiT explicitly tailors its\ngenerations to address specific aspects requiring enhancement. Experimental\nresults demonstrate that Reflect-DiT improves performance on the GenEval\nbenchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it\nachieves a new state-of-the-art score of 0.81 on GenEval while generating only\n20 samples per prompt, surpassing the previous best score of 0.80, which was\nobtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples\nunder the best-of-N approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12532", "pdf": "https://arxiv.org/pdf/2503.12532", "abs": "https://arxiv.org/abs/2503.12532", "authors": ["Fanbin Lu", "Zhisheng Zhong", "Ziqin Wei", "Shu Liu", "Chi-Wing Fu", "Jiaya Jia"], "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scaling law"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12001", "pdf": "https://arxiv.org/pdf/2503.12001", "abs": "https://arxiv.org/abs/2503.12001", "authors": ["Peizhen Zheng", "Longfei Wei", "Dongjing Jiang", "Jianfei Zhang"], "title": "3D Gaussian Splatting against Moving Objects for High-Fidelity Street Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "The accurate reconstruction of dynamic street scenes is critical for\napplications in autonomous driving, augmented reality, and virtual reality.\nTraditional methods relying on dense point clouds and triangular meshes\nstruggle with moving objects, occlusions, and real-time processing constraints,\nlimiting their effectiveness in complex urban environments. While multi-view\nstereo and neural radiance fields have advanced 3D reconstruction, they face\nchallenges in computational efficiency and handling scene dynamics. This paper\nproposes a novel 3D Gaussian point distribution method for dynamic street scene\nreconstruction. Our approach introduces an adaptive transparency mechanism that\neliminates moving objects while preserving high-fidelity static scene details.\nAdditionally, iterative refinement of Gaussian point distribution enhances\ngeometric accuracy and texture representation. We integrate directional\nencoding with spatial position optimization to optimize storage and rendering\nefficiency, reducing redundancy while maintaining scene integrity. Experimental\nresults demonstrate that our method achieves high reconstruction quality,\nimproved rendering performance, and adaptability in large-scale dynamic\nenvironments. These contributions establish a robust framework for real-time,\nhigh-precision 3D reconstruction, advancing the practicality of dynamic scene\nmodeling across multiple applications. The source code for this work is\navailable to the public at https://github.com/deepcoxcom/3dgs", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "iterative refinement"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12150", "pdf": "https://arxiv.org/pdf/2503.12150", "abs": "https://arxiv.org/abs/2503.12150", "authors": ["Hongyu Sun", "Qiuhong Ke", "Ming Cheng", "Yongcai Wang", "Deying Li", "Chenhui Gou", "Jianfei Cai"], "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables", "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12271", "pdf": "https://arxiv.org/pdf/2503.12271", "abs": "https://arxiv.org/abs/2503.12271", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Akash Gokul", "Arsh Koneru", "Yusuke Kato", "Kazuki Kozuka", "Aditya Grover"], "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection", "categories": ["cs.CV"], "comment": "17 pages, 9 figures", "summary": "The predominant approach to advancing text-to-image generation has been\ntraining-time scaling, where larger models are trained on more data using\ngreater computational resources. While effective, this approach is\ncomputationally expensive, leading to growing interest in inference-time\nscaling to improve performance. Currently, inference-time scaling for\ntext-to-image diffusion models is largely limited to best-of-N sampling, where\nmultiple images are generated per prompt and a selection model chooses the best\noutput. Inspired by the recent success of reasoning models like DeepSeek-R1 in\nthe language domain, we introduce an alternative to naive best-of-N sampling by\nequipping text-to-image Diffusion Transformers with in-context reflection\ncapabilities. We propose Reflect-DiT, a method that enables Diffusion\nTransformers to refine their generations using in-context examples of\npreviously generated images alongside textual feedback describing necessary\nimprovements. Instead of passively relying on random sampling and hoping for a\nbetter result in a future generation, Reflect-DiT explicitly tailors its\ngenerations to address specific aspects requiring enhancement. Experimental\nresults demonstrate that Reflect-DiT improves performance on the GenEval\nbenchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it\nachieves a new state-of-the-art score of 0.81 on GenEval while generating only\n20 samples per prompt, surpassing the previous best score of 0.80, which was\nobtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples\nunder the best-of-N approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12532", "pdf": "https://arxiv.org/pdf/2503.12532", "abs": "https://arxiv.org/abs/2503.12532", "authors": ["Fanbin Lu", "Zhisheng Zhong", "Ziqin Wei", "Shu Liu", "Chi-Wing Fu", "Jiaya Jia"], "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scaling law"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12866", "pdf": "https://arxiv.org/pdf/2503.12866", "abs": "https://arxiv.org/abs/2503.12866", "authors": ["Chenyu Zhang", "Kunlun Xu", "Zichen Liu", "Yuxin Peng", "Jiahuan Zhou"], "title": "SCAP: Transductive Test-Time Adaptation via Supportive Clique-based Attribute Prompting", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Vision-language models (VLMs) encounter considerable challenges when adapting\nto domain shifts stemming from changes in data distribution. Test-time\nadaptation (TTA) has emerged as a promising approach to enhance VLM performance\nunder such conditions. In practice, test data often arrives in batches, leading\nto increasing interest in the transductive TTA setting. However, existing TTA\nmethods primarily focus on individual test samples, overlooking crucial\ncross-sample correlations within a batch. While recent ViT-based TTA methods\nhave introduced batch-level adaptation, they remain suboptimal for VLMs due to\ninadequate integration of the text modality. To address these limitations, we\npropose a novel transductive TTA framework, Supportive Clique-based Attribute\nPrompting (SCAP), which effectively combines visual and textual information to\nenhance adaptation by generating fine-grained attribute prompts across test\nbatches. SCAP first forms supportive cliques of test samples in an unsupervised\nmanner based on visual similarity and learns an attribute prompt for each\nclique, capturing shared attributes critical for adaptation. For each test\nsample, SCAP aggregates attribute prompts from its associated cliques,\nproviding enriched contextual information. To ensure adaptability over time, we\nincorporate a retention module that dynamically updates attribute prompts and\ntheir associated attributes as new data arrives. Comprehensive experiments\nacross multiple benchmarks demonstrate that SCAP outperforms existing\nstate-of-the-art methods, significantly advancing VLM generalization under\ndomain shifts. Our code is available at\nhttps://github.com/zhoujiahuan1991/CVPR2025-SCAP.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13012", "pdf": "https://arxiv.org/pdf/2503.13012", "abs": "https://arxiv.org/abs/2503.13012", "authors": ["Xingguo Lv", "Xingbo Dong", "Liwen Wang", "Jiewen Yang", "Lei Zhao", "Bin Pu", "Zhe Jin", "Xuejun Li"], "title": "Test-Time Domain Generalization via Universe Learning: A Multi-Graph Matching Approach for Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite domain generalization (DG) has significantly addressed the\nperformance degradation of pre-trained models caused by domain shifts, it often\nfalls short in real-world deployment. Test-time adaptation (TTA), which adjusts\na learned model using unlabeled test data, presents a promising solution.\nHowever, most existing TTA methods struggle to deliver strong performance in\nmedical image segmentation, primarily because they overlook the crucial prior\nknowledge inherent to medical images. To address this challenge, we incorporate\nmorphological information and propose a framework based on multi-graph\nmatching. Specifically, we introduce learnable universe embeddings that\nintegrate morphological priors during multi-source training, along with novel\nunsupervised test-time paradigms for domain adaptation. This approach\nguarantees cycle-consistency in multi-matching while enabling the model to more\neffectively capture the invariant priors of unseen data, significantly\nmitigating the effects of domain shifts. Extensive experiments demonstrate that\nour method outperforms other state-of-the-art approaches on two medical image\nsegmentation benchmarks for both multi-source and single-source domain\ngeneralization tasks. The source code is available at\nhttps://github.com/Yore0/TTDG-MGM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13424", "pdf": "https://arxiv.org/pdf/2503.13424", "abs": "https://arxiv.org/abs/2503.13424", "authors": ["Xinyu Lian", "Zichao Yu", "Ruiming Liang", "Yitong Wang", "Li Ray Luo", "Kaixu Chen", "Yuanzhen Zhou", "Qihong Tang", "Xudong Xu", "Zhaoyang Lyu", "Bo Dai", "Jiangmiao Pang"], "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated Objects via Procedural Generation", "categories": ["cs.CV"], "comment": "Project page: https://infinite-mobility.github.io 10 pages,12 figures", "summary": "Large-scale articulated objects with high quality are desperately needed for\nmultiple tasks related to embodied AI. Most existing methods for creating\narticulated objects are either data-driven or simulation based, which are\nlimited by the scale and quality of the training data or the fidelity and heavy\nlabour of the simulation. In this paper, we propose Infinite Mobility, a novel\nmethod for synthesizing high-fidelity articulated objects through procedural\ngeneration. User study and quantitative evaluation demonstrate that our method\ncan produce results that excel current state-of-the-art methods and are\ncomparable to human-annotated datasets in both physics property and mesh\nquality. Furthermore, we show that our synthetic data can be used as training\ndata for generative models, enabling next-step scaling up. Code is available at\nhttps://github.com/Intern-Nexus/Infinite-Mobility", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13309", "pdf": "https://arxiv.org/pdf/2503.13309", "abs": "https://arxiv.org/abs/2503.13309", "authors": ["Farnoush Bayatmakou", "Reza Taleei", "Milad Amir Toutounchian", "Arash Mohammadi"], "title": "Integrating AI for Human-Centric Breast Cancer Diagnostics: A Multi-Scale and Multi-View Swin Transformer Framework", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite advancements in Computer-Aided Diagnosis (CAD) systems, breast cancer\nremains one of the leading causes of cancer-related deaths among women\nworldwide. Recent breakthroughs in Artificial Intelligence (AI) have shown\nsignificant promise in development of advanced Deep Learning (DL) architectures\nfor breast cancer diagnosis through mammography. In this context, the paper\nfocuses on the integration of AI within a Human-Centric workflow to enhance\nbreast cancer diagnostics. Key challenges are, however, largely overlooked such\nas reliance on detailed tumor annotations and susceptibility to missing views,\nparticularly during test time. To address these issues, we propose a hybrid,\nmulti-scale and multi-view Swin Transformer-based framework (MSMV-Swin) that\nenhances diagnostic robustness and accuracy. The proposed MSMV-Swin framework\nis designed to work as a decision-support tool, helping radiologists analyze\nmulti-view mammograms more effectively. More specifically, the MSMV-Swin\nframework leverages the Segment Anything Model (SAM) to isolate the breast\nlobe, reducing background noise and enabling comprehensive feature extraction.\nThe multi-scale nature of the proposed MSMV-Swin framework accounts for\ntumor-specific regions as well as the spatial characteristics of tissues\nsurrounding the tumor, capturing both localized and contextual information. The\nintegration of contextual and localized data ensures that MSMV-Swin's outputs\nalign with the way radiologists interpret mammograms, fostering better human-AI\ninteraction and trust. A hybrid fusion structure is then designed to ensure\nrobustness against missing views, a common occurrence in clinical practice when\nonly a single mammogram view is available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11742", "pdf": "https://arxiv.org/pdf/2503.11742", "abs": "https://arxiv.org/abs/2503.11742", "authors": ["Moreno D'Incà", "Elia Peruzzo", "Xingqian Xu", "Humphrey Shi", "Nicu Sebe", "Massimiliano Mancini"], "title": "Safe Vision-Language Models via Unsafe Weights Manipulation", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Vision-language models (VLMs) often inherit the biases and unsafe\nassociations present within their large-scale training dataset. While recent\napproaches mitigate unsafe behaviors, their evaluation focuses on how safe the\nmodel is on unsafe inputs, ignoring potential shortcomings on safe ones. In\nthis paper, we first revise safety evaluation by introducing SafeGround, a new\nset of metrics that evaluate safety at different levels of granularity. With\nthis metric, we uncover a surprising issue of training-based methods: they make\nthe model less safe on safe inputs. From this finding, we take a different\ndirection and explore whether it is possible to make a model safer without\ntraining, introducing Unsafe Weights Manipulation (UWM). UWM uses a calibration\nset of safe and unsafe instances to compare activations between safe and unsafe\ncontent, identifying the most important parameters for processing the latter.\nTheir values are then manipulated via negation. Experiments show that UWM\nachieves the best tradeoff between safety and knowledge preservation,\nconsistently improving VLMs on unsafe queries while outperforming even\ntraining-based state-of-the-art methods on safe ones.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11781", "pdf": "https://arxiv.org/pdf/2503.11781", "abs": "https://arxiv.org/abs/2503.11781", "authors": ["Artem Nikonorov", "Georgy Perevozchikov", "Andrei Korepanov", "Nancy Mehta", "Mahmoud Afifi", "Egor Ershov", "Radu Timofte"], "title": "Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks", "categories": ["cs.CV"], "comment": null, "summary": "We present cmKAN, a versatile framework for color matching. Given an input\nimage with colors from a source color distribution, our method effectively and\naccurately maps these colors to match a target color distribution in both\nsupervised and unsupervised settings. Our framework leverages the spline\ncapabilities of Kolmogorov-Arnold Networks (KANs) to model the color matching\nbetween source and target distributions. Specifically, we developed a\nhypernetwork that generates spatially varying weight maps to control the\nnonlinear splines of a KAN, enabling accurate color matching. As part of this\nwork, we introduce a first large-scale dataset of paired images captured by two\ndistinct cameras and evaluate the efficacy of our and existing methods in\nmatching colors. We evaluated our approach across various color-matching tasks,\nincluding: (1) raw-to-raw mapping, where the source color distribution is in\none camera's raw color space and the target in another camera's raw space; (2)\nraw-to-sRGB mapping, where the source color distribution is in a camera's raw\nspace and the target is in the display sRGB space, emulating the color\nrendering of a camera ISP; and (3) sRGB-to-sRGB mapping, where the goal is to\ntransfer colors from a source sRGB space (e.g., produced by a source camera\nISP) to a target sRGB space (e.g., from a different camera ISP). The results\nshow that our method outperforms existing approaches by 37.3% on average for\nsupervised and unsupervised cases while remaining lightweight compared to other\nmethods. The codes, dataset, and pre-trained models are available at:\nhttps://github.com/gosha20777/cmKAN", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11792", "pdf": "https://arxiv.org/pdf/2503.11792", "abs": "https://arxiv.org/abs/2503.11792", "authors": ["Peizhi Yan", "Rabab K. Ward", "Dan Wang", "Qiang Tang", "Shan Du"], "title": "StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model", "categories": ["cs.CV"], "comment": "13 pages, work was completed in 2023", "summary": "For 3D face modeling, the recently developed 3D-aware neural rendering\nmethods are able to render photorealistic face images with arbitrary viewing\ndirections. The training of the parametric controllable 3D-aware face models,\nhowever, still relies on a large-scale dataset that is lab-collected. To\naddress this issue, this paper introduces \"StyleMorpheus\", the first\nstyle-based neural 3D Morphable Face Model (3DMM) that is trained on\nin-the-wild images. It inherits 3DMM's disentangled controllability (over face\nidentity, expression, and appearance) but without the need for accurately\nreconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder\nstructure. The encoder aims at learning a representative disentangled\nparametric code space and the decoder improves the disentanglement using shape\nand appearance-related style codes in the different sub-modules of the network.\nFurthermore, we fine-tune the decoder through style-based generative\nadversarial learning to achieve photorealistic 3D rendering quality. The\nproposed style-based design enables StyleMorpheus to achieve state-of-the-art\n3D-aware face reconstruction results, while also allowing disentangled control\nof the reconstructed face. Our model achieves real-time rendering speed,\nallowing its use in virtual reality applications. We also demonstrate the\ncapability of the proposed style-based design in face editing applications such\nas style mixing and color editing. Project homepage:\nhttps://github.com/ubc-3d-vision-lab/StyleMorpheus.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11807", "pdf": "https://arxiv.org/pdf/2503.11807", "abs": "https://arxiv.org/abs/2503.11807", "authors": ["Sanayya A", "Amoolya Shetty", "Abhijeet Sharma", "Venkatesh Ravichandran", "Masthan Wali Gosuvarapalli", "Sarthak Jain", "Priyamvada Nanjundiah", "Ujjal Kr Dutta", "Divya Sharma"], "title": "Mitigating Bad Ground Truth in Supervised Machine Learning based Crop Classification: A Multi-Level Framework with Sentinel-2 Images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted In IEEE India Geoscience and Remote Sensing Symposium\n  (InGARSS) 2024", "summary": "In agricultural management, precise Ground Truth (GT) data is crucial for\naccurate Machine Learning (ML) based crop classification. Yet, issues like crop\nmislabeling and incorrect land identification are common. We propose a\nmulti-level GT cleaning framework while utilizing multi-temporal Sentinel-2\ndata to address these issues. Specifically, this framework utilizes generating\nembeddings for farmland, clustering similar crop profiles, and identification\nof outliers indicating GT errors. We validated clusters with False Colour\nComposite (FCC) checks and used distance-based metrics to scale and automate\nthis verification process. The importance of cleaning the GT data became\napparent when the models were trained on the clean and unclean data. For\ninstance, when we trained a Random Forest model with the clean GT data, we\nachieved upto 70\\% absolute percentage points higher for the F1 score metric.\nThis approach advances crop classification methodologies, with potential for\napplications towards improving loan underwriting and agricultural\ndecision-making.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11849", "pdf": "https://arxiv.org/pdf/2503.11849", "abs": "https://arxiv.org/abs/2503.11849", "authors": ["Yi Wang", "Zhitong Xiong", "Chenying Liu", "Adam J. Stewart", "Thomas Dujardin", "Nikolaos Ioannis Bountos", "Angelos Zavras", "Franziska Gerken", "Ioannis Papoutsis", "Laura Leal-Taixé", "Xiao Xiang Zhu"], "title": "Towards a Unified Copernicus Foundation Model for Earth Vision", "categories": ["cs.CV"], "comment": "31 pages, 32 figures", "summary": "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps://github.com/zhu-xlab/Copernicus-FM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11935", "pdf": "https://arxiv.org/pdf/2503.11935", "abs": "https://arxiv.org/abs/2503.11935", "authors": ["Jun Yu", "Yang Zheng", "Lei Wang", "Yongqi Wang", "Shengfan Xu"], "title": "Design of an Expression Recognition Solution Employing the Global Channel-Spatial Attention Mechanism", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition is a challenging classification task with broad\napplication prospects in the field of human - computer interaction. This paper\naims to introduce the methods of our upcoming 8th Affective Behavior Analysis\nin the Wild (ABAW) competition to be held at CVPR2025. To address issues such\nas low recognition accuracy caused by subtle expression changes and multi -\nscales in facial expression recognition in videos, we propose global channel -\nspatial attention and median - enhanced spatial - channel attention to\nstrengthen feature processing for speech and images respectively. Secondly, to\nfully utilize the complementarity between the speech and facial expression\nmodalities, a speech - and - facial - expression key - frame alignment\ntechnique is adopted to calculate the weights of speech and facial expressions.\nThese weights are input into the feature fusion layer for multi - scale dilated\nfusion, which effectively improves the recognition rate of facial expression\nrecognition. In the facial expression recognition task of the 6th ABAW\ncompetition, our method achieved excellent results on the official validation\nset, which fully demonstrates the effectiveness and competitiveness of the\nproposed method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11958", "pdf": "https://arxiv.org/pdf/2503.11958", "abs": "https://arxiv.org/abs/2503.11958", "authors": ["Chong Su", "Yingbin Fu", "Zheyuan Hu", "Jing Yang", "Param Hanji", "Shaojun Wang", "Xuan Zhao", "Cengiz Öztireli", "Fangcheng Zhong"], "title": "CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Chong Su and Yingbin Fu contributed equally to this work", "summary": "We introduce CHOrD, a novel framework for scalable synthesis of 3D indoor\nscenes, designed to create house-scale, collision-free, and hierarchically\nstructured indoor digital twins. In contrast to existing methods that directly\nsynthesize the scene layout as a scene graph or object list, CHOrD incorporates\na 2D image-based intermediate layout representation, enabling effective\nprevention of collision artifacts by successfully capturing them as\nout-of-distribution (OOD) scenarios during generation. Furthermore, unlike\nexisting methods, CHOrD is capable of generating scene layouts that adhere to\ncomplex floor plans with multi-modal controls, enabling the creation of\ncoherent, house-wide layouts robust to both geometric and semantic variations\nin room structures. Additionally, we propose a novel dataset with expanded\ncoverage of household items and room configurations, as well as significantly\nimproved data quality. CHOrD demonstrates state-of-the-art performance on both\nthe 3D-FRONT and our proposed datasets, delivering photorealistic, spatially\ncoherent indoor scene synthesis adaptable to arbitrary floor plan variations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11995", "pdf": "https://arxiv.org/pdf/2503.11995", "abs": "https://arxiv.org/abs/2503.11995", "authors": ["Shun Zou", "Yi Zou", "Mingya Zhang", "Shipeng Luo", "Zhihao Chen", "Guangwei Gao"], "title": "Fraesormer: Learning Adaptive Sparse Transformer for Efficient Food Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "In recent years, Transformer has witnessed significant progress in food\nrecognition. However, most existing approaches still face two critical\nchallenges in lightweight food recognition: (1) the quadratic complexity and\nredundant feature representation from interactions with irrelevant tokens; (2)\nstatic feature recognition and single-scale representation, which overlook the\nunstructured, non-fixed nature of food images and the need for multi-scale\nfeatures. To address these, we propose an adaptive and efficient sparse\nTransformer architecture (Fraesormer) with two core designs: Adaptive Top-k\nSparse Partial Attention (ATK-SPA) and Hierarchical Scale-Sensitive Feature\nGating Network (HSSFGN). ATK-SPA uses a learnable Gated Dynamic Top-K Operator\n(GDTKO) to retain critical attention scores, filtering low query-key matches\nthat hinder feature aggregation. It also introduces a partial channel mechanism\nto reduce redundancy and promote expert information flow, enabling local-global\ncollaborative modeling. HSSFGN employs gating mechanism to achieve multi-scale\nfeature representation, enhancing contextual semantic information. Extensive\nexperiments show that Fraesormer outperforms state-of-the-art methods. code is\navailable at https://zs1314.github.io/Fraesormer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12006", "pdf": "https://arxiv.org/pdf/2503.12006", "abs": "https://arxiv.org/abs/2503.12006", "authors": ["Zhe Shan", "Yang Liu", "Lei Zhou", "Cheng Yan", "Heng Wang", "Xia Xie"], "title": "ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "The availability of large-scale remote sensing video data underscores the\nimportance of high-quality interactive segmentation. However, challenges such\nas small object sizes, ambiguous features, and limited generalization make it\ndifficult for current methods to achieve this goal. In this work, we propose\nROS-SAM, a method designed to achieve high-quality interactive segmentation\nwhile preserving generalization across diverse remote sensing data. The ROS-SAM\nis built upon three key innovations: 1) LoRA-based fine-tuning, which enables\nefficient domain adaptation while maintaining SAM's generalization ability, 2)\nEnhancement of deep network layers to improve the discriminability of extracted\nfeatures, thereby reducing misclassifications, and 3) Integration of global\ncontext with local boundary details in the mask decoder to generate\nhigh-quality segmentation masks. Additionally, we design the data pipeline to\nensure the model learns to better handle objects at varying scales during\ntraining while focusing on high-quality predictions during inference.\nExperiments on remote sensing video datasets show that the redesigned data\npipeline boosts the IoU by 6%, while ROS-SAM increases the IoU by 13%. Finally,\nwhen evaluated on existing remote sensing object tracking datasets, ROS-SAM\ndemonstrates impressive zero-shot capabilities, generating masks that closely\nresemble manual annotations. These results confirm ROS-SAM as a powerful tool\nfor fine-grained segmentation in remote sensing applications. Code is available\nat https://github.com/ShanZard/ROS-SAM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12009", "pdf": "https://arxiv.org/pdf/2503.12009", "abs": "https://arxiv.org/abs/2503.12009", "authors": ["Xin Jin", "Haisheng Su", "Kai Liu", "Cong Ma", "Wei Wu", "Fei Hui", "Junchi Yan"], "title": "UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Recent advances in LiDAR 3D detection have demonstrated the effectiveness of\nTransformer-based frameworks in capturing the global dependencies from point\ncloud spaces, which serialize the 3D voxels into the flattened 1D sequence for\niterative self-attention. However, the spatial structure of 3D voxels will be\ninevitably destroyed during the serialization process. Besides, due to the\nconsiderable number of 3D voxels and quadratic complexity of Transformers,\nmultiple sequences are grouped before feeding to Transformers, leading to a\nlimited receptive field. Inspired by the impressive performance of State Space\nModels (SSM) achieved in the field of 2D vision tasks, in this paper, we\npropose a novel Unified Mamba (UniMamba), which seamlessly integrates the\nmerits of 3D convolution and SSM in a concise multi-head manner, aiming to\nperform \"local and global\" spatial context aggregation efficiently and\nsimultaneously. Specifically, a UniMamba block is designed which mainly\nconsists of spatial locality modeling, complementary Z-order serialization and\nlocal-global sequential aggregator. The spatial locality modeling module\nintegrates 3D submanifold convolution to capture the dynamic spatial position\nembedding before serialization. Then the efficient Z-order curve is adopted for\nserialization both horizontally and vertically. Furthermore, the local-global\nsequential aggregator adopts the channel grouping strategy to efficiently\nencode both \"local and global\" spatial inter-dependencies using multi-head SSM.\nAdditionally, an encoder-decoder architecture with stacked UniMamba blocks is\nformed to facilitate multi-scale spatial learning hierarchically. Extensive\nexperiments are conducted on three popular datasets: nuScenes, Waymo and\nArgoverse 2. Particularly, our UniMamba achieves 70.2 mAP on the nuScenes\ndataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12014", "pdf": "https://arxiv.org/pdf/2503.12014", "abs": "https://arxiv.org/abs/2503.12014", "authors": ["Shun Zou", "Yi Zou", "Mingya Zhang", "Shipeng Luo", "Guangwei Gao", "Guojun Qi"], "title": "Learning Dual-Domain Multi-Scale Representations for Single Image Deraining", "categories": ["cs.CV"], "comment": "6 pages, 5 figures, code: https://zs1314.github.io/DMSR", "summary": "Existing image deraining methods typically rely on single-input,\nsingle-output, and single-scale architectures, which overlook the joint\nmulti-scale information between external and internal features. Furthermore,\nsingle-domain representations are often too restrictive, limiting their ability\nto handle the complexities of real-world rain scenarios. To address these\nchallenges, we propose a novel Dual-Domain Multi-Scale Representation Network\n(DMSR). The key idea is to exploit joint multi-scale representations from both\nexternal and internal domains in parallel while leveraging the strengths of\nboth spatial and frequency domains to capture more comprehensive properties.\nSpecifically, our method consists of two main components: the Multi-Scale\nProgressive Spatial Refinement Module (MPSRM) and the Frequency Domain Scale\nMixer (FDSM). The MPSRM enables the interaction and coupling of multi-scale\nexpert information within the internal domain using a hierarchical modulation\nand fusion strategy. The FDSM extracts multi-scale local information in the\nspatial domain, while also modeling global dependencies in the frequency\ndomain. Extensive experiments show that our model achieves state-of-the-art\nperformance across six benchmark datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12018", "pdf": "https://arxiv.org/pdf/2503.12018", "abs": "https://arxiv.org/abs/2503.12018", "authors": ["Zhe Jin", "Tat-Seng Chua"], "title": "Compose Your Aesthetics: Empowering Text-to-Image Models with the Principles of Art", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) diffusion models (DM) have garnered widespread adoption\ndue to their capability in generating high-fidelity outputs and accessibility\nto anyone able to put imagination into words. However, DMs are often\npredisposed to generate unappealing outputs, much like the random images on the\ninternet they were trained on. Existing approaches to address this are founded\non the implicit premise that visual aesthetics is universal, which is limiting.\nAesthetics in the T2I context should be about personalization and we propose\nthe novel task of aesthetics alignment which seeks to align user-specified\naesthetics with the T2I generation output. Inspired by how artworks provide an\ninvaluable perspective to approach aesthetics, we codify visual aesthetics\nusing the compositional framework artists employ, known as the Principles of\nArt (PoA). To facilitate this study, we introduce CompArt, a large-scale\ncompositional art dataset building on top of WikiArt with PoA analysis\nannotated by a capable Multimodal LLM. Leveraging the expressive power of LLMs\nand training a lightweight and transferrable adapter, we demonstrate that T2I\nDMs can effectively offer 10 compositional controls through user-specified PoA\nconditions. Additionally, we design an appropriate evaluation framework to\nassess the efficacy of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12024", "pdf": "https://arxiv.org/pdf/2503.12024", "abs": "https://arxiv.org/abs/2503.12024", "authors": ["Byeongjun Park", "Hyojun Go", "Hyelin Nam", "Byung-Hoon Kim", "Hyungjin Chung", "Changick Kim"], "title": "SteerX: Creating Any Camera-Free 3D and 4D Scenes with Geometric Steering", "categories": ["cs.CV"], "comment": "Project page: https://byeongjun-park.github.io/SteerX/", "summary": "Recent progress in 3D/4D scene generation emphasizes the importance of\nphysical alignment throughout video generation and scene reconstruction.\nHowever, existing methods improve the alignment separately at each stage,\nmaking it difficult to manage subtle misalignments arising from another stage.\nHere, we present SteerX, a zero-shot inference-time steering method that\nunifies scene reconstruction into the generation process, tilting data\ndistributions toward better geometric alignment. To this end, we introduce two\ngeometric reward functions for 3D/4D scene generation by using pose-free\nfeed-forward scene reconstruction models. Through extensive experiments, we\ndemonstrate the effectiveness of SteerX in improving 3D/4D scene generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12049", "pdf": "https://arxiv.org/pdf/2503.12049", "abs": "https://arxiv.org/abs/2503.12049", "authors": ["Ruijie Lu", "Yixin Chen", "Yu Liu", "Jiaxiang Tang", "Junfeng Ni", "Diwen Wan", "Gang Zeng", "Siyuan Huang"], "title": "TACO: Taming Diffusion for in-the-wild Video Amodal Completion", "categories": ["cs.CV"], "comment": "Project page: https://jason-aplp.github.io/TACO", "summary": "Humans can infer complete shapes and appearances of objects from limited\nvisual cues, relying on extensive prior knowledge of the physical world.\nHowever, completing partially observable objects while ensuring consistency\nacross video frames remains challenging for existing models, especially for\nunstructured, in-the-wild videos. This paper tackles the task of Video Amodal\nCompletion (VAC), which aims to generate the complete object consistently\nthroughout the video given a visual prompt specifying the object of interest.\nLeveraging the rich, consistent manifolds learned by pre-trained video\ndiffusion models, we propose a conditional diffusion model, TACO, that\nrepurposes these manifolds for VAC. To enable its effective and robust\ngeneralization to challenging in-the-wild scenarios, we curate a large-scale\nsynthetic dataset with multiple difficulty levels by systematically imposing\nocclusions onto un-occluded videos. Building on this, we devise a progressive\nfine-tuning paradigm that starts with simpler recovery tasks and gradually\nadvances to more complex ones. We demonstrate TACO's versatility on a wide\nrange of in-the-wild videos from Internet, as well as on diverse, unseen\ndatasets commonly used in autonomous driving, robotic manipulation, and scene\nunderstanding. Moreover, we show that TACO can be effectively applied to\nvarious downstream tasks like object reconstruction and pose estimation,\nhighlighting its potential to facilitate physical world understanding and\nreasoning. Our project page is available at https://jason-aplp.github.io/TACO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12061", "pdf": "https://arxiv.org/pdf/2503.12061", "abs": "https://arxiv.org/abs/2503.12061", "authors": ["Yuqing Yan", "Yirui Wu"], "title": "EHNet: An Efficient Hybrid Network for Crowd Counting and Localization", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, crowd counting and localization have become crucial\ntechniques in computer vision, with applications spanning various domains. The\npresence of multi-scale crowd distributions within a single image remains a\nfundamental challenge in crowd counting tasks. To address these challenges, we\nintroduce the Efficient Hybrid Network (EHNet), a novel framework for efficient\ncrowd counting and localization. By reformulating crowd counting into a point\nregression framework, EHNet leverages the Spatial-Position Attention Module\n(SPAM) to capture comprehensive spatial contexts and long-range dependencies.\nAdditionally, we develop an Adaptive Feature Aggregation Module (AFAM) to\neffectively fuse and harmonize multi-scale feature representations. Building\nupon these, we introduce the Multi-Scale Attentive Decoder (MSAD). Experimental\nresults on four benchmark datasets demonstrate that EHNet achieves competitive\nperformance with reduced computational overhead, outperforming existing methods\non ShanghaiTech Part \\_A, ShanghaiTech Part \\_B, UCF-CC-50, and UCF-QNRF. Our\ncode is in https://anonymous.4open.science/r/EHNet.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12063", "pdf": "https://arxiv.org/pdf/2503.12063", "abs": "https://arxiv.org/abs/2503.12063", "authors": ["Yuqing Yan", "Yirui Wu"], "title": "DLA-Count: Dynamic Label Assignment Network for Dense Cell Distribution Counting", "categories": ["cs.CV"], "comment": null, "summary": "Cell counting remains a fundamental yet challenging task in medical and\nbiological research due to the diverse morphology of cells, their dense\ndistribution, and variations in image quality. We present DLA-Count, a\nbreakthrough approach to cell counting that introduces three key innovations:\n(1) K-adjacent Hungarian Matching (KHM), which dramatically improves cell\nmatching in dense regions, (2) Multi-scale Deformable Gaussian Convolution\n(MDGC), which adapts to varying cell morphologies, and (3) Gaussian-enhanced\nFeature Decoder (GFD) for efficient multi-scale feature fusion. Our extensive\nexperiments on four challenging cell counting datasets (ADI, MBM, VGG, and DCC)\ndemonstrate that our method outperforms previous methods across diverse\ndatasets, with improvements in Mean Absolute Error of up to 46.7\\% on ADI and\n42.5\\% on MBM datasets. Our code is available at\nhttps://anonymous.4open.science/r/DLA-Count.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12067", "pdf": "https://arxiv.org/pdf/2503.12067", "abs": "https://arxiv.org/abs/2503.12067", "authors": ["Amir M. Mansourian", "Rozhan Ahmadi", "Masoud Ghafouri", "Amir Mohammad Babaei", "Elaheh Badali Golezani", "Zeynab Yasamani Ghamchi", "Vida Ramezanian", "Alireza Taherian", "Kimia Dinashi", "Amirali Miri", "Shohreh Kasaei"], "title": "A Comprehensive Survey on Knowledge Distillation", "categories": ["cs.CV"], "comment": "47 pages, 10 figures, 13 tables", "summary": "Deep Neural Networks (DNNs) have achieved notable performance in the fields\nof computer vision and natural language processing with various applications in\nboth academia and industry. However, with recent advancements in DNNs and\ntransformer models with a tremendous number of parameters, deploying these\nlarge models on edge devices causes serious issues such as high runtime and\nmemory consumption. This is especially concerning with the recent large-scale\nfoundation models, Vision-Language Models (VLMs), and Large Language Models\n(LLMs). Knowledge Distillation (KD) is one of the prominent techniques proposed\nto address the aforementioned problems using a teacher-student architecture.\nMore specifically, a lightweight student model is trained using additional\nknowledge from a cumbersome teacher model. In this work, a comprehensive survey\nof knowledge distillation methods is proposed. This includes reviewing KD from\ndifferent aspects: distillation sources, distillation schemes, distillation\nalgorithms, distillation by modalities, applications of distillation, and\ncomparison among existing methods. In contrast to most existing surveys, which\nare either outdated or simply update former surveys, this work proposes a\ncomprehensive survey with a new point of view and representation structure that\ncategorizes and investigates the most recent methods in knowledge distillation.\nThis survey considers various critically important subcategories, including KD\nfor diffusion models, 3D inputs, foundational models, transformers, and LLMs.\nFurthermore, existing challenges in KD and possible future research directions\nare discussed. Github page of the project:\nhttps://github.com/IPL-Sharif/KD_Survey", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12069", "pdf": "https://arxiv.org/pdf/2503.12069", "abs": "https://arxiv.org/abs/2503.12069", "authors": ["Wei Lai", "Tianyu Ding", "ren dongdong", "Lei Wang", "Jing Huo", "Yang Gao", "Wenbin Li"], "title": "Robust Dataset Distillation by Matching Adversarial Trajectories", "categories": ["cs.CV"], "comment": null, "summary": "Dataset distillation synthesizes compact datasets that enable models to\nachieve performance comparable to training on the original large-scale\ndatasets. However, existing distillation methods overlook the robustness of the\nmodel, resulting in models that are vulnerable to adversarial attacks when\ntrained on distilled data. To address this limitation, we introduce the task of\n``robust dataset distillation\", a novel paradigm that embeds adversarial\nrobustness into the synthetic datasets during the distillation process. We\npropose Matching Adversarial Trajectories (MAT), a method that integrates\nadversarial training into trajectory-based dataset distillation. MAT\nincorporates adversarial samples during trajectory generation to obtain robust\ntraining trajectories, which are then used to guide the distillation process.\nAs experimentally demonstrated, even through natural training on our distilled\ndataset, models can achieve enhanced adversarial robustness while maintaining\ncompetitive accuracy compared to existing distillation methods. Our work\nhighlights robust dataset distillation as a new and important research\ndirection and provides a strong baseline for future research to bridge the gap\nbetween efficient training and adversarial robustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12093", "pdf": "https://arxiv.org/pdf/2503.12093", "abs": "https://arxiv.org/abs/2503.12093", "authors": ["Oren Shrout", "Ayellet Tal"], "title": "SFMNet: Sparse Focal Modulation for 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "We propose SFMNet, a novel 3D sparse detector that combines the efficiency of\nsparse convolutions with the ability to model long-range dependencies. While\ntraditional sparse convolution techniques efficiently capture local structures,\nthey struggle with modeling long-range relationships. However, capturing\nlong-range dependencies is fundamental for 3D object detection. In contrast,\ntransformers are designed to capture these long-range dependencies through\nattention mechanisms. But, they come with high computational costs, due to\ntheir quadratic query-key-value interactions. Furthermore, directly applying\nattention to non-empty voxels is inefficient due to the sparse nature of 3D\nscenes. Our SFMNet is built on a novel Sparse Focal Modulation (SFM) module,\nwhich integrates short- and long-range contexts with linear complexity by\nleveraging a new hierarchical sparse convolution design. This approach enables\nSFMNet to achieve high detection performance with improved efficiency, making\nit well-suited for large-scale LiDAR scenes. We show that our detector achieves\nstate-of-the-art performance on autonomous driving datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12096", "pdf": "https://arxiv.org/pdf/2503.12096", "abs": "https://arxiv.org/abs/2503.12096", "authors": ["Ashshak Sharifdeen", "Muhammad Akhtar Munir", "Sanoojan Baliah", "Salman Khan", "Muhammad Haris Khan"], "title": "O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Test-time prompt tuning for vision-language models (VLMs) is getting\nattention because of their ability to learn with unlabeled data without\nfine-tuning. Although test-time prompt tuning methods for VLMs can boost\naccuracy, the resulting models tend to demonstrate poor calibration, which\ncasts doubts on the reliability and trustworthiness of these models. Notably,\nmore attention needs to be devoted to calibrating the test-time prompt tuning\nin vision-language models. To this end, we propose a new approach, called O-TPT\nthat introduces orthogonality constraints on the textual features corresponding\nto the learnable prompts for calibrating test-time prompt tuning in VLMs.\nTowards introducing orthogonality constraints, we make the following\ncontributions. First, we uncover new insights behind the suboptimal calibration\nperformance of existing methods relying on textual feature dispersion. Second,\nwe show that imposing a simple orthogonalization of textual features is a more\neffective approach towards obtaining textual dispersion. We conduct extensive\nexperiments on various datasets with different backbones and baselines. The\nresults indicate that our method consistently outperforms the prior state of\nthe art in significantly reducing the overall average calibration error. Also,\nour method surpasses the zero-shot calibration performance on fine-grained\nclassification tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12165", "pdf": "https://arxiv.org/pdf/2503.12165", "abs": "https://arxiv.org/abs/2503.12165", "authors": ["Zijian He", "Yuwei Ning", "Yipeng Qin", "Wangrun Wang", "Sibei Yang", "Liang Lin", "Guanbin Li"], "title": "VTON 360: High-Fidelity Virtual Try-On from Any Viewing Direction", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Virtual Try-On (VTON) is a transformative technology in e-commerce and\nfashion design, enabling realistic digital visualization of clothing on\nindividuals. In this work, we propose VTON 360, a novel 3D VTON method that\naddresses the open challenge of achieving high-fidelity VTON that supports\nany-view rendering. Specifically, we leverage the equivalence between a 3D\nmodel and its rendered multi-view 2D images, and reformulate 3D VTON as an\nextension of 2D VTON that ensures 3D consistent results across multiple views.\nTo achieve this, we extend 2D VTON models to include multi-view garments and\nclothing-agnostic human body images as input, and propose several novel\ntechniques to enhance them, including: i) a pseudo-3D pose representation using\nnormal maps derived from the SMPL-X 3D human model, ii) a multi-view spatial\nattention mechanism that models the correlations between features from\ndifferent viewing angles, and iii) a multi-view CLIP embedding that enhances\nthe garment CLIP features used in 2D VTON with camera information. Extensive\nexperiments on large-scale real datasets and clothing images from e-commerce\nplatforms demonstrate the effectiveness of our approach. Project page:\nhttps://scnuhealthy.github.io/VTON360.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12332", "pdf": "https://arxiv.org/pdf/2503.12332", "abs": "https://arxiv.org/abs/2503.12332", "authors": ["Yunze Liu", "Peiran Wu", "Cheng Liang", "Junxiao Shen", "Limin Wang", "Li Yi"], "title": "VideoMAP: Toward Scalable Mamba-based Video Autoregressive Pretraining", "categories": ["cs.CV"], "comment": null, "summary": "Recent Mamba-based architectures for video understanding demonstrate\npromising computational efficiency and competitive performance, yet struggle\nwith overfitting issues that hinder their scalability. To overcome this\nchallenge, we introduce VideoMAP, a Hybrid Mamba-Transformer framework\nfeaturing a novel pre-training approach. VideoMAP uses a 4:1\nMamba-to-Transformer ratio, effectively balancing computational cost and model\ncapacity. This architecture, combined with our proposed frame-wise masked\nautoregressive pre-training strategy, delivers significant performance gains\nwhen scaling to larger models. Additionally, VideoMAP exhibits impressive\nsample efficiency, significantly outperforming existing methods with less\ntraining data. Experiments show that VideoMAP outperforms existing models\nacross various datasets, including Kinetics-400, Something-Something V2,\nBreakfast, and COIN. Furthermore, we demonstrate the potential of VideoMAP as a\nvisual encoder for multimodal large language models, highlighting its ability\nto reduce memory usage and enable the processing of longer video sequences. The\ncode is open-source at https://github.com/yunzeliu/MAP", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12355", "pdf": "https://arxiv.org/pdf/2503.12355", "abs": "https://arxiv.org/abs/2503.12355", "authors": ["Kumar Krishna Agrawal", "Long Lian", "Longchao Liu", "Natalia Harguindeguy", "Boyi Li", "Alexander Bick", "Maggie Chung", "Trevor Darrell", "Adam Yala"], "title": "Atlas: Multi-Scale Attention Improves Long Context Image Modeling", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Efficiently modeling massive images is a long-standing challenge in machine\nlearning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on\ntwo key ideas, (i) multi-scale representations (ii) bi-directional cross-scale\ncommunication. MSA creates O(log N) scales to represent the image across\nprogressively coarser features and leverages cross-attention to propagate\ninformation across scales. We then introduce Atlas, a novel neural network\narchitecture based on MSA. We demonstrate that Atlas significantly improves the\ncompute-performance tradeoff of long-context image modeling in a\nhigh-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves\n91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster.\nAtlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96%\nbetter than LongViT. In comparisons against MambaVision-S, we find Atlas-S\nachieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px\nrespectively, while obtaining similar runtimes. Code for reproducing our\nexperiments and pretrained models is available at\nhttps://github.com/yalalab/atlas.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12369", "pdf": "https://arxiv.org/pdf/2503.12369", "abs": "https://arxiv.org/abs/2503.12369", "authors": ["Ruoyu Wang", "Yukai Ma", "Yi Yao", "Sheng Tao", "Haoang Li", "Zongzhi Zhu", "Yong Liu", "Xingxing Zuo"], "title": "L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model", "categories": ["cs.CV"], "comment": null, "summary": "Semantic Scene Completion (SSC) constitutes a pivotal element in autonomous\ndriving perception systems, tasked with inferring the 3D semantic occupancy of\na scene from sensory data. To improve accuracy, prior research has implemented\nvarious computationally demanding and memory-intensive 3D operations, imposing\nsignificant computational requirements on the platform during training and\ntesting. This paper proposes L2COcc, a lightweight camera-centric SSC framework\nthat also accommodates LiDAR inputs. With our proposed efficient voxel\ntransformer (EVT) and cross-modal knowledge modules, including feature\nsimilarity distillation (FSD), TPV distillation (TPVD) and prediction alignment\ndistillation (PAD), our method substantially reduce computational burden while\nmaintaining high accuracy. The experimental evaluations demonstrate that our\nproposed method surpasses the current state-of-the-art vision-based SSC methods\nregarding accuracy on both the SemanticKITTI and SSCBench-KITTI-360 benchmarks,\nrespectively. Additionally, our method is more lightweight, exhibiting a\nreduction in both memory consumption and inference time by over 23% compared to\nthe current state-of-the-arts method. Code is available at our project\npage:https://studyingfufu.github.io/L2COcc/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12382", "pdf": "https://arxiv.org/pdf/2503.12382", "abs": "https://arxiv.org/abs/2503.12382", "authors": ["Kang You", "Tong Chen", "Dandan Ding", "M. Salman Asif", "Zhan Ma"], "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Despite the substantial advancements demonstrated by learning-based neural\nmodels in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time\ncompression - an indispensable criterion for numerous industrial applications -\nremains a formidable challenge. This paper proposes RENO, the first real-time\nneural codec for 3D LiDAR point clouds, achieving superior performance with a\nlightweight model. RENO skips the octree construction and directly builds upon\nthe multiscale sparse tensor representation. Instead of the multi-stage\ninferring, RENO devises sparse occupancy codes, which exploit cross-scale\ncorrelation and derive voxels' occupancy in a one-shot manner, greatly saving\nprocessing time. Experimental results demonstrate that the proposed RENO\nachieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform\n(e.g., one RTX 3090 GPU) for both encoding and decoding processes, while\nproviding 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco,\nrespectively, at a similar quality. RENO model size is merely 1MB, making it\nattractive for practical applications. The source code is available at\nhttps://github.com/NJUVISION/RENO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12383", "pdf": "https://arxiv.org/pdf/2503.12383", "abs": "https://arxiv.org/abs/2503.12383", "authors": ["Songen Gu", "Haoxuan Song", "Binjie Liu", "Qian Yu", "Sanyi Zhang", "Haiyong Jiang", "Jin Huang", "Feng Tian"], "title": "VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native\n3D object generation framework that incorporates a 3D Gaussian Splatting\nrepresentation. As part of our work, we introduce VRSS, the first large-scale\npaired dataset containing VR sketches, text, images, and 3DGS, bridging the gap\nin multi-modal VR sketch-based generation. Our approach features the following\nkey innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage\nalignment strategy that bridges the domain gap between sparse VR sketch\nembeddings and rich CLIP embeddings, facilitating both VR sketch-based\nretrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We\ndisentangle the 3D generation process by using explicit VR sketches for\ngeometric conditioning and text descriptions for appearance control. To\nfacilitate this, we propose a generalizable VR sketch encoder that effectively\naligns different modalities. 3) Efficient and high-fidelity 3D native\ngeneration. Our method leverages a 3D-native generation approach that enables\nfast and texture-rich 3D object synthesis. Experiments conducted on our VRSS\ndataset demonstrate that our method achieves high-quality, multi-modal VR\nsketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian\nmethod will be beneficial for the 3D generation community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12385", "pdf": "https://arxiv.org/pdf/2503.12385", "abs": "https://arxiv.org/abs/2503.12385", "authors": ["Yutao Hu", "Sen Li", "Jincheng Yan", "Wenqi Shao", "Xiaoyan Luo"], "title": "Car-1000: A New Large Scale Fine-Grained Visual Categorization Dataset", "categories": ["cs.CV"], "comment": "accepted to The Eleventh Workshop on Fine-Grained Visual\n  Categorization in CVPR 2024", "summary": "Fine-grained visual categorization (FGVC) is a challenging but significant\ntask in computer vision, which aims to recognize different sub-categories of\nbirds, cars, airplanes, etc. Among them, recognizing models of different cars\nhas significant application value in autonomous driving, traffic surveillance\nand scene understanding, which has received considerable attention in the past\nfew years. However, Stanford-Car, the most widely used fine-grained dataset for\ncar recognition, only has 196 different categories and only includes vehicle\nmodels produced earlier than 2013. Due to the rapid advancements in the\nautomotive industry during recent years, the appearances of various car models\nhave become increasingly intricate and sophisticated. Consequently, the\nprevious Stanford-Car dataset fails to capture this evolving landscape and\ncannot satisfy the requirements of automotive industry. To address these\nchallenges, in our paper, we introduce Car-1000, a large-scale dataset designed\nspecifically for fine-grained visual categorization of diverse car models.\nCar-1000 encompasses vehicles from 165 different automakers, spanning a wide\nrange of 1000 distinct car models. Additionally, we have reproduced several\nstate-of-the-art FGVC methods on the Car-1000 dataset, establishing a new\nbenchmark for research in this field. We hope that our work will offer a fresh\nperspective for future FGVC researchers. Our dataset is available at\nhttps://github.com/toggle1995/Car-1000.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12404", "pdf": "https://arxiv.org/pdf/2503.12404", "abs": "https://arxiv.org/abs/2503.12404", "authors": ["Jianhao Yang", "Wenshuo Yu", "Yuanchao Lv", "Jiance Sun", "Bokang Sun", "Mingyang Liu"], "title": "SAM2-ELNet: Label Enhancement and Automatic Annotation for Remote Sensing Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing image segmentation is crucial for environmental monitoring,\ndisaster assessment, and resource management, directly affecting the accuracy\nand efficiency of surface information extraction. The performance of existing\nsupervised models in remote sensing image segmentation tasks highly depends on\nthe quality of label data. However, current label data mainly relies on manual\nannotation, which comes with high time costs and is subject to subjective\ninterference, resulting in distortion of label boundaries and often a loss of\ndetail. To solve the above problems, our work proposes an Edge-enhanced\nLabeling Network, called SAM2-ELNet, which incorporates a labeling module and\nan edge attention mechanism. This model effectively addresses issues such as\nlabel detail loss, fragmentation, and inaccurate boundaries. Due to the\nscarcity of manually annotated remote sensing data, the feature extraction\ncapabilities of traditional neural networks are limited. Our method uses the\nHiera backbone of the pre-trained self-supervised large model segment anything\nmodel 2 (SAM2) as the encoder, achieves high-quality and efficient feature\nextraction even with small samples by fine-tuning on downstream tasks. This\nstudy compared the training effects of original and enhanced labels on the\nmanually annotated Deep-SAR Oil Spill (SOS) dataset. Results showed that the\nmodel trained with enhanced labels performed better and had a lower final loss,\nindicating closer alignment with the real data distribution. Our work also\nexplores the potential of extending the model into an efficient automatic\nannotation framework through generalization experiments, facilitating\nlarge-scale remote sensing image interpretation and intelligent recognition.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12419", "pdf": "https://arxiv.org/pdf/2503.12419", "abs": "https://arxiv.org/abs/2503.12419", "authors": ["Luming Wang", "Hao Shi", "Xiaoting Yin", "Kailun Yang", "Kaiwei Wang"], "title": "EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera", "categories": ["cs.CV", "cs.RO", "eess.IV", "physics.optics"], "comment": "The dataset and models are made publicly available at\n  https://github.com/3190105222/EgoEv_Gesture", "summary": "Egocentric gesture recognition is a pivotal technology for enhancing natural\nhuman-computer interaction, yet traditional RGB-based solutions suffer from\nmotion blur and illumination variations in dynamic scenarios. While event\ncameras show distinct advantages in handling high dynamic range with ultra-low\npower consumption, existing RGB-based architectures face inherent limitations\nin processing asynchronous event streams due to their synchronous frame-based\nnature. Moreover, from an egocentric perspective, event cameras record data\nthat include events generated by both head movements and hand gestures, thereby\nincreasing the complexity of gesture recognition. To address this, we propose a\nnovel network architecture specifically designed for event data processing,\nincorporating (1) a lightweight CNN with asymmetric depthwise convolutions to\nreduce parameters while preserving spatiotemporal features, (2) a plug-and-play\nstate-space model as context block that decouples head movement noise from\ngesture dynamics, and (3) a parameter-free Bins-Temporal Shift Module (BSTM)\nthat shifts features along bins and temporal dimensions to fuse sparse events\nefficiently. We further build the EgoEvGesture dataset, the first large-scale\ndataset for egocentric gesture recognition using event cameras. Experimental\nresults demonstrate that our method achieves 62.7% accuracy in heterogeneous\ntesting with only 7M parameters, 3.1% higher than state-of-the-art approaches.\nNotable misclassifications in freestyle motions stem from high inter-personal\nvariability and unseen test patterns differing from training data. Moreover,\nour approach achieved a remarkable accuracy of 96.97% on DVS128 Gesture,\ndemonstrating strong cross-dataset generalization capability. The dataset and\nmodels are made publicly available at\nhttps://github.com/3190105222/EgoEv_Gesture.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12460", "pdf": "https://arxiv.org/pdf/2503.12460", "abs": "https://arxiv.org/abs/2503.12460", "authors": ["Zhicheng Wang", "Zhiyu Pan", "Zhan Peng", "Jian Cheng", "Liwen Xiao", "Wei Jiang", "Zhiguo Cao"], "title": "Exploring Contextual Attribute Density in Referring Expression Counting", "categories": ["cs.CV"], "comment": "CVPR25", "summary": "Referring expression counting (REC) algorithms are for more flexible and\ninteractive counting ability across varied fine-grained text expressions.\nHowever, the requirement for fine-grained attribute understanding poses\nchallenges for prior arts, as they struggle to accurately align attribute\ninformation with correct visual patterns. Given the proven importance of\n''visual density'', it is presumed that the limitations of current REC\napproaches stem from an under-exploration of ''contextual attribute density''\n(CAD). In the scope of REC, we define CAD as the measure of the information\nintensity of one certain fine-grained attribute in visual regions. To model the\nCAD, we propose a U-shape CAD estimator in which referring expression and\nmulti-scale visual features from GroundingDINO can interact with each other.\nWith additional density supervision, we can effectively encode CAD, which is\nsubsequently decoded via a novel attention procedure with CAD-refined queries.\nIntegrating all these contributions, our framework significantly outperforms\nstate-of-the-art REC methods, achieves $30\\%$ error reduction in counting\nmetrics and a $10\\%$ improvement in localization accuracy. The surprising\nresults shed light on the significance of contextual attribute density for REC.\nCode will be at github.com/Xu3XiWang/CAD-GD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12472", "pdf": "https://arxiv.org/pdf/2503.12472", "abs": "https://arxiv.org/abs/2503.12472", "authors": ["Wenbo Dai", "Lijing Lu", "Zhihang Li"], "title": "Diffusion-based Synthetic Data Generation for Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "The performance of models is intricately linked to the abundance of training\ndata. In Visible-Infrared person Re-IDentification (VI-ReID) tasks, collecting\nand annotating large-scale images of each individual under various cameras and\nmodalities is tedious, time-expensive, costly and must comply with data\nprotection laws, posing a severe challenge in meeting dataset requirements.\nCurrent research investigates the generation of synthetic data as an efficient\nand privacy-ensuring alternative to collecting real data in the field. However,\na specific data synthesis technique tailored for VI-ReID models has yet to be\nexplored. In this paper, we present a novel data generation framework, dubbed\nDiffusion-based VI-ReID data Expansion (DiVE), that automatically obtain\nmassive RGB-IR paired images with identity preserving by decoupling identity\nand modality to improve the performance of VI-ReID models. Specifically,\nidentity representation is acquired from a set of samples sharing the same ID,\nwhereas the modality of images is learned by fine-tuning the Stable Diffusion\n(SD) on modality-specific data. DiVE extend the text-driven image synthesis to\nidentity-preserving RGB-IR multimodal image synthesis. This approach\nsignificantly reduces data collection and annotation costs by directly\nincorporating synthetic data into ReID model training. Experiments have\ndemonstrated that VI-ReID models trained on synthetic data produced by DiVE\nconsistently exhibit notable enhancements. In particular, the state-of-the-art\nmethod, CAJ, trained with synthetic images, achieves an improvement of about\n$9\\%$ in mAP over the baseline on the LLCM dataset. Code:\nhttps://github.com/BorgDiven/DiVE", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12496", "pdf": "https://arxiv.org/pdf/2503.12496", "abs": "https://arxiv.org/abs/2503.12496", "authors": ["Tianyuan Qu", "Longxiang Tang", "Bohao Peng", "Senqiao Yang", "Bei Yu", "Jiaya Jia"], "title": "Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?", "categories": ["cs.CV"], "comment": null, "summary": "The rise of Large Vision-Language Models (LVLMs) has significantly advanced\nvideo understanding. However, efficiently processing long videos remains a\nchallenge due to the ``Sampling Dilemma'': low-density sampling risks missing\ncritical information, while high-density sampling introduces redundancy. To\naddress this issue, we introduce LSDBench, the first benchmark designed to\nevaluate LVLMs on long-video tasks by constructing high Necessary Sampling\nDensity (NSD) questions, where NSD represents the minimum sampling density\nrequired to accurately answer a given question. LSDBench focuses on dense,\nshort-duration actions to rigorously assess the sampling strategies employed by\nLVLMs. To tackle the challenges posed by high-NSD questions, we propose a novel\nReasoning-Driven Hierarchical Sampling (RHS) framework, which combines global\nlocalization of question-relevant cues with local dense sampling for precise\ninference. Additionally, we develop a lightweight Semantic-Guided Frame\nSelector to prioritize informative frames, enabling RHS to achieve comparable\nor superior performance with significantly fewer sampled frames. Together, our\nLSDBench and RHS framework address the unique challenges of high-NSD long-video\ntasks, setting a new standard for evaluating and improving LVLMs in this\ndomain.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12542", "pdf": "https://arxiv.org/pdf/2503.12542", "abs": "https://arxiv.org/abs/2503.12542", "authors": ["Peiran Wu", "Yunze Liu", "Chonghan Liu", "Miao Liu", "Junxiao Shen"], "title": "ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos", "categories": ["cs.CV"], "comment": null, "summary": "Humans excel at spatio-temporal reasoning, effortlessly interpreting dynamic\nvisual events from an egocentric viewpoint. However, whether multimodal large\nlanguage models (MLLMs) can similarly comprehend the 4D world remains\nuncertain. This paper explores multimodal spatio-temporal reasoning from an\negocentric perspective, aiming to equip MLLMs with human-like reasoning\ncapabilities. To support this objective, we introduce Ego-ST Bench, a novel\nbenchmark containing over 5,000 question-answer pairs across four categories,\nsystematically evaluating spatial, temporal, and integrated spatio-temporal\nreasoning. Additionally, we propose the ST-R1 Video model, a video-based\nreasoning model that incorporates reverse thinking into its reinforcement\nlearning process, significantly enhancing performance. We combine\nlong-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative\nPolicy Optimization (GRPO) reinforcement learning, achieving notable\nimprovements with limited high-quality data. Ego-ST Bench and ST-R1 provide\nvaluable insights and resources for advancing video-based spatio-temporal\nreasoning research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12552", "pdf": "https://arxiv.org/pdf/2503.12552", "abs": "https://arxiv.org/abs/2503.12552", "authors": ["Tianyu Li", "Yihang Qiu", "Zhenhua Wu", "Carl Lindström", "Peng Su", "Matthias Nießner", "Hongyang Li"], "title": "MTGS: Multi-Traversal Gaussian Splatting", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Multi-traversal data, commonly collected through daily commutes or by\nself-driving fleets, provides multiple viewpoints for scene reconstruction\nwithin a road block. This data offers significant potential for high-quality\nnovel view synthesis, which is crucial for applications such as autonomous\nvehicle simulators. However, inherent challenges in multi-traversal data often\nresult in suboptimal reconstruction quality, including variations in appearance\nand the presence of dynamic objects. To address these issues, we propose\nMulti-Traversal Gaussian Splatting (MTGS), a novel approach that reconstructs\nhigh-quality driving scenes from arbitrarily collected multi-traversal data by\nmodeling a shared static geometry while separately handling dynamic elements\nand appearance variations. Our method employs a multi-traversal dynamic scene\ngraph with a shared static node and traversal-specific dynamic nodes,\ncomplemented by color correction nodes with learnable spherical harmonics\ncoefficient residuals. This approach enables high-fidelity novel view synthesis\nand provides flexibility to navigate any viewpoint. We conduct extensive\nexperiments on a large-scale driving dataset, nuPlan, with multi-traversal\ndata. Our results demonstrate that MTGS improves LPIPS by 23.5% and geometry\naccuracy by 46.3% compared to single-traversal baselines. The code and data\nwould be available to the public.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11742", "pdf": "https://arxiv.org/pdf/2503.11742", "abs": "https://arxiv.org/abs/2503.11742", "authors": ["Moreno D'Incà", "Elia Peruzzo", "Xingqian Xu", "Humphrey Shi", "Nicu Sebe", "Massimiliano Mancini"], "title": "Safe Vision-Language Models via Unsafe Weights Manipulation", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Vision-language models (VLMs) often inherit the biases and unsafe\nassociations present within their large-scale training dataset. While recent\napproaches mitigate unsafe behaviors, their evaluation focuses on how safe the\nmodel is on unsafe inputs, ignoring potential shortcomings on safe ones. In\nthis paper, we first revise safety evaluation by introducing SafeGround, a new\nset of metrics that evaluate safety at different levels of granularity. With\nthis metric, we uncover a surprising issue of training-based methods: they make\nthe model less safe on safe inputs. From this finding, we take a different\ndirection and explore whether it is possible to make a model safer without\ntraining, introducing Unsafe Weights Manipulation (UWM). UWM uses a calibration\nset of safe and unsafe instances to compare activations between safe and unsafe\ncontent, identifying the most important parameters for processing the latter.\nTheir values are then manipulated via negation. Experiments show that UWM\nachieves the best tradeoff between safety and knowledge preservation,\nconsistently improving VLMs on unsafe queries while outperforming even\ntraining-based state-of-the-art methods on safe ones.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11781", "pdf": "https://arxiv.org/pdf/2503.11781", "abs": "https://arxiv.org/abs/2503.11781", "authors": ["Artem Nikonorov", "Georgy Perevozchikov", "Andrei Korepanov", "Nancy Mehta", "Mahmoud Afifi", "Egor Ershov", "Radu Timofte"], "title": "Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks", "categories": ["cs.CV"], "comment": null, "summary": "We present cmKAN, a versatile framework for color matching. Given an input\nimage with colors from a source color distribution, our method effectively and\naccurately maps these colors to match a target color distribution in both\nsupervised and unsupervised settings. Our framework leverages the spline\ncapabilities of Kolmogorov-Arnold Networks (KANs) to model the color matching\nbetween source and target distributions. Specifically, we developed a\nhypernetwork that generates spatially varying weight maps to control the\nnonlinear splines of a KAN, enabling accurate color matching. As part of this\nwork, we introduce a first large-scale dataset of paired images captured by two\ndistinct cameras and evaluate the efficacy of our and existing methods in\nmatching colors. We evaluated our approach across various color-matching tasks,\nincluding: (1) raw-to-raw mapping, where the source color distribution is in\none camera's raw color space and the target in another camera's raw space; (2)\nraw-to-sRGB mapping, where the source color distribution is in a camera's raw\nspace and the target is in the display sRGB space, emulating the color\nrendering of a camera ISP; and (3) sRGB-to-sRGB mapping, where the goal is to\ntransfer colors from a source sRGB space (e.g., produced by a source camera\nISP) to a target sRGB space (e.g., from a different camera ISP). The results\nshow that our method outperforms existing approaches by 37.3% on average for\nsupervised and unsupervised cases while remaining lightweight compared to other\nmethods. The codes, dataset, and pre-trained models are available at:\nhttps://github.com/gosha20777/cmKAN", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11792", "pdf": "https://arxiv.org/pdf/2503.11792", "abs": "https://arxiv.org/abs/2503.11792", "authors": ["Peizhi Yan", "Rabab K. Ward", "Dan Wang", "Qiang Tang", "Shan Du"], "title": "StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model", "categories": ["cs.CV"], "comment": "13 pages, work was completed in 2023", "summary": "For 3D face modeling, the recently developed 3D-aware neural rendering\nmethods are able to render photorealistic face images with arbitrary viewing\ndirections. The training of the parametric controllable 3D-aware face models,\nhowever, still relies on a large-scale dataset that is lab-collected. To\naddress this issue, this paper introduces \"StyleMorpheus\", the first\nstyle-based neural 3D Morphable Face Model (3DMM) that is trained on\nin-the-wild images. It inherits 3DMM's disentangled controllability (over face\nidentity, expression, and appearance) but without the need for accurately\nreconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder\nstructure. The encoder aims at learning a representative disentangled\nparametric code space and the decoder improves the disentanglement using shape\nand appearance-related style codes in the different sub-modules of the network.\nFurthermore, we fine-tune the decoder through style-based generative\nadversarial learning to achieve photorealistic 3D rendering quality. The\nproposed style-based design enables StyleMorpheus to achieve state-of-the-art\n3D-aware face reconstruction results, while also allowing disentangled control\nof the reconstructed face. Our model achieves real-time rendering speed,\nallowing its use in virtual reality applications. We also demonstrate the\ncapability of the proposed style-based design in face editing applications such\nas style mixing and color editing. Project homepage:\nhttps://github.com/ubc-3d-vision-lab/StyleMorpheus.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11807", "pdf": "https://arxiv.org/pdf/2503.11807", "abs": "https://arxiv.org/abs/2503.11807", "authors": ["Sanayya A", "Amoolya Shetty", "Abhijeet Sharma", "Venkatesh Ravichandran", "Masthan Wali Gosuvarapalli", "Sarthak Jain", "Priyamvada Nanjundiah", "Ujjal Kr Dutta", "Divya Sharma"], "title": "Mitigating Bad Ground Truth in Supervised Machine Learning based Crop Classification: A Multi-Level Framework with Sentinel-2 Images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted In IEEE India Geoscience and Remote Sensing Symposium\n  (InGARSS) 2024", "summary": "In agricultural management, precise Ground Truth (GT) data is crucial for\naccurate Machine Learning (ML) based crop classification. Yet, issues like crop\nmislabeling and incorrect land identification are common. We propose a\nmulti-level GT cleaning framework while utilizing multi-temporal Sentinel-2\ndata to address these issues. Specifically, this framework utilizes generating\nembeddings for farmland, clustering similar crop profiles, and identification\nof outliers indicating GT errors. We validated clusters with False Colour\nComposite (FCC) checks and used distance-based metrics to scale and automate\nthis verification process. The importance of cleaning the GT data became\napparent when the models were trained on the clean and unclean data. For\ninstance, when we trained a Random Forest model with the clean GT data, we\nachieved upto 70\\% absolute percentage points higher for the F1 score metric.\nThis approach advances crop classification methodologies, with potential for\napplications towards improving loan underwriting and agricultural\ndecision-making.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11849", "pdf": "https://arxiv.org/pdf/2503.11849", "abs": "https://arxiv.org/abs/2503.11849", "authors": ["Yi Wang", "Zhitong Xiong", "Chenying Liu", "Adam J. Stewart", "Thomas Dujardin", "Nikolaos Ioannis Bountos", "Angelos Zavras", "Franziska Gerken", "Ioannis Papoutsis", "Laura Leal-Taixé", "Xiao Xiang Zhu"], "title": "Towards a Unified Copernicus Foundation Model for Earth Vision", "categories": ["cs.CV"], "comment": "31 pages, 32 figures", "summary": "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps://github.com/zhu-xlab/Copernicus-FM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11935", "pdf": "https://arxiv.org/pdf/2503.11935", "abs": "https://arxiv.org/abs/2503.11935", "authors": ["Jun Yu", "Yang Zheng", "Lei Wang", "Yongqi Wang", "Shengfan Xu"], "title": "Design of an Expression Recognition Solution Employing the Global Channel-Spatial Attention Mechanism", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition is a challenging classification task with broad\napplication prospects in the field of human - computer interaction. This paper\naims to introduce the methods of our upcoming 8th Affective Behavior Analysis\nin the Wild (ABAW) competition to be held at CVPR2025. To address issues such\nas low recognition accuracy caused by subtle expression changes and multi -\nscales in facial expression recognition in videos, we propose global channel -\nspatial attention and median - enhanced spatial - channel attention to\nstrengthen feature processing for speech and images respectively. Secondly, to\nfully utilize the complementarity between the speech and facial expression\nmodalities, a speech - and - facial - expression key - frame alignment\ntechnique is adopted to calculate the weights of speech and facial expressions.\nThese weights are input into the feature fusion layer for multi - scale dilated\nfusion, which effectively improves the recognition rate of facial expression\nrecognition. In the facial expression recognition task of the 6th ABAW\ncompetition, our method achieved excellent results on the official validation\nset, which fully demonstrates the effectiveness and competitiveness of the\nproposed method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11958", "pdf": "https://arxiv.org/pdf/2503.11958", "abs": "https://arxiv.org/abs/2503.11958", "authors": ["Chong Su", "Yingbin Fu", "Zheyuan Hu", "Jing Yang", "Param Hanji", "Shaojun Wang", "Xuan Zhao", "Cengiz Öztireli", "Fangcheng Zhong"], "title": "CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Chong Su and Yingbin Fu contributed equally to this work", "summary": "We introduce CHOrD, a novel framework for scalable synthesis of 3D indoor\nscenes, designed to create house-scale, collision-free, and hierarchically\nstructured indoor digital twins. In contrast to existing methods that directly\nsynthesize the scene layout as a scene graph or object list, CHOrD incorporates\na 2D image-based intermediate layout representation, enabling effective\nprevention of collision artifacts by successfully capturing them as\nout-of-distribution (OOD) scenarios during generation. Furthermore, unlike\nexisting methods, CHOrD is capable of generating scene layouts that adhere to\ncomplex floor plans with multi-modal controls, enabling the creation of\ncoherent, house-wide layouts robust to both geometric and semantic variations\nin room structures. Additionally, we propose a novel dataset with expanded\ncoverage of household items and room configurations, as well as significantly\nimproved data quality. CHOrD demonstrates state-of-the-art performance on both\nthe 3D-FRONT and our proposed datasets, delivering photorealistic, spatially\ncoherent indoor scene synthesis adaptable to arbitrary floor plan variations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11995", "pdf": "https://arxiv.org/pdf/2503.11995", "abs": "https://arxiv.org/abs/2503.11995", "authors": ["Shun Zou", "Yi Zou", "Mingya Zhang", "Shipeng Luo", "Zhihao Chen", "Guangwei Gao"], "title": "Fraesormer: Learning Adaptive Sparse Transformer for Efficient Food Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "In recent years, Transformer has witnessed significant progress in food\nrecognition. However, most existing approaches still face two critical\nchallenges in lightweight food recognition: (1) the quadratic complexity and\nredundant feature representation from interactions with irrelevant tokens; (2)\nstatic feature recognition and single-scale representation, which overlook the\nunstructured, non-fixed nature of food images and the need for multi-scale\nfeatures. To address these, we propose an adaptive and efficient sparse\nTransformer architecture (Fraesormer) with two core designs: Adaptive Top-k\nSparse Partial Attention (ATK-SPA) and Hierarchical Scale-Sensitive Feature\nGating Network (HSSFGN). ATK-SPA uses a learnable Gated Dynamic Top-K Operator\n(GDTKO) to retain critical attention scores, filtering low query-key matches\nthat hinder feature aggregation. It also introduces a partial channel mechanism\nto reduce redundancy and promote expert information flow, enabling local-global\ncollaborative modeling. HSSFGN employs gating mechanism to achieve multi-scale\nfeature representation, enhancing contextual semantic information. Extensive\nexperiments show that Fraesormer outperforms state-of-the-art methods. code is\navailable at https://zs1314.github.io/Fraesormer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12006", "pdf": "https://arxiv.org/pdf/2503.12006", "abs": "https://arxiv.org/abs/2503.12006", "authors": ["Zhe Shan", "Yang Liu", "Lei Zhou", "Cheng Yan", "Heng Wang", "Xia Xie"], "title": "ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "The availability of large-scale remote sensing video data underscores the\nimportance of high-quality interactive segmentation. However, challenges such\nas small object sizes, ambiguous features, and limited generalization make it\ndifficult for current methods to achieve this goal. In this work, we propose\nROS-SAM, a method designed to achieve high-quality interactive segmentation\nwhile preserving generalization across diverse remote sensing data. The ROS-SAM\nis built upon three key innovations: 1) LoRA-based fine-tuning, which enables\nefficient domain adaptation while maintaining SAM's generalization ability, 2)\nEnhancement of deep network layers to improve the discriminability of extracted\nfeatures, thereby reducing misclassifications, and 3) Integration of global\ncontext with local boundary details in the mask decoder to generate\nhigh-quality segmentation masks. Additionally, we design the data pipeline to\nensure the model learns to better handle objects at varying scales during\ntraining while focusing on high-quality predictions during inference.\nExperiments on remote sensing video datasets show that the redesigned data\npipeline boosts the IoU by 6%, while ROS-SAM increases the IoU by 13%. Finally,\nwhen evaluated on existing remote sensing object tracking datasets, ROS-SAM\ndemonstrates impressive zero-shot capabilities, generating masks that closely\nresemble manual annotations. These results confirm ROS-SAM as a powerful tool\nfor fine-grained segmentation in remote sensing applications. Code is available\nat https://github.com/ShanZard/ROS-SAM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12009", "pdf": "https://arxiv.org/pdf/2503.12009", "abs": "https://arxiv.org/abs/2503.12009", "authors": ["Xin Jin", "Haisheng Su", "Kai Liu", "Cong Ma", "Wei Wu", "Fei Hui", "Junchi Yan"], "title": "UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Recent advances in LiDAR 3D detection have demonstrated the effectiveness of\nTransformer-based frameworks in capturing the global dependencies from point\ncloud spaces, which serialize the 3D voxels into the flattened 1D sequence for\niterative self-attention. However, the spatial structure of 3D voxels will be\ninevitably destroyed during the serialization process. Besides, due to the\nconsiderable number of 3D voxels and quadratic complexity of Transformers,\nmultiple sequences are grouped before feeding to Transformers, leading to a\nlimited receptive field. Inspired by the impressive performance of State Space\nModels (SSM) achieved in the field of 2D vision tasks, in this paper, we\npropose a novel Unified Mamba (UniMamba), which seamlessly integrates the\nmerits of 3D convolution and SSM in a concise multi-head manner, aiming to\nperform \"local and global\" spatial context aggregation efficiently and\nsimultaneously. Specifically, a UniMamba block is designed which mainly\nconsists of spatial locality modeling, complementary Z-order serialization and\nlocal-global sequential aggregator. The spatial locality modeling module\nintegrates 3D submanifold convolution to capture the dynamic spatial position\nembedding before serialization. Then the efficient Z-order curve is adopted for\nserialization both horizontally and vertically. Furthermore, the local-global\nsequential aggregator adopts the channel grouping strategy to efficiently\nencode both \"local and global\" spatial inter-dependencies using multi-head SSM.\nAdditionally, an encoder-decoder architecture with stacked UniMamba blocks is\nformed to facilitate multi-scale spatial learning hierarchically. Extensive\nexperiments are conducted on three popular datasets: nuScenes, Waymo and\nArgoverse 2. Particularly, our UniMamba achieves 70.2 mAP on the nuScenes\ndataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12014", "pdf": "https://arxiv.org/pdf/2503.12014", "abs": "https://arxiv.org/abs/2503.12014", "authors": ["Shun Zou", "Yi Zou", "Mingya Zhang", "Shipeng Luo", "Guangwei Gao", "Guojun Qi"], "title": "Learning Dual-Domain Multi-Scale Representations for Single Image Deraining", "categories": ["cs.CV"], "comment": "6 pages, 5 figures, code: https://zs1314.github.io/DMSR", "summary": "Existing image deraining methods typically rely on single-input,\nsingle-output, and single-scale architectures, which overlook the joint\nmulti-scale information between external and internal features. Furthermore,\nsingle-domain representations are often too restrictive, limiting their ability\nto handle the complexities of real-world rain scenarios. To address these\nchallenges, we propose a novel Dual-Domain Multi-Scale Representation Network\n(DMSR). The key idea is to exploit joint multi-scale representations from both\nexternal and internal domains in parallel while leveraging the strengths of\nboth spatial and frequency domains to capture more comprehensive properties.\nSpecifically, our method consists of two main components: the Multi-Scale\nProgressive Spatial Refinement Module (MPSRM) and the Frequency Domain Scale\nMixer (FDSM). The MPSRM enables the interaction and coupling of multi-scale\nexpert information within the internal domain using a hierarchical modulation\nand fusion strategy. The FDSM extracts multi-scale local information in the\nspatial domain, while also modeling global dependencies in the frequency\ndomain. Extensive experiments show that our model achieves state-of-the-art\nperformance across six benchmark datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12018", "pdf": "https://arxiv.org/pdf/2503.12018", "abs": "https://arxiv.org/abs/2503.12018", "authors": ["Zhe Jin", "Tat-Seng Chua"], "title": "Compose Your Aesthetics: Empowering Text-to-Image Models with the Principles of Art", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) diffusion models (DM) have garnered widespread adoption\ndue to their capability in generating high-fidelity outputs and accessibility\nto anyone able to put imagination into words. However, DMs are often\npredisposed to generate unappealing outputs, much like the random images on the\ninternet they were trained on. Existing approaches to address this are founded\non the implicit premise that visual aesthetics is universal, which is limiting.\nAesthetics in the T2I context should be about personalization and we propose\nthe novel task of aesthetics alignment which seeks to align user-specified\naesthetics with the T2I generation output. Inspired by how artworks provide an\ninvaluable perspective to approach aesthetics, we codify visual aesthetics\nusing the compositional framework artists employ, known as the Principles of\nArt (PoA). To facilitate this study, we introduce CompArt, a large-scale\ncompositional art dataset building on top of WikiArt with PoA analysis\nannotated by a capable Multimodal LLM. Leveraging the expressive power of LLMs\nand training a lightweight and transferrable adapter, we demonstrate that T2I\nDMs can effectively offer 10 compositional controls through user-specified PoA\nconditions. Additionally, we design an appropriate evaluation framework to\nassess the efficacy of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12024", "pdf": "https://arxiv.org/pdf/2503.12024", "abs": "https://arxiv.org/abs/2503.12024", "authors": ["Byeongjun Park", "Hyojun Go", "Hyelin Nam", "Byung-Hoon Kim", "Hyungjin Chung", "Changick Kim"], "title": "SteerX: Creating Any Camera-Free 3D and 4D Scenes with Geometric Steering", "categories": ["cs.CV"], "comment": "Project page: https://byeongjun-park.github.io/SteerX/", "summary": "Recent progress in 3D/4D scene generation emphasizes the importance of\nphysical alignment throughout video generation and scene reconstruction.\nHowever, existing methods improve the alignment separately at each stage,\nmaking it difficult to manage subtle misalignments arising from another stage.\nHere, we present SteerX, a zero-shot inference-time steering method that\nunifies scene reconstruction into the generation process, tilting data\ndistributions toward better geometric alignment. To this end, we introduce two\ngeometric reward functions for 3D/4D scene generation by using pose-free\nfeed-forward scene reconstruction models. Through extensive experiments, we\ndemonstrate the effectiveness of SteerX in improving 3D/4D scene generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12049", "pdf": "https://arxiv.org/pdf/2503.12049", "abs": "https://arxiv.org/abs/2503.12049", "authors": ["Ruijie Lu", "Yixin Chen", "Yu Liu", "Jiaxiang Tang", "Junfeng Ni", "Diwen Wan", "Gang Zeng", "Siyuan Huang"], "title": "TACO: Taming Diffusion for in-the-wild Video Amodal Completion", "categories": ["cs.CV"], "comment": "Project page: https://jason-aplp.github.io/TACO", "summary": "Humans can infer complete shapes and appearances of objects from limited\nvisual cues, relying on extensive prior knowledge of the physical world.\nHowever, completing partially observable objects while ensuring consistency\nacross video frames remains challenging for existing models, especially for\nunstructured, in-the-wild videos. This paper tackles the task of Video Amodal\nCompletion (VAC), which aims to generate the complete object consistently\nthroughout the video given a visual prompt specifying the object of interest.\nLeveraging the rich, consistent manifolds learned by pre-trained video\ndiffusion models, we propose a conditional diffusion model, TACO, that\nrepurposes these manifolds for VAC. To enable its effective and robust\ngeneralization to challenging in-the-wild scenarios, we curate a large-scale\nsynthetic dataset with multiple difficulty levels by systematically imposing\nocclusions onto un-occluded videos. Building on this, we devise a progressive\nfine-tuning paradigm that starts with simpler recovery tasks and gradually\nadvances to more complex ones. We demonstrate TACO's versatility on a wide\nrange of in-the-wild videos from Internet, as well as on diverse, unseen\ndatasets commonly used in autonomous driving, robotic manipulation, and scene\nunderstanding. Moreover, we show that TACO can be effectively applied to\nvarious downstream tasks like object reconstruction and pose estimation,\nhighlighting its potential to facilitate physical world understanding and\nreasoning. Our project page is available at https://jason-aplp.github.io/TACO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12061", "pdf": "https://arxiv.org/pdf/2503.12061", "abs": "https://arxiv.org/abs/2503.12061", "authors": ["Yuqing Yan", "Yirui Wu"], "title": "EHNet: An Efficient Hybrid Network for Crowd Counting and Localization", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, crowd counting and localization have become crucial\ntechniques in computer vision, with applications spanning various domains. The\npresence of multi-scale crowd distributions within a single image remains a\nfundamental challenge in crowd counting tasks. To address these challenges, we\nintroduce the Efficient Hybrid Network (EHNet), a novel framework for efficient\ncrowd counting and localization. By reformulating crowd counting into a point\nregression framework, EHNet leverages the Spatial-Position Attention Module\n(SPAM) to capture comprehensive spatial contexts and long-range dependencies.\nAdditionally, we develop an Adaptive Feature Aggregation Module (AFAM) to\neffectively fuse and harmonize multi-scale feature representations. Building\nupon these, we introduce the Multi-Scale Attentive Decoder (MSAD). Experimental\nresults on four benchmark datasets demonstrate that EHNet achieves competitive\nperformance with reduced computational overhead, outperforming existing methods\non ShanghaiTech Part \\_A, ShanghaiTech Part \\_B, UCF-CC-50, and UCF-QNRF. Our\ncode is in https://anonymous.4open.science/r/EHNet.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12063", "pdf": "https://arxiv.org/pdf/2503.12063", "abs": "https://arxiv.org/abs/2503.12063", "authors": ["Yuqing Yan", "Yirui Wu"], "title": "DLA-Count: Dynamic Label Assignment Network for Dense Cell Distribution Counting", "categories": ["cs.CV"], "comment": null, "summary": "Cell counting remains a fundamental yet challenging task in medical and\nbiological research due to the diverse morphology of cells, their dense\ndistribution, and variations in image quality. We present DLA-Count, a\nbreakthrough approach to cell counting that introduces three key innovations:\n(1) K-adjacent Hungarian Matching (KHM), which dramatically improves cell\nmatching in dense regions, (2) Multi-scale Deformable Gaussian Convolution\n(MDGC), which adapts to varying cell morphologies, and (3) Gaussian-enhanced\nFeature Decoder (GFD) for efficient multi-scale feature fusion. Our extensive\nexperiments on four challenging cell counting datasets (ADI, MBM, VGG, and DCC)\ndemonstrate that our method outperforms previous methods across diverse\ndatasets, with improvements in Mean Absolute Error of up to 46.7\\% on ADI and\n42.5\\% on MBM datasets. Our code is available at\nhttps://anonymous.4open.science/r/DLA-Count.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12067", "pdf": "https://arxiv.org/pdf/2503.12067", "abs": "https://arxiv.org/abs/2503.12067", "authors": ["Amir M. Mansourian", "Rozhan Ahmadi", "Masoud Ghafouri", "Amir Mohammad Babaei", "Elaheh Badali Golezani", "Zeynab Yasamani Ghamchi", "Vida Ramezanian", "Alireza Taherian", "Kimia Dinashi", "Amirali Miri", "Shohreh Kasaei"], "title": "A Comprehensive Survey on Knowledge Distillation", "categories": ["cs.CV"], "comment": "47 pages, 10 figures, 13 tables", "summary": "Deep Neural Networks (DNNs) have achieved notable performance in the fields\nof computer vision and natural language processing with various applications in\nboth academia and industry. However, with recent advancements in DNNs and\ntransformer models with a tremendous number of parameters, deploying these\nlarge models on edge devices causes serious issues such as high runtime and\nmemory consumption. This is especially concerning with the recent large-scale\nfoundation models, Vision-Language Models (VLMs), and Large Language Models\n(LLMs). Knowledge Distillation (KD) is one of the prominent techniques proposed\nto address the aforementioned problems using a teacher-student architecture.\nMore specifically, a lightweight student model is trained using additional\nknowledge from a cumbersome teacher model. In this work, a comprehensive survey\nof knowledge distillation methods is proposed. This includes reviewing KD from\ndifferent aspects: distillation sources, distillation schemes, distillation\nalgorithms, distillation by modalities, applications of distillation, and\ncomparison among existing methods. In contrast to most existing surveys, which\nare either outdated or simply update former surveys, this work proposes a\ncomprehensive survey with a new point of view and representation structure that\ncategorizes and investigates the most recent methods in knowledge distillation.\nThis survey considers various critically important subcategories, including KD\nfor diffusion models, 3D inputs, foundational models, transformers, and LLMs.\nFurthermore, existing challenges in KD and possible future research directions\nare discussed. Github page of the project:\nhttps://github.com/IPL-Sharif/KD_Survey", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12069", "pdf": "https://arxiv.org/pdf/2503.12069", "abs": "https://arxiv.org/abs/2503.12069", "authors": ["Wei Lai", "Tianyu Ding", "ren dongdong", "Lei Wang", "Jing Huo", "Yang Gao", "Wenbin Li"], "title": "Robust Dataset Distillation by Matching Adversarial Trajectories", "categories": ["cs.CV"], "comment": null, "summary": "Dataset distillation synthesizes compact datasets that enable models to\nachieve performance comparable to training on the original large-scale\ndatasets. However, existing distillation methods overlook the robustness of the\nmodel, resulting in models that are vulnerable to adversarial attacks when\ntrained on distilled data. To address this limitation, we introduce the task of\n``robust dataset distillation\", a novel paradigm that embeds adversarial\nrobustness into the synthetic datasets during the distillation process. We\npropose Matching Adversarial Trajectories (MAT), a method that integrates\nadversarial training into trajectory-based dataset distillation. MAT\nincorporates adversarial samples during trajectory generation to obtain robust\ntraining trajectories, which are then used to guide the distillation process.\nAs experimentally demonstrated, even through natural training on our distilled\ndataset, models can achieve enhanced adversarial robustness while maintaining\ncompetitive accuracy compared to existing distillation methods. Our work\nhighlights robust dataset distillation as a new and important research\ndirection and provides a strong baseline for future research to bridge the gap\nbetween efficient training and adversarial robustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12093", "pdf": "https://arxiv.org/pdf/2503.12093", "abs": "https://arxiv.org/abs/2503.12093", "authors": ["Oren Shrout", "Ayellet Tal"], "title": "SFMNet: Sparse Focal Modulation for 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "We propose SFMNet, a novel 3D sparse detector that combines the efficiency of\nsparse convolutions with the ability to model long-range dependencies. While\ntraditional sparse convolution techniques efficiently capture local structures,\nthey struggle with modeling long-range relationships. However, capturing\nlong-range dependencies is fundamental for 3D object detection. In contrast,\ntransformers are designed to capture these long-range dependencies through\nattention mechanisms. But, they come with high computational costs, due to\ntheir quadratic query-key-value interactions. Furthermore, directly applying\nattention to non-empty voxels is inefficient due to the sparse nature of 3D\nscenes. Our SFMNet is built on a novel Sparse Focal Modulation (SFM) module,\nwhich integrates short- and long-range contexts with linear complexity by\nleveraging a new hierarchical sparse convolution design. This approach enables\nSFMNet to achieve high detection performance with improved efficiency, making\nit well-suited for large-scale LiDAR scenes. We show that our detector achieves\nstate-of-the-art performance on autonomous driving datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12096", "pdf": "https://arxiv.org/pdf/2503.12096", "abs": "https://arxiv.org/abs/2503.12096", "authors": ["Ashshak Sharifdeen", "Muhammad Akhtar Munir", "Sanoojan Baliah", "Salman Khan", "Muhammad Haris Khan"], "title": "O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Test-time prompt tuning for vision-language models (VLMs) is getting\nattention because of their ability to learn with unlabeled data without\nfine-tuning. Although test-time prompt tuning methods for VLMs can boost\naccuracy, the resulting models tend to demonstrate poor calibration, which\ncasts doubts on the reliability and trustworthiness of these models. Notably,\nmore attention needs to be devoted to calibrating the test-time prompt tuning\nin vision-language models. To this end, we propose a new approach, called O-TPT\nthat introduces orthogonality constraints on the textual features corresponding\nto the learnable prompts for calibrating test-time prompt tuning in VLMs.\nTowards introducing orthogonality constraints, we make the following\ncontributions. First, we uncover new insights behind the suboptimal calibration\nperformance of existing methods relying on textual feature dispersion. Second,\nwe show that imposing a simple orthogonalization of textual features is a more\neffective approach towards obtaining textual dispersion. We conduct extensive\nexperiments on various datasets with different backbones and baselines. The\nresults indicate that our method consistently outperforms the prior state of\nthe art in significantly reducing the overall average calibration error. Also,\nour method surpasses the zero-shot calibration performance on fine-grained\nclassification tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12165", "pdf": "https://arxiv.org/pdf/2503.12165", "abs": "https://arxiv.org/abs/2503.12165", "authors": ["Zijian He", "Yuwei Ning", "Yipeng Qin", "Wangrun Wang", "Sibei Yang", "Liang Lin", "Guanbin Li"], "title": "VTON 360: High-Fidelity Virtual Try-On from Any Viewing Direction", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Virtual Try-On (VTON) is a transformative technology in e-commerce and\nfashion design, enabling realistic digital visualization of clothing on\nindividuals. In this work, we propose VTON 360, a novel 3D VTON method that\naddresses the open challenge of achieving high-fidelity VTON that supports\nany-view rendering. Specifically, we leverage the equivalence between a 3D\nmodel and its rendered multi-view 2D images, and reformulate 3D VTON as an\nextension of 2D VTON that ensures 3D consistent results across multiple views.\nTo achieve this, we extend 2D VTON models to include multi-view garments and\nclothing-agnostic human body images as input, and propose several novel\ntechniques to enhance them, including: i) a pseudo-3D pose representation using\nnormal maps derived from the SMPL-X 3D human model, ii) a multi-view spatial\nattention mechanism that models the correlations between features from\ndifferent viewing angles, and iii) a multi-view CLIP embedding that enhances\nthe garment CLIP features used in 2D VTON with camera information. Extensive\nexperiments on large-scale real datasets and clothing images from e-commerce\nplatforms demonstrate the effectiveness of our approach. Project page:\nhttps://scnuhealthy.github.io/VTON360.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12332", "pdf": "https://arxiv.org/pdf/2503.12332", "abs": "https://arxiv.org/abs/2503.12332", "authors": ["Yunze Liu", "Peiran Wu", "Cheng Liang", "Junxiao Shen", "Limin Wang", "Li Yi"], "title": "VideoMAP: Toward Scalable Mamba-based Video Autoregressive Pretraining", "categories": ["cs.CV"], "comment": null, "summary": "Recent Mamba-based architectures for video understanding demonstrate\npromising computational efficiency and competitive performance, yet struggle\nwith overfitting issues that hinder their scalability. To overcome this\nchallenge, we introduce VideoMAP, a Hybrid Mamba-Transformer framework\nfeaturing a novel pre-training approach. VideoMAP uses a 4:1\nMamba-to-Transformer ratio, effectively balancing computational cost and model\ncapacity. This architecture, combined with our proposed frame-wise masked\nautoregressive pre-training strategy, delivers significant performance gains\nwhen scaling to larger models. Additionally, VideoMAP exhibits impressive\nsample efficiency, significantly outperforming existing methods with less\ntraining data. Experiments show that VideoMAP outperforms existing models\nacross various datasets, including Kinetics-400, Something-Something V2,\nBreakfast, and COIN. Furthermore, we demonstrate the potential of VideoMAP as a\nvisual encoder for multimodal large language models, highlighting its ability\nto reduce memory usage and enable the processing of longer video sequences. The\ncode is open-source at https://github.com/yunzeliu/MAP", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12355", "pdf": "https://arxiv.org/pdf/2503.12355", "abs": "https://arxiv.org/abs/2503.12355", "authors": ["Kumar Krishna Agrawal", "Long Lian", "Longchao Liu", "Natalia Harguindeguy", "Boyi Li", "Alexander Bick", "Maggie Chung", "Trevor Darrell", "Adam Yala"], "title": "Atlas: Multi-Scale Attention Improves Long Context Image Modeling", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Efficiently modeling massive images is a long-standing challenge in machine\nlearning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on\ntwo key ideas, (i) multi-scale representations (ii) bi-directional cross-scale\ncommunication. MSA creates O(log N) scales to represent the image across\nprogressively coarser features and leverages cross-attention to propagate\ninformation across scales. We then introduce Atlas, a novel neural network\narchitecture based on MSA. We demonstrate that Atlas significantly improves the\ncompute-performance tradeoff of long-context image modeling in a\nhigh-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves\n91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster.\nAtlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96%\nbetter than LongViT. In comparisons against MambaVision-S, we find Atlas-S\nachieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px\nrespectively, while obtaining similar runtimes. Code for reproducing our\nexperiments and pretrained models is available at\nhttps://github.com/yalalab/atlas.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12369", "pdf": "https://arxiv.org/pdf/2503.12369", "abs": "https://arxiv.org/abs/2503.12369", "authors": ["Ruoyu Wang", "Yukai Ma", "Yi Yao", "Sheng Tao", "Haoang Li", "Zongzhi Zhu", "Yong Liu", "Xingxing Zuo"], "title": "L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model", "categories": ["cs.CV"], "comment": null, "summary": "Semantic Scene Completion (SSC) constitutes a pivotal element in autonomous\ndriving perception systems, tasked with inferring the 3D semantic occupancy of\na scene from sensory data. To improve accuracy, prior research has implemented\nvarious computationally demanding and memory-intensive 3D operations, imposing\nsignificant computational requirements on the platform during training and\ntesting. This paper proposes L2COcc, a lightweight camera-centric SSC framework\nthat also accommodates LiDAR inputs. With our proposed efficient voxel\ntransformer (EVT) and cross-modal knowledge modules, including feature\nsimilarity distillation (FSD), TPV distillation (TPVD) and prediction alignment\ndistillation (PAD), our method substantially reduce computational burden while\nmaintaining high accuracy. The experimental evaluations demonstrate that our\nproposed method surpasses the current state-of-the-art vision-based SSC methods\nregarding accuracy on both the SemanticKITTI and SSCBench-KITTI-360 benchmarks,\nrespectively. Additionally, our method is more lightweight, exhibiting a\nreduction in both memory consumption and inference time by over 23% compared to\nthe current state-of-the-arts method. Code is available at our project\npage:https://studyingfufu.github.io/L2COcc/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12382", "pdf": "https://arxiv.org/pdf/2503.12382", "abs": "https://arxiv.org/abs/2503.12382", "authors": ["Kang You", "Tong Chen", "Dandan Ding", "M. Salman Asif", "Zhan Ma"], "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Despite the substantial advancements demonstrated by learning-based neural\nmodels in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time\ncompression - an indispensable criterion for numerous industrial applications -\nremains a formidable challenge. This paper proposes RENO, the first real-time\nneural codec for 3D LiDAR point clouds, achieving superior performance with a\nlightweight model. RENO skips the octree construction and directly builds upon\nthe multiscale sparse tensor representation. Instead of the multi-stage\ninferring, RENO devises sparse occupancy codes, which exploit cross-scale\ncorrelation and derive voxels' occupancy in a one-shot manner, greatly saving\nprocessing time. Experimental results demonstrate that the proposed RENO\nachieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform\n(e.g., one RTX 3090 GPU) for both encoding and decoding processes, while\nproviding 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco,\nrespectively, at a similar quality. RENO model size is merely 1MB, making it\nattractive for practical applications. The source code is available at\nhttps://github.com/NJUVISION/RENO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12383", "pdf": "https://arxiv.org/pdf/2503.12383", "abs": "https://arxiv.org/abs/2503.12383", "authors": ["Songen Gu", "Haoxuan Song", "Binjie Liu", "Qian Yu", "Sanyi Zhang", "Haiyong Jiang", "Jin Huang", "Feng Tian"], "title": "VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native\n3D object generation framework that incorporates a 3D Gaussian Splatting\nrepresentation. As part of our work, we introduce VRSS, the first large-scale\npaired dataset containing VR sketches, text, images, and 3DGS, bridging the gap\nin multi-modal VR sketch-based generation. Our approach features the following\nkey innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage\nalignment strategy that bridges the domain gap between sparse VR sketch\nembeddings and rich CLIP embeddings, facilitating both VR sketch-based\nretrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We\ndisentangle the 3D generation process by using explicit VR sketches for\ngeometric conditioning and text descriptions for appearance control. To\nfacilitate this, we propose a generalizable VR sketch encoder that effectively\naligns different modalities. 3) Efficient and high-fidelity 3D native\ngeneration. Our method leverages a 3D-native generation approach that enables\nfast and texture-rich 3D object synthesis. Experiments conducted on our VRSS\ndataset demonstrate that our method achieves high-quality, multi-modal VR\nsketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian\nmethod will be beneficial for the 3D generation community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12385", "pdf": "https://arxiv.org/pdf/2503.12385", "abs": "https://arxiv.org/abs/2503.12385", "authors": ["Yutao Hu", "Sen Li", "Jincheng Yan", "Wenqi Shao", "Xiaoyan Luo"], "title": "Car-1000: A New Large Scale Fine-Grained Visual Categorization Dataset", "categories": ["cs.CV"], "comment": "accepted to The Eleventh Workshop on Fine-Grained Visual\n  Categorization in CVPR 2024", "summary": "Fine-grained visual categorization (FGVC) is a challenging but significant\ntask in computer vision, which aims to recognize different sub-categories of\nbirds, cars, airplanes, etc. Among them, recognizing models of different cars\nhas significant application value in autonomous driving, traffic surveillance\nand scene understanding, which has received considerable attention in the past\nfew years. However, Stanford-Car, the most widely used fine-grained dataset for\ncar recognition, only has 196 different categories and only includes vehicle\nmodels produced earlier than 2013. Due to the rapid advancements in the\nautomotive industry during recent years, the appearances of various car models\nhave become increasingly intricate and sophisticated. Consequently, the\nprevious Stanford-Car dataset fails to capture this evolving landscape and\ncannot satisfy the requirements of automotive industry. To address these\nchallenges, in our paper, we introduce Car-1000, a large-scale dataset designed\nspecifically for fine-grained visual categorization of diverse car models.\nCar-1000 encompasses vehicles from 165 different automakers, spanning a wide\nrange of 1000 distinct car models. Additionally, we have reproduced several\nstate-of-the-art FGVC methods on the Car-1000 dataset, establishing a new\nbenchmark for research in this field. We hope that our work will offer a fresh\nperspective for future FGVC researchers. Our dataset is available at\nhttps://github.com/toggle1995/Car-1000.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12404", "pdf": "https://arxiv.org/pdf/2503.12404", "abs": "https://arxiv.org/abs/2503.12404", "authors": ["Jianhao Yang", "Wenshuo Yu", "Yuanchao Lv", "Jiance Sun", "Bokang Sun", "Mingyang Liu"], "title": "SAM2-ELNet: Label Enhancement and Automatic Annotation for Remote Sensing Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing image segmentation is crucial for environmental monitoring,\ndisaster assessment, and resource management, directly affecting the accuracy\nand efficiency of surface information extraction. The performance of existing\nsupervised models in remote sensing image segmentation tasks highly depends on\nthe quality of label data. However, current label data mainly relies on manual\nannotation, which comes with high time costs and is subject to subjective\ninterference, resulting in distortion of label boundaries and often a loss of\ndetail. To solve the above problems, our work proposes an Edge-enhanced\nLabeling Network, called SAM2-ELNet, which incorporates a labeling module and\nan edge attention mechanism. This model effectively addresses issues such as\nlabel detail loss, fragmentation, and inaccurate boundaries. Due to the\nscarcity of manually annotated remote sensing data, the feature extraction\ncapabilities of traditional neural networks are limited. Our method uses the\nHiera backbone of the pre-trained self-supervised large model segment anything\nmodel 2 (SAM2) as the encoder, achieves high-quality and efficient feature\nextraction even with small samples by fine-tuning on downstream tasks. This\nstudy compared the training effects of original and enhanced labels on the\nmanually annotated Deep-SAR Oil Spill (SOS) dataset. Results showed that the\nmodel trained with enhanced labels performed better and had a lower final loss,\nindicating closer alignment with the real data distribution. Our work also\nexplores the potential of extending the model into an efficient automatic\nannotation framework through generalization experiments, facilitating\nlarge-scale remote sensing image interpretation and intelligent recognition.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12419", "pdf": "https://arxiv.org/pdf/2503.12419", "abs": "https://arxiv.org/abs/2503.12419", "authors": ["Luming Wang", "Hao Shi", "Xiaoting Yin", "Kailun Yang", "Kaiwei Wang"], "title": "EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera", "categories": ["cs.CV", "cs.RO", "eess.IV", "physics.optics"], "comment": "The dataset and models are made publicly available at\n  https://github.com/3190105222/EgoEv_Gesture", "summary": "Egocentric gesture recognition is a pivotal technology for enhancing natural\nhuman-computer interaction, yet traditional RGB-based solutions suffer from\nmotion blur and illumination variations in dynamic scenarios. While event\ncameras show distinct advantages in handling high dynamic range with ultra-low\npower consumption, existing RGB-based architectures face inherent limitations\nin processing asynchronous event streams due to their synchronous frame-based\nnature. Moreover, from an egocentric perspective, event cameras record data\nthat include events generated by both head movements and hand gestures, thereby\nincreasing the complexity of gesture recognition. To address this, we propose a\nnovel network architecture specifically designed for event data processing,\nincorporating (1) a lightweight CNN with asymmetric depthwise convolutions to\nreduce parameters while preserving spatiotemporal features, (2) a plug-and-play\nstate-space model as context block that decouples head movement noise from\ngesture dynamics, and (3) a parameter-free Bins-Temporal Shift Module (BSTM)\nthat shifts features along bins and temporal dimensions to fuse sparse events\nefficiently. We further build the EgoEvGesture dataset, the first large-scale\ndataset for egocentric gesture recognition using event cameras. Experimental\nresults demonstrate that our method achieves 62.7% accuracy in heterogeneous\ntesting with only 7M parameters, 3.1% higher than state-of-the-art approaches.\nNotable misclassifications in freestyle motions stem from high inter-personal\nvariability and unseen test patterns differing from training data. Moreover,\nour approach achieved a remarkable accuracy of 96.97% on DVS128 Gesture,\ndemonstrating strong cross-dataset generalization capability. The dataset and\nmodels are made publicly available at\nhttps://github.com/3190105222/EgoEv_Gesture.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12460", "pdf": "https://arxiv.org/pdf/2503.12460", "abs": "https://arxiv.org/abs/2503.12460", "authors": ["Zhicheng Wang", "Zhiyu Pan", "Zhan Peng", "Jian Cheng", "Liwen Xiao", "Wei Jiang", "Zhiguo Cao"], "title": "Exploring Contextual Attribute Density in Referring Expression Counting", "categories": ["cs.CV"], "comment": "CVPR25", "summary": "Referring expression counting (REC) algorithms are for more flexible and\ninteractive counting ability across varied fine-grained text expressions.\nHowever, the requirement for fine-grained attribute understanding poses\nchallenges for prior arts, as they struggle to accurately align attribute\ninformation with correct visual patterns. Given the proven importance of\n''visual density'', it is presumed that the limitations of current REC\napproaches stem from an under-exploration of ''contextual attribute density''\n(CAD). In the scope of REC, we define CAD as the measure of the information\nintensity of one certain fine-grained attribute in visual regions. To model the\nCAD, we propose a U-shape CAD estimator in which referring expression and\nmulti-scale visual features from GroundingDINO can interact with each other.\nWith additional density supervision, we can effectively encode CAD, which is\nsubsequently decoded via a novel attention procedure with CAD-refined queries.\nIntegrating all these contributions, our framework significantly outperforms\nstate-of-the-art REC methods, achieves $30\\%$ error reduction in counting\nmetrics and a $10\\%$ improvement in localization accuracy. The surprising\nresults shed light on the significance of contextual attribute density for REC.\nCode will be at github.com/Xu3XiWang/CAD-GD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12472", "pdf": "https://arxiv.org/pdf/2503.12472", "abs": "https://arxiv.org/abs/2503.12472", "authors": ["Wenbo Dai", "Lijing Lu", "Zhihang Li"], "title": "Diffusion-based Synthetic Data Generation for Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "The performance of models is intricately linked to the abundance of training\ndata. In Visible-Infrared person Re-IDentification (VI-ReID) tasks, collecting\nand annotating large-scale images of each individual under various cameras and\nmodalities is tedious, time-expensive, costly and must comply with data\nprotection laws, posing a severe challenge in meeting dataset requirements.\nCurrent research investigates the generation of synthetic data as an efficient\nand privacy-ensuring alternative to collecting real data in the field. However,\na specific data synthesis technique tailored for VI-ReID models has yet to be\nexplored. In this paper, we present a novel data generation framework, dubbed\nDiffusion-based VI-ReID data Expansion (DiVE), that automatically obtain\nmassive RGB-IR paired images with identity preserving by decoupling identity\nand modality to improve the performance of VI-ReID models. Specifically,\nidentity representation is acquired from a set of samples sharing the same ID,\nwhereas the modality of images is learned by fine-tuning the Stable Diffusion\n(SD) on modality-specific data. DiVE extend the text-driven image synthesis to\nidentity-preserving RGB-IR multimodal image synthesis. This approach\nsignificantly reduces data collection and annotation costs by directly\nincorporating synthetic data into ReID model training. Experiments have\ndemonstrated that VI-ReID models trained on synthetic data produced by DiVE\nconsistently exhibit notable enhancements. In particular, the state-of-the-art\nmethod, CAJ, trained with synthetic images, achieves an improvement of about\n$9\\%$ in mAP over the baseline on the LLCM dataset. Code:\nhttps://github.com/BorgDiven/DiVE", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12496", "pdf": "https://arxiv.org/pdf/2503.12496", "abs": "https://arxiv.org/abs/2503.12496", "authors": ["Tianyuan Qu", "Longxiang Tang", "Bohao Peng", "Senqiao Yang", "Bei Yu", "Jiaya Jia"], "title": "Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?", "categories": ["cs.CV"], "comment": null, "summary": "The rise of Large Vision-Language Models (LVLMs) has significantly advanced\nvideo understanding. However, efficiently processing long videos remains a\nchallenge due to the ``Sampling Dilemma'': low-density sampling risks missing\ncritical information, while high-density sampling introduces redundancy. To\naddress this issue, we introduce LSDBench, the first benchmark designed to\nevaluate LVLMs on long-video tasks by constructing high Necessary Sampling\nDensity (NSD) questions, where NSD represents the minimum sampling density\nrequired to accurately answer a given question. LSDBench focuses on dense,\nshort-duration actions to rigorously assess the sampling strategies employed by\nLVLMs. To tackle the challenges posed by high-NSD questions, we propose a novel\nReasoning-Driven Hierarchical Sampling (RHS) framework, which combines global\nlocalization of question-relevant cues with local dense sampling for precise\ninference. Additionally, we develop a lightweight Semantic-Guided Frame\nSelector to prioritize informative frames, enabling RHS to achieve comparable\nor superior performance with significantly fewer sampled frames. Together, our\nLSDBench and RHS framework address the unique challenges of high-NSD long-video\ntasks, setting a new standard for evaluating and improving LVLMs in this\ndomain.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12542", "pdf": "https://arxiv.org/pdf/2503.12542", "abs": "https://arxiv.org/abs/2503.12542", "authors": ["Peiran Wu", "Yunze Liu", "Chonghan Liu", "Miao Liu", "Junxiao Shen"], "title": "ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos", "categories": ["cs.CV"], "comment": null, "summary": "Humans excel at spatio-temporal reasoning, effortlessly interpreting dynamic\nvisual events from an egocentric viewpoint. However, whether multimodal large\nlanguage models (MLLMs) can similarly comprehend the 4D world remains\nuncertain. This paper explores multimodal spatio-temporal reasoning from an\negocentric perspective, aiming to equip MLLMs with human-like reasoning\ncapabilities. To support this objective, we introduce Ego-ST Bench, a novel\nbenchmark containing over 5,000 question-answer pairs across four categories,\nsystematically evaluating spatial, temporal, and integrated spatio-temporal\nreasoning. Additionally, we propose the ST-R1 Video model, a video-based\nreasoning model that incorporates reverse thinking into its reinforcement\nlearning process, significantly enhancing performance. We combine\nlong-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative\nPolicy Optimization (GRPO) reinforcement learning, achieving notable\nimprovements with limited high-quality data. Ego-ST Bench and ST-R1 provide\nvaluable insights and resources for advancing video-based spatio-temporal\nreasoning research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12552", "pdf": "https://arxiv.org/pdf/2503.12552", "abs": "https://arxiv.org/abs/2503.12552", "authors": ["Tianyu Li", "Yihang Qiu", "Zhenhua Wu", "Carl Lindström", "Peng Su", "Matthias Nießner", "Hongyang Li"], "title": "MTGS: Multi-Traversal Gaussian Splatting", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Multi-traversal data, commonly collected through daily commutes or by\nself-driving fleets, provides multiple viewpoints for scene reconstruction\nwithin a road block. This data offers significant potential for high-quality\nnovel view synthesis, which is crucial for applications such as autonomous\nvehicle simulators. However, inherent challenges in multi-traversal data often\nresult in suboptimal reconstruction quality, including variations in appearance\nand the presence of dynamic objects. To address these issues, we propose\nMulti-Traversal Gaussian Splatting (MTGS), a novel approach that reconstructs\nhigh-quality driving scenes from arbitrarily collected multi-traversal data by\nmodeling a shared static geometry while separately handling dynamic elements\nand appearance variations. Our method employs a multi-traversal dynamic scene\ngraph with a shared static node and traversal-specific dynamic nodes,\ncomplemented by color correction nodes with learnable spherical harmonics\ncoefficient residuals. This approach enables high-fidelity novel view synthesis\nand provides flexibility to navigate any viewpoint. We conduct extensive\nexperiments on a large-scale driving dataset, nuPlan, with multi-traversal\ndata. Our results demonstrate that MTGS improves LPIPS by 23.5% and geometry\naccuracy by 46.3% compared to single-traversal baselines. The code and data\nwould be available to the public.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12617", "pdf": "https://arxiv.org/pdf/2503.12617", "abs": "https://arxiv.org/abs/2503.12617", "authors": ["Anthony Lamelas", "Harrison Muchnic"], "title": "Scaling Semantic Categories: Investigating the Impact on Vision Transformer Labeling Performance", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "4 pages, 7 figures, submitted to CVPR (feedback pending)", "summary": "This study explores the impact of scaling semantic categories on the image\nclassification performance of vision transformers (ViTs). In this specific\ncase, the CLIP server provided by Jina AI is used for experimentation. The\nresearch hypothesizes that as the number of ground truth and artificially\nintroduced semantically equivalent categories increases, the labeling accuracy\nof ViTs improves until a theoretical maximum or limit is reached. A wide\nvariety of image datasets were chosen to test this hypothesis. These datasets\nwere processed through a custom function in Python designed to evaluate the\nmodel's accuracy, with adjustments being made to account for format differences\nbetween datasets. By exponentially introducing new redundant categories, the\nexperiment assessed accuracy trends until they plateaued, decreased, or\nfluctuated inconsistently. The findings show that while semantic scaling\ninitially increases model performance, the benefits diminish or reverse after\nsurpassing a critical threshold, providing insight into the limitations and\npossible optimization of category labeling strategies for ViTs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12745", "pdf": "https://arxiv.org/pdf/2503.12745", "abs": "https://arxiv.org/abs/2503.12745", "authors": ["Patrick Rim", "Hyoungseob Park", "S. Gangopadhyay", "Ziyao Zeng", "Younjoon Chung", "Alex Wong"], "title": "ProtoDepth: Unsupervised Continual Depth Completion with Prototypes", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "We present ProtoDepth, a novel prototype-based approach for continual\nlearning of unsupervised depth completion, the multimodal 3D reconstruction\ntask of predicting dense depth maps from RGB images and sparse point clouds.\nThe unsupervised learning paradigm is well-suited for continual learning, as\nground truth is not needed. However, when training on new non-stationary\ndistributions, depth completion models will catastrophically forget previously\nlearned information. We address forgetting by learning prototype sets that\nadapt the latent features of a frozen pretrained model to new domains. Since\nthe original weights are not modified, ProtoDepth does not forget when\ntest-time domain identity is known. To extend ProtoDepth to the challenging\nsetting where the test-time domain identity is withheld, we propose to learn\ndomain descriptors that enable the model to select the appropriate prototype\nset for inference. We evaluate ProtoDepth on benchmark dataset sequences, where\nwe reduce forgetting compared to baselines by 52.2% for indoor and 53.2% for\noutdoor to achieve the state of the art.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12772", "pdf": "https://arxiv.org/pdf/2503.12772", "abs": "https://arxiv.org/abs/2503.12772", "authors": ["Sung-Yeon Park", "Can Cui", "Yunsheng Ma", "Ahmadreza Moradipari", "Rohit Gupta", "Kyungtae Han", "Ziran Wang"], "title": "NuPlanQA: A Large-Scale Dataset and Benchmark for Multi-View Driving Scene Understanding in Multi-Modal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Recent advances in multi-modal large language models (MLLMs) have\ndemonstrated strong performance across various domains; however, their ability\nto comprehend driving scenes remains less proven. The complexity of driving\nscenarios, which includes multi-view information, poses significant challenges\nfor existing MLLMs. In this paper, we introduce NuPlanQA-Eval, a multi-view,\nmulti-modal evaluation benchmark for driving scene understanding. To further\nsupport generalization to multi-view driving scenarios, we also propose\nNuPlanQA-1M, a large-scale dataset comprising 1M real-world visual\nquestion-answering (VQA) pairs. For context-aware analysis of traffic scenes,\nwe categorize our dataset into nine subtasks across three core skills: Road\nEnvironment Perception, Spatial Relations Recognition, and Ego-Centric\nReasoning. Furthermore, we present BEV-LLM, integrating Bird's-Eye-View (BEV)\nfeatures from multi-view images into MLLMs. Our evaluation results reveal key\nchallenges that existing MLLMs face in driving scene-specific perception and\nspatial reasoning from ego-centric perspectives. In contrast, BEV-LLM\ndemonstrates remarkable adaptability to this domain, outperforming other models\nin six of the nine subtasks. These findings highlight how BEV integration\nenhances multi-view MLLMs while also identifying key areas that require further\nrefinement for effective adaptation to driving scenes. To facilitate further\nresearch, we publicly release NuPlanQA at\nhttps://github.com/sungyeonparkk/NuPlanQA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12779", "pdf": "https://arxiv.org/pdf/2503.12779", "abs": "https://arxiv.org/abs/2503.12779", "authors": ["Haoxiao Wang", "Kaichen Zhou", "Binrui Gu", "Zhiyuan Feng", "Weijie Wang", "Peilin Sun", "Yicheng Xiao", "Jianhua Zhang", "Hao Dong"], "title": "TransDiff: Diffusion-Based Method for Manipulating Transparent Objects Using a Single RGB-D Image", "categories": ["cs.CV"], "comment": "Accepted by ICRA 2025", "summary": "Manipulating transparent objects presents significant challenges due to the\ncomplexities introduced by their reflection and refraction properties, which\nconsiderably hinder the accurate estimation of their 3D shapes. To address\nthese challenges, we propose a single-view RGB-D-based depth completion\nframework, TransDiff, that leverages the Denoising Diffusion Probabilistic\nModels(DDPM) to achieve material-agnostic object grasping in desktop.\nSpecifically, we leverage features extracted from RGB images, including\nsemantic segmentation, edge maps, and normal maps, to condition the depth map\ngeneration process. Our method learns an iterative denoising process that\ntransforms a random depth distribution into a depth map, guided by initially\nrefined depth information, ensuring more accurate depth estimation in scenarios\ninvolving transparent objects. Additionally, we propose a novel training method\nto better align the noisy depth and RGB image features, which are used as\nconditions to refine depth estimation step by step. Finally, we utilized an\nimproved inference process to accelerate the denoising procedure. Through\ncomprehensive experimental validation, we demonstrate that our method\nsignificantly outperforms the baselines in both synthetic and real-world\nbenchmarks with acceptable inference time. The demo of our method can be found\non https://wang-haoxiao.github.io/TransDiff/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12781", "pdf": "https://arxiv.org/pdf/2503.12781", "abs": "https://arxiv.org/abs/2503.12781", "authors": ["Zhang Jiaxing", "Tang Hao"], "title": "SAM2 for Image and Video Segmentation: A Comprehensive Survey", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 4 figures, 7 Tables", "summary": "Despite significant advances in deep learning for image and video\nsegmentation, existing models continue to face challenges in cross-domain\nadaptability and generalization. Image and video segmentation are fundamental\ntasks in computer vision with wide-ranging applications in healthcare,\nagriculture, industrial inspection, and autonomous driving. With the advent of\nlarge-scale foundation models, SAM2 - an improved version of SAM (Segment\nAnything Model)has been optimized for segmentation tasks, demonstrating\nenhanced performance in complex scenarios. However, SAM2's adaptability and\nlimitations in specific domains require further investigation. This paper\nsystematically analyzes the application of SAM2 in image and video segmentation\nand evaluates its performance in various fields. We begin by introducing the\nfoundational concepts of image segmentation, categorizing foundation models,\nand exploring the technical characteristics of SAM and SAM2. Subsequently, we\ndelve into SAM2's applications in static image and video segmentation,\nemphasizing its performance in specialized areas such as medical imaging and\nthe challenges of cross-domain adaptability. As part of our research, we\nreviewed over 200 related papers to provide a comprehensive analysis of the\ntopic. Finally, the paper highlights the strengths and weaknesses of SAM2 in\nsegmentation tasks, identifies the technical challenges it faces, and proposes\nfuture development directions. This review provides valuable insights and\npractical recommendations for optimizing and applying SAM2 in real-world\nscenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12783", "pdf": "https://arxiv.org/pdf/2503.12783", "abs": "https://arxiv.org/abs/2503.12783", "authors": ["Jianan Li", "Huan Chen", "Wangcai Zhao", "Rui Chen", "Tingfa Xu"], "title": "Mixed-granularity Implicit Representation for Continuous Hyperspectral Compressive Reconstruction", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by TNNLS", "summary": "Hyperspectral Images (HSIs) are crucial across numerous fields but are\nhindered by the long acquisition times associated with traditional\nspectrometers. The Coded Aperture Snapshot Spectral Imaging (CASSI) system\nmitigates this issue through a compression technique that accelerates the\nacquisition process. However, reconstructing HSIs from compressed data presents\nchallenges due to fixed spatial and spectral resolution constraints. This study\nintroduces a novel method using implicit neural representation for continuous\nhyperspectral image reconstruction. We propose the Mixed Granularity Implicit\nRepresentation (MGIR) framework, which includes a Hierarchical Spectral-Spatial\nImplicit Encoder for efficient multi-scale implicit feature extraction. This is\ncomplemented by a Mixed-Granularity Local Feature Aggregator that adaptively\nintegrates local features across scales, combined with a decoder that merges\ncoordinate information for precise reconstruction. By leveraging implicit\nneural representations, the MGIR framework enables reconstruction at any\ndesired spatial-spectral resolution, significantly enhancing the flexibility\nand adaptability of the CASSI system. Extensive experimental evaluations\nconfirm that our model produces reconstructed images at arbitrary resolutions\nand matches state-of-the-art methods across varying spectral-spatial\ncompression ratios. The code will be released at https://github.com/chh11/MGIR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12820", "pdf": "https://arxiv.org/pdf/2503.12820", "abs": "https://arxiv.org/abs/2503.12820", "authors": ["Kailin Li", "Zhenxin Li", "Shiyi Lan", "Yuan Xie", "Zhizhong Zhang", "Jiayi Liu", "Zuxuan Wu", "Zhiding Yu", "Jose M. Alvarez"], "title": "Hydra-MDP++: Advancing End-to-End Driving via Expert-Guided Hydra-Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Hydra-MDP++ introduces a novel teacher-student knowledge distillation\nframework with a multi-head decoder that learns from human demonstrations and\nrule-based experts. Using a lightweight ResNet-34 network without complex\ncomponents, the framework incorporates expanded evaluation metrics, including\ntraffic light compliance (TL), lane-keeping ability (LK), and extended comfort\n(EC) to address unsafe behaviors not captured by traditional NAVSIM-derived\nteachers. Like other end-to-end autonomous driving approaches, \\hydra processes\nraw images directly without relying on privileged perception signals.\nHydra-MDP++ achieves state-of-the-art performance by integrating these\ncomponents with a 91.0% drive score on NAVSIM through scaling to a V2-99 image\nencoder, demonstrating its effectiveness in handling diverse driving scenarios\nwhile maintaining computational efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12853", "pdf": "https://arxiv.org/pdf/2503.12853", "abs": "https://arxiv.org/abs/2503.12853", "authors": ["Yanlin Xiang", "Qingyuan He", "Ting Xu", "Ran Hao", "Jiacheng Hu", "Hanchao Zhang"], "title": "Adaptive Transformer Attention and Multi-Scale Fusion for Spine 3D Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This study proposes a 3D semantic segmentation method for the spine based on\nthe improved SwinUNETR to improve segmentation accuracy and robustness. Aiming\nat the complex anatomical structure of spinal images, this paper introduces a\nmulti-scale fusion mechanism to enhance the feature extraction capability by\nusing information of different scales, thereby improving the recognition\naccuracy of the model for the target area. In addition, the introduction of the\nadaptive attention mechanism enables the model to dynamically adjust the\nattention to the key area, thereby optimizing the boundary segmentation effect.\nThe experimental results show that compared with 3D CNN, 3D U-Net, and 3D U-Net\n+ Transformer, the model of this study has achieved significant improvements in\nmIoU, mDice, and mAcc indicators, and has better segmentation performance. The\nablation experiment further verifies the effectiveness of the proposed improved\nmethod, proving that multi-scale fusion and adaptive attention mechanism have a\npositive effect on the segmentation task. Through the visualization analysis of\nthe inference results, the model can better restore the real anatomical\nstructure of the spinal image. Future research can further optimize the\nTransformer structure and expand the data scale to improve the generalization\nability of the model. This study provides an efficient solution for the task of\nmedical image segmentation, which is of great significance to intelligent\nmedical image analysis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12855", "pdf": "https://arxiv.org/pdf/2503.12855", "abs": "https://arxiv.org/abs/2503.12855", "authors": ["Yujie Lu", "Yale Song", "William Wang", "Lorenzo Torresani", "Tushar Nagarajan"], "title": "VITED: Video Temporal Evidence Distillation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We investigate complex video question answering via chain-of-evidence\nreasoning -- identifying sequences of temporal spans from multiple relevant\nparts of the video, together with visual evidence within them. Existing models\nstruggle with multi-step reasoning as they uniformly sample a fixed number of\nframes, which can miss critical evidence distributed nonuniformly throughout\nthe video. Moreover, they lack the ability to temporally localize such evidence\nin the broader context of the full video, which is required for answering\ncomplex questions. We propose a framework to enhance existing VideoQA datasets\nwith evidence reasoning chains, automatically constructed by searching for\noptimal intervals of interest in the video with supporting evidence, that\nmaximizes the likelihood of answering a given question. We train our model\n(VITED) to generate these evidence chains directly, enabling it to both\nlocalize evidence windows as well as perform multi-step reasoning across them\nin long-form video content. We show the value of our evidence-distilled models\non a suite of long video QA benchmarks where we outperform state-of-the-art\napproaches that lack evidence reasoning capabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12885", "pdf": "https://arxiv.org/pdf/2503.12885", "abs": "https://arxiv.org/abs/2503.12885", "authors": ["Dewei Zhou", "Mingwei Li", "Zongxin Yang", "Yi Yang"], "title": "DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale Text-to-Image Models", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Image-conditioned generation methods, such as depth- and canny-conditioned\napproaches, have demonstrated remarkable abilities for precise image synthesis.\nHowever, existing models still struggle to accurately control the content of\nmultiple instances (or regions). Even state-of-the-art models like FLUX and\n3DIS face challenges, such as attribute leakage between instances, which limits\nuser control. To address these issues, we introduce DreamRenderer, a\ntraining-free approach built upon the FLUX model. DreamRenderer enables users\nto control the content of each instance via bounding boxes or masks, while\nensuring overall visual harmony. We propose two key innovations: 1) Bridge\nImage Tokens for Hard Text Attribute Binding, which uses replicated image\ntokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely\non text data, bind the correct visual attributes for each instance during Joint\nAttention; 2) Hard Image Attribute Binding applied only to vital layers.\nThrough our analysis of FLUX, we identify the critical layers responsible for\ninstance attribute rendering and apply Hard Image Attribute Binding only in\nthese layers, using soft binding in the others. This approach ensures precise\ncontrol while preserving image quality. Evaluations on the COCO-POS and\nCOCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success\nRatio by 17.7% over FLUX and enhances the performance of layout-to-image models\nlike GLIGEN and 3DIS by up to 26.8%. Project Page:\nhttps://limuloo.github.io/DreamRenderer/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12905", "pdf": "https://arxiv.org/pdf/2503.12905", "abs": "https://arxiv.org/abs/2503.12905", "authors": ["Yuanbin Qian", "Shuhan Ye", "Chong Wang", "Xiaojie Cai", "Jiangbo Qian", "Jiafei Wu"], "title": "UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks", "categories": ["cs.CV", "cs.NE"], "comment": "Accepted by AAAI 2025", "summary": "Video anomaly detection plays a significant role in intelligent surveillance\nsystems. To enhance model's anomaly recognition ability, previous works have\ntypically involved RGB, optical flow, and text features. Recently, dynamic\nvision sensors (DVS) have emerged as a promising technology, which capture\nvisual information as discrete events with a very high dynamic range and\ntemporal resolution. It reduces data redundancy and enhances the capture\ncapacity of moving objects compared to conventional camera. To introduce this\nrich dynamic information into the surveillance field, we created the first DVS\nvideo anomaly detection benchmark, namely UCF-Crime-DVS. To fully utilize this\nnew data modality, a multi-scale spiking fusion network (MSF) is designed based\non spiking neural networks (SNNs). This work explores the potential application\nof dynamic information from event data in video anomaly detection. Our\nexperiments demonstrate the effectiveness of our framework on UCF-Crime-DVS and\nits superior performance compared to other models, establishing a new baseline\nfor SNN-based weakly supervised video anomaly detection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12910", "pdf": "https://arxiv.org/pdf/2503.12910", "abs": "https://arxiv.org/abs/2503.12910", "authors": ["Jingyi Yuan", "Pengyu Jie", "Junyin Zhang", "Ziao Li", "Chenqiang Gao"], "title": "MFP-CLIP: Exploring the Efficacy of Multi-Form Prompts for Zero-Shot Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recently, zero-shot anomaly detection (ZSAD) has emerged as a pivotal\nparadigm for identifying defects in unseen categories without requiring target\nsamples in training phase. However, existing ZSAD methods struggle with the\nboundary of small and complex defects due to insufficient representations. Most\nof them use the single manually designed prompts, failing to work for diverse\nobjects and anomalies. In this paper, we propose MFP-CLIP, a novel prompt-based\nCLIP framework which explores the efficacy of multi-form prompts for zero-shot\nindustrial anomaly detection. We employ an image to text prompting(I2TP)\nmechanism to better represent the object in the image. MFP-CLIP enhances\nperception to multi-scale and complex anomalies by self prompting(SP) and a\nmulti-patch feature aggregation(MPFA) module. To precisely localize defects, we\nintroduce the mask prompting(MP) module to guide model to focus on potential\nanomaly regions. Extensive experiments are conducted on two wildly used\nindustrial anomaly detection benchmarks, MVTecAD and VisA, demonstrating\nMFP-CLIP's superiority in ZSAD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12964", "pdf": "https://arxiv.org/pdf/2503.12964", "abs": "https://arxiv.org/abs/2503.12964", "authors": ["Zeeshan Patel", "Ethan He", "Parth Mannan", "Xiaowei Ren", "Ryan Wolf", "Niket Agarwal", "Jacob Huffman", "Zhuoyao Wang", "Carl Wang", "Jack Chang", "Yan Bai", "Tommy Huang", "Linnan Wang", "Sahil Jain", "Shanmugam Ramasamy", "Joseph Jennings", "Ekaterina Sirazitdinova", "Oleg Sudakov", "Mingyuan Ma", "Bobby Chen", "Forrest Lin", "Hao Wang", "Vasanth Rao Naik Sabavat", "Sriharsha Niverty", "Rong Ou", "Pallab Bhattacharya", "David Page", "Nima Tajbakhsh", "Ashwath Aithal"], "title": "Training Video Foundation Models with NVIDIA NeMo", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Video Foundation Models (VFMs) have recently been used to simulate the real\nworld to train physical AI systems and develop creative visual experiences.\nHowever, there are significant challenges in training large-scale, high quality\nVFMs that can generate high-quality videos. We present a scalable, open-source\nVFM training pipeline with NVIDIA NeMo, providing accelerated video dataset\ncuration, multimodal data loading, and parallelized video diffusion model\ntraining and inference. We also provide a comprehensive performance analysis\nhighlighting best practices for efficient VFM training and inference.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12974", "pdf": "https://arxiv.org/pdf/2503.12974", "abs": "https://arxiv.org/abs/2503.12974", "authors": ["Xueying Jiang", "Wenhao Li", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "Exploring 3D Activity Reasoning and Planning: From Implicit Human Intentions to Route-Aware Planning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "3D activity reasoning and planning has attracted increasing attention in\nhuman-robot interaction and embodied AI thanks to the recent advance in\nmultimodal learning. However, most existing works share two constraints: 1)\nheavy reliance on explicit instructions with little reasoning on implicit user\nintention; 2) negligence of inter-step route planning on robot moves. To bridge\nthe gaps, we propose 3D activity reasoning and planning, a novel 3D task that\nreasons the intended activities from implicit instructions and decomposes them\ninto steps with inter-step routes and planning under the guidance of\nfine-grained 3D object shapes and locations from scene segmentation. We tackle\nthe new 3D task from two perspectives. First, we construct ReasonPlan3D, a\nlarge-scale benchmark that covers diverse 3D scenes with rich implicit\ninstructions and detailed annotations for multi-step task planning, inter-step\nroute planning, and fine-grained segmentation. Second, we design a novel\nframework that introduces progressive plan generation with contextual\nconsistency across multiple steps, as well as a scene graph that is updated\ndynamically for capturing critical objects and their spatial relations.\nExtensive experiments demonstrate the effectiveness of our benchmark and\nframework in reasoning activities from implicit human instructions, producing\naccurate stepwise task plans, and seamlessly integrating route planning for\nmulti-step moves. The dataset and code will be released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency", "fine-grained"], "score": 4}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13004", "pdf": "https://arxiv.org/pdf/2503.13004", "abs": "https://arxiv.org/abs/2503.13004", "authors": ["Jiaxu Liu", "Li Li", "Hubert P. H. Shum", "Toby P. Breckon"], "title": "TFDM: Time-Variant Frequency-Based Point Cloud Diffusion with Mamba", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models currently demonstrate impressive performance over various\ngenerative tasks. Recent work on image diffusion highlights the strong\ncapabilities of Mamba (state space models) due to its efficient handling of\nlong-range dependencies and sequential data modeling. Unfortunately, joint\nconsideration of state space models with 3D point cloud generation remains\nlimited. To harness the powerful capabilities of the Mamba model for 3D point\ncloud generation, we propose a novel diffusion framework containing dual latent\nMamba block (DM-Block) and a time-variant frequency encoder (TF-Encoder). The\nDM-Block apply a space-filling curve to reorder points into sequences suitable\nfor Mamba state-space modeling, while operating in a latent space to mitigate\nthe computational overhead that arises from direct 3D data processing.\nMeanwhile, the TF-Encoder takes advantage of the ability of the diffusion model\nto refine fine details in later recovery stages by prioritizing key points\nwithin the U-Net architecture. This frequency-based mechanism ensures enhanced\ndetail quality in the final stages of generation. Experimental results on the\nShapeNet-v2 dataset demonstrate that our method achieves state-of-the-art\nperformance (ShapeNet-v2: 0.14\\% on 1-NNA-Abs50 EMD and 57.90\\% on COV EMD) on\ncertain metrics for specific categories while reducing computational parameters\nand inference time by up to 10$\\times$ and 9$\\times$, respectively. Source code\nis available in Supplementary Materials and will be released upon accpetance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13073", "pdf": "https://arxiv.org/pdf/2503.13073", "abs": "https://arxiv.org/abs/2503.13073", "authors": ["Zhicheng Zhao", "Jinquan Yan", "Chenglong Li", "Xiao Wang", "Jin Tang"], "title": "DehazeMamba: SAR-guided Optical Remote Sensing Image Dehazing with Adaptive State Space Model", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Optical remote sensing image dehazing presents significant challenges due to\nits extensive spatial scale and highly non-uniform haze distribution, which\ntraditional single-image dehazing methods struggle to address effectively.\nWhile Synthetic Aperture Radar (SAR) imagery offers inherently haze-free\nreference information for large-scale scenes, existing SAR-guided dehazing\napproaches face two critical limitations: the integration of SAR information\noften diminishes the quality of haze-free regions, and the instability of\nfeature quality further exacerbates cross-modal domain shift. To overcome these\nchallenges, we introduce DehazeMamba, a novel SAR-guided dehazing network built\non a progressive haze decoupling fusion strategy. Our approach incorporates two\nkey innovations: a Haze Perception and Decoupling Module (HPDM) that\ndynamically identifies haze-affected regions through optical-SAR difference\nanalysis, and a Progressive Fusion Module (PFM) that mitigates domain shift\nthrough a two-stage fusion process based on feature quality assessment. To\nfacilitate research in this domain, we present MRSHaze, a large-scale benchmark\ndataset comprising 8,000 pairs of temporally synchronized, precisely\ngeo-registered SAR-optical images with high resolution and diverse haze\nconditions. Extensive experiments demonstrate that DehazeMamba significantly\noutperforms state-of-the-art methods, achieving a 0.73 dB improvement in PSNR\nand substantial enhancements in downstream tasks such as semantic segmentation.\nThe dataset is available at\nhttps://github.com/mmic-lcl/Datasets-and-benchmark-code.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13111", "pdf": "https://arxiv.org/pdf/2503.13111", "abs": "https://arxiv.org/abs/2503.13111", "authors": ["Erik Daxberger", "Nina Wenzel", "David Griffiths", "Haiming Gang", "Justin Lazarow", "Gefen Kohavi", "Kai Kang", "Marcin Eichner", "Yinfei Yang", "Afshin Dehghan", "Peter Grasch"], "title": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models. We will publish our SFT dataset\nand benchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13139", "pdf": "https://arxiv.org/pdf/2503.13139", "abs": "https://arxiv.org/abs/2503.13139", "authors": ["Weiyu Guo", "Ziyang Chen", "Shaoguang Wang", "Jianxiang He", "Yijie Xu", "Jinhui Ye", "Ying Sun", "Hui Xiong"], "title": "Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL", "eess.IV"], "comment": "18 pages, under review", "summary": "Understanding long video content is a complex endeavor that often relies on\ndensely sampled frame captions or end-to-end feature selectors, yet these\ntechniques commonly overlook the logical relationships between textual queries\nand visual elements. In practice, computational constraints necessitate coarse\nframe subsampling, a challenge analogous to ``finding a needle in a haystack.''\nTo address this issue, we introduce a semantics-driven search framework that\nreformulates keyframe selection under the paradigm of Visual Semantic-Logical\nSearch. Specifically, we systematically define four fundamental logical\ndependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute\ndependency, and 4) causal order. These relations dynamically update frame\nsampling distributions through an iterative refinement process, enabling\ncontext-aware identification of semantically critical frames tailored to\nspecific query requirements. Our method establishes new SOTA performance on the\nmanually annotated benchmark in key-frame selection metrics. Furthermore, when\napplied to downstream video question-answering tasks, the proposed approach\ndemonstrates the best performance gains over existing methods on LongVideoBench\nand Video-MME, validating its effectiveness in bridging the logical gap between\ntextual queries and visual-temporal reasoning. The code will be publicly\navailable.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13160", "pdf": "https://arxiv.org/pdf/2503.13160", "abs": "https://arxiv.org/abs/2503.13160", "authors": ["Zihao Liu", "Xiaoyu Wu", "Jianqin Wu", "Xuxu Wang", "Linlin Yang"], "title": "Language-guided Open-world Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Video anomaly detection models aim to detect anomalies that deviate from what\nis expected. In open-world scenarios, the expected events may change as\nrequirements change. For example, not wearing a mask is considered abnormal\nduring a flu outbreak but normal otherwise. However, existing methods assume\nthat the definition of anomalies is invariable, and thus are not applicable to\nthe open world. To address this, we propose a novel open-world VAD paradigm\nwith variable definitions, allowing guided detection through user-provided\nnatural language at inference time. This paradigm necessitates establishing a\nrobust mapping from video and textual definition to anomaly score. Therefore,\nwe propose LaGoVAD (Language-guided Open-world VAD), a model that dynamically\nadapts anomaly definitions through two regularization strategies: diversifying\nthe relative durations of anomalies via dynamic video synthesis, and enhancing\nfeature robustness through contrastive learning with negative mining. Training\nsuch adaptable models requires diverse anomaly definitions, but existing\ndatasets typically provide given labels without semantic descriptions. To\nbridge this gap, we collect PreVAD (Pre-training Video Anomaly Dataset), the\nlargest and most diverse video anomaly dataset to date, featuring 35,279\nannotated videos with multi-level category labels and descriptions that\nexplicitly define anomalies. Zero-shot experiments on seven datasets\ndemonstrate SOTA performance. Data and code will be released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13179", "pdf": "https://arxiv.org/pdf/2503.13179", "abs": "https://arxiv.org/abs/2503.13179", "authors": ["Yi Zhang", "Wenye Zhou", "Ruonan Lin"], "title": "A super-resolution reconstruction method for lightweight building images based on an expanding feature modulation network", "categories": ["cs.CV"], "comment": null, "summary": "This study proposes a lightweight method for building image super-resolution\nusing a Dilated Contextual Feature Modulation Network (DCFMN). The process\nincludes obtaining high-resolution images, down-sampling them to\nlow-resolution, enhancing the low-resolution images, constructing and training\na lightweight network model, and generating super-resolution outputs. To\naddress challenges such as regular textures and long-range dependencies in\nbuilding images, the DCFMN integrates an expansion separable modulation unit\nand a local feature enhancement module. The former employs multiple expansion\nconvolutions equivalent to a large kernel to efficiently aggregate multi-scale\nfeatures while leveraging a simple attention mechanism for adaptivity. The\nlatter encodes local features, mixes channel information, and ensures no\nadditional computational burden during inference through reparameterization.\nThis approach effectively resolves the limitations of existing lightweight\nsuper-resolution networks in modeling long-range dependencies, achieving\naccurate and efficient global feature modeling without increasing computational\ncosts, and significantly improving both reconstruction quality and lightweight\nefficiency for building image super-resolution models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13203", "pdf": "https://arxiv.org/pdf/2503.13203", "abs": "https://arxiv.org/abs/2503.13203", "authors": ["Corentin Sautier", "Gilles Puy", "Alexandre Boulch", "Renaud Marlet", "Vincent Lepetit"], "title": "Clustering is back: Reaching state-of-the-art LiDAR instance segmentation without training", "categories": ["cs.CV"], "comment": null, "summary": "Panoptic segmentation of LiDAR point clouds is fundamental to outdoor scene\nunderstanding, with autonomous driving being a primary application. While\nstate-of-the-art approaches typically rely on end-to-end deep learning\narchitectures and extensive manual annotations of instances, the significant\ncost and time investment required for labeling large-scale point cloud datasets\nremains a major bottleneck in this field. In this work, we demonstrate that\ncompetitive panoptic segmentation can be achieved using only semantic labels,\nwith instances predicted without any training or annotations. Our method\nachieves performance comparable to current state-of-the-art supervised methods\non standard benchmarks including SemanticKITTI and nuScenes, and outperforms\nevery publicly available method on SemanticKITTI as a drop-in instance head\nreplacement, while running in real-time on a single-threaded CPU and requiring\nno instance labels. Our method is fully explainable, and requires no learning\nor parameter tuning. Code is available at https://github.com/valeoai/Alpine/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13229", "pdf": "https://arxiv.org/pdf/2503.13229", "abs": "https://arxiv.org/abs/2503.13229", "authors": ["Yongkang Cheng", "Shaoli Huang"], "title": "HoloGest: Decoupled Diffusion and Motion Priors for Generating Holisticly Expressive Co-speech Gestures", "categories": ["cs.CV"], "comment": "Accepted by 3DV 2025", "summary": "Animating virtual characters with holistic co-speech gestures is a\nchallenging but critical task. Previous systems have primarily focused on the\nweak correlation between audio and gestures, leading to physically unnatural\noutcomes that degrade the user experience. To address this problem, we\nintroduce HoleGest, a novel neural network framework based on decoupled\ndiffusion and motion priors for the automatic generation of high-quality,\nexpressive co-speech gestures. Our system leverages large-scale human motion\ndatasets to learn a robust prior with low audio dependency and high motion\nreliance, enabling stable global motion and detailed finger movements. To\nimprove the generation efficiency of diffusion-based models, we integrate\nimplicit joint constraints with explicit geometric and conditional constraints,\ncapturing complex motion distributions between large strides. This integration\nsignificantly enhances generation speed while maintaining high-quality motion.\nFurthermore, we design a shared embedding space for gesture-transcription text\nalignment, enabling the generation of semantically correct gesture actions.\nExtensive experiments and user feedback demonstrate the effectiveness and\npotential applications of our model, with our method achieving a level of\nrealism close to the ground truth, providing an immersive user experience. Our\ncode, model, and demo are are available at\nhttps://cyk990422.github.io/HoloGest.github.io/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13272", "pdf": "https://arxiv.org/pdf/2503.13272", "abs": "https://arxiv.org/abs/2503.13272", "authors": ["Katja Schwarz", "Norman Mueller", "Peter Kontschieder"], "title": "Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "Synthesizing consistent and photorealistic 3D scenes is an open problem in\ncomputer vision. Video diffusion models generate impressive videos but cannot\ndirectly synthesize 3D representations, i.e., lack 3D consistency in the\ngenerated sequences. In addition, directly training generative 3D models is\nchallenging due to a lack of 3D training data at scale. In this work, we\npresent Generative Gaussian Splatting (GGS) -- a novel approach that integrates\na 3D representation with a pre-trained latent video diffusion model.\nSpecifically, our model synthesizes a feature field parameterized via 3D\nGaussian primitives. The feature field is then either rendered to feature maps\nand decoded into multi-view images, or directly upsampled into a 3D radiance\nfield. We evaluate our approach on two common benchmark datasets for scene\nsynthesis, RealEstate10K and ScanNet+, and find that our proposed GGS model\nsignificantly improves both the 3D consistency of the generated multi-view\nimages, and the quality of the generated 3D scenes over all relevant baselines.\nCompared to a similar model without 3D representation, GGS improves FID on the\ngenerated 3D scenes by ~20% on both RealEstate10K and ScanNet+. Project page:\nhttps://katjaschwarz.github.io/ggs/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13319", "pdf": "https://arxiv.org/pdf/2503.13319", "abs": "https://arxiv.org/abs/2503.13319", "authors": ["Shitong Shao", "Hongwei Yi", "Hanzhong Guo", "Tian Ye", "Daquan Zhou", "Michael Lingelbach", "Zhiqiang Xu", "Zeke Xie"], "title": "MagicDistillation: Weak-to-Strong Video Distillation for Large-Scale Portrait Few-Step Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Fine-tuning open-source large-scale VDMs for the portrait video synthesis\ntask can result in significant improvements across multiple dimensions, such as\nvisual quality and natural facial motion dynamics. Despite their advancements,\nhow to achieve step distillation and reduce the substantial computational\noverhead of large-scale VDMs remains unexplored. To fill this gap, this paper\nproposes Weak-to-Strong Video Distillation (W2SVD) to mitigate both the issue\nof insufficient training memory and the problem of training collapse observed\nin vanilla DMD during the training process. Specifically, we first leverage\nLoRA to fine-tune the fake diffusion transformer (DiT) to address the\nout-of-memory issue. Then, we employ the W2S distribution matching to adjust\nthe real DiT's parameter, subtly shifting it toward the fake DiT's parameter.\nThis adjustment is achieved by utilizing the weak weight of the low-rank\nbranch, effectively alleviate the conundrum where the video synthesized by the\nfew-step generator deviates from the real data distribution, leading to\ninaccuracies in the KL divergence approximation. Additionally, we minimize the\ndistance between the fake data distribution and the ground truth distribution\nto further enhance the visual quality of the synthesized videos. As\nexperimentally demonstrated on HunyuanVideo, W2SVD surpasses the standard\nEuler, LCM, DMD and even the 28-step standard sampling in FID/FVD and VBench in\n1/4-step video synthesis. The project page is in\nhttps://w2svd.github.io/W2SVD/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13360", "pdf": "https://arxiv.org/pdf/2503.13360", "abs": "https://arxiv.org/abs/2503.13360", "authors": ["Hai-Long Sun", "Zhun Sun", "Houwen Peng", "Han-Jia Ye"], "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "The project page is available at\n  https://sun-hailong.github.io/projects/TVC", "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13385", "pdf": "https://arxiv.org/pdf/2503.13385", "abs": "https://arxiv.org/abs/2503.13385", "authors": ["Qing Zhou", "Junyu Gao", "Qi Wang"], "title": "Scale Efficient Training for Large Datasets", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by CVPR2025", "summary": "The rapid growth of dataset scales has been a key driver in advancing deep\nlearning research. However, as dataset scale increases, the training process\nbecomes increasingly inefficient due to the presence of low-value samples,\nincluding excessive redundant samples, overly challenging samples, and\ninefficient easy samples that contribute little to model improvement.To address\nthis challenge, we propose Scale Efficient Training (SeTa) for large datasets,\na dynamic sample pruning approach that losslessly reduces training time. To\nremove low-value samples, SeTa first performs random pruning to eliminate\nredundant samples, then clusters the remaining samples according to their\nlearning difficulty measured by loss. Building upon this clustering, a sliding\nwindow strategy is employed to progressively remove both overly challenging and\ninefficient easy clusters following an easy-to-hard curriculum.We conduct\nextensive experiments on large-scale synthetic datasets, including ToCa, SS1M,\nand ST+MJ, each containing over 3 million samples.SeTa reduces training costs\nby up to 50\\% while maintaining or improving performance, with minimal\ndegradation even at 70\\% cost reduction. Furthermore, experiments on various\nscale real datasets across various backbones (CNNs, Transformers, and Mambas)\nand diverse tasks (instruction tuning, multi-view stereo, geo-localization,\ncomposed image retrieval, referring image segmentation) demonstrate the\npowerful effectiveness and universality of our approach. Code is available at\nhttps://github.com/mrazhou/SeTa.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13433", "pdf": "https://arxiv.org/pdf/2503.13433", "abs": "https://arxiv.org/abs/2503.13433", "authors": ["Johan Edstedt"], "title": "Less Biased Noise Scale Estimation for Threshold-Robust RANSAC", "categories": ["cs.CV"], "comment": null, "summary": "The gold-standard for robustly estimating relative pose through image\nmatching is RANSAC. While RANSAC is powerful, it requires setting the inlier\nthreshold that determines whether the error of a correspondence under an\nestimated model is sufficiently small to be included in its consensus set.\nSetting this threshold is typically done by hand, and is difficult to tune\nwithout a access to ground truth data. Thus, a method capable of automatically\ndetermining the optimal threshold would be desirable. In this paper we revisit\ninlier noise scale estimation, which is an attractive approach as the inlier\nnoise scale is linear to the optimal threshold. We revisit the noise scale\nestimation method SIMFIT and find bias in the estimate of the noise scale. In\nparticular, we fix underestimates from using the same data for fitting the\nmodel as estimating the inlier noise, and from not taking the threshold itself\ninto account. Secondly, since the optimal threshold within a scene is\napproximately constant we propose a multi-pair extension of SIMFIT++, by\nfiltering of estimates, which improves results. Our approach yields robust\nperformance across a range of thresholds, shown in Figure 1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13434", "pdf": "https://arxiv.org/pdf/2503.13434", "abs": "https://arxiv.org/abs/2503.13434", "authors": ["Yaowei Li", "Lingen Li", "Zhaoyang Zhang", "Xiaoyu Li", "Guangzhi Wang", "Hongxiang Li", "Xiaodong Cun", "Ying Shan", "Yuexian Zou"], "title": "BlobCtrl: A Unified and Flexible Framework for Element-level Image Generation and Editing", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Project Webpage: https://liyaowei-stu.github.io/project/BlobCtrl/", "summary": "Element-level visual manipulation is essential in digital content creation,\nbut current diffusion-based methods lack the precision and flexibility of\ntraditional tools. In this work, we introduce BlobCtrl, a framework that\nunifies element-level generation and editing using a probabilistic blob-based\nrepresentation. By employing blobs as visual primitives, our approach\neffectively decouples and represents spatial location, semantic content, and\nidentity information, enabling precise element-level manipulation. Our key\ncontributions include: 1) a dual-branch diffusion architecture with\nhierarchical feature fusion for seamless foreground-background integration; 2)\na self-supervised training paradigm with tailored data augmentation and score\nfunctions; and 3) controllable dropout strategies to balance fidelity and\ndiversity. To support further research, we introduce BlobData for large-scale\ntraining and BlobBench for systematic evaluation. Experiments show that\nBlobCtrl excels in various element-level manipulation tasks while maintaining\ncomputational efficiency, offering a practical solution for precise and\nflexible visual content creation. Project page:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11851", "pdf": "https://arxiv.org/pdf/2503.11851", "abs": "https://arxiv.org/abs/2503.11851", "authors": ["Jutika Borah", "Hidam Kumarjit Singh"], "title": "DCAT: Dual Cross-Attention Fusion for Disease Classification in Radiological Images with Uncertainty Estimation", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "18 pages, 8 figures, 5 tables", "summary": "Accurate and reliable image classification is crucial in radiology, where\ndiagnostic decisions significantly impact patient outcomes. Conventional deep\nlearning models tend to produce overconfident predictions despite underlying\nuncertainties, potentially leading to misdiagnoses. Attention mechanisms have\nemerged as powerful tools in deep learning, enabling models to focus on\nrelevant parts of the input data. Combined with feature fusion, they can be\neffective in addressing uncertainty challenges. Cross-attention has become\nincreasingly important in medical image analysis for capturing dependencies\nacross features and modalities. This paper proposes a novel dual\ncross-attention fusion model for medical image analysis by addressing key\nchallenges in feature integration and interpretability. Our approach introduces\na bidirectional cross-attention mechanism with refined channel and spatial\nattention that dynamically fuses feature maps from EfficientNetB4 and ResNet34\nleveraging multi-network contextual dependencies. The refined features through\nchannel and spatial attention highlights discriminative patterns crucial for\naccurate classification. The proposed model achieved AUC of 99.75%, 100%,\n99.93% and 98.69% and AUPR of 99.81%, 100%, 99.97%, and 96.36% on Covid-19,\nTuberculosis, Pneumonia Chest X-ray images and Retinal OCT images respectively.\nThe entropy values and several high uncertain samples give an interpretable\nvisualization from the model enhancing transparency. By combining multi-scale\nfeature extraction, bidirectional attention and uncertainty estimation, our\nproposed model strongly impacts medical image analysis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11978", "pdf": "https://arxiv.org/pdf/2503.11978", "abs": "https://arxiv.org/abs/2503.11978", "authors": ["Eric M. Chen", "Di Liu", "Sizhuo Ma", "Michael Vasilkovsky", "Bing Zhou", "Qiang Gao", "Wenzhou Wang", "Jiahao Luo", "Dimitris N. Metaxas", "Vincent Sitzmann", "Jian Wang"], "title": "Snapmoji: Instant Generation of Animatable Dual-Stylized Avatars", "categories": ["cs.GR", "cs.CV"], "comment": "N/A", "summary": "The increasing popularity of personalized avatar systems, such as Snapchat\nBitmojis and Apple Memojis, highlights the growing demand for digital\nself-representation. Despite their widespread use, existing avatar platforms\nface significant limitations, including restricted expressivity due to\npredefined assets, tedious customization processes, or inefficient rendering\nrequirements. Addressing these shortcomings, we introduce Snapmoji, an avatar\ngeneration system that instantly creates animatable, dual-stylized avatars from\na selfie. We propose Gaussian Domain Adaptation (GDA), which is pre-trained on\nlarge-scale Gaussian models using 3D data from sources such as Objaverse and\nfine-tuned with 2D style transfer tasks, endowing it with a rich 3D prior. This\nenables Snapmoji to transform a selfie into a primary stylized avatar, like the\nBitmoji style, and apply a secondary style, such as Plastic Toy or Alien, all\nwhile preserving the user's identity and the primary style's integrity. Our\nsystem is capable of producing 3D Gaussian avatars that support dynamic\nanimation, including accurate facial expression transfer. Designed for\nefficiency, Snapmoji achieves selfie-to-avatar conversion in just 0.9 seconds\nand supports real-time interactions on mobile devices at 30 to 40 frames per\nsecond. Extensive testing confirms that Snapmoji outperforms existing methods\nin versatility and speed, making it a convenient tool for automatic avatar\ncreation in various styles.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12042", "pdf": "https://arxiv.org/pdf/2503.12042", "abs": "https://arxiv.org/abs/2503.12042", "authors": ["Zhedong Zhang", "Liang Li", "Chenggang Yan", "Chunshan Liu", "Anton van den Hengel", "Yuankai Qi"], "title": "Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody Adapting for Movie Dubbing", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "Accepted by CVPR2025", "summary": "Movie dubbing describes the process of transforming a script into speech that\naligns temporally and emotionally with a given movie clip while exemplifying\nthe speaker's voice demonstrated in a short reference audio clip. This task\ndemands the model bridge character performances and complicated prosody\nstructures to build a high-quality video-synchronized dubbing track. The\nlimited scale of movie dubbing datasets, along with the background noise\ninherent in audio data, hinder the acoustic modeling performance of trained\nmodels. To address these issues, we propose an acoustic-prosody disentangled\ntwo-stage method to achieve high-quality dubbing generation with precise\nprosody alignment. First, we propose a prosody-enhanced acoustic pre-training\nto develop robust acoustic modeling capabilities. Then, we freeze the\npre-trained acoustic system and design a disentangled framework to model\nprosodic text features and dubbing style while maintaining acoustic quality.\nAdditionally, we incorporate an in-domain emotion analysis module to reduce the\nimpact of visual domain shifts across different movies, thereby enhancing\nemotion-prosody alignment. Extensive experiments show that our method performs\nfavorably against the state-of-the-art models on two primary benchmarks. The\ndemos are available at https://zzdoog.github.io/ProDubber/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12466", "pdf": "https://arxiv.org/pdf/2503.12466", "abs": "https://arxiv.org/abs/2503.12466", "authors": ["Jiahang Cao", "Qiang Zhang", "Hanzhong Guo", "Jiaxu Wang", "Hao Cheng", "Renjing Xu"], "title": "Modality-Composable Diffusion Policy via Inference-Time Distribution-level Composition", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to ICLR 2025 Generative Models for Robot Learning Workshop", "summary": "Diffusion Policy (DP) has attracted significant attention as an effective\nmethod for policy representation due to its capacity to model\nmulti-distribution dynamics. However, current DPs are often based on a single\nvisual modality (e.g., RGB or point cloud), limiting their accuracy and\ngeneralization potential. Although training a generalized DP capable of\nhandling heterogeneous multimodal data would enhance performance, it entails\nsubstantial computational and data-related costs. To address these challenges,\nwe propose a novel policy composition method: by leveraging multiple\npre-trained DPs based on individual visual modalities, we can combine their\ndistributional scores to form a more expressive Modality-Composable Diffusion\nPolicy (MCDP), without the need for additional training. Through extensive\nempirical experiments on the RoboTwin dataset, we demonstrate the potential of\nMCDP to improve both adaptability and performance. This exploration aims to\nprovide valuable insights into the flexible composition of existing DPs,\nfacilitating the development of generalizable cross-modality, cross-domain, and\neven cross-embodiment policies. Our code is open-sourced at\nhttps://github.com/AndyCao1125/MCDP.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13051", "pdf": "https://arxiv.org/pdf/2503.13051", "abs": "https://arxiv.org/abs/2503.13051", "authors": ["Kai Uwe Barthel", "Florian Barthel", "Peter Eisert"], "title": "Permutation Learning with Only N Parameters: From SoftSort to Self-Organizing Gaussians", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "Sorting and permutation learning are key concepts in optimization and machine\nlearning, especially when organizing high-dimensional data into meaningful\nspatial layouts. The Gumbel-Sinkhorn method, while effective, requires N*N\nparameters to determine a full permutation matrix, making it computationally\nexpensive for large datasets. Low-rank matrix factorization approximations\nreduce memory requirements to 2MN (with M << N), but they still struggle with\nvery large problems. SoftSort, by providing a continuous relaxation of the\nargsort operator, allows differentiable 1D sorting, but it faces challenges\nwith multidimensional data and complex permutations. In this paper, we present\na novel method for learning permutations using only N parameters, which\ndramatically reduces storage costs. Our approach builds on SoftSort, but\nextends it by iteratively shuffling the N indices of the elements to be sorted\nthrough a separable learning process. This modification significantly improves\nsorting quality, especially for multidimensional data and complex optimization\ncriteria, and outperforms pure SoftSort. Our method offers improved memory\nefficiency and scalability compared to existing approaches, while maintaining\nhigh-quality permutation learning. Its dramatically reduced memory requirements\nmake it particularly well-suited for large-scale optimization tasks, such as\n\"Self-Organizing Gaussians\", where efficient and scalable permutation learning\nis critical.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13082", "pdf": "https://arxiv.org/pdf/2503.13082", "abs": "https://arxiv.org/abs/2503.13082", "authors": ["Runyu Jiao", "Alice Fasoli", "Francesco Giuliari", "Matteo Bortolon", "Sergio Povoli", "Guofeng Mei", "Yiming Wang", "Fabio Poiesi"], "title": "Free-form language-based robotic reasoning and grasping", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project website: https://tev-fbk.github.io/FreeGrasp/", "summary": "Performing robotic grasping from a cluttered bin based on human instructions\nis a challenging task, as it requires understanding both the nuances of\nfree-form language and the spatial relationships between objects.\nVision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have\ndemonstrated remarkable reasoning capabilities across both text and images. But\ncan they truly be used for this task in a zero-shot setting? And what are their\nlimitations? In this paper, we explore these research questions via the\nfree-form language-based robotic grasping task, and propose a novel method,\nFreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about\nhuman instructions and object spatial arrangements. Our method detects all\nobjects as keypoints and uses these keypoints to annotate marks on images,\naiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our\nmethod to determine whether a requested object is directly graspable or if\nother objects must be grasped and removed first. Since no existing dataset is\nspecifically designed for this task, we introduce a synthetic dataset\nFreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated\ninstructions and ground-truth grasping sequences. We conduct extensive analyses\nwith both FreeGraspData and real-world validation with a gripper-equipped\nrobotic arm, demonstrating state-of-the-art performance in grasp reasoning and\nexecution. Project website: https://tev-fbk.github.io/FreeGrasp/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13205", "pdf": "https://arxiv.org/pdf/2503.13205", "abs": "https://arxiv.org/abs/2503.13205", "authors": ["Zhen Chen", "Zhihao Peng", "Xusheng Liang", "Cheng Wang", "Peigan Liang", "Linsheng Zeng", "Minjie Ju", "Yixuan Yuan"], "title": "MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for Inpatient Pathways", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.MA"], "comment": null, "summary": "Inpatient pathways demand complex clinical decision-making based on\ncomprehensive patient information, posing critical challenges for clinicians.\nDespite advancements in large language models (LLMs) in medical applications,\nlimited research focused on artificial intelligence (AI) inpatient pathways\nsystems, due to the lack of large-scale inpatient datasets. Moreover, existing\nmedical benchmarks typically concentrated on medical question-answering and\nexaminations, ignoring the multifaceted nature of clinical decision-making in\ninpatient settings. To address these gaps, we first developed the Inpatient\nPathway Decision Support (IPDS) benchmark from the MIMIC-IV database,\nencompassing 51,274 cases across nine triage departments and 17 major disease\ncategories alongside 16 standardized treatment options. Then, we proposed the\nMulti-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways\nwith three clinical agents, including a triage agent managing the patient\nadmission, a diagnosis agent serving as the primary decision maker at the\ndepartment, and a treatment agent providing treatment plans. Additionally, our\nMAP framework includes a chief agent overseeing the inpatient pathways to guide\nand promote these three clinician agents. Extensive experiments showed our MAP\nimproved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM\nHuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant\nclinical compliance, outperforming three board-certified clinicians by 10%-12%,\nestablishing a foundation for inpatient pathways systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13441", "pdf": "https://arxiv.org/pdf/2503.13441", "abs": "https://arxiv.org/abs/2503.13441", "authors": ["Ri-Zhao Qiu", "Shiqi Yang", "Xuxin Cheng", "Chaitanya Chawla", "Jialong Li", "Tairan He", "Ge Yan", "Lars Paulsen", "Ge Yang", "Sha Yi", "Guanya Shi", "Xiaolong Wang"], "title": "Humanoid Policy ~ Human Policy", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Code and data: https://human-as-robot.github.io/", "summary": "Training manipulation policies for humanoid robots with diverse data enhances\ntheir robustness and generalization across tasks and platforms. However,\nlearning solely from robot demonstrations is labor-intensive, requiring\nexpensive tele-operated data collection which is difficult to scale. This paper\ninvestigates a more scalable data source, egocentric human demonstrations, to\nserve as cross-embodiment training data for robot learning. We mitigate the\nembodiment gap between humanoids and humans from both the data and modeling\nperspectives. We collect an egocentric task-oriented dataset (PH2D) that is\ndirectly aligned with humanoid manipulation demonstrations. We then train a\nhuman-humanoid behavior policy, which we term Human Action Transformer (HAT).\nThe state-action space of HAT is unified for both humans and humanoid robots\nand can be differentiably retargeted to robot actions. Co-trained with\nsmaller-scale robot data, HAT directly models humanoid robots and humans as\ndifferent embodiments without additional supervision. We show that human data\nimproves both generalization and robustness of HAT with significantly better\ndata collection efficiency. Code and data: https://human-as-robot.github.io/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13446", "pdf": "https://arxiv.org/pdf/2503.13446", "abs": "https://arxiv.org/abs/2503.13446", "authors": ["Zhenyu Wu", "Yuheng Zhou", "Xiuwei Xu", "Ziwei Wang", "Haibin Yan"], "title": "MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CVPR 2025. Project Page:\n  https://gary3410.github.io/momanipVLA/", "summary": "Mobile manipulation is the fundamental challenge for robotics to assist\nhumans with diverse tasks and environments in everyday life. However,\nconventional mobile manipulation approaches often struggle to generalize across\ndifferent tasks and environments because of the lack of large-scale training.\nIn contrast, recent advances in vision-language-action (VLA) models have shown\nimpressive generalization capabilities, but these foundation models are\ndeveloped for fixed-base manipulation tasks. Therefore, we propose an efficient\npolicy adaptation framework named MoManipVLA to transfer pre-trained VLA models\nof fix-base manipulation to mobile manipulation, so that high generalization\nability across tasks and environments can be achieved in mobile manipulation\npolicy. Specifically, we utilize pre-trained VLA models to generate waypoints\nof the end-effector with high generalization ability. We design motion planning\nobjectives for the mobile base and the robot arm, which aim at maximizing the\nphysical feasibility of the trajectory. Finally, we present an efficient\nbi-level objective optimization framework for trajectory generation, where the\nupper-level optimization predicts waypoints for base movement to enhance the\nmanipulator policy space, and the lower-level optimization selects the optimal\nend-effector trajectory to complete the manipulation task. In this way,\nMoManipVLA can adjust the position of the robot base in a zero-shot manner,\nthus making the waypoints predicted from the fixed-base VLA models feasible.\nExtensive experimental results on OVMM and the real world demonstrate that\nMoManipVLA achieves a 4.2% higher success rate than the state-of-the-art mobile\nmanipulation, and only requires 50 training cost for real world deployment due\nto the strong generalization ability in the pre-trained VLA models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
