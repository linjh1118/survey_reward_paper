{"id": "2503.13531", "pdf": "https://arxiv.org/pdf/2503.13531", "abs": "https://arxiv.org/abs/2503.13531", "authors": ["Jin Kim", "Byunghwee Lee", "Taekho You", "Jinhyuk Yun"], "title": "Context-aware Multimodal AI Reveals Hidden Pathways in Five Centuries of Art Evolution", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "comment": "30 pages, 4 figures. Some example paintings are blurred to avoid\n  potential copyright violations", "summary": "The rise of multimodal generative AI is transforming the intersection of\ntechnology and art, offering deeper insights into large-scale artwork. Although\nits creative capabilities have been widely explored, its potential to represent\nartwork in latent spaces remains underexamined. We use cutting-edge generative\nAI, specifically Stable Diffusion, to analyze 500 years of Western paintings by\nextracting two types of latent information with the model: formal aspects\n(e.g., colors) and contextual aspects (e.g., subject). Our findings reveal that\ncontextual information differentiates between artistic periods, styles, and\nindividual artists more successfully than formal elements. Additionally, using\ncontextual keywords extracted from paintings, we show how artistic expression\nevolves alongside societal changes. Our generative experiment, infusing\nprospective contexts into historical artworks, successfully reproduces the\nevolutionary trajectory of artworks, highlighting the significance of mutual\ninteraction between society and art. This study demonstrates how multimodal AI\nexpands traditional formal analysis by integrating temporal, cultural, and\nhistorical contexts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13587", "pdf": "https://arxiv.org/pdf/2503.13587", "abs": "https://arxiv.org/abs/2503.13587", "authors": ["Dingkang Liang", "Dingyuan Zhang", "Xin Zhou", "Sifan Tu", "Tianrui Feng", "Xiaofan Li", "Yumeng Zhang", "Mingyang Du", "Xiao Tan", "Xiang Bai"], "title": "Seeing the Future, Perceiving the Future: A Unified Driving World Model for Future Generation and Perception", "categories": ["cs.CV"], "comment": "The project page is at https://github.com/dk-liang/UniFuture", "summary": "We present UniFuture, a simple yet effective driving world model that\nseamlessly integrates future scene generation and perception within a single\nframework. Unlike existing models focusing solely on pixel-level future\nprediction or geometric reasoning, our approach jointly models future\nappearance (i.e., RGB image) and geometry (i.e., depth), ensuring coherent\npredictions. Specifically, during the training, we first introduce a\nDual-Latent Sharing scheme, which transfers image and depth sequence in a\nshared latent space, allowing both modalities to benefit from shared feature\nlearning. Additionally, we propose a Multi-scale Latent Interaction mechanism,\nwhich facilitates bidirectional refinement between image and depth features at\nmultiple spatial scales, effectively enhancing geometry consistency and\nperceptual alignment. During testing, our UniFuture can easily predict\nhigh-consistency future image-depth pairs by only using the current image as\ninput. Extensive experiments on the nuScenes dataset demonstrate that UniFuture\noutperforms specialized models on future generation and perception tasks,\nhighlighting the advantages of a unified, structurally-aware world model. The\nproject page is at https://github.com/dk-liang/UniFuture.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13652", "pdf": "https://arxiv.org/pdf/2503.13652", "abs": "https://arxiv.org/abs/2503.13652", "authors": ["Maan Qraitem", "Piotr Teterwak", "Kate Saenko", "Bryan A. Plummer"], "title": "Web Artifact Attacks Disrupt Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) (e.g., CLIP, LLaVA) are trained on large-scale,\nlightly curated web datasets, leading them to learn unintended correlations\nbetween semantic concepts and unrelated visual signals. These associations\ndegrade model accuracy by causing predictions to rely on incidental patterns\nrather than genuine visual understanding. Prior work has weaponized these\ncorrelations as an attack vector to manipulate model predictions, such as\ninserting a deceiving class text onto the image in a typographic attack. These\nattacks succeed due to VLMs' text-heavy bias-a result of captions that echo\nvisible words rather than describing content. However, this attack has focused\nsolely on text that matches the target class exactly, overlooking a broader\nrange of correlations, including non-matching text and graphical symbols, which\narise from the abundance of branding content in web-scale data. To address this\ngap, we introduce artifact-based attacks: a novel class of manipulations that\nmislead models using both non-matching text and graphical elements. Unlike\ntypographic attacks, these artifacts are not predefined, making them harder to\ndefend against but also more challenging to find. We address this by framing\nartifact attacks as a search problem and demonstrate their effectiveness across\nfive datasets, with some artifacts reinforcing each other to reach 100% attack\nsuccess rates. These attacks transfer across models with up to 90%\neffectiveness, making it possible to attack unseen models. To defend against\nthese attacks, we extend prior work's artifact aware prompting to the graphical\nsetting. We see a moderate reduction of success rates of up to 15% relative to\nstandard prompts, suggesting a promising direction for enhancing model\nrobustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13730", "pdf": "https://arxiv.org/pdf/2503.13730", "abs": "https://arxiv.org/abs/2503.13730", "authors": ["Forouzan Fallah", "Maitreya Patel", "Agneet Chatterjee", "Vlad I. Morariu", "Chitta Baral", "Yezhou Yang"], "title": "TextInVision: Text and Prompt Complexity Driven Visual Text Generation Benchmark", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Generating images with embedded text is crucial for the automatic production\nof visual and multimodal documents, such as educational materials and\nadvertisements. However, existing diffusion-based text-to-image models often\nstruggle to accurately embed text within images, facing challenges in spelling\naccuracy, contextual relevance, and visual coherence. Evaluating the ability of\nsuch models to embed text within a generated image is complicated due to the\nlack of comprehensive benchmarks. In this work, we introduce TextInVision, a\nlarge-scale, text and prompt complexity driven benchmark designed to evaluate\nthe ability of diffusion models to effectively integrate visual text into\nimages. We crafted a diverse set of prompts and texts that consider various\nattributes and text characteristics. Additionally, we prepared an image dataset\nto test Variational Autoencoder (VAE) models across different character\nrepresentations, highlighting that VAE architectures can also pose challenges\nin text generation within diffusion frameworks. Through extensive analysis of\nmultiple models, we identify common errors and highlight issues such as\nspelling inaccuracies and contextual mismatches. By pinpointing the failure\npoints across different prompts and texts, our research lays the foundation for\nfuture advancements in AI-generated multimodal content.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13740", "pdf": "https://arxiv.org/pdf/2503.13740", "abs": "https://arxiv.org/abs/2503.13740", "authors": ["Yuxuan Jiang", "Chengxi Zeng", "Siyue Teng", "Fan Zhang", "Xiaoqing Zhu", "Joel Sole", "David Bull"], "title": "C2D-ISR: Optimizing Attention-based Image Super-resolution from Continuous to Discrete Scales", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, attention mechanisms have been exploited in single image\nsuper-resolution (SISR), achieving impressive reconstruction results. However,\nthese advancements are still limited by the reliance on simple training\nstrategies and network architectures designed for discrete up-sampling scales,\nwhich hinder the model's ability to effectively capture information across\nmultiple scales. To address these limitations, we propose a novel framework,\n\\textbf{C2D-ISR}, for optimizing attention-based image super-resolution models\nfrom both performance and complexity perspectives. Our approach is based on a\ntwo-stage training methodology and a hierarchical encoding mechanism. The new\ntraining methodology involves continuous-scale training for discrete scale\nmodels, enabling the learning of inter-scale correlations and multi-scale\nfeature representation. In addition, we generalize the hierarchical encoding\nmechanism with existing attention-based network structures, which can achieve\nimproved spatial feature fusion, cross-scale information aggregation, and more\nimportantly, much faster inference. We have evaluated the C2D-ISR framework\nbased on three efficient attention-based backbones, SwinIR-L, SRFormer-L and\nMambaIRv2-L, and demonstrated significant improvements over the other existing\noptimization framework, HiT, in terms of super-resolution performance (up to\n0.2dB) and computational complexity reduction (up to 11%). The source code will\nbe made publicly available at www.github.com.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13777", "pdf": "https://arxiv.org/pdf/2503.13777", "abs": "https://arxiv.org/abs/2503.13777", "authors": ["Xuyang Fang", "Sion Hannuna", "Neill Campbell"], "title": "8-Calves Image dataset", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "We introduce the 8-Calves dataset, a benchmark for evaluating object\ndetection and identity classification in occlusion-rich, temporally consistent\nenvironments. The dataset comprises a 1-hour video (67,760 frames) of eight\nHolstein Friesian calves in a barn, with ground truth bounding boxes and\nidentities, alongside 900 static frames for detection tasks. Each calf exhibits\na unique coat pattern, enabling precise identity distinction.\n  For cow detection, we fine-tuned 28 models (25 YOLO variants, 3 transformers)\non 600 frames, testing on the full video. Results reveal smaller YOLO models\n(e.g., YOLOV9c) outperform larger counterparts despite potential bias from a\nYOLOv8m-based labeling pipeline. For identity classification, embeddings from\n23 pretrained vision models (ResNet, ConvNextV2, ViTs) were evaluated via\nlinear classifiers and KNN. Modern architectures like ConvNextV2 excelled,\nwhile larger models frequently overfit, highlighting inefficiencies in scaling.\n  Key findings include: (1) Minimal, targeted augmentations (e.g., rotation)\noutperform complex strategies on simpler datasets; (2) Pretraining strategies\n(e.g., BEiT, DinoV2) significantly boost identity recognition; (3) Temporal\ncontinuity and natural motion patterns offer unique challenges absent in\nsynthetic or domain-specific benchmarks. The dataset's controlled design and\nextended sequences (1 hour vs. prior 10-minute benchmarks) make it a pragmatic\ntool for stress-testing occlusion handling, temporal consistency, and\nefficiency.\n  The link to the dataset is https://github.com/tonyFang04/8-calves.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13794", "pdf": "https://arxiv.org/pdf/2503.13794", "abs": "https://arxiv.org/abs/2503.13794", "authors": ["Yang Zhou", "Shiyu Zhao", "Yuxiao Chen", "Zhenting Wang", "Dimitris N. Metaxas"], "title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large foundation models trained on large-scale visual-text data can\nsignificantly enhance Open Vocabulary Object Detection (OVD) through data\ngeneration. However, this may lead to biased synthetic data and overfitting to\nspecific configurations. It can sidestep biases of manually curated data\ngeneration by directly leveraging hidden states of Large Language Models\n(LLMs), which is surprisingly rarely explored. This paper presents a systematic\nmethod to enhance visual grounding by utilizing decoder layers of the LLM of a\nMLLM. We introduce a zero-initialized cross-attention adapter to enable\nefficient knowledge transfer from LLMs to object detectors, an new approach\ncalled LED (LLM Enhanced Open-Vocabulary Object Detection). We demonstrate that\nintermediate hidden states from early LLM layers retain strong spatial-semantic\ncorrelations that are beneficial to grounding tasks. Experiments show that our\nadaptation strategy significantly enhances the performance on complex free-form\ntext queries while remaining the same on plain categories. With our adaptation,\nQwen2-0.5B with Swin-T as the vision encoder improves GroundingDINO by 2.33% on\nOmnilabel, at the overhead of 8.7% more GFLOPs. Qwen2-0.5B with a larger vision\nencoder can further boost the performance by 6.22%. We further validate our\ndesign by ablating on varied adapter architectures, sizes of LLMs, and which\nlayers to add adaptation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13799", "pdf": "https://arxiv.org/pdf/2503.13799", "abs": "https://arxiv.org/abs/2503.13799", "authors": ["Liangrui Pan", "Xiaoyu Li", "Yutao Dou", "Qiya Song", "Jiadi Luo", "Qingchun Liang", "Shaoliang Peng"], "title": "SMILE: a Scale-aware Multiple Instance Learning Method for Multicenter STAS Lung Cancer Histopathology Diagnosis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Spread through air spaces (STAS) represents a newly identified aggressive\npattern in lung cancer, which is known to be associated with adverse prognostic\nfactors and complex pathological features. Pathologists currently rely on time\nconsuming manual assessments, which are highly subjective and prone to\nvariation. This highlights the urgent need for automated and precise diag\nnostic solutions. 2,970 lung cancer tissue slides are comprised from multiple\ncenters, re-diagnosed them, and constructed and publicly released three lung\ncancer STAS datasets: STAS CSU (hospital), STAS TCGA, and STAS CPTAC. All STAS\ndatasets provide corresponding pathological feature diagnoses and related\nclinical data. To address the bias, sparse and heterogeneous nature of STAS, we\npropose an scale-aware multiple instance learning(SMILE) method for STAS\ndiagnosis of lung cancer. By introducing a scale-adaptive attention mechanism,\nthe SMILE can adaptively adjust high attention instances, reducing\nover-reliance on local regions and promoting consistent detection of STAS\nlesions. Extensive experiments show that SMILE achieved competitive diagnostic\nresults on STAS CSU, diagnosing 251 and 319 STAS samples in CPTAC\nandTCGA,respectively, surpassing clinical average AUC. The 11 open baseline\nresults are the first to be established for STAS research, laying the\nfoundation for the future expansion, interpretability, and clinical integration\nof computational pathology technologies. The datasets and code are available at\nhttps://anonymous.4open.science/r/IJCAI25-1DA1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13806", "pdf": "https://arxiv.org/pdf/2503.13806", "abs": "https://arxiv.org/abs/2503.13806", "authors": ["Wenjie Zhang", "Ziyang Zhang", "Mengnan He", "Jiancheng Ye"], "title": "Organ-aware Multi-scale Medical Image Segmentation Using Text Prompt Engineering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate segmentation is essential for effective treatment planning and\ndisease monitoring. Existing medical image segmentation methods predominantly\nrely on uni-modal visual inputs, such as images or videos, requiring\nlabor-intensive manual annotations. Additionally, medical imaging techniques\ncapture multiple intertwined organs within a single scan, further complicating\nsegmentation accuracy. To address these challenges, MedSAM, a large-scale\nmedical segmentation model based on the Segment Anything Model (SAM), was\ndeveloped to enhance segmentation accuracy by integrating image features with\nuser-provided prompts. While MedSAM has demonstrated strong performance across\nvarious medical segmentation tasks, it primarily relies on geometric prompts\n(e.g., points and bounding boxes) and lacks support for text-based prompts,\nwhich could help specify subtle or ambiguous anatomical structures. To overcome\nthese limitations, we propose the Organ-aware Multi-scale Text-guided Medical\nImage Segmentation Model (OMT-SAM) for multi-organ segmentation. Our approach\nintroduces CLIP encoders as a novel image-text prompt encoder, operating with\nthe geometric prompt encoder to provide informative contextual guidance. We\npair descriptive textual prompts with corresponding images, processing them\nthrough pre-trained CLIP encoders and a cross-attention mechanism to generate\nfused image-text embeddings. Additionally, we extract multi-scale visual\nfeatures from MedSAM, capturing fine-grained anatomical details at different\nlevels of granularity. We evaluate OMT-SAM on the FLARE 2021 dataset,\nbenchmarking its performance against existing segmentation methods. Empirical\nresults demonstrate that OMT-SAM achieves a mean Dice Similarity Coefficient of\n0.937, outperforming MedSAM (0.893) and other segmentation models, highlighting\nits superior capability in handling complex medical image segmentation tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13816", "pdf": "https://arxiv.org/pdf/2503.13816", "abs": "https://arxiv.org/abs/2503.13816", "authors": ["Zhixuan Liu", "Haokun Zhu", "Rui Chen", "Jonathan Francis", "Soonmin Hwang", "Ji Zhang", "Jean Oh"], "title": "MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple Depth Views in Multi-Room Environments", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a novel diffusion-based approach for generating\nprivacy-preserving digital twins of multi-room indoor environments from depth\nimages only. Central to our approach is a novel Multi-view Overlapped Scene\nAlignment with Implicit Consistency (MOSAIC) model that explicitly considers\ncross-view dependencies within the same scene in the probabilistic sense.\nMOSAIC operates through a novel inference-time optimization that avoids error\naccumulation common in sequential or single-room constraint in panorama-based\napproaches. MOSAIC scales to complex scenes with zero extra training and\nprovably reduces the variance during denoising processes when more overlapping\nviews are added, leading to improved generation quality. Experiments show that\nMOSAIC outperforms state-of-the-art baselines on image fidelity metrics in\nreconstructing complex multi-room environments. Project page is available at:\nhttps://mosaic-cmubig.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13821", "pdf": "https://arxiv.org/pdf/2503.13821", "abs": "https://arxiv.org/abs/2503.13821", "authors": ["Chi Hsuan Wu", "Kumar Ashutosh", "Kristen Grauman"], "title": "Stitch-a-Recipe: Video Demonstration from Multistep Descriptions", "categories": ["cs.CV"], "comment": null, "summary": "When obtaining visual illustrations from text descriptions, today's methods\ntake a description with-a single text context caption, or an action\ndescription-and retrieve or generate the matching visual context. However,\nprior work does not permit visual illustration of multistep descriptions, e.g.\na cooking recipe composed of multiple steps. Furthermore, simply handling each\nstep description in isolation would result in an incoherent demonstration. We\npropose Stitch-a-Recipe, a novel retrieval-based method to assemble a video\ndemonstration from a multistep description. The resulting video contains clips,\npossibly from different sources, that accurately reflect all the step\ndescriptions, while being visually coherent. We formulate a training pipeline\nthat creates large-scale weakly supervised data containing diverse and novel\nrecipes and injects hard negatives that promote both correctness and coherence.\nValidated on in-the-wild instructional videos, Stitch-a-Recipe achieves\nstate-of-the-art performance, with quantitative gains up to 24% as well as\ndramatic wins in a human preference study.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13828", "pdf": "https://arxiv.org/pdf/2503.13828", "abs": "https://arxiv.org/abs/2503.13828", "authors": ["Chunlei Li", "Yilei Shi", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "title": "Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical Anomaly Detection", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "Unsupervised anomaly detection using deep learning has garnered significant\nresearch attention due to its broad applicability, particularly in medical\nimaging where labeled anomalous data are scarce. While earlier approaches\nleverage generative models like autoencoders and generative adversarial\nnetworks (GANs), they often fall short due to overgeneralization. Recent\nmethods explore various strategies, including memory banks, normalizing flows,\nself-supervised learning, and knowledge distillation, to enhance\ndiscrimination. Among these, knowledge distillation, particularly reverse\ndistillation, has shown promise. Following this paradigm, we propose a novel\nscale-aware contrastive reverse distillation model that addresses two key\nlimitations of existing reverse distillation methods: insufficient feature\ndiscriminability and inability to handle anomaly scale variations.\nSpecifically, we introduce a contrastive student-teacher learning approach to\nderive more discriminative representations by generating and exploring\nout-of-normal distributions. Further, we design a scale adaptation mechanism to\nsoftly weight contrastive distillation losses at different scales to account\nfor the scale variation issue. Extensive experiments on benchmark datasets\ndemonstrate state-of-the-art performance, validating the efficacy of the\nproposed method. Code is available at https://github.com/MedAITech/SCRD4AD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13858", "pdf": "https://arxiv.org/pdf/2503.13858", "abs": "https://arxiv.org/abs/2503.13858", "authors": ["Hongyu Ke", "Jack Morris", "Kentaro Oguchi", "Xiaofei Cao", "Yongkang Liu", "Haoxin Wang", "Yi Ding"], "title": "MamBEV: Enabling State Space Models to Learn Birds-Eye-View Representations", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "3D visual perception tasks, such as 3D detection from multi-camera images,\nare essential components of autonomous driving and assistance systems. However,\ndesigning computationally efficient methods remains a significant challenge. In\nthis paper, we propose a Mamba-based framework called MamBEV, which learns\nunified Bird's Eye View (BEV) representations using linear spatio-temporal\nSSM-based attention. This approach supports multiple 3D perception tasks with\nsignificantly improved computational and memory efficiency. Furthermore, we\nintroduce SSM based cross-attention, analogous to standard cross attention,\nwhere BEV query representations can interact with relevant image features.\nExtensive experiments demonstrate MamBEV's promising performance across diverse\nvisual perception metrics, highlighting its advantages in input scaling\nefficiency compared to existing benchmark models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13881", "pdf": "https://arxiv.org/pdf/2503.13881", "abs": "https://arxiv.org/abs/2503.13881", "authors": ["Donggon Jang", "Yucheol Cho", "Suin Lee", "Taehyeon Kim", "Dae-Shik Kim"], "title": "MMR: A Large-scale Benchmark Dataset for Multi-target and Multi-granularity Reasoning Segmentation", "categories": ["cs.CV"], "comment": "ICLR 2025, Code and dataset are available at\n  \\url{https://github.com/jdg900/MMR}", "summary": "The fusion of Large Language Models with vision models is pioneering new\npossibilities in user-interactive vision-language tasks. A notable application\nis reasoning segmentation, where models generate pixel-level segmentation masks\nby comprehending implicit meanings in human instructions. However, seamless\nhuman-AI interaction demands more than just object-level recognition; it\nrequires understanding both objects and the functions of their detailed parts,\nparticularly in multi-target scenarios. For example, when instructing a robot\nto \\textit{turn on the TV\"}, there could be various ways to accomplish this\ncommand. Recognizing multiple objects capable of turning on the TV, such as the\nTV itself or a remote control (multi-target), provides more flexible options\nand aids in finding the optimized scenario. Furthermore, understanding specific\nparts of these objects, like the TV's button or the remote's button\n(part-level), is important for completing the action. Unfortunately, current\nreasoning segmentation datasets predominantly focus on a single target\nobject-level reasoning, which limits the detailed recognition of an object's\nparts in multi-target contexts. To address this gap, we construct a large-scale\ndataset called Multi-target and Multi-granularity Reasoning (MMR). MMR\ncomprises 194K complex and implicit instructions that consider multi-target,\nobject-level, and part-level aspects, based on pre-existing image-mask sets.\nThis dataset supports diverse and context-aware interactions by hierarchically\nproviding object and part information. Moreover, we propose a straightforward\nyet effective framework for multi-target, object-level, and part-level\nreasoning segmentation. Experimental results on MMR show that the proposed\nmethod can reason effectively in multi-target and multi-granularity scenarios,\nwhile the existing reasoning segmentation model still has room for improvement.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13891", "pdf": "https://arxiv.org/pdf/2503.13891", "abs": "https://arxiv.org/abs/2503.13891", "authors": ["Xiaoying Xing", "Chia-Wen Kuo", "Li Fuxin", "Yulei Niu", "Fan Chen", "Ming Li", "Ying Wu", "Longyin Wen", "Sijie Zhu"], "title": "Where do Large Vision-Language Models Look at when Answering Questions?", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown promising performance in\nvision-language understanding and reasoning tasks. However, their visual\nunderstanding behaviors remain underexplored. A fundamental question arises: to\nwhat extent do LVLMs rely on visual input, and which image regions contribute\nto their responses? It is non-trivial to interpret the free-form generation of\nLVLMs due to their complicated visual architecture (e.g., multiple encoders and\nmulti-resolution) and variable-length outputs. In this paper, we extend\nexisting heatmap visualization methods (e.g., iGOS++) to support LVLMs for\nopen-ended visual question answering. We propose a method to select visually\nrelevant tokens that reflect the relevance between generated answers and input\nimage. Furthermore, we conduct a comprehensive analysis of state-of-the-art\nLVLMs on benchmarks designed to require visual information to answer. Our\nfindings offer several insights into LVLM behavior, including the relationship\nbetween focus region and answer correctness, differences in visual attention\nacross architectures, and the impact of LLM scale on visual understanding. The\ncode and data are available at\nhttps://github.com/bytedance/LVLM_Interpretation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13895", "pdf": "https://arxiv.org/pdf/2503.13895", "abs": "https://arxiv.org/abs/2503.13895", "authors": ["Xinliang Zhang", "Lei Zhu", "Shuang Zeng", "Hangzhou He", "Ourui Fu", "Zhengjian Yao", "Zhaoheng Xie", "Yanye Lu"], "title": "Exploiting Inherent Class Label: Towards Robust Scribble Supervised Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Scribble-based weakly supervised semantic segmentation leverages only a few\nannotated pixels as labels to train a segmentation model, presenting\nsignificant potential for reducing the human labor involved in the annotation\nprocess. This approach faces two primary challenges: first, the sparsity of\nscribble annotations can lead to inconsistent predictions due to limited\nsupervision; second, the variability in scribble annotations, reflecting\ndiffering human annotator preferences, can prevent the model from consistently\ncapturing the discriminative regions of objects, potentially leading to\nunstable predictions. To address these issues, we propose a holistic framework,\nthe class-driven scribble promotion network, for robust scribble-supervised\nsemantic segmentation. This framework not only utilizes the provided scribble\nannotations but also leverages their associated class labels to generate\nreliable pseudo-labels. Within the network, we introduce a localization\nrectification module to mitigate noisy labels and a distance perception module\nto identify reliable regions surrounding scribble annotations and\npseudo-labels. In addition, we introduce new large-scale benchmarks,\nScribbleCOCO and ScribbleCityscapes, accompanied by a scribble simulation\nalgorithm that enables evaluation across varying scribble styles. Our method\ndemonstrates competitive performance in both accuracy and robustness,\nunderscoring its superiority over existing approaches. The datasets and the\ncodes will be made publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13914", "pdf": "https://arxiv.org/pdf/2503.13914", "abs": "https://arxiv.org/abs/2503.13914", "authors": ["Barza Nisar", "Steven L. Waslander"], "title": "PSA-SSL: Pose and Size-aware Self-Supervised Learning on LiDAR Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised learning (SSL) on 3D point clouds has the potential to learn\nfeature representations that can transfer to diverse sensors and multiple\ndownstream perception tasks. However, recent SSL approaches fail to define\npretext tasks that retain geometric information such as object pose and scale,\nwhich can be detrimental to the performance of downstream localization and\ngeometry-sensitive 3D scene understanding tasks, such as 3D semantic\nsegmentation and 3D object detection. We propose PSA-SSL, a novel extension to\npoint cloud SSL that learns object pose and size-aware (PSA) features. Our\napproach defines a self-supervised bounding box regression pretext task, which\nretains object pose and size information. Furthermore, we incorporate LiDAR\nbeam pattern augmentation on input point clouds, which encourages learning\nsensor-agnostic features. Our experiments demonstrate that with a single\npretrained model, our light-weight yet effective extensions achieve significant\nimprovements on 3D semantic segmentation with limited labels across popular\nautonomous driving datasets (Waymo, nuScenes, SemanticKITTI). Moreover, our\napproach outperforms other state-of-the-art SSL methods on 3D semantic\nsegmentation (using up to 10 times less labels), as well as on 3D object\ndetection. Our code will be released on https://github.com/TRAILab/PSA-SSL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13935", "pdf": "https://arxiv.org/pdf/2503.13935", "abs": "https://arxiv.org/abs/2503.13935", "authors": ["Bowen Yuan", "Yuxia Fu", "Zijian Wang", "Yadan Luo", "Zi Huang"], "title": "SCORE: Soft Label Compression-Centric Dataset Condensation via Coding Rate Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Dataset Condensation (DC) aims to obtain a condensed dataset that allows\nmodels trained on the condensed dataset to achieve performance comparable to\nthose trained on the full dataset. Recent DC approaches increasingly focus on\nencoding knowledge into realistic images with soft labeling, for their\nscalability to ImageNet-scale datasets and strong capability of cross-domain\ngeneralization. However, this strong performance comes at a substantial storage\ncost which could significantly exceed the storage cost of the original dataset.\nWe argue that the three key properties to alleviate this performance-storage\ndilemma are informativeness, discriminativeness, and compressibility of the\ncondensed data. Towards this end, this paper proposes a \\textbf{S}oft label\ncompression-centric dataset condensation framework using \\textbf{CO}ding\n\\textbf{R}at\\textbf{E} (SCORE). SCORE formulates dataset condensation as a\nmin-max optimization problem, which aims to balance the three key properties\nfrom an information-theoretic perspective. In particular, we theoretically\ndemonstrate that our coding rate-inspired objective function is submodular, and\nits optimization naturally enforces low-rank structure in the soft label set\ncorresponding to each condensed data. Extensive experiments on large-scale\ndatasets, including ImageNet-1K and Tiny-ImageNet, demonstrate that SCORE\noutperforms existing methods in most cases. Even with 30$\\times$ compression of\nsoft labels, performance decreases by only 5.5\\% and 2.7\\% for ImageNet-1K with\nIPC 10 and 50, respectively. Code will be released upon paper acceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13952", "pdf": "https://arxiv.org/pdf/2503.13952", "abs": "https://arxiv.org/abs/2503.13952", "authors": ["Xinqing Li", "Ruiqi Song", "Qingyu Xie", "Ye Wu", "Nanxin Zeng", "Yunfeng Ai"], "title": "SimWorld: A Unified Benchmark for Simulator-Conditioned Scene Generation via World Model", "categories": ["cs.CV", "I.4.8; I.2.10"], "comment": "8 pages, 4 figures", "summary": "With the rapid advancement of autonomous driving technology, a lack of data\nhas become a major obstacle to enhancing perception model accuracy. Researchers\nare now exploring controllable data generation using world models to diversify\ndatasets. However, previous work has been limited to studying image generation\nquality on specific public datasets. There is still relatively little research\non how to build data generation engines for real-world application scenes to\nachieve large-scale data generation for challenging scenes. In this paper, a\nsimulator-conditioned scene generation engine based on world model is proposed.\nBy constructing a simulation system consistent with real-world scenes,\nsimulation data and labels, which serve as the conditions for data generation\nin the world model, for any scenes can be collected. It is a novel data\ngeneration pipeline by combining the powerful scene simulation capabilities of\nthe simulation engine with the robust data generation capabilities of the world\nmodel. In addition, a benchmark with proportionally constructed virtual and\nreal data, is provided for exploring the capabilities of world models in\nreal-world scenes. Quantitative results show that these generated images\nsignificantly improve downstream perception models performance. Finally, we\nexplored the generative performance of the world model in urban autonomous\ndriving scenarios. All the data and code will be available at\nhttps://github.com/Li-Zn-H/SimWorld.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13956", "pdf": "https://arxiv.org/pdf/2503.13956", "abs": "https://arxiv.org/abs/2503.13956", "authors": ["Yixuan Li", "Changli Tang", "Jimin Zhuang", "Yudong Yang", "Guangzhi Sun", "Wei Li", "Zejun Ma", "Chao Zhang"], "title": "Improving LLM Video Understanding with 16 Frames Per Second", "categories": ["cs.CV"], "comment": null, "summary": "Human vision is dynamic and continuous. However, in video understanding with\nmultimodal large language models (LLMs), existing methods primarily rely on\nstatic features extracted from images sampled at a fixed low frame rate of\nframe-per-second (FPS) $\\leqslant$2, leading to critical visual information\nloss. In this paper, we introduce F-16, the first multimodal LLM designed for\nhigh-frame-rate video understanding. By increasing the frame rate to 16 FPS and\ncompressing visual tokens within each 1-second clip, F-16 efficiently captures\ndynamic visual features while preserving key semantic information. Experimental\nresults demonstrate that higher frame rates considerably enhance video\nunderstanding across multiple benchmarks, providing a new approach to improving\nvideo LLMs beyond scaling model size or training data. F-16 achieves\nstate-of-the-art performance among 7-billion-parameter video LLMs on both\ngeneral and fine-grained video understanding benchmarks, such as Video-MME and\nTemporalBench. Furthermore, F-16 excels in complex spatiotemporal tasks,\nincluding high-speed sports analysis (\\textit{e.g.}, basketball, football,\ngymnastics, and diving), outperforming SOTA proprietary visual models like\nGPT-4o and Gemini-1.5-pro. Additionally, we introduce a novel decoding method\nfor F-16 that enables highly efficient low-frame-rate inference without\nrequiring model retraining. Upon acceptance, we will release the source code,\nmodel checkpoints, and data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13969", "pdf": "https://arxiv.org/pdf/2503.13969", "abs": "https://arxiv.org/abs/2503.13969", "authors": ["HaoBin Qin", "Jiale Fang", "Keisuke Fujii"], "title": "SoccerSynth Field: enhancing field detection with synthetic data from virtual soccer simulator", "categories": ["cs.CV"], "comment": null, "summary": "Field detection in team sports is an essential task in sports video analysis.\nHowever, collecting large-scale and diverse real-world datasets for training\ndetection models is often cost and time-consuming. Synthetic datasets, which\nallow controlled variability in lighting, textures, and camera angles, will be\na promising alternative for addressing these problems. This study addresses the\nchallenges of high costs and difficulties in collecting real-world datasets by\ninvestigating the effectiveness of pretraining models using synthetic datasets.\nIn this paper, we propose the effectiveness of using a synthetic dataset\n(SoccerSynth-Field) for soccer field detection. A synthetic soccer field\ndataset was created to pretrain models, and the performance of these models was\ncompared with models trained on real-world datasets. The results demonstrate\nthat models pretrained on the synthetic dataset exhibit superior performance in\ndetecting soccer fields. This highlights the effectiveness of synthetic data in\nenhancing model robustness and accuracy, offering a cost-effective and scalable\nsolution for advancing detection tasks in sports field detection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13991", "pdf": "https://arxiv.org/pdf/2503.13991", "abs": "https://arxiv.org/abs/2503.13991", "authors": ["Bo Peng", "Jintao Chen", "Mufeng Yao", "Chenhao Zhang", "Jianghui Zhang", "Mingmin Chi", "Jiang Tao"], "title": "GraphTEN: Graph Enhanced Texture Encoding Network", "categories": ["cs.CV", "cs.AI", "68T45", "I.2.10; I.4.7"], "comment": "6 pages, 7 figures, conference paper", "summary": "Texture recognition is a fundamental problem in computer vision and pattern\nrecognition. Recent progress leverages feature aggregation into discriminative\ndescriptions based on convolutional neural networks (CNNs). However, modeling\nnon-local context relations through visual primitives remains challenging due\nto the variability and randomness of texture primitives in spatial\ndistributions. In this paper, we propose a graph-enhanced texture encoding\nnetwork (GraphTEN) designed to capture both local and global features of\ntexture primitives. GraphTEN models global associations through fully connected\ngraphs and captures cross-scale dependencies of texture primitives via\nbipartite graphs. Additionally, we introduce a patch encoding module that\nutilizes a codebook to achieve an orderless representation of texture by\nencoding multi-scale patch features into a unified feature space. The proposed\nGraphTEN achieves superior performance compared to state-of-the-art methods\nacross five publicly available datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14064", "pdf": "https://arxiv.org/pdf/2503.14064", "abs": "https://arxiv.org/abs/2503.14064", "authors": ["Xinhao Xiang", "Xiao Liu", "Zizhong Li", "Zhuosheng Liu", "Jiawei Zhang"], "title": "AIGVE-Tool: AI-Generated Video Evaluation Toolkit with Multifaceted Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement in AI-generated video synthesis has led to a growth\ndemand for standardized and effective evaluation metrics. Existing metrics lack\na unified framework for systematically categorizing methodologies, limiting a\nholistic understanding of the evaluation landscape. Additionally, fragmented\nimplementations and the absence of standardized interfaces lead to redundant\nprocessing overhead. Furthermore, many prior approaches are constrained by\ndataset-specific dependencies, limiting their applicability across diverse\nvideo domains. To address these challenges, we introduce AIGVE-Tool\n(AI-Generated Video Evaluation Toolkit), a unified framework that provides a\nstructured and extensible evaluation pipeline for a comprehensive AI-generated\nvideo evaluation. Organized within a novel five-category taxonomy, AIGVE-Tool\nintegrates multiple evaluation methodologies while allowing flexible\ncustomization through a modular configuration system. Additionally, we propose\nAIGVE-Bench, a large-scale benchmark dataset created with five SOTA video\ngeneration models based on hand-crafted instructions and prompts. This dataset\nsystematically evaluates various video generation models across nine critical\nquality dimensions. Extensive experiments demonstrate the effectiveness of\nAIGVE-Tool in providing standardized and reliable evaluation results,\nhighlighting specific strengths and limitations of current models and\nfacilitating the advancements of next-generation AI-generated video techniques.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14070", "pdf": "https://arxiv.org/pdf/2503.14070", "abs": "https://arxiv.org/abs/2503.14070", "authors": ["Yang Ye", "Junliang Guo", "Haoyu Wu", "Tianyu He", "Tim Pearce", "Tabish Rashid", "Katja Hofmann", "Jiang Bian"], "title": "Fast Autoregressive Video Generation with Diagonal Decoding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autoregressive Transformer models have demonstrated impressive performance in\nvideo generation, but their sequential token-by-token decoding process poses a\nmajor bottleneck, particularly for long videos represented by tens of thousands\nof tokens. In this paper, we propose Diagonal Decoding (DiagD), a training-free\ninference acceleration algorithm for autoregressively pre-trained models that\nexploits spatial and temporal correlations in videos. Our method generates\ntokens along diagonal paths in the spatial-temporal token grid, enabling\nparallel decoding within each frame as well as partially overlapping across\nconsecutive frames. The proposed algorithm is versatile and adaptive to various\ngenerative models and tasks, while providing flexible control over the\ntrade-off between inference speed and visual quality. Furthermore, we propose a\ncost-effective finetuning strategy that aligns the attention patterns of the\nmodel with our decoding order, further mitigating the training-inference gap on\nsmall-scale models. Experiments on multiple autoregressive video generation\nmodels and datasets demonstrate that DiagD achieves up to $10\\times$ speedup\ncompared to naive sequential decoding, while maintaining comparable visual\nfidelity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14109", "pdf": "https://arxiv.org/pdf/2503.14109", "abs": "https://arxiv.org/abs/2503.14109", "authors": ["Nicolas Gonthier"], "title": "Operational Change Detection for Geographical Information: Overview and Challenges", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint under review", "summary": "Rapid evolution of territories due to climate change and human impact\nrequires prompt and effective updates to geospatial databases maintained by the\nNational Mapping Agency. This paper presents a comprehensive overview of change\ndetection methods tailored for the operational updating of large-scale\ngeographic databases. This review first outlines the fundamental definition of\nchange, emphasizing its multifaceted nature, from temporal to semantic\ncharacterization. It categorizes automatic change detection methods into four\nmain families: rule-based, statistical, machine learning, and simulation\nmethods. The strengths, limitations, and applicability of every family are\ndiscussed in the context of various input data. Then, key applications for\nNational Mapping Agencies are identified, particularly the optimization of\ngeospatial database updating, change-based phenomena, and dynamics monitoring.\nFinally, the paper highlights the current challenges for leveraging change\ndetection such as the variability of change definition, the missing of relevant\nlarge-scale datasets, the diversity of input data, the unstudied no-change\ndetection, the human in the loop integration and the operational constraints.\nThe discussion underscores the necessity for ongoing innovation in change\ndetection techniques to address the future needs of geographic information\nsystems for national mapping agencies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14140", "pdf": "https://arxiv.org/pdf/2503.14140", "abs": "https://arxiv.org/abs/2503.14140", "authors": ["Zining Wang", "Tongkun Guan", "Pei Fu", "Chen Duan", "Qianyi Jiang", "Zhentao Guo", "Shan Guo", "Junfeng Luo", "Wei Shen", "Xiaokang Yang"], "title": "Marten: Visual Question Answering with Mask Generation for Multi-modal Document Understanding", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Multi-modal Large Language Models (MLLMs) have introduced a novel dimension\nto document understanding, i.e., they endow large language models with visual\ncomprehension capabilities; however, how to design a suitable image-text\npre-training task for bridging the visual and language modality in\ndocument-level MLLMs remains underexplored. In this study, we introduce a novel\nvisual-language alignment method that casts the key issue as a Visual Question\nAnswering with Mask generation (VQAMask) task, optimizing two tasks\nsimultaneously: VQA-based text parsing and mask generation. The former allows\nthe model to implicitly align images and text at the semantic level. The latter\nintroduces an additional mask generator (discarded during inference) to\nexplicitly ensure alignment between visual texts within images and their\ncorresponding image regions at a spatially-aware level. Together, they can\nprevent model hallucinations when parsing visual text and effectively promote\nspatially-aware feature representation learning. To support the proposed\nVQAMask task, we construct a comprehensive image-mask generation pipeline and\nprovide a large-scale dataset with 6M data (MTMask6M). Subsequently, we\ndemonstrate that introducing the proposed mask generation task yields\ncompetitive document-level understanding performance. Leveraging the proposed\nVQAMask, we introduce Marten, a training-efficient MLLM tailored for\ndocument-level understanding. Extensive experiments show that our Marten\nconsistently achieves significant improvements among 8B-MLLMs in\ndocument-centric tasks. Code and datasets are available at\nhttps://github.com/PriNing/Marten.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering", "dimension"], "score": 3}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14219", "pdf": "https://arxiv.org/pdf/2503.14219", "abs": "https://arxiv.org/abs/2503.14219", "authors": ["Yizhou Li", "Yusuke Monno", "Masatoshi Okutomi", "Yuuichi Tanaka", "Seiichi Kataoka", "Teruaki Kosiba"], "title": "Segmentation-Guided Neural Radiance Fields for Novel Street View Synthesis", "categories": ["cs.CV", "eess.IV"], "comment": "Presented at VISAPP2025. Project page:\n  http://www.ok.sc.e.titech.ac.jp/res/NVS/index.html", "summary": "Recent advances in Neural Radiance Fields (NeRF) have shown great potential\nin 3D reconstruction and novel view synthesis, particularly for indoor and\nsmall-scale scenes. However, extending NeRF to large-scale outdoor environments\npresents challenges such as transient objects, sparse cameras and textures, and\nvarying lighting conditions. In this paper, we propose a segmentation-guided\nenhancement to NeRF for outdoor street scenes, focusing on complex urban\nenvironments. Our approach extends ZipNeRF and utilizes Grounded SAM for\nsegmentation mask generation, enabling effective handling of transient objects,\nmodeling of the sky, and regularization of the ground. We also introduce\nappearance embeddings to adapt to inconsistent lighting across view sequences.\nExperimental results demonstrate that our method outperforms the baseline\nZipNeRF, improving novel view synthesis quality with fewer artifacts and\nsharper details.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14228", "pdf": "https://arxiv.org/pdf/2503.14228", "abs": "https://arxiv.org/abs/2503.14228", "authors": ["Nobuhiko Wakai", "Satoshi Sato", "Yasunori Ishii", "Takayoshi Yamashita"], "title": "Panoramic Distortion-Aware Tokenization for Person Detection and Localization Using Transformers in Overhead Fisheye Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Person detection methods are used widely in applications including visual\nsurveillance, pedestrian detection, and robotics. However, accurate detection\nof persons from overhead fisheye images remains an open challenge because of\nfactors including person rotation and small-sized persons. To address the\nperson rotation problem, we convert the fisheye images into panoramic images.\nFor smaller people, we focused on the geometry of the panoramas. Conventional\ndetection methods tend to focus on larger people because these larger people\nyield large significant areas for feature maps. In equirectangular panoramic\nimages, we find that a person's height decreases linearly near the top of the\nimages. Using this finding, we leverage the significance values and aggregate\ntokens that are sorted based on these values to balance the significant areas.\nIn this leveraging process, we introduce panoramic distortion-aware\ntokenization. This tokenization procedure divides a panoramic image using\nself-similarity figures that enable determination of optimal divisions without\ngaps, and we leverage the maximum significant values in each tile of token\ngroups to preserve the significant areas of smaller people. To achieve higher\ndetection accuracy, we propose a person detection and localization method that\ncombines panoramic-image remapping and the tokenization procedure. Extensive\nexperiments demonstrated that our method outperforms conventional methods when\napplied to large-scale datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14231", "pdf": "https://arxiv.org/pdf/2503.14231", "abs": "https://arxiv.org/abs/2503.14231", "authors": ["Ziyao Ling", "Giovanni Delnevo", "Paola Salomoni", "Silvia Mirri"], "title": "Multi-task Learning for Identification of Porcelain in Song and Yuan Dynasties", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Chinese porcelain holds immense historical and cultural value, making its\naccurate classification essential for archaeological research and cultural\nheritage preservation. Traditional classification methods rely heavily on\nexpert analysis, which is time-consuming, subjective, and difficult to scale.\nThis paper explores the application of DL and transfer learning techniques to\nautomate the classification of porcelain artifacts across four key attributes:\ndynasty, glaze, ware, and type. We evaluate four Convolutional Neural Networks\n(CNNs) - ResNet50, MobileNetV2, VGG16, and InceptionV3 - comparing their\nperformance with and without pre-trained weights. Our results demonstrate that\ntransfer learning significantly enhances classification accuracy, particularly\nfor complex tasks like type classification, where models trained from scratch\nexhibit lower performance. MobileNetV2 and ResNet50 consistently achieve high\naccuracy and robustness across all tasks, while VGG16 struggles with more\ndiverse classifications. We further discuss the impact of dataset limitations\nand propose future directions, including domain-specific pre-training,\nintegration of attention mechanisms, explainable AI methods, and generalization\nto other cultural artifacts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14237", "pdf": "https://arxiv.org/pdf/2503.14237", "abs": "https://arxiv.org/abs/2503.14237", "authors": ["Chenting Wang", "Kunchang Li", "Tianxiang Jiang", "Xiangyu Zeng", "Yi Wang", "Limin Wang"], "title": "Make Your Training Flexible: Towards Deployment-Efficient Video Models", "categories": ["cs.CV"], "comment": null, "summary": "Popular video training methods mainly operate on a fixed number of tokens\nsampled from a predetermined spatiotemporal grid, resulting in sub-optimal\naccuracy-computation trade-offs due to inherent video redundancy. They also\nlack adaptability to varying computational budgets for downstream tasks,\nhindering applications of the most competitive model in real-world scenes. We\nthus propose a new test setting, Token Optimization, for maximized input\ninformation across budgets, which optimizes the size-limited set of input\ntokens through token selection from more suitably sampled videos. To this end,\nwe propose a novel augmentation tool termed Flux. By making the sampling grid\nflexible and leveraging token selection, it is easily adopted in most popular\nvideo training frameworks, boosting model robustness with nearly no additional\ncost. We integrate Flux in large-scale video pre-training, and the resulting\nFluxViT establishes new state-of-the-art results across extensive tasks at\nstandard costs. Notably, with 1/4 tokens only, it can still match the\nperformance of previous state-of-the-art models with Token Optimization,\nyielding nearly 90\\% savings. All models and data are available at\nhttps://github.com/OpenGVLab/FluxViT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14273", "pdf": "https://arxiv.org/pdf/2503.14273", "abs": "https://arxiv.org/abs/2503.14273", "authors": ["Matthew J. Allen", "Harry J. F. Owen", "Stuart W. D. Grieve", "Emily R. Lines"], "title": "Manual Labelling Artificially Inflates Deep Learning-Based Segmentation Performance on Closed Canopy: Validation Using TLS", "categories": ["cs.CV", "cs.AI", "I.4; I.4.6; I.4.8; I.4.9; I.5; I.5.4"], "comment": "17 pages, 3 figures", "summary": "Monitoring forest dynamics at an individual tree scale is essential for\naccurately assessing ecosystem responses to climate change, yet traditional\nmethods relying on field-based forest inventories are labor-intensive and\nlimited in spatial coverage. Advances in remote sensing using drone-acquired\nRGB imagery combined with deep learning models have promised precise individual\ntree crown (ITC) segmentation; however, existing methods are frequently\nvalidated against human-annotated images, lacking rigorous independent ground\ntruth. In this study, we generate high-fidelity validation labels from\nco-located Terrestrial Laser Scanning (TLS) data for drone imagery of mixed\nunmanaged boreal and Mediterranean forests. We evaluate the performance of two\nwidely used deep learning ITC segmentation models - DeepForest (RetinaNet) and\nDetectree2 (Mask R-CNN) - on these data, and compare to performance on further\nMediterranean forest data labelled manually. When validated against TLS-derived\nground truth from Mediterranean forests, model performance decreased\nsignificantly compared to assessment based on hand-labelled from an\necologically similar site (AP50: 0.094 vs. 0.670). Restricting evaluation to\nonly canopy trees shrank this gap considerably (Canopy AP50: 0.365), although\nperformance was still far lower than on similar hand-labelled data. Models also\nperformed poorly on boreal forest data (AP50: 0.142), although again increasing\nwhen evaluated on canopy trees only (Canopy AP50: 0.308). Both models showed\nvery poor localisation accuracy at stricter IoU thresholds, even when\nrestricted to canopy trees (Max AP75: 0.051). Similar results have been\nobserved in studies using aerial LiDAR data, suggesting fundamental limitations\nin aerial-based segmentation approaches in closed canopy forests.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14295", "pdf": "https://arxiv.org/pdf/2503.14295", "abs": "https://arxiv.org/abs/2503.14295", "authors": ["Baiqin Wang", "Xiangyu Zhu", "Fan Shen", "Hao Xu", "Zhen Lei"], "title": "PC-Talk: Precise Facial Animation Control for Audio-Driven Talking Face Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in audio-driven talking face generation have made great\nprogress in lip synchronization. However, current methods often lack sufficient\ncontrol over facial animation such as speaking style and emotional expression,\nresulting in uniform outputs. In this paper, we focus on improving two key\nfactors: lip-audio alignment and emotion control, to enhance the diversity and\nuser-friendliness of talking videos. Lip-audio alignment control focuses on\nelements like speaking style and the scale of lip movements, whereas emotion\ncontrol is centered on generating realistic emotional expressions, allowing for\nmodifications in multiple attributes such as intensity. To achieve precise\ncontrol of facial animation, we propose a novel framework, PC-Talk, which\nenables lip-audio alignment and emotion control through implicit keypoint\ndeformations. First, our lip-audio alignment control module facilitates precise\nediting of speaking styles at the word level and adjusts lip movement scales to\nsimulate varying vocal loudness levels, maintaining lip synchronization with\nthe audio. Second, our emotion control module generates vivid emotional facial\nfeatures with pure emotional deformation. This module also enables the fine\nmodification of intensity and the combination of multiple emotions across\ndifferent facial regions. Our method demonstrates outstanding control\ncapabilities and achieves state-of-the-art performance on both HDTF and MEAD\ndatasets in extensive experiments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14346", "pdf": "https://arxiv.org/pdf/2503.14346", "abs": "https://arxiv.org/abs/2503.14346", "authors": ["X. Anadón", "Javier Rodríguez-Puigvert", "J. M. M. Montiel"], "title": "3D Densification for Multi-Map Monocular VSLAM in Endoscopy", "categories": ["cs.CV"], "comment": null, "summary": "Multi-map Sparse Monocular visual Simultaneous Localization and Mapping\napplied to monocular endoscopic sequences has proven efficient to robustly\nrecover tracking after the frequent losses in endoscopy due to motion blur,\ntemporal occlusion, tools interaction or water jets. The sparse multi-maps are\nadequate for robust camera localization, however they are very poor for\nenvironment representation, they are noisy, with a high percentage of\ninaccurately reconstructed 3D points, including significant outliers, and more\nimportantly with an unacceptable low density for clinical applications.\n  We propose a method to remove outliers and densify the maps of the state of\nthe art for sparse endoscopy multi-map CudaSIFT-SLAM. The NN LightDepth for\nup-to-scale depth dense predictions are aligned with the sparse CudaSIFT\nsubmaps by means of the robust to spurious LMedS. Our system mitigates the\ninherent scale ambiguity in monocular depth estimation while filtering\noutliers, leading to reliable densified 3D maps.\n  We provide experimental evidence of accurate densified maps 4.15 mm RMS\naccuracy at affordable computing time in the C3VD phantom colon dataset. We\nreport qualitative results on the real colonoscopy from the Endomapper dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14350", "pdf": "https://arxiv.org/pdf/2503.14350", "abs": "https://arxiv.org/abs/2503.14350", "authors": ["Shoubin Yu", "Difan Liu", "Ziqiao Ma", "Yicong Hong", "Yang Zhou", "Hao Tan", "Joyce Chai", "Mohit Bansal"], "title": "VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "First three authors contributed equally. Project page:\n  https://veggie-gen.github.io/", "summary": "Recent video diffusion models have enhanced video editing, but it remains\nchallenging to handle instructional editing and diverse tasks (e.g., adding,\nremoving, changing) within a unified framework. In this paper, we introduce\nVEGGIE, a Video Editor with Grounded Generation from Instructions, a simple\nend-to-end framework that unifies video concept editing, grounding, and\nreasoning based on diverse user instructions. Specifically, given a video and\ntext query, VEGGIE first utilizes an MLLM to interpret user intentions in\ninstructions and ground them to the video contexts, generating frame-specific\ngrounded task queries for pixel-space responses. A diffusion model then renders\nthese plans and generates edited videos that align with user intent. To support\ndiverse tasks and complex instructions, we employ a curriculum learning\nstrategy: first aligning the MLLM and video diffusion model with large-scale\ninstructional image editing data, followed by end-to-end fine-tuning on\nhigh-quality multitask video data. Additionally, we introduce a novel data\nsynthesis pipeline to generate paired instructional video editing data for\nmodel training. It transforms static image data into diverse, high-quality\nvideo editing samples by leveraging Image-to-Video models to inject dynamics.\nVEGGIE shows strong performance in instructional video editing with different\nediting skills, outperforming the best instructional baseline as a versatile\nmodel, while other models struggle with multi-tasking. VEGGIE also excels in\nvideo object grounding and reasoning segmentation, where other baselines fail.\nWe further reveal how the multiple tasks help each other and highlight\npromising applications like zero-shot multimodal instructional and in-context\nvideo editing.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14445", "pdf": "https://arxiv.org/pdf/2503.14445", "abs": "https://arxiv.org/abs/2503.14445", "authors": ["Stanislaw Szymanowicz", "Jason Y. Zhang", "Pratul Srinivasan", "Ruiqi Gao", "Arthur Brussee", "Aleksander Holynski", "Ricardo Martin-Brualla", "Jonathan T. Barron", "Philipp Henzler"], "title": "Bolt3D: Generating 3D Scenes in Seconds", "categories": ["cs.CV"], "comment": "Project page: https://szymanowiczs.github.io/bolt3d", "summary": "We present a latent diffusion model for fast feed-forward 3D scene\ngeneration. Given one or more images, our model Bolt3D directly samples a 3D\nscene representation in less than seven seconds on a single GPU. We achieve\nthis by leveraging powerful and scalable existing 2D diffusion network\narchitectures to produce consistent high-fidelity 3D scene representations. To\ntrain this model, we create a large-scale multiview-consistent dataset of 3D\ngeometry and appearance by applying state-of-the-art dense 3D reconstruction\ntechniques to existing multiview image datasets. Compared to prior multiview\ngenerative models that require per-scene optimization for 3D reconstruction,\nBolt3D reduces the inference cost by a factor of up to 300 times.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14489", "pdf": "https://arxiv.org/pdf/2503.14489", "abs": "https://arxiv.org/abs/2503.14489", "authors": ["Jensen", "Zhou", "Hang Gao", "Vikram Voleti", "Aaryaman Vasishta", "Chun-Han Yao", "Mark Boss", "Philip Torr", "Christian Rupprecht", "Varun Jampani"], "title": "Stable Virtual Camera: Generative View Synthesis with Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We present Stable Virtual Camera (Seva), a generalist diffusion model that\ncreates novel views of a scene, given any number of input views and target\ncameras. Existing works struggle to generate either large viewpoint changes or\ntemporally smooth samples, while relying on specific task configurations. Our\napproach overcomes these limitations through simple model design, optimized\ntraining recipe, and flexible sampling strategy that generalize across view\nsynthesis tasks at test time. As a result, our samples maintain high\nconsistency without requiring additional 3D representation-based distillation,\nthus streamlining view synthesis in the wild. Furthermore, we show that our\nmethod can generate high-quality videos lasting up to half a minute with\nseamless loop closure. Extensive benchmarking demonstrates that Seva\noutperforms existing methods across different datasets and settings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14492", "pdf": "https://arxiv.org/pdf/2503.14492", "abs": "https://arxiv.org/abs/2503.14492", "authors": ["NVIDIA", ":", "Hassan Abu Alhaija", "Jose Alvarez", "Maciej Bala", "Tiffany Cai", "Tianshi Cao", "Liz Cha", "Joshua Chen", "Mike Chen", "Francesco Ferroni", "Sanja Fidler", "Dieter Fox", "Yunhao Ge", "Jinwei Gu", "Ali Hassani", "Michael Isaev", "Pooya Jannaty", "Shiyi Lan", "Tobias Lasser", "Huan Ling", "Ming-Yu Liu", "Xian Liu", "Yifan Lu", "Alice Luo", "Qianli Ma", "Hanzi Mao", "Fabio Ramos", "Xuanchi Ren", "Tianchang Shen", "Shitao Tang", "Ting-Chun Wang", "Jay Wu", "Jiashu Xu", "Stella Xu", "Kevin Xie", "Yuchong Ye", "Xiaodong Yang", "Xiaohui Zeng", "Yu Zeng"], "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14504", "pdf": "https://arxiv.org/pdf/2503.14504", "abs": "https://arxiv.org/abs/2503.14504", "authors": ["Tao Yu", "Yi-Fan Zhang†", "Chaoyou Fu", "Junkang Wu", "Jinda Lu", "Kun Wang", "Xingyu Lu", "Yunhang Shen", "Guibin Zhang", "Dingjie Song", "Yibo Yan", "Tianlong Xu", "Qingsong Wen", "Zhang Zhang", "Yan Huang", "Liang Wang", "Tieniu Tan"], "title": "Aligning Multimodal LLM with Human Preference: A Survey", "categories": ["cs.CV"], "comment": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment", "summary": "Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "truthfulness", "safety"], "score": 3}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13588", "pdf": "https://arxiv.org/pdf/2503.13588", "abs": "https://arxiv.org/abs/2503.13588", "authors": ["Shiran Yuan", "Hao Zhao"], "title": "Next-Scale Autoregressive Models are Zero-Shot Single-Image Object View Synthesizers", "categories": ["cs.GR", "cs.CV"], "comment": "Full codebase, training set, and eval benchmark at\n  https://github.com/Shiran-Yuan/ArchonView", "summary": "Methods based on diffusion backbones have recently revolutionized novel view\nsynthesis (NVS). However, those models require pretrained 2D diffusion\ncheckpoints (e.g., Stable Diffusion) as the basis for geometrical priors. Since\nsuch checkpoints require exorbitant amounts of data and compute to train, this\ngreatly limits the scalability of diffusion-based NVS models. We present\nNext-Scale Autoregression Conditioned by View (ArchonView), a method that\nsignificantly exceeds state-of-the-art methods despite being trained from\nscratch with 3D rendering data only and no 2D pretraining. We achieve this by\nincorporating both global (pose-augmented semantics) and local (multi-scale\nhierarchical encodings) conditioning into a backbone based on the next-scale\nautoregression paradigm. Our model also exhibits robust performance even for\ndifficult camera poses where previous methods fail, and is several times faster\nin inference speed compared to diffusion. We experimentally verify that\nperformance scales with model and dataset size, and conduct extensive\ndemonstration of our method's synthesis quality across several tasks. Our code\nis open-sourced at https://github.com/Shiran-Yuan/ArchonView.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14034", "pdf": "https://arxiv.org/pdf/2503.14034", "abs": "https://arxiv.org/abs/2503.14034", "authors": ["Xi Shen", "Julian Gamboa", "Tabassom Hamidfar", "Shamima Mitu", "Selim M. Shahriar"], "title": "Shift, Scale and Rotation Invariant Multiple Object Detection using Balanced Joint Transform Correlator", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The Polar Mellin Transform (PMT) is a well-known technique that converts\nimages into shift, scale and rotation invariant signatures for object detection\nusing opto-electronic correlators. However, this technique cannot be properly\napplied when there are multiple targets in a single input. Here, we propose a\nSegmented PMT (SPMT) that extends this methodology for cases where multiple\nobjects are present within the same frame. Simulations show that this SPMT can\nbe integrated into an opto-electronic joint transform correlator to create a\ncorrelation system capable of detecting multiple objects simultaneously,\npresenting robust detection capabilities across various transformation\nconditions, with remarkable discrimination between matching and non-matching\ntargets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14377", "pdf": "https://arxiv.org/pdf/2503.14377", "abs": "https://arxiv.org/abs/2503.14377", "authors": ["Negin Baghbanzadeh", "Adibvafa Fallahpour", "Yasaman Parhizkar", "Franklin Ogidi", "Shuvendu Roy", "Sajad Ashkezari", "Vahid Reza Khazaie", "Michael Colacci", "Ali Etemad", "Arash Afkanpour", "Elham Dolatabadi"], "title": "Advancing Medical Representation Learning Through High-Quality Data", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Despite the growing scale of medical Vision-Language datasets, the impact of\ndataset quality on model performance remains under-explored. We introduce\nOpen-PMC, a high-quality medical dataset from PubMed Central, containing 2.2\nmillion image-text pairs, enriched with image modality annotations, subfigures,\nand summarized in-text references. Notably, the in-text references provide\nricher medical context, extending beyond the abstract information typically\nfound in captions. Through extensive experiments, we benchmark Open-PMC against\nlarger datasets across retrieval and zero-shot classification tasks. Our\nresults show that dataset quality-not just size-drives significant performance\ngains. We complement our benchmark with an in-depth analysis of feature\nrepresentation. Our findings highlight the crucial role of data curation\nquality in advancing multimodal medical AI. We release Open-PMC, along with the\ntrained models and our codebase.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-19.jsonl"}
