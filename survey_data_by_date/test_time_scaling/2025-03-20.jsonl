{"id": "2503.15299", "pdf": "https://arxiv.org/pdf/2503.15299", "abs": "https://arxiv.org/abs/2503.15299", "authors": ["Zorik Gekhman", "Eyal Ben David", "Hadas Orgad", "Eran Ofek", "Yonatan Belinkov", "Idan Szpector", "Jonathan Herzig", "Roi Reichart"], "title": "Inside-Out: Hidden Factual Knowledge in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a\nmodel can internally know an answer perfectly, yet fail to generate it even\nonce, despite large-scale repeated sampling of 1,000 answers. This reveals\nfundamental limitations in the generation capabilities of LLMs, which (3) puts\na practical constraint on scaling test-time compute via repeated answer\nsampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale", "test-time compute"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15299", "pdf": "https://arxiv.org/pdf/2503.15299", "abs": "https://arxiv.org/abs/2503.15299", "authors": ["Zorik Gekhman", "Eyal Ben David", "Hadas Orgad", "Eran Ofek", "Yonatan Belinkov", "Idan Szpector", "Jonathan Herzig", "Roi Reichart"], "title": "Inside-Out: Hidden Factual Knowledge in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a\nmodel can internally know an answer perfectly, yet fail to generate it even\nonce, despite large-scale repeated sampling of 1,000 answers. This reveals\nfundamental limitations in the generation capabilities of LLMs, which (3) puts\na practical constraint on scaling test-time compute via repeated answer\nsampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale", "test-time compute"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14564", "pdf": "https://arxiv.org/pdf/2503.14564", "abs": "https://arxiv.org/abs/2503.14564", "authors": ["Guowei Wang", "Changxing Ding"], "title": "Effortless Active Labeling for Long-Term Test-Time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025.Code:https://github.com/flash1803/EATTA", "summary": "Long-term test-time adaptation (TTA) is a challenging task due to error\naccumulation. Recent approaches tackle this issue by actively labeling a small\nproportion of samples in each batch, yet the annotation burden quickly grows as\nthe batch number increases. In this paper, we investigate how to achieve\neffortless active labeling so that a maximum of one sample is selected for\nannotation in each batch. First, we annotate the most valuable sample in each\nbatch based on the single-step optimization perspective in the TTA context. In\nthis scenario, the samples that border between the source- and target-domain\ndata distributions are considered the most feasible for the model to learn in\none iteration. Then, we introduce an efficient strategy to identify these\nsamples using feature perturbation. Second, we discover that the gradient\nmagnitudes produced by the annotated and unannotated samples have significant\nvariations. Therefore, we propose balancing their impact on model optimization\nusing two dynamic weights. Extensive experiments on the popular ImageNet-C, -R,\n-K, -A and PACS databases demonstrate that our approach consistently\noutperforms state-of-the-art methods with significantly lower annotation costs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15463", "pdf": "https://arxiv.org/pdf/2503.15463", "abs": "https://arxiv.org/abs/2503.15463", "authors": ["Jia-Nan Li", "Jian Guan", "Songhao Wu", "Wei Wu", "Rui Yan"], "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14919", "pdf": "https://arxiv.org/pdf/2503.14919", "abs": "https://arxiv.org/abs/2503.14919", "authors": ["Junyu Shi", "Lijiang Liu", "Yong Sun", "Zhiyuan Zhang", "Jinni Zhou", "Qiang Nie"], "title": "GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Scaling up motion datasets is crucial to enhance motion generation\ncapabilities. However, training on large-scale multi-source datasets introduces\ndata heterogeneity challenges due to variations in motion content. To address\nthis, we propose Generative Pretrained Multi-path Motion Model (GenM$^3$), a\ncomprehensive framework designed to learn unified motion representations.\nGenM$^3$ comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that\nadapts to different dataset distributions to learn a unified discrete motion\nrepresentation, and 2) a Multi-path Motion Transformer (MMT) that improves\nintra-modal representations by using separate modality-specific pathways, each\nwith densely activated experts to accommodate variations within that modality,\nand improves inter-modal alignment by the text-motion shared pathway. To enable\nlarge-scale training, we integrate and unify 11 high-quality motion datasets\n(approximately 220 hours of motion data) and augment it with textual\nannotations (nearly 10,000 motion sequences labeled by a large language model\nand 300+ by human experts). After training on our integrated dataset, GenM$^3$\nachieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark, surpassing\nstate-of-the-art methods by a large margin. It also demonstrates strong\nzero-shot generalization on IDEA400 dataset, highlighting its effectiveness and\nadaptability across diverse motion scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15106", "pdf": "https://arxiv.org/pdf/2503.15106", "abs": "https://arxiv.org/abs/2503.15106", "authors": ["Amir Hamza", "Andrea Caraffa", "Davide Boscaini", "Fabio Poiesi"], "title": "Distilling 3D distinctive local descriptors for 6D pose estimation", "categories": ["cs.CV"], "comment": "Project Website: https://tev-fbk.github.io/dGeDi/", "summary": "Three-dimensional local descriptors are crucial for encoding geometric\nsurface properties, making them essential for various point cloud understanding\ntasks. Among these descriptors, GeDi has demonstrated strong zero-shot 6D pose\nestimation capabilities but remains computationally impractical for real-world\napplications due to its expensive inference process. \\textit{Can we retain\nGeDi's effectiveness while significantly improving its efficiency?} In this\npaper, we explore this question by introducing a knowledge distillation\nframework that trains an efficient student model to regress local descriptors\nfrom a GeDi teacher. Our key contributions include: an efficient large-scale\ntraining procedure that ensures robustness to occlusions and partial\nobservations while operating under compute and storage constraints, and a novel\nloss formulation that handles weak supervision from non-distinctive teacher\ndescriptors. We validate our approach on five BOP Benchmark datasets and\ndemonstrate a significant reduction in inference time while maintaining\ncompetitive performance with existing methods, bringing zero-shot 6D pose\nestimation closer to real-time feasibility. Project Website:\nhttps://tev-fbk.github.io/dGeDi/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15293", "pdf": "https://arxiv.org/pdf/2503.15293", "abs": "https://arxiv.org/abs/2503.15293", "authors": ["Hangtao Zhang", "Yichen Wang", "Shihui Yan", "Chenyu Zhu", "Ziqi Zhou", "Linshan Hou", "Shengshan Hu", "Minghui Li", "Yanjun Zhang", "Leo Yu Zhang"], "title": "Test-Time Backdoor Detection for Object Detection Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Object detection models are vulnerable to backdoor attacks, where attackers\npoison a small subset of training samples by embedding a predefined trigger to\nmanipulate prediction. Detecting poisoned samples (i.e., those containing\ntriggers) at test time can prevent backdoor activation. However, unlike image\nclassification tasks, the unique characteristics of object detection --\nparticularly its output of numerous objects -- pose fresh challenges for\nbackdoor detection. The complex attack effects (e.g., \"ghost\" object emergence\nor \"vanishing\" object) further render current defenses fundamentally\ninadequate. To this end, we design TRAnsformation Consistency Evaluation\n(TRACE), a brand-new method for detecting poisoned samples at test time in\nobject detection. Our journey begins with two intriguing observations: (1)\npoisoned samples exhibit significantly more consistent detection results than\nclean ones across varied backgrounds. (2) clean samples show higher detection\nconsistency when introduced to different focal information. Based on these\nphenomena, TRACE applies foreground and background transformations to each test\nsample, then assesses transformation consistency by calculating the variance in\nobjects confidences. TRACE achieves black-box, universal backdoor detection,\nwith extensive experiments showing a 30% improvement in AUROC over\nstate-of-the-art defenses and resistance to adaptive attacks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14564", "pdf": "https://arxiv.org/pdf/2503.14564", "abs": "https://arxiv.org/abs/2503.14564", "authors": ["Guowei Wang", "Changxing Ding"], "title": "Effortless Active Labeling for Long-Term Test-Time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025.Code:https://github.com/flash1803/EATTA", "summary": "Long-term test-time adaptation (TTA) is a challenging task due to error\naccumulation. Recent approaches tackle this issue by actively labeling a small\nproportion of samples in each batch, yet the annotation burden quickly grows as\nthe batch number increases. In this paper, we investigate how to achieve\neffortless active labeling so that a maximum of one sample is selected for\nannotation in each batch. First, we annotate the most valuable sample in each\nbatch based on the single-step optimization perspective in the TTA context. In\nthis scenario, the samples that border between the source- and target-domain\ndata distributions are considered the most feasible for the model to learn in\none iteration. Then, we introduce an efficient strategy to identify these\nsamples using feature perturbation. Second, we discover that the gradient\nmagnitudes produced by the annotated and unannotated samples have significant\nvariations. Therefore, we propose balancing their impact on model optimization\nusing two dynamic weights. Extensive experiments on the popular ImageNet-C, -R,\n-K, -A and PACS databases demonstrate that our approach consistently\noutperforms state-of-the-art methods with significantly lower annotation costs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15463", "pdf": "https://arxiv.org/pdf/2503.15463", "abs": "https://arxiv.org/abs/2503.15463", "authors": ["Jia-Nan Li", "Jian Guan", "Songhao Wu", "Wei Wu", "Rui Yan"], "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14919", "pdf": "https://arxiv.org/pdf/2503.14919", "abs": "https://arxiv.org/abs/2503.14919", "authors": ["Junyu Shi", "Lijiang Liu", "Yong Sun", "Zhiyuan Zhang", "Jinni Zhou", "Qiang Nie"], "title": "GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Scaling up motion datasets is crucial to enhance motion generation\ncapabilities. However, training on large-scale multi-source datasets introduces\ndata heterogeneity challenges due to variations in motion content. To address\nthis, we propose Generative Pretrained Multi-path Motion Model (GenM$^3$), a\ncomprehensive framework designed to learn unified motion representations.\nGenM$^3$ comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that\nadapts to different dataset distributions to learn a unified discrete motion\nrepresentation, and 2) a Multi-path Motion Transformer (MMT) that improves\nintra-modal representations by using separate modality-specific pathways, each\nwith densely activated experts to accommodate variations within that modality,\nand improves inter-modal alignment by the text-motion shared pathway. To enable\nlarge-scale training, we integrate and unify 11 high-quality motion datasets\n(approximately 220 hours of motion data) and augment it with textual\nannotations (nearly 10,000 motion sequences labeled by a large language model\nand 300+ by human experts). After training on our integrated dataset, GenM$^3$\nachieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark, surpassing\nstate-of-the-art methods by a large margin. It also demonstrates strong\nzero-shot generalization on IDEA400 dataset, highlighting its effectiveness and\nadaptability across diverse motion scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15106", "pdf": "https://arxiv.org/pdf/2503.15106", "abs": "https://arxiv.org/abs/2503.15106", "authors": ["Amir Hamza", "Andrea Caraffa", "Davide Boscaini", "Fabio Poiesi"], "title": "Distilling 3D distinctive local descriptors for 6D pose estimation", "categories": ["cs.CV"], "comment": "Project Website: https://tev-fbk.github.io/dGeDi/", "summary": "Three-dimensional local descriptors are crucial for encoding geometric\nsurface properties, making them essential for various point cloud understanding\ntasks. Among these descriptors, GeDi has demonstrated strong zero-shot 6D pose\nestimation capabilities but remains computationally impractical for real-world\napplications due to its expensive inference process. \\textit{Can we retain\nGeDi's effectiveness while significantly improving its efficiency?} In this\npaper, we explore this question by introducing a knowledge distillation\nframework that trains an efficient student model to regress local descriptors\nfrom a GeDi teacher. Our key contributions include: an efficient large-scale\ntraining procedure that ensures robustness to occlusions and partial\nobservations while operating under compute and storage constraints, and a novel\nloss formulation that handles weak supervision from non-distinctive teacher\ndescriptors. We validate our approach on five BOP Benchmark datasets and\ndemonstrate a significant reduction in inference time while maintaining\ncompetitive performance with existing methods, bringing zero-shot 6D pose\nestimation closer to real-time feasibility. Project Website:\nhttps://tev-fbk.github.io/dGeDi/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15293", "pdf": "https://arxiv.org/pdf/2503.15293", "abs": "https://arxiv.org/abs/2503.15293", "authors": ["Hangtao Zhang", "Yichen Wang", "Shihui Yan", "Chenyu Zhu", "Ziqi Zhou", "Linshan Hou", "Shengshan Hu", "Minghui Li", "Yanjun Zhang", "Leo Yu Zhang"], "title": "Test-Time Backdoor Detection for Object Detection Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Object detection models are vulnerable to backdoor attacks, where attackers\npoison a small subset of training samples by embedding a predefined trigger to\nmanipulate prediction. Detecting poisoned samples (i.e., those containing\ntriggers) at test time can prevent backdoor activation. However, unlike image\nclassification tasks, the unique characteristics of object detection --\nparticularly its output of numerous objects -- pose fresh challenges for\nbackdoor detection. The complex attack effects (e.g., \"ghost\" object emergence\nor \"vanishing\" object) further render current defenses fundamentally\ninadequate. To this end, we design TRAnsformation Consistency Evaluation\n(TRACE), a brand-new method for detecting poisoned samples at test time in\nobject detection. Our journey begins with two intriguing observations: (1)\npoisoned samples exhibit significantly more consistent detection results than\nclean ones across varied backgrounds. (2) clean samples show higher detection\nconsistency when introduced to different focal information. Based on these\nphenomena, TRACE applies foreground and background transformations to each test\nsample, then assesses transformation consistency by calculating the variance in\nobjects confidences. TRACE achieves black-box, universal backdoor detection,\nwith extensive experiments showing a 30% improvement in AUROC over\nstate-of-the-art defenses and resistance to adaptive attacks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14526", "pdf": "https://arxiv.org/pdf/2503.14526", "abs": "https://arxiv.org/abs/2503.14526", "authors": ["Yu Fang", "Yue Yang", "Xinghao Zhu", "Kaiyuan Zheng", "Gedas Bertasius", "Daniel Szafir", "Mingyu Ding"], "title": "ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis", "categories": ["cs.CV", "cs.GR", "cs.RO"], "comment": "Website: https://yuffish.github.io/rebot/", "summary": "Vision-language-action (VLA) models present a promising paradigm by training\npolicies directly on real robot datasets like Open X-Embodiment. However, the\nhigh cost of real-world data collection hinders further data scaling, thereby\nrestricting the generalizability of VLAs. In this paper, we introduce ReBot, a\nnovel real-to-sim-to-real approach for scaling real robot datasets and adapting\nVLA models to target domains, which is the last-mile deployment challenge in\nrobot manipulation. Specifically, ReBot replays real-world robot trajectories\nin simulation to diversify manipulated objects (real-to-sim), and integrates\nthe simulated movements with inpainted real-world background to synthesize\nphysically realistic and temporally consistent robot videos (sim-to-real). Our\napproach has several advantages: 1) it enjoys the benefit of real data to\nminimize the sim-to-real gap; 2) it leverages the scalability of simulation;\nand 3) it can generalize a pretrained VLA to a target domain with fully\nautomated data pipelines. Extensive experiments in both simulation and\nreal-world environments show that ReBot significantly enhances the performance\nand robustness of VLAs. For example, in SimplerEnv with the WidowX robot, ReBot\nimproved the in-domain performance of Octo by 7.2% and OpenVLA by 21.8%, and\nout-of-domain generalization by 19.9% and 9.4%, respectively. For real-world\nevaluation with a Franka robot, ReBot increased the success rates of Octo by\n17% and OpenVLA by 20%. More information can be found at:\nhttps://yuffish.github.io/rebot/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14698", "pdf": "https://arxiv.org/pdf/2503.14698", "abs": "https://arxiv.org/abs/2503.14698", "authors": ["Yiming Wang", "Lucy Chai", "Xuan Luo", "Michael Niemeyer", "Manuel Lagunas", "Stephen Lombardi", "Siyu Tang", "Tiancheng Sun"], "title": "SplatVoxel: History-Aware Novel View Streaming without Temporal Training", "categories": ["cs.CV"], "comment": null, "summary": "We study the problem of novel view streaming from sparse-view videos, which\naims to generate a continuous sequence of high-quality, temporally consistent\nnovel views as new input frames arrive. However, existing novel view synthesis\nmethods struggle with temporal coherence and visual fidelity, leading to\nflickering and inconsistency. To address these challenges, we introduce\nhistory-awareness, leveraging previous frames to reconstruct the scene and\nimprove quality and stability. We propose a hybrid splat-voxel feed-forward\nscene reconstruction approach that combines Gaussian Splatting to propagate\ninformation over time, with a hierarchical voxel grid for temporal fusion.\nGaussian primitives are efficiently warped over time using a motion graph that\nextends 2D tracking models to 3D motion, while a sparse voxel transformer\nintegrates new temporal observations in an error-aware manner. Crucially, our\nmethod does not require training on multi-view video datasets, which are\ncurrently limited in size and diversity, and can be directly applied to\nsparse-view video streams in a history-aware manner at inference time. Our\napproach achieves state-of-the-art performance in both static and streaming\nscene reconstruction, effectively reducing temporal artifacts and visual\nartifacts while running at interactive rates (15 fps with 350ms delay) on a\nsingle H100 GPU. Project Page: https://19reborn.github.io/SplatVoxel/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14774", "pdf": "https://arxiv.org/pdf/2503.14774", "abs": "https://arxiv.org/abs/2503.14774", "authors": ["David Serrano-Lozano", "Aditya Arora", "Luis Herranz", "Konstantinos G. Derpanis", "Michael S. Brown", "Javier Vazquez-Corral"], "title": "Revisiting Image Fusion for Multi-Illuminant White-Balance Correction", "categories": ["cs.CV"], "comment": "10 pages", "summary": "White balance (WB) correction in scenes with multiple illuminants remains a\npersistent challenge in computer vision. Recent methods explored fusion-based\napproaches, where a neural network linearly blends multiple sRGB versions of an\ninput image, each processed with predefined WB presets. However, we demonstrate\nthat these methods are suboptimal for common multi-illuminant scenarios.\nAdditionally, existing fusion-based methods rely on sRGB WB datasets lacking\ndedicated multi-illuminant images, limiting both training and evaluation. To\naddress these challenges, we introduce two key contributions. First, we propose\nan efficient transformer-based model that effectively captures spatial\ndependencies across sRGB WB presets, substantially improving upon linear fusion\ntechniques. Second, we introduce a large-scale multi-illuminant dataset\ncomprising over 16,000 sRGB images rendered with five different WB settings,\nalong with WB-corrected images. Our method achieves up to 100\\% improvement\nover existing techniques on our new multi-illuminant image fusion dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15351", "pdf": "https://arxiv.org/pdf/2503.15351", "abs": "https://arxiv.org/abs/2503.15351", "authors": ["I-Fan Lin", "Faegheh Hasibi", "Suzan Verberne"], "title": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15354", "pdf": "https://arxiv.org/pdf/2503.15354", "abs": "https://arxiv.org/abs/2503.15354", "authors": ["Yining Lu", "Noah Ziems", "Hy Dang", "Meng Jiang"], "title": "Optimizing Decomposition for Optimal Claim Verification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14867", "pdf": "https://arxiv.org/pdf/2503.14867", "abs": "https://arxiv.org/abs/2503.14867", "authors": ["Caoshuo Li", "Tanzhe Li", "Xiaobin Hu", "Donghao Luo", "Taisong Jin"], "title": "DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recently, Vision Graph Neural Network (ViG) has gained considerable attention\nin computer vision. Despite its groundbreaking innovation, Vision Graph Neural\nNetwork encounters key issues including the quadratic computational complexity\ncaused by its K-Nearest Neighbor (KNN) graph construction and the limitation of\npairwise relations of normal graphs. To address the aforementioned challenges,\nwe propose a novel vision architecture, termed Dilated Vision HyperGraph Neural\nNetwork (DVHGNN), which is designed to leverage multi-scale hypergraph to\nefficiently capture high-order correlations among objects. Specifically, the\nproposed method tailors Clustering and Dilated HyperGraph Construction (DHGC)\nto adaptively capture multi-scale dependencies among the data samples.\nFurthermore, a dynamic hypergraph convolution mechanism is proposed to\nfacilitate adaptive feature exchange and fusion at the hypergraph level.\nExtensive qualitative and quantitative evaluations of the benchmark image\ndatasets demonstrate that the proposed DVHGNN significantly outperforms the\nstate-of-the-art vision backbones. For instance, our DVHGNN-S achieves an\nimpressive top-1 accuracy of 83.1% on ImageNet-1K, surpassing ViG-S by +1.0%\nand ViHGNN-S by +0.6%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14895", "pdf": "https://arxiv.org/pdf/2503.14895", "abs": "https://arxiv.org/abs/2503.14895", "authors": ["Shuo Li", "Jiajun Sun", "Guodong Zheng", "Xiaoran Fan", "Yujiong Shen", "Yi Lu", "Zhiheng Xi", "Yuming Yang", "Wenming Tan", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14911", "pdf": "https://arxiv.org/pdf/2503.14911", "abs": "https://arxiv.org/abs/2503.14911", "authors": ["Siyuan Yan", "Ming Hu", "Yiwen Jiang", "Xieji Li", "Hao Fei", "Philipp Tschandl", "Harald Kittler", "Zongyuan Ge"], "title": "Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology", "categories": ["cs.CV"], "comment": "23 pages", "summary": "The emergence of vision-language models has transformed medical AI, enabling\nunprecedented advances in diagnostic capability and clinical applications.\nHowever, progress in dermatology has lagged behind other medical domains due to\nthe lack of standard image-text pairs. Existing dermatological datasets are\nlimited in both scale and depth, offering only single-label annotations across\na narrow range of diseases instead of rich textual descriptions, and lacking\nthe crucial clinical context needed for real-world applications. To address\nthese limitations, we present Derm1M, the first large-scale vision-language\ndataset for dermatology, comprising 1,029,761 image-text pairs. Built from\ndiverse educational resources and structured around a standard ontology\ncollaboratively developed by experts, Derm1M provides comprehensive coverage\nfor over 390 skin conditions across four hierarchical levels and 130 clinical\nconcepts with rich contextual information such as medical history, symptoms,\nand skin tone. To demonstrate Derm1M potential in advancing both AI research\nand clinical application, we pretrained a series of CLIP-like models,\ncollectively called DermLIP, on this dataset. The DermLIP family significantly\noutperforms state-of-the-art foundation models on eight diverse datasets across\nmultiple tasks, including zero-shot skin disease classification, clinical and\nartifacts concept identification, few-shot/full-shot learning, and cross-modal\nretrieval. Our dataset and code will be public.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14559", "pdf": "https://arxiv.org/pdf/2503.14559", "abs": "https://arxiv.org/abs/2503.14559", "authors": ["Weixiong Lin", "Chen Ju", "Haicheng Wang", "Shengchao Hu", "Shuai Xiao", "Mengting Chen", "Yuheng Jiao", "Mingshuai Yao", "Jinsong Lan", "Qingwen Liu", "Ying Chen"], "title": "Squeeze Out Tokens from Sample for Finer-Grained Data Governance", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Widely observed data scaling laws, in which error falls off as a power of the\ntraining size, demonstrate the diminishing returns of unselective data\nexpansion. Hence, data governance is proposed to downsize datasets through\npruning non-informative samples. Yet, isolating the impact of a specific sample\non overall model performance is challenging, due to the vast computation\nrequired for tryout all sample combinations. Current data governors circumvent\nthis complexity by estimating sample contributions through heuristic-derived\nscalar scores, thereby discarding low-value ones. Despite thorough sample\nsieving, retained samples contain substantial undesired tokens intrinsically,\nunderscoring the potential for further compression and purification. In this\nwork, we upgrade data governance from a 'sieving' approach to a 'juicing' one.\nInstead of scanning for least-flawed samples, our dual-branch DataJuicer\napplies finer-grained intra-sample governance. It squeezes out informative\ntokens and boosts image-text alignments. Specifically, the vision branch\nretains salient image patches and extracts relevant object classes, while the\ntext branch incorporates these classes to enhance captions. Consequently,\nDataJuicer yields more refined datasets through finer-grained governance.\nExtensive experiments across datasets demonstrate that DataJuicer significantly\noutperforms existing DataSieve in image-text retrieval, classification, and\ndense visual reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14895", "pdf": "https://arxiv.org/pdf/2503.14895", "abs": "https://arxiv.org/abs/2503.14895", "authors": ["Shuo Li", "Jiajun Sun", "Guodong Zheng", "Xiaoran Fan", "Yujiong Shen", "Yi Lu", "Zhiheng Xi", "Yuming Yang", "Wenming Tan", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14941", "pdf": "https://arxiv.org/pdf/2503.14941", "abs": "https://arxiv.org/abs/2503.14941", "authors": ["Qihui Zhang", "Munan Ning", "Zheyuan Liu", "Yanbo Wang", "Jiayi Ye", "Yue Huang", "Shuo Yang", "Xiao Chen", "Yibing Song", "Li Yuan"], "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation", "question answering"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15166", "pdf": "https://arxiv.org/pdf/2503.15166", "abs": "https://arxiv.org/abs/2503.15166", "authors": ["Ã€lex Pujol Vidal", "Sergio Escalera", "Kamal Nasrollahi", "Thomas B. Moeslund"], "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Preprint", "summary": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14953", "pdf": "https://arxiv.org/pdf/2503.14953", "abs": "https://arxiv.org/abs/2503.14953", "authors": ["Yang Liu", "Wentao Feng", "Zhuoyao Liu", "Shudong Huang", "Jiancheng Lv"], "title": "Aligning Information Capacity Between Vision and Language via Dense-to-Sparse Feature Distillation for Image-Text Matching", "categories": ["cs.CV"], "comment": null, "summary": "Enabling Visual Semantic Models to effectively handle multi-view description\nmatching has been a longstanding challenge. Existing methods typically learn a\nset of embeddings to find the optimal match for each view's text and compute\nsimilarity. However, the visual and text embeddings learned through these\napproaches have limited information capacity and are prone to interference from\nlocally similar negative samples. To address this issue, we argue that the\ninformation capacity of embeddings is crucial and propose Dense-to-Sparse\nFeature Distilled Visual Semantic Embedding (D2S-VSE), which enhances the\ninformation capacity of sparse text by leveraging dense text distillation.\nSpecifically, D2S-VSE is a two-stage framework. In the pre-training stage, we\nalign images with dense text to enhance the information capacity of visual\nsemantic embeddings. In the fine-tuning stage, we optimize two tasks\nsimultaneously, distilling dense text embeddings to sparse text embeddings\nwhile aligning images and sparse texts, enhancing the information capacity of\nsparse text embeddings. Our proposed D2S-VSE model is extensively evaluated on\nthe large-scale MS-COCO and Flickr30K datasets, demonstrating its superiority\nover recent state-of-the-art methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14955", "pdf": "https://arxiv.org/pdf/2503.14955", "abs": "https://arxiv.org/abs/2503.14955", "authors": ["Bike Chen", "Antti TikanmÃ¤ki", "Juha RÃ¶ning"], "title": "Depth-Aware Range Image-Based Model for Point Cloud Segmentation", "categories": ["cs.CV"], "comment": "No Comments", "summary": "Point cloud segmentation (PCS) aims to separate points into different and\nmeaningful groups. The task plays an important role in robotics because PCS\nenables robots to understand their physical environments directly. To process\nsparse and large-scale outdoor point clouds in real time, range image-based\nmodels are commonly adopted. However, in a range image, the lack of explicit\ndepth information inevitably causes some separate objects in 3D space to touch\neach other, bringing difficulty for the range image-based models in correctly\nsegmenting the objects. Moreover, previous PCS models are usually derived from\nthe existing color image-based models and unable to make full use of the\nimplicit but ordered depth information inherent in the range image, thereby\nachieving inferior performance. In this paper, we propose Depth-Aware Module\n(DAM) and Fast FMVNet V3. DAM perceives the ordered depth information in the\nrange image by explicitly modelling the interdependence among channels. Fast\nFMVNet V3 incorporates DAM by integrating it into the last block in each\narchitecture stage. Extensive experiments conducted on SemanticKITTI, nuScenes,\nand SemanticPOSS demonstrate that DAM brings a significant improvement for Fast\nFMVNet V3 with negligible computational cost.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14960", "pdf": "https://arxiv.org/pdf/2503.14960", "abs": "https://arxiv.org/abs/2503.14960", "authors": ["Seungyeon Cho", "Tae-Kyun Kim"], "title": "Body-Hand Modality Expertized Networks with Cross-attention for Fine-grained Skeleton Action Recognition", "categories": ["cs.CV"], "comment": "7 figures, 8 pages", "summary": "Skeleton-based Human Action Recognition (HAR) is a vital technology in\nrobotics and human-robot interaction. However, most existing methods\nconcentrate primarily on full-body movements and often overlook subtle hand\nmotions that are critical for distinguishing fine-grained actions. Recent work\nleverages a unified graph representation that combines body, hand, and foot\nkeypoints to capture detailed body dynamics. Yet, these models often blur fine\nhand details due to the disparity between body and hand action characteristics\nand the loss of subtle features during the spatial-pooling. In this paper, we\npropose BHaRNet (Body-Hand action Recognition Network), a novel framework that\naugments a typical body-expert model with a hand-expert model. Our model\njointly trains both streams with an ensemble loss that fosters cooperative\nspecialization, functioning in a manner reminiscent of a Mixture-of-Experts\n(MoE). Moreover, cross-attention is employed via an expertized branch method\nand a pooling-attention module to enable feature-level interactions and\nselectively fuse complementary information. Inspired by MMNet, we also\ndemonstrate the applicability of our approach to multi-modal tasks by\nleveraging RGB information, where body features guide RGB learning to capture\nricher contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60,\nNTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet\nachieves SOTA accuracies -- improving from 86.4\\% to 93.0\\% in hand-intensive\nactions -- while maintaining fewer GFLOPs and parameters than the relevant\nunified methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14983", "pdf": "https://arxiv.org/pdf/2503.14983", "abs": "https://arxiv.org/abs/2503.14983", "authors": ["Zanting Ye", "Xiaolong Niu", "Xuanbin Wu", "Wenxiang Yi", "Yuan Chang", "Lijun Lu"], "title": "Semi-KAN: KAN Provides an Effective Representation for Semi-Supervised Learning in Medical Image Segmentation", "categories": ["cs.CV"], "comment": "18 pages, 7 figures, 6 tables", "summary": "Deep learning-based medical image segmentation has shown remarkable success;\nhowever, it typically requires extensive pixel-level annotations, which are\nboth expensive and time-intensive. Semi-supervised medical image segmentation\n(SSMIS) offers a viable alternative, driven by advancements in CNNs and ViTs.\nHowever, these networks often rely on single fixed activation functions and\nlinear modeling patterns, limiting their ability to effectively learn robust\nrepresentations. Given the limited availability of labeled date, achieving\nrobust representation learning becomes crucial. Inspired by Kolmogorov-Arnold\nNetworks (KANs), we propose Semi-KAN, which leverages the untapped potential of\nKANs to enhance backbone architectures for representation learning in SSMIS.\nOur findings indicate that: (1) compared to networks with fixed activation\nfunctions, KANs exhibit superior representation learning capabilities with\nfewer parameters, and (2) KANs excel in high-semantic feature spaces. Building\non these insights, we integrate KANs into tokenized intermediate\nrepresentations, applying them selectively at the encoder's bottleneck and the\ndecoder's top layers within a U-Net pipeline to extract high-level semantic\nfeatures. Although learnable activation functions improve feature expansion,\nthey introduce significant computational overhead with only marginal\nperformance gains. To mitigate this, we reduce the feature dimensions and\nemploy horizontal scaling to capture multiple pattern representations.\nFurthermore, we design a multi-branch U-Net architecture with uncertainty\nestimation to effectively learn diverse pattern representations. Extensive\nexperiments on four public datasets demonstrate that Semi-KAN surpasses\nbaseline networks, utilizing fewer KAN layers and lower computational cost,\nthereby underscoring the potential of KANs as a promising approach for SSMIS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14998", "pdf": "https://arxiv.org/pdf/2503.14998", "abs": "https://arxiv.org/abs/2503.14998", "authors": ["Marta Hasny", "Maxime Di Folco", "Keno Bressem", "Julia Schnabel"], "title": "TGV: Tabular Data-Guided Learning of Visual Cardiac Representations", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning methods in computer vision typically rely on different\nviews of the same image to form pairs. However, in medical imaging, we often\nseek to compare entire patients with different phenotypes rather than just\nmultiple augmentations of one scan. We propose harnessing clinically relevant\ntabular data to identify distinct patient phenotypes and form more meaningful\npairs in a contrastive learning framework. Our method uses tabular attributes\nto guide the training of visual representations, without requiring a joint\nembedding space. We demonstrate its strength using short-axis cardiac MR images\nand clinical attributes from the UK Biobank, where tabular data helps to more\neffectively distinguish between patient subgroups. Evaluation on downstream\ntasks, including fine-tuning and zero-shot prediction of cardiovascular artery\ndiseases and cardiac phenotypes, shows that incorporating tabular data yields\nstronger visual representations than conventional methods that rely solely on\nimage augmentations or combined image-tabular embeddings. Furthermore, we\ndemonstrate that image encoders trained with tabular guidance are capable of\nembedding demographic information in their representations, allowing them to\nuse insights from tabular data for unimodal predictions, making them\nwell-suited to real-world medical settings where extensive clinical annotations\nmay not be routinely available at inference time. The code will be available on\nGitHub.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15166", "pdf": "https://arxiv.org/pdf/2503.15166", "abs": "https://arxiv.org/abs/2503.15166", "authors": ["Ã€lex Pujol Vidal", "Sergio Escalera", "Kamal Nasrollahi", "Thomas B. Moeslund"], "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Preprint", "summary": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15285", "pdf": "https://arxiv.org/pdf/2503.15285", "abs": "https://arxiv.org/abs/2503.15285", "authors": ["Yuanchao Yue", "Zhengxin Li", "Wei Zhang", "Hui Yuan"], "title": "PAPI-Reg: Patch-to-Pixel Solution for Efficient Cross-Modal Registration between LiDAR Point Cloud and Camera Image", "categories": ["cs.CV"], "comment": null, "summary": "The primary requirement for cross-modal data fusion is the precise alignment\nof data from different sensors. However, the calibration between LiDAR point\nclouds and camera images is typically time-consuming and needs external\ncalibration board or specific environmental features. Cross-modal registration\neffectively solves this problem by aligning the data directly without requiring\nexternal calibration. However, due to the domain gap between the point cloud\nand the image, existing methods rarely achieve satisfactory registration\naccuracy while maintaining real-time performance. To address this issue, we\npropose a framework that projects point clouds into several 2D representations\nfor matching with camera images, which not only leverages the geometric\ncharacteristic of LiDAR point clouds more effectively but also bridge the\ndomain gap between the point cloud and image. Moreover, to tackle the\nchallenges of cross modal differences and the limited overlap between LiDAR\npoint clouds and images in the image matching task, we introduce a multi-scale\nfeature extraction network to effectively extract features from both camera\nimages and the projection maps of LiDAR point cloud. Additionally, we propose a\npatch-to-pixel matching network to provide more effective supervision and\nachieve higher accuracy. We validate the performance of our model through\nexperiments on the KITTI and nuScenes datasets. Our network achieves real-time\nperformance and extremely high registration accuracy. On the KITTI dataset, our\nmodel achieves a registration accuracy rate of over 99\\%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15300", "pdf": "https://arxiv.org/pdf/2503.15300", "abs": "https://arxiv.org/abs/2503.15300", "authors": ["Weixiao Gao", "Liangliang Nan", "Hugo Ledoux"], "title": "SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes", "categories": ["cs.CV"], "comment": "22 pages, 24 figures", "summary": "Semantic segmentation in urban scene analysis has mainly focused on images or\npoint clouds, while textured meshes - offering richer spatial representation -\nremain underexplored. This paper introduces SUM Parts, the first large-scale\ndataset for urban textured meshes with part-level semantic labels, covering\nabout 2.5 km2 with 21 classes. The dataset was created using our own annotation\ntool, which supports both face- and texture-based annotations with efficient\ninteractive selection. We also provide a comprehensive evaluation of 3D\nsemantic segmentation and interactive annotation methods on this dataset. Our\nproject page is available at https://tudelft3d.github.io/SUMParts/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15369", "pdf": "https://arxiv.org/pdf/2503.15369", "abs": "https://arxiv.org/abs/2503.15369", "authors": ["Yinan Liang", "Ziwei Wang", "Xiuwei Xu", "Jie Zhou", "Jiwen Lu"], "title": "EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "While multimodal large language models demonstrate strong performance in\ncomplex reasoning tasks, they pose significant challenges related to model\ncomplexity during deployment, especially for resource-limited devices. In this\npaper, we propose an automatic pruning method for large vision-language models\nto enhance the efficiency of multimodal reasoning. Conventional methods rely on\nthe training data of the original model to select the proper pruning ratio for\ndifferent network components. However, these methods are impractical for large\nvision-language models due to the unaffordable search costs caused by web-scale\ntraining corpus. In contrast, our approach only leverages a small number of\nsamples to search for the desired pruning policy by maximizing its\ngeneralization ability on unknown training data while maintaining the model\naccuracy, which enables the achievement of an optimal trade-off between\naccuracy and efficiency for large visual language models. Specifically, we\nformulate the generalization gap of the pruning strategy using the structural\nrisk minimization principle. Based on both task performance and generalization\ncapability, we iteratively search for the optimal pruning policy within a given\nsearch space and optimize the vision projector to evolve the search space with\nhigher upper bound of performance. We conduct extensive experiments on the\nScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual\nquestion answering. Using only 64 samples for pruning policy search,\nEfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a\n$\\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15406", "pdf": "https://arxiv.org/pdf/2503.15406", "abs": "https://arxiv.org/abs/2503.15406", "authors": ["Jisu Nam", "Soowon Son", "Zhan Xu", "Jing Shi", "Difan Liu", "Feng Liu", "Aashish Misraa", "Seungryong Kim", "Yang Zhou"], "title": "Visual Persona: Foundation Model for Full-Body Human Customization", "categories": ["cs.CV"], "comment": "CVPR 2025, Project page is available at\n  https://cvlab-kaist.github.io/Visual-Persona", "summary": "We introduce Visual Persona, a foundation model for text-to-image full-body\nhuman customization that, given a single in-the-wild human image, generates\ndiverse images of the individual guided by text descriptions. Unlike prior\nmethods that focus solely on preserving facial identity, our approach captures\ndetailed full-body appearance, aligning with text descriptions for body\nstructure and scene variations. Training this model requires large-scale paired\nhuman data, consisting of multiple images per individual with consistent\nfull-body identities, which is notoriously difficult to obtain. To address\nthis, we propose a data curation pipeline leveraging vision-language models to\nevaluate full-body appearance consistency, resulting in Visual Persona-500K, a\ndataset of 580k paired human images across 100k unique identities. For precise\nappearance transfer, we introduce a transformer encoder-decoder architecture\nadapted to a pre-trained text-to-image diffusion model, which augments the\ninput image into distinct body regions, encodes these regions as local\nappearance features, and projects them into dense identity embeddings\nindependently to condition the diffusion model for synthesizing customized\nimages. Visual Persona consistently surpasses existing approaches, generating\nhigh-quality, customized images from in-the-wild inputs. Extensive ablation\nstudies validate design choices, and we demonstrate the versatility of Visual\nPersona across various downstream tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15412", "pdf": "https://arxiv.org/pdf/2503.15412", "abs": "https://arxiv.org/abs/2503.15412", "authors": ["Fereshteh Forghani", "Jason J. Yu", "Tristan Aumentado-Armstrong", "Konstantinos G. Derpanis", "Marcus A. Brubaker"], "title": "Learn Your Scales: Towards Scale-Consistent Generative Novel View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Conventional depth-free multi-view datasets are captured using a moving\nmonocular camera without metric calibration. The scales of camera positions in\nthis monocular setting are ambiguous. Previous methods have acknowledged scale\nambiguity in multi-view data via various ad-hoc normalization pre-processing\nsteps, but have not directly analyzed the effect of incorrect scene scales on\ntheir application. In this paper, we seek to understand and address the effect\nof scale ambiguity when used to train generative novel view synthesis methods\n(GNVS). In GNVS, new views of a scene or object can be minimally synthesized\ngiven a single image and are, thus, unconstrained, necessitating the use of\ngenerative methods. The generative nature of these models captures all aspects\nof uncertainty, including any uncertainty of scene scales, which act as\nnuisance variables for the task. We study the effect of scene scale ambiguity\nin GNVS when sampled from a single image by isolating its effect on the\nresulting models and, based on these intuitions, define new metrics that\nmeasure the scale inconsistency of generated views. We then propose a framework\nto estimate scene scales jointly with the GNVS model in an end-to-end fashion.\nEmpirically, we show that our method reduces the scale inconsistency of\ngenerated views without the complexity or downsides of previous scale\nnormalization methods. Further, we show that removing this ambiguity improves\ngenerated image quality of the resulting GNVS model.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15457", "pdf": "https://arxiv.org/pdf/2503.15457", "abs": "https://arxiv.org/abs/2503.15457", "authors": ["Yuanzhi Zhu", "Xi Wang", "StÃ©phane LathuiliÃ¨re", "Vicky Kalogeiton"], "title": "Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step Generator", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling\ntechnique. Despite their remarkable results, they typically suffer from slow\ninference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a\nnovel approach that distills masked diffusion models into a one-step generator.\nDi$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using\nintermediate-step information for one-step generation, which we solve through\ntoken-level distribution matching that optimizes model output logits by an\n'on-policy framework' with the help of an auxiliary model; and (2) the lack of\nentropy in the initial distribution, which we address through a token\ninitialization strategy that injects randomness while maintaining similarity to\nteacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on\nboth class-conditional and text-conditional image generation, impressively\nachieving performance competitive to multi-step teacher outputs while\ndrastically reducing inference time. To our knowledge, we are the first to\nsuccessfully achieve one-step distillation of masked diffusion models and the\nfirst to apply discrete distillation to text-to-image generation, opening new\npaths for efficient generative modeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15470", "pdf": "https://arxiv.org/pdf/2503.15470", "abs": "https://arxiv.org/abs/2503.15470", "authors": ["Boshen Xu", "Yuting Mei", "Xinbi Liu", "Sipeng Zheng", "Qin Jin"], "title": "EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": "Code will be released at: https://github.com/xuboshen/EgoDTM", "summary": "Egocentric video-language pretraining has significantly advanced video\nrepresentation learning. Humans perceive and interact with a fully 3D world,\ndeveloping spatial awareness that extends beyond text-based understanding.\nHowever, most previous works learn from 1D text or 2D visual cues, such as\nbounding boxes, which inherently lack 3D understanding. To bridge this gap, we\nintroduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained\nthrough large-scale 3D-aware video pretraining and video-text contrastive\nlearning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently\nlearn 3D-awareness from pseudo depth maps generated by depth estimation models.\nTo further facilitate 3D-aware video pretraining, we enrich the original brief\ncaptions with hand-object visual cues by organically combining several\nfoundation models. Extensive experiments demonstrate EgoDTM's superior\nperformance across diverse downstream tasks, highlighting its superior 3D-aware\nvisual understanding. Our code will be released at\nhttps://github.com/xuboshen/EgoDTM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14559", "pdf": "https://arxiv.org/pdf/2503.14559", "abs": "https://arxiv.org/abs/2503.14559", "authors": ["Weixiong Lin", "Chen Ju", "Haicheng Wang", "Shengchao Hu", "Shuai Xiao", "Mengting Chen", "Yuheng Jiao", "Mingshuai Yao", "Jinsong Lan", "Qingwen Liu", "Ying Chen"], "title": "Squeeze Out Tokens from Sample for Finer-Grained Data Governance", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Widely observed data scaling laws, in which error falls off as a power of the\ntraining size, demonstrate the diminishing returns of unselective data\nexpansion. Hence, data governance is proposed to downsize datasets through\npruning non-informative samples. Yet, isolating the impact of a specific sample\non overall model performance is challenging, due to the vast computation\nrequired for tryout all sample combinations. Current data governors circumvent\nthis complexity by estimating sample contributions through heuristic-derived\nscalar scores, thereby discarding low-value ones. Despite thorough sample\nsieving, retained samples contain substantial undesired tokens intrinsically,\nunderscoring the potential for further compression and purification. In this\nwork, we upgrade data governance from a 'sieving' approach to a 'juicing' one.\nInstead of scanning for least-flawed samples, our dual-branch DataJuicer\napplies finer-grained intra-sample governance. It squeezes out informative\ntokens and boosts image-text alignments. Specifically, the vision branch\nretains salient image patches and extracts relevant object classes, while the\ntext branch incorporates these classes to enhance captions. Consequently,\nDataJuicer yields more refined datasets through finer-grained governance.\nExtensive experiments across datasets demonstrate that DataJuicer significantly\noutperforms existing DataSieve in image-text retrieval, classification, and\ndense visual reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15058", "pdf": "https://arxiv.org/pdf/2503.15058", "abs": "https://arxiv.org/abs/2503.15058", "authors": ["Francesco Di Feola", "Ludovica Pompilio", "Cecilia Assolito", "Valerio Guarrasi", "Paolo Soda"], "title": "Texture-Aware StarGAN for CT data harmonisation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Computed Tomography (CT) plays a pivotal role in medical diagnosis; however,\nvariability across reconstruction kernels hinders data-driven approaches, such\nas deep learning models, from achieving reliable and generalized performance.\nTo this end, CT data harmonization has emerged as a promising solution to\nminimize such non-biological variances by standardizing data across different\nsources or conditions. In this context, Generative Adversarial Networks (GANs)\nhave proved to be a powerful framework for harmonization, framing it as a\nstyle-transfer problem. However, GAN-based approaches still face limitations in\ncapturing complex relationships within the images, which are essential for\neffective harmonization. In this work, we propose a novel texture-aware StarGAN\nfor CT data harmonization, enabling one-to-many translations across different\nreconstruction kernels. Although the StarGAN model has been successfully\napplied in other domains, its potential for CT data harmonization remains\nunexplored. Furthermore, our approach introduces a multi-scale texture loss\nfunction that embeds texture information across different spatial and angular\nscales into the harmonization process, effectively addressing kernel-induced\ntexture variations. We conducted extensive experimentation on a publicly\navailable dataset, utilizing a total of 48667 chest CT slices from 197 patients\ndistributed over three different reconstruction kernels, demonstrating the\nsuperiority of our method over the baseline StarGAN.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15390", "pdf": "https://arxiv.org/pdf/2503.15390", "abs": "https://arxiv.org/abs/2503.15390", "authors": ["Yumin Zhang", "Yan Gao", "Haoran Duan", "Hanqing Guo", "Tejal Shah", "Rajiv Ranjan", "Bo Wei"], "title": "FedSCA: Federated Tuning with Similarity-guided Collaborative Aggregation for Heterogeneous Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Transformer-based foundation models (FMs) have recently demonstrated\nremarkable performance in medical image segmentation. However, scaling these\nmodels is challenging due to the limited size of medical image datasets within\nisolated hospitals, where data centralization is restricted due to privacy\nconcerns. These constraints, combined with the data-intensive nature of FMs,\nhinder their broader application. Integrating federated learning (FL) with\nfoundation models (FLFM) fine-tuning offers a potential solution to these\nchallenges by enabling collaborative model training without data sharing, thus\nallowing FMs to take advantage of a diverse pool of sensitive medical image\ndata across hospitals/clients. However, non-independent and identically\ndistributed (non-IID) data among clients, paired with computational and\ncommunication constraints in federated environments, presents an additional\nchallenge that limits further performance improvements and remains inadequately\naddressed in existing studies. In this work, we propose a novel FLFM\nfine-tuning framework, \\underline{\\textbf{Fed}}erated tuning with\n\\underline{\\textbf{S}}imilarity-guided \\underline{\\textbf{C}}ollaborative\n\\underline{\\textbf{A}}ggregation (FedSCA), encompassing all phases of the FL\nprocess. This includes (1) specially designed parameter-efficient fine-tuning\n(PEFT) for local client training to enhance computational efficiency; (2)\npartial low-level adapter transmission for communication efficiency; and (3)\nsimilarity-guided collaborative aggregation (SGCA) on the server side to\naddress non-IID issues. Extensive experiments on three FL benchmarks for\nmedical image segmentation demonstrate the effectiveness of our proposed\nFedSCA, establishing new SOTA performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14526", "pdf": "https://arxiv.org/pdf/2503.14526", "abs": "https://arxiv.org/abs/2503.14526", "authors": ["Yu Fang", "Yue Yang", "Xinghao Zhu", "Kaiyuan Zheng", "Gedas Bertasius", "Daniel Szafir", "Mingyu Ding"], "title": "ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis", "categories": ["cs.CV", "cs.GR", "cs.RO"], "comment": "Website: https://yuffish.github.io/rebot/", "summary": "Vision-language-action (VLA) models present a promising paradigm by training\npolicies directly on real robot datasets like Open X-Embodiment. However, the\nhigh cost of real-world data collection hinders further data scaling, thereby\nrestricting the generalizability of VLAs. In this paper, we introduce ReBot, a\nnovel real-to-sim-to-real approach for scaling real robot datasets and adapting\nVLA models to target domains, which is the last-mile deployment challenge in\nrobot manipulation. Specifically, ReBot replays real-world robot trajectories\nin simulation to diversify manipulated objects (real-to-sim), and integrates\nthe simulated movements with inpainted real-world background to synthesize\nphysically realistic and temporally consistent robot videos (sim-to-real). Our\napproach has several advantages: 1) it enjoys the benefit of real data to\nminimize the sim-to-real gap; 2) it leverages the scalability of simulation;\nand 3) it can generalize a pretrained VLA to a target domain with fully\nautomated data pipelines. Extensive experiments in both simulation and\nreal-world environments show that ReBot significantly enhances the performance\nand robustness of VLAs. For example, in SimplerEnv with the WidowX robot, ReBot\nimproved the in-domain performance of Octo by 7.2% and OpenVLA by 21.8%, and\nout-of-domain generalization by 19.9% and 9.4%, respectively. For real-world\nevaluation with a Franka robot, ReBot increased the success rates of Octo by\n17% and OpenVLA by 20%. More information can be found at:\nhttps://yuffish.github.io/rebot/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14698", "pdf": "https://arxiv.org/pdf/2503.14698", "abs": "https://arxiv.org/abs/2503.14698", "authors": ["Yiming Wang", "Lucy Chai", "Xuan Luo", "Michael Niemeyer", "Manuel Lagunas", "Stephen Lombardi", "Siyu Tang", "Tiancheng Sun"], "title": "SplatVoxel: History-Aware Novel View Streaming without Temporal Training", "categories": ["cs.CV"], "comment": null, "summary": "We study the problem of novel view streaming from sparse-view videos, which\naims to generate a continuous sequence of high-quality, temporally consistent\nnovel views as new input frames arrive. However, existing novel view synthesis\nmethods struggle with temporal coherence and visual fidelity, leading to\nflickering and inconsistency. To address these challenges, we introduce\nhistory-awareness, leveraging previous frames to reconstruct the scene and\nimprove quality and stability. We propose a hybrid splat-voxel feed-forward\nscene reconstruction approach that combines Gaussian Splatting to propagate\ninformation over time, with a hierarchical voxel grid for temporal fusion.\nGaussian primitives are efficiently warped over time using a motion graph that\nextends 2D tracking models to 3D motion, while a sparse voxel transformer\nintegrates new temporal observations in an error-aware manner. Crucially, our\nmethod does not require training on multi-view video datasets, which are\ncurrently limited in size and diversity, and can be directly applied to\nsparse-view video streams in a history-aware manner at inference time. Our\napproach achieves state-of-the-art performance in both static and streaming\nscene reconstruction, effectively reducing temporal artifacts and visual\nartifacts while running at interactive rates (15 fps with 350ms delay) on a\nsingle H100 GPU. Project Page: https://19reborn.github.io/SplatVoxel/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14774", "pdf": "https://arxiv.org/pdf/2503.14774", "abs": "https://arxiv.org/abs/2503.14774", "authors": ["David Serrano-Lozano", "Aditya Arora", "Luis Herranz", "Konstantinos G. Derpanis", "Michael S. Brown", "Javier Vazquez-Corral"], "title": "Revisiting Image Fusion for Multi-Illuminant White-Balance Correction", "categories": ["cs.CV"], "comment": "10 pages", "summary": "White balance (WB) correction in scenes with multiple illuminants remains a\npersistent challenge in computer vision. Recent methods explored fusion-based\napproaches, where a neural network linearly blends multiple sRGB versions of an\ninput image, each processed with predefined WB presets. However, we demonstrate\nthat these methods are suboptimal for common multi-illuminant scenarios.\nAdditionally, existing fusion-based methods rely on sRGB WB datasets lacking\ndedicated multi-illuminant images, limiting both training and evaluation. To\naddress these challenges, we introduce two key contributions. First, we propose\nan efficient transformer-based model that effectively captures spatial\ndependencies across sRGB WB presets, substantially improving upon linear fusion\ntechniques. Second, we introduce a large-scale multi-illuminant dataset\ncomprising over 16,000 sRGB images rendered with five different WB settings,\nalong with WB-corrected images. Our method achieves up to 100\\% improvement\nover existing techniques on our new multi-illuminant image fusion dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15351", "pdf": "https://arxiv.org/pdf/2503.15351", "abs": "https://arxiv.org/abs/2503.15351", "authors": ["I-Fan Lin", "Faegheh Hasibi", "Suzan Verberne"], "title": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15354", "pdf": "https://arxiv.org/pdf/2503.15354", "abs": "https://arxiv.org/abs/2503.15354", "authors": ["Yining Lu", "Noah Ziems", "Hy Dang", "Meng Jiang"], "title": "Optimizing Decomposition for Optimal Claim Verification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14867", "pdf": "https://arxiv.org/pdf/2503.14867", "abs": "https://arxiv.org/abs/2503.14867", "authors": ["Caoshuo Li", "Tanzhe Li", "Xiaobin Hu", "Donghao Luo", "Taisong Jin"], "title": "DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recently, Vision Graph Neural Network (ViG) has gained considerable attention\nin computer vision. Despite its groundbreaking innovation, Vision Graph Neural\nNetwork encounters key issues including the quadratic computational complexity\ncaused by its K-Nearest Neighbor (KNN) graph construction and the limitation of\npairwise relations of normal graphs. To address the aforementioned challenges,\nwe propose a novel vision architecture, termed Dilated Vision HyperGraph Neural\nNetwork (DVHGNN), which is designed to leverage multi-scale hypergraph to\nefficiently capture high-order correlations among objects. Specifically, the\nproposed method tailors Clustering and Dilated HyperGraph Construction (DHGC)\nto adaptively capture multi-scale dependencies among the data samples.\nFurthermore, a dynamic hypergraph convolution mechanism is proposed to\nfacilitate adaptive feature exchange and fusion at the hypergraph level.\nExtensive qualitative and quantitative evaluations of the benchmark image\ndatasets demonstrate that the proposed DVHGNN significantly outperforms the\nstate-of-the-art vision backbones. For instance, our DVHGNN-S achieves an\nimpressive top-1 accuracy of 83.1% on ImageNet-1K, surpassing ViG-S by +1.0%\nand ViHGNN-S by +0.6%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14895", "pdf": "https://arxiv.org/pdf/2503.14895", "abs": "https://arxiv.org/abs/2503.14895", "authors": ["Shuo Li", "Jiajun Sun", "Guodong Zheng", "Xiaoran Fan", "Yujiong Shen", "Yi Lu", "Zhiheng Xi", "Yuming Yang", "Wenming Tan", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14911", "pdf": "https://arxiv.org/pdf/2503.14911", "abs": "https://arxiv.org/abs/2503.14911", "authors": ["Siyuan Yan", "Ming Hu", "Yiwen Jiang", "Xieji Li", "Hao Fei", "Philipp Tschandl", "Harald Kittler", "Zongyuan Ge"], "title": "Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology", "categories": ["cs.CV"], "comment": "23 pages", "summary": "The emergence of vision-language models has transformed medical AI, enabling\nunprecedented advances in diagnostic capability and clinical applications.\nHowever, progress in dermatology has lagged behind other medical domains due to\nthe lack of standard image-text pairs. Existing dermatological datasets are\nlimited in both scale and depth, offering only single-label annotations across\na narrow range of diseases instead of rich textual descriptions, and lacking\nthe crucial clinical context needed for real-world applications. To address\nthese limitations, we present Derm1M, the first large-scale vision-language\ndataset for dermatology, comprising 1,029,761 image-text pairs. Built from\ndiverse educational resources and structured around a standard ontology\ncollaboratively developed by experts, Derm1M provides comprehensive coverage\nfor over 390 skin conditions across four hierarchical levels and 130 clinical\nconcepts with rich contextual information such as medical history, symptoms,\nand skin tone. To demonstrate Derm1M potential in advancing both AI research\nand clinical application, we pretrained a series of CLIP-like models,\ncollectively called DermLIP, on this dataset. The DermLIP family significantly\noutperforms state-of-the-art foundation models on eight diverse datasets across\nmultiple tasks, including zero-shot skin disease classification, clinical and\nartifacts concept identification, few-shot/full-shot learning, and cross-modal\nretrieval. Our dataset and code will be public.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14559", "pdf": "https://arxiv.org/pdf/2503.14559", "abs": "https://arxiv.org/abs/2503.14559", "authors": ["Weixiong Lin", "Chen Ju", "Haicheng Wang", "Shengchao Hu", "Shuai Xiao", "Mengting Chen", "Yuheng Jiao", "Mingshuai Yao", "Jinsong Lan", "Qingwen Liu", "Ying Chen"], "title": "Squeeze Out Tokens from Sample for Finer-Grained Data Governance", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Widely observed data scaling laws, in which error falls off as a power of the\ntraining size, demonstrate the diminishing returns of unselective data\nexpansion. Hence, data governance is proposed to downsize datasets through\npruning non-informative samples. Yet, isolating the impact of a specific sample\non overall model performance is challenging, due to the vast computation\nrequired for tryout all sample combinations. Current data governors circumvent\nthis complexity by estimating sample contributions through heuristic-derived\nscalar scores, thereby discarding low-value ones. Despite thorough sample\nsieving, retained samples contain substantial undesired tokens intrinsically,\nunderscoring the potential for further compression and purification. In this\nwork, we upgrade data governance from a 'sieving' approach to a 'juicing' one.\nInstead of scanning for least-flawed samples, our dual-branch DataJuicer\napplies finer-grained intra-sample governance. It squeezes out informative\ntokens and boosts image-text alignments. Specifically, the vision branch\nretains salient image patches and extracts relevant object classes, while the\ntext branch incorporates these classes to enhance captions. Consequently,\nDataJuicer yields more refined datasets through finer-grained governance.\nExtensive experiments across datasets demonstrate that DataJuicer significantly\noutperforms existing DataSieve in image-text retrieval, classification, and\ndense visual reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14895", "pdf": "https://arxiv.org/pdf/2503.14895", "abs": "https://arxiv.org/abs/2503.14895", "authors": ["Shuo Li", "Jiajun Sun", "Guodong Zheng", "Xiaoran Fan", "Yujiong Shen", "Yi Lu", "Zhiheng Xi", "Yuming Yang", "Wenming Tan", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14941", "pdf": "https://arxiv.org/pdf/2503.14941", "abs": "https://arxiv.org/abs/2503.14941", "authors": ["Qihui Zhang", "Munan Ning", "Zheyuan Liu", "Yanbo Wang", "Jiayi Ye", "Yue Huang", "Shuo Yang", "Xiao Chen", "Yibing Song", "Li Yuan"], "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation", "question answering"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15166", "pdf": "https://arxiv.org/pdf/2503.15166", "abs": "https://arxiv.org/abs/2503.15166", "authors": ["Ã€lex Pujol Vidal", "Sergio Escalera", "Kamal Nasrollahi", "Thomas B. Moeslund"], "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Preprint", "summary": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14953", "pdf": "https://arxiv.org/pdf/2503.14953", "abs": "https://arxiv.org/abs/2503.14953", "authors": ["Yang Liu", "Wentao Feng", "Zhuoyao Liu", "Shudong Huang", "Jiancheng Lv"], "title": "Aligning Information Capacity Between Vision and Language via Dense-to-Sparse Feature Distillation for Image-Text Matching", "categories": ["cs.CV"], "comment": null, "summary": "Enabling Visual Semantic Models to effectively handle multi-view description\nmatching has been a longstanding challenge. Existing methods typically learn a\nset of embeddings to find the optimal match for each view's text and compute\nsimilarity. However, the visual and text embeddings learned through these\napproaches have limited information capacity and are prone to interference from\nlocally similar negative samples. To address this issue, we argue that the\ninformation capacity of embeddings is crucial and propose Dense-to-Sparse\nFeature Distilled Visual Semantic Embedding (D2S-VSE), which enhances the\ninformation capacity of sparse text by leveraging dense text distillation.\nSpecifically, D2S-VSE is a two-stage framework. In the pre-training stage, we\nalign images with dense text to enhance the information capacity of visual\nsemantic embeddings. In the fine-tuning stage, we optimize two tasks\nsimultaneously, distilling dense text embeddings to sparse text embeddings\nwhile aligning images and sparse texts, enhancing the information capacity of\nsparse text embeddings. Our proposed D2S-VSE model is extensively evaluated on\nthe large-scale MS-COCO and Flickr30K datasets, demonstrating its superiority\nover recent state-of-the-art methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14955", "pdf": "https://arxiv.org/pdf/2503.14955", "abs": "https://arxiv.org/abs/2503.14955", "authors": ["Bike Chen", "Antti TikanmÃ¤ki", "Juha RÃ¶ning"], "title": "Depth-Aware Range Image-Based Model for Point Cloud Segmentation", "categories": ["cs.CV"], "comment": "No Comments", "summary": "Point cloud segmentation (PCS) aims to separate points into different and\nmeaningful groups. The task plays an important role in robotics because PCS\nenables robots to understand their physical environments directly. To process\nsparse and large-scale outdoor point clouds in real time, range image-based\nmodels are commonly adopted. However, in a range image, the lack of explicit\ndepth information inevitably causes some separate objects in 3D space to touch\neach other, bringing difficulty for the range image-based models in correctly\nsegmenting the objects. Moreover, previous PCS models are usually derived from\nthe existing color image-based models and unable to make full use of the\nimplicit but ordered depth information inherent in the range image, thereby\nachieving inferior performance. In this paper, we propose Depth-Aware Module\n(DAM) and Fast FMVNet V3. DAM perceives the ordered depth information in the\nrange image by explicitly modelling the interdependence among channels. Fast\nFMVNet V3 incorporates DAM by integrating it into the last block in each\narchitecture stage. Extensive experiments conducted on SemanticKITTI, nuScenes,\nand SemanticPOSS demonstrate that DAM brings a significant improvement for Fast\nFMVNet V3 with negligible computational cost.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14960", "pdf": "https://arxiv.org/pdf/2503.14960", "abs": "https://arxiv.org/abs/2503.14960", "authors": ["Seungyeon Cho", "Tae-Kyun Kim"], "title": "Body-Hand Modality Expertized Networks with Cross-attention for Fine-grained Skeleton Action Recognition", "categories": ["cs.CV"], "comment": "7 figures, 8 pages", "summary": "Skeleton-based Human Action Recognition (HAR) is a vital technology in\nrobotics and human-robot interaction. However, most existing methods\nconcentrate primarily on full-body movements and often overlook subtle hand\nmotions that are critical for distinguishing fine-grained actions. Recent work\nleverages a unified graph representation that combines body, hand, and foot\nkeypoints to capture detailed body dynamics. Yet, these models often blur fine\nhand details due to the disparity between body and hand action characteristics\nand the loss of subtle features during the spatial-pooling. In this paper, we\npropose BHaRNet (Body-Hand action Recognition Network), a novel framework that\naugments a typical body-expert model with a hand-expert model. Our model\njointly trains both streams with an ensemble loss that fosters cooperative\nspecialization, functioning in a manner reminiscent of a Mixture-of-Experts\n(MoE). Moreover, cross-attention is employed via an expertized branch method\nand a pooling-attention module to enable feature-level interactions and\nselectively fuse complementary information. Inspired by MMNet, we also\ndemonstrate the applicability of our approach to multi-modal tasks by\nleveraging RGB information, where body features guide RGB learning to capture\nricher contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60,\nNTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet\nachieves SOTA accuracies -- improving from 86.4\\% to 93.0\\% in hand-intensive\nactions -- while maintaining fewer GFLOPs and parameters than the relevant\nunified methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14983", "pdf": "https://arxiv.org/pdf/2503.14983", "abs": "https://arxiv.org/abs/2503.14983", "authors": ["Zanting Ye", "Xiaolong Niu", "Xuanbin Wu", "Wenxiang Yi", "Yuan Chang", "Lijun Lu"], "title": "Semi-KAN: KAN Provides an Effective Representation for Semi-Supervised Learning in Medical Image Segmentation", "categories": ["cs.CV"], "comment": "18 pages, 7 figures, 6 tables", "summary": "Deep learning-based medical image segmentation has shown remarkable success;\nhowever, it typically requires extensive pixel-level annotations, which are\nboth expensive and time-intensive. Semi-supervised medical image segmentation\n(SSMIS) offers a viable alternative, driven by advancements in CNNs and ViTs.\nHowever, these networks often rely on single fixed activation functions and\nlinear modeling patterns, limiting their ability to effectively learn robust\nrepresentations. Given the limited availability of labeled date, achieving\nrobust representation learning becomes crucial. Inspired by Kolmogorov-Arnold\nNetworks (KANs), we propose Semi-KAN, which leverages the untapped potential of\nKANs to enhance backbone architectures for representation learning in SSMIS.\nOur findings indicate that: (1) compared to networks with fixed activation\nfunctions, KANs exhibit superior representation learning capabilities with\nfewer parameters, and (2) KANs excel in high-semantic feature spaces. Building\non these insights, we integrate KANs into tokenized intermediate\nrepresentations, applying them selectively at the encoder's bottleneck and the\ndecoder's top layers within a U-Net pipeline to extract high-level semantic\nfeatures. Although learnable activation functions improve feature expansion,\nthey introduce significant computational overhead with only marginal\nperformance gains. To mitigate this, we reduce the feature dimensions and\nemploy horizontal scaling to capture multiple pattern representations.\nFurthermore, we design a multi-branch U-Net architecture with uncertainty\nestimation to effectively learn diverse pattern representations. Extensive\nexperiments on four public datasets demonstrate that Semi-KAN surpasses\nbaseline networks, utilizing fewer KAN layers and lower computational cost,\nthereby underscoring the potential of KANs as a promising approach for SSMIS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14998", "pdf": "https://arxiv.org/pdf/2503.14998", "abs": "https://arxiv.org/abs/2503.14998", "authors": ["Marta Hasny", "Maxime Di Folco", "Keno Bressem", "Julia Schnabel"], "title": "TGV: Tabular Data-Guided Learning of Visual Cardiac Representations", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning methods in computer vision typically rely on different\nviews of the same image to form pairs. However, in medical imaging, we often\nseek to compare entire patients with different phenotypes rather than just\nmultiple augmentations of one scan. We propose harnessing clinically relevant\ntabular data to identify distinct patient phenotypes and form more meaningful\npairs in a contrastive learning framework. Our method uses tabular attributes\nto guide the training of visual representations, without requiring a joint\nembedding space. We demonstrate its strength using short-axis cardiac MR images\nand clinical attributes from the UK Biobank, where tabular data helps to more\neffectively distinguish between patient subgroups. Evaluation on downstream\ntasks, including fine-tuning and zero-shot prediction of cardiovascular artery\ndiseases and cardiac phenotypes, shows that incorporating tabular data yields\nstronger visual representations than conventional methods that rely solely on\nimage augmentations or combined image-tabular embeddings. Furthermore, we\ndemonstrate that image encoders trained with tabular guidance are capable of\nembedding demographic information in their representations, allowing them to\nuse insights from tabular data for unimodal predictions, making them\nwell-suited to real-world medical settings where extensive clinical annotations\nmay not be routinely available at inference time. The code will be available on\nGitHub.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15166", "pdf": "https://arxiv.org/pdf/2503.15166", "abs": "https://arxiv.org/abs/2503.15166", "authors": ["Ã€lex Pujol Vidal", "Sergio Escalera", "Kamal Nasrollahi", "Thomas B. Moeslund"], "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Preprint", "summary": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15285", "pdf": "https://arxiv.org/pdf/2503.15285", "abs": "https://arxiv.org/abs/2503.15285", "authors": ["Yuanchao Yue", "Zhengxin Li", "Wei Zhang", "Hui Yuan"], "title": "PAPI-Reg: Patch-to-Pixel Solution for Efficient Cross-Modal Registration between LiDAR Point Cloud and Camera Image", "categories": ["cs.CV"], "comment": null, "summary": "The primary requirement for cross-modal data fusion is the precise alignment\nof data from different sensors. However, the calibration between LiDAR point\nclouds and camera images is typically time-consuming and needs external\ncalibration board or specific environmental features. Cross-modal registration\neffectively solves this problem by aligning the data directly without requiring\nexternal calibration. However, due to the domain gap between the point cloud\nand the image, existing methods rarely achieve satisfactory registration\naccuracy while maintaining real-time performance. To address this issue, we\npropose a framework that projects point clouds into several 2D representations\nfor matching with camera images, which not only leverages the geometric\ncharacteristic of LiDAR point clouds more effectively but also bridge the\ndomain gap between the point cloud and image. Moreover, to tackle the\nchallenges of cross modal differences and the limited overlap between LiDAR\npoint clouds and images in the image matching task, we introduce a multi-scale\nfeature extraction network to effectively extract features from both camera\nimages and the projection maps of LiDAR point cloud. Additionally, we propose a\npatch-to-pixel matching network to provide more effective supervision and\nachieve higher accuracy. We validate the performance of our model through\nexperiments on the KITTI and nuScenes datasets. Our network achieves real-time\nperformance and extremely high registration accuracy. On the KITTI dataset, our\nmodel achieves a registration accuracy rate of over 99\\%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15300", "pdf": "https://arxiv.org/pdf/2503.15300", "abs": "https://arxiv.org/abs/2503.15300", "authors": ["Weixiao Gao", "Liangliang Nan", "Hugo Ledoux"], "title": "SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes", "categories": ["cs.CV"], "comment": "22 pages, 24 figures", "summary": "Semantic segmentation in urban scene analysis has mainly focused on images or\npoint clouds, while textured meshes - offering richer spatial representation -\nremain underexplored. This paper introduces SUM Parts, the first large-scale\ndataset for urban textured meshes with part-level semantic labels, covering\nabout 2.5 km2 with 21 classes. The dataset was created using our own annotation\ntool, which supports both face- and texture-based annotations with efficient\ninteractive selection. We also provide a comprehensive evaluation of 3D\nsemantic segmentation and interactive annotation methods on this dataset. Our\nproject page is available at https://tudelft3d.github.io/SUMParts/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15369", "pdf": "https://arxiv.org/pdf/2503.15369", "abs": "https://arxiv.org/abs/2503.15369", "authors": ["Yinan Liang", "Ziwei Wang", "Xiuwei Xu", "Jie Zhou", "Jiwen Lu"], "title": "EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "While multimodal large language models demonstrate strong performance in\ncomplex reasoning tasks, they pose significant challenges related to model\ncomplexity during deployment, especially for resource-limited devices. In this\npaper, we propose an automatic pruning method for large vision-language models\nto enhance the efficiency of multimodal reasoning. Conventional methods rely on\nthe training data of the original model to select the proper pruning ratio for\ndifferent network components. However, these methods are impractical for large\nvision-language models due to the unaffordable search costs caused by web-scale\ntraining corpus. In contrast, our approach only leverages a small number of\nsamples to search for the desired pruning policy by maximizing its\ngeneralization ability on unknown training data while maintaining the model\naccuracy, which enables the achievement of an optimal trade-off between\naccuracy and efficiency for large visual language models. Specifically, we\nformulate the generalization gap of the pruning strategy using the structural\nrisk minimization principle. Based on both task performance and generalization\ncapability, we iteratively search for the optimal pruning policy within a given\nsearch space and optimize the vision projector to evolve the search space with\nhigher upper bound of performance. We conduct extensive experiments on the\nScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual\nquestion answering. Using only 64 samples for pruning policy search,\nEfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a\n$\\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15406", "pdf": "https://arxiv.org/pdf/2503.15406", "abs": "https://arxiv.org/abs/2503.15406", "authors": ["Jisu Nam", "Soowon Son", "Zhan Xu", "Jing Shi", "Difan Liu", "Feng Liu", "Aashish Misraa", "Seungryong Kim", "Yang Zhou"], "title": "Visual Persona: Foundation Model for Full-Body Human Customization", "categories": ["cs.CV"], "comment": "CVPR 2025, Project page is available at\n  https://cvlab-kaist.github.io/Visual-Persona", "summary": "We introduce Visual Persona, a foundation model for text-to-image full-body\nhuman customization that, given a single in-the-wild human image, generates\ndiverse images of the individual guided by text descriptions. Unlike prior\nmethods that focus solely on preserving facial identity, our approach captures\ndetailed full-body appearance, aligning with text descriptions for body\nstructure and scene variations. Training this model requires large-scale paired\nhuman data, consisting of multiple images per individual with consistent\nfull-body identities, which is notoriously difficult to obtain. To address\nthis, we propose a data curation pipeline leveraging vision-language models to\nevaluate full-body appearance consistency, resulting in Visual Persona-500K, a\ndataset of 580k paired human images across 100k unique identities. For precise\nappearance transfer, we introduce a transformer encoder-decoder architecture\nadapted to a pre-trained text-to-image diffusion model, which augments the\ninput image into distinct body regions, encodes these regions as local\nappearance features, and projects them into dense identity embeddings\nindependently to condition the diffusion model for synthesizing customized\nimages. Visual Persona consistently surpasses existing approaches, generating\nhigh-quality, customized images from in-the-wild inputs. Extensive ablation\nstudies validate design choices, and we demonstrate the versatility of Visual\nPersona across various downstream tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15412", "pdf": "https://arxiv.org/pdf/2503.15412", "abs": "https://arxiv.org/abs/2503.15412", "authors": ["Fereshteh Forghani", "Jason J. Yu", "Tristan Aumentado-Armstrong", "Konstantinos G. Derpanis", "Marcus A. Brubaker"], "title": "Learn Your Scales: Towards Scale-Consistent Generative Novel View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Conventional depth-free multi-view datasets are captured using a moving\nmonocular camera without metric calibration. The scales of camera positions in\nthis monocular setting are ambiguous. Previous methods have acknowledged scale\nambiguity in multi-view data via various ad-hoc normalization pre-processing\nsteps, but have not directly analyzed the effect of incorrect scene scales on\ntheir application. In this paper, we seek to understand and address the effect\nof scale ambiguity when used to train generative novel view synthesis methods\n(GNVS). In GNVS, new views of a scene or object can be minimally synthesized\ngiven a single image and are, thus, unconstrained, necessitating the use of\ngenerative methods. The generative nature of these models captures all aspects\nof uncertainty, including any uncertainty of scene scales, which act as\nnuisance variables for the task. We study the effect of scene scale ambiguity\nin GNVS when sampled from a single image by isolating its effect on the\nresulting models and, based on these intuitions, define new metrics that\nmeasure the scale inconsistency of generated views. We then propose a framework\nto estimate scene scales jointly with the GNVS model in an end-to-end fashion.\nEmpirically, we show that our method reduces the scale inconsistency of\ngenerated views without the complexity or downsides of previous scale\nnormalization methods. Further, we show that removing this ambiguity improves\ngenerated image quality of the resulting GNVS model.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15457", "pdf": "https://arxiv.org/pdf/2503.15457", "abs": "https://arxiv.org/abs/2503.15457", "authors": ["Yuanzhi Zhu", "Xi Wang", "StÃ©phane LathuiliÃ¨re", "Vicky Kalogeiton"], "title": "Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step Generator", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling\ntechnique. Despite their remarkable results, they typically suffer from slow\ninference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a\nnovel approach that distills masked diffusion models into a one-step generator.\nDi$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using\nintermediate-step information for one-step generation, which we solve through\ntoken-level distribution matching that optimizes model output logits by an\n'on-policy framework' with the help of an auxiliary model; and (2) the lack of\nentropy in the initial distribution, which we address through a token\ninitialization strategy that injects randomness while maintaining similarity to\nteacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on\nboth class-conditional and text-conditional image generation, impressively\nachieving performance competitive to multi-step teacher outputs while\ndrastically reducing inference time. To our knowledge, we are the first to\nsuccessfully achieve one-step distillation of masked diffusion models and the\nfirst to apply discrete distillation to text-to-image generation, opening new\npaths for efficient generative modeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15470", "pdf": "https://arxiv.org/pdf/2503.15470", "abs": "https://arxiv.org/abs/2503.15470", "authors": ["Boshen Xu", "Yuting Mei", "Xinbi Liu", "Sipeng Zheng", "Qin Jin"], "title": "EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": "Code will be released at: https://github.com/xuboshen/EgoDTM", "summary": "Egocentric video-language pretraining has significantly advanced video\nrepresentation learning. Humans perceive and interact with a fully 3D world,\ndeveloping spatial awareness that extends beyond text-based understanding.\nHowever, most previous works learn from 1D text or 2D visual cues, such as\nbounding boxes, which inherently lack 3D understanding. To bridge this gap, we\nintroduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained\nthrough large-scale 3D-aware video pretraining and video-text contrastive\nlearning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently\nlearn 3D-awareness from pseudo depth maps generated by depth estimation models.\nTo further facilitate 3D-aware video pretraining, we enrich the original brief\ncaptions with hand-object visual cues by organically combining several\nfoundation models. Extensive experiments demonstrate EgoDTM's superior\nperformance across diverse downstream tasks, highlighting its superior 3D-aware\nvisual understanding. Our code will be released at\nhttps://github.com/xuboshen/EgoDTM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14559", "pdf": "https://arxiv.org/pdf/2503.14559", "abs": "https://arxiv.org/abs/2503.14559", "authors": ["Weixiong Lin", "Chen Ju", "Haicheng Wang", "Shengchao Hu", "Shuai Xiao", "Mengting Chen", "Yuheng Jiao", "Mingshuai Yao", "Jinsong Lan", "Qingwen Liu", "Ying Chen"], "title": "Squeeze Out Tokens from Sample for Finer-Grained Data Governance", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Widely observed data scaling laws, in which error falls off as a power of the\ntraining size, demonstrate the diminishing returns of unselective data\nexpansion. Hence, data governance is proposed to downsize datasets through\npruning non-informative samples. Yet, isolating the impact of a specific sample\non overall model performance is challenging, due to the vast computation\nrequired for tryout all sample combinations. Current data governors circumvent\nthis complexity by estimating sample contributions through heuristic-derived\nscalar scores, thereby discarding low-value ones. Despite thorough sample\nsieving, retained samples contain substantial undesired tokens intrinsically,\nunderscoring the potential for further compression and purification. In this\nwork, we upgrade data governance from a 'sieving' approach to a 'juicing' one.\nInstead of scanning for least-flawed samples, our dual-branch DataJuicer\napplies finer-grained intra-sample governance. It squeezes out informative\ntokens and boosts image-text alignments. Specifically, the vision branch\nretains salient image patches and extracts relevant object classes, while the\ntext branch incorporates these classes to enhance captions. Consequently,\nDataJuicer yields more refined datasets through finer-grained governance.\nExtensive experiments across datasets demonstrate that DataJuicer significantly\noutperforms existing DataSieve in image-text retrieval, classification, and\ndense visual reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15058", "pdf": "https://arxiv.org/pdf/2503.15058", "abs": "https://arxiv.org/abs/2503.15058", "authors": ["Francesco Di Feola", "Ludovica Pompilio", "Cecilia Assolito", "Valerio Guarrasi", "Paolo Soda"], "title": "Texture-Aware StarGAN for CT data harmonisation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Computed Tomography (CT) plays a pivotal role in medical diagnosis; however,\nvariability across reconstruction kernels hinders data-driven approaches, such\nas deep learning models, from achieving reliable and generalized performance.\nTo this end, CT data harmonization has emerged as a promising solution to\nminimize such non-biological variances by standardizing data across different\nsources or conditions. In this context, Generative Adversarial Networks (GANs)\nhave proved to be a powerful framework for harmonization, framing it as a\nstyle-transfer problem. However, GAN-based approaches still face limitations in\ncapturing complex relationships within the images, which are essential for\neffective harmonization. In this work, we propose a novel texture-aware StarGAN\nfor CT data harmonization, enabling one-to-many translations across different\nreconstruction kernels. Although the StarGAN model has been successfully\napplied in other domains, its potential for CT data harmonization remains\nunexplored. Furthermore, our approach introduces a multi-scale texture loss\nfunction that embeds texture information across different spatial and angular\nscales into the harmonization process, effectively addressing kernel-induced\ntexture variations. We conducted extensive experimentation on a publicly\navailable dataset, utilizing a total of 48667 chest CT slices from 197 patients\ndistributed over three different reconstruction kernels, demonstrating the\nsuperiority of our method over the baseline StarGAN.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15390", "pdf": "https://arxiv.org/pdf/2503.15390", "abs": "https://arxiv.org/abs/2503.15390", "authors": ["Yumin Zhang", "Yan Gao", "Haoran Duan", "Hanqing Guo", "Tejal Shah", "Rajiv Ranjan", "Bo Wei"], "title": "FedSCA: Federated Tuning with Similarity-guided Collaborative Aggregation for Heterogeneous Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Transformer-based foundation models (FMs) have recently demonstrated\nremarkable performance in medical image segmentation. However, scaling these\nmodels is challenging due to the limited size of medical image datasets within\nisolated hospitals, where data centralization is restricted due to privacy\nconcerns. These constraints, combined with the data-intensive nature of FMs,\nhinder their broader application. Integrating federated learning (FL) with\nfoundation models (FLFM) fine-tuning offers a potential solution to these\nchallenges by enabling collaborative model training without data sharing, thus\nallowing FMs to take advantage of a diverse pool of sensitive medical image\ndata across hospitals/clients. However, non-independent and identically\ndistributed (non-IID) data among clients, paired with computational and\ncommunication constraints in federated environments, presents an additional\nchallenge that limits further performance improvements and remains inadequately\naddressed in existing studies. In this work, we propose a novel FLFM\nfine-tuning framework, \\underline{\\textbf{Fed}}erated tuning with\n\\underline{\\textbf{S}}imilarity-guided \\underline{\\textbf{C}}ollaborative\n\\underline{\\textbf{A}}ggregation (FedSCA), encompassing all phases of the FL\nprocess. This includes (1) specially designed parameter-efficient fine-tuning\n(PEFT) for local client training to enhance computational efficiency; (2)\npartial low-level adapter transmission for communication efficiency; and (3)\nsimilarity-guided collaborative aggregation (SGCA) on the server side to\naddress non-IID issues. Extensive experiments on three FL benchmarks for\nmedical image segmentation demonstrate the effectiveness of our proposed\nFedSCA, establishing new SOTA performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
