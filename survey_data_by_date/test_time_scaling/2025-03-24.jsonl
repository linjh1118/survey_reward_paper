{"id": "2503.17363", "pdf": "https://arxiv.org/pdf/2503.17363", "abs": "https://arxiv.org/abs/2503.17363", "authors": ["Yansi Li", "Jiahao Xu", "Tian Liang", "Xingyu Chen", "Zhiwei He", "Qiuzhi Liu", "Rui Wang", "Zhuosheng Zhang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural Language Self-Critique", "categories": ["cs.CL"], "comment": null, "summary": "Enhancing the reasoning capabilities of large language models (LLMs),\nparticularly for complex tasks requiring multi-step logical deductions, remains\na significant challenge. Traditional inference time scaling methods utilize\nscalar reward signals from process reward models to evaluate candidate\nreasoning steps, but these scalar rewards lack the nuanced qualitative\ninformation essential for understanding and justifying each step. In this\npaper, we propose a novel inference-time scaling approach -- stepwise natural\nlanguage self-critique (PANEL), which employs self-generated natural language\ncritiques as feedback to guide the step-level search process. By generating\nrich, human-readable critiques for each candidate reasoning step, PANEL retains\nessential qualitative information, facilitating better-informed decision-making\nduring inference. This approach bypasses the need for task-specific verifiers\nand the associated training overhead, making it broadly applicable across\ndiverse tasks. Experimental results on challenging reasoning benchmarks,\nincluding AIME and GPQA, demonstrate that PANEL significantly enhances\nreasoning performance, outperforming traditional scalar reward-based methods.\nOur code is available at https://github.com/puddingyeah/PANEL to support and\nencourage future research in this promising field.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "inference time", "scaling"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16616", "pdf": "https://arxiv.org/pdf/2503.16616", "abs": "https://arxiv.org/abs/2503.16616", "authors": ["Xiaoran Zhang", "Byung-Woo Hong", "Hyoungseob Park", "Daniel H. Pak", "Anne-Marie Rickmann", "Lawrence H. Staib", "James S. Duncan", "Alex Wong"], "title": "Progressive Test Time Energy Adaptation for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "We propose a model-agnostic, progressive test-time energy adaptation approach\nfor medical image segmentation. Maintaining model performance across diverse\nmedical datasets is challenging, as distribution shifts arise from inconsistent\nimaging protocols and patient variations. Unlike domain adaptation methods that\nrequire multiple passes through target data - impractical in clinical settings\n- our approach adapts pretrained models progressively as they process test\ndata. Our method leverages a shape energy model trained on source data, which\nassigns an energy score at the patch level to segmentation maps: low energy\nrepresents in-distribution (accurate) shapes, while high energy signals\nout-of-distribution (erroneous) predictions. By minimizing this energy score at\ntest time, we refine the segmentation model to align with the target\ndistribution. To validate the effectiveness and adaptability, we evaluated our\nframework on eight public MRI (bSSFP, T1- and T2-weighted) and X-ray datasets\nspanning cardiac, spinal cord, and lung segmentation. We consistently\noutperform baselines both quantitatively and qualitatively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17352", "pdf": "https://arxiv.org/pdf/2503.17352", "abs": "https://arxiv.org/abs/2503.17352", "authors": ["Yihe Deng", "Hritik Bansal", "Fan Yin", "Nanyun Peng", "Wei Wang", "Kai-Wei Chang"], "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement", "categories": ["cs.CV", "cs.CL"], "comment": "23 pages, 11 figures, 8 tables", "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-verification", "self-correction"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17213", "pdf": "https://arxiv.org/pdf/2503.17213", "abs": "https://arxiv.org/abs/2503.17213", "authors": ["Ting Sun", "Cheng Cui", "Yuning Du", "Yi Liu"], "title": "PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction", "categories": ["cs.CV", "cs.AI"], "comment": "Github Repo: https://github.com/PaddlePaddle/PaddleX", "summary": "Document layout analysis is a critical preprocessing step in document\nintelligence, enabling the detection and localization of structural elements\nsuch as titles, text blocks, tables, and formulas. Despite its importance,\nexisting layout detection models face significant challenges in generalizing\nacross diverse document types, handling complex layouts, and achieving\nreal-time performance for large-scale data processing. To address these\nlimitations, we present PP-DocLayout, which achieves high precision and\nefficiency in recognizing 23 types of layout regions across diverse document\nformats. To meet different needs, we offer three models of varying scales.\nPP-DocLayout-L is a high-precision model based on the RT-DETR-L detector,\nachieving 90.4% mAP@0.5 and an end-to-end inference time of 13.4 ms per page on\na T4 GPU. PP-DocLayout-M is a balanced model, offering 75.2% mAP@0.5 with an\ninference time of 12.7 ms per page on a T4 GPU. PP-DocLayout-S is a\nhigh-efficiency model designed for resource-constrained environments and\nreal-time applications, with an inference time of 8.1 ms per page on a T4 GPU\nand 14.5 ms on a CPU. This work not only advances the state of the art in\ndocument layout analysis but also provides a robust solution for constructing\nhigh-quality training data, enabling advancements in document intelligence and\nmultimodal AI systems. Code and models are available at\nhttps://github.com/PaddlePaddle/PaddleX .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17352", "pdf": "https://arxiv.org/pdf/2503.17352", "abs": "https://arxiv.org/abs/2503.17352", "authors": ["Yihe Deng", "Hritik Bansal", "Fan Yin", "Nanyun Peng", "Wei Wang", "Kai-Wei Chang"], "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement", "categories": ["cs.CV", "cs.CL"], "comment": "23 pages, 11 figures, 8 tables", "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-verification", "self-correction"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16527", "pdf": "https://arxiv.org/pdf/2503.16527", "abs": "https://arxiv.org/abs/2503.16527", "authors": ["Ang Li", "Haozhe Chen", "Hongseok Namkoong", "Tianyi Peng"], "title": "LLM Generated Persona is a Promise with a Catch", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "comment": null, "summary": "The use of large language models (LLMs) to simulate human behavior has gained\nsignificant attention, particularly through personas that approximate\nindividual characteristics. Persona-based simulations hold promise for\ntransforming disciplines that rely on population-level feedback, including\nsocial science, economic analysis, marketing research, and business operations.\nTraditional methods to collect realistic persona data face significant\nchallenges. They are prohibitively expensive and logistically challenging due\nto privacy constraints, and often fail to capture multi-dimensional attributes,\nparticularly subjective qualities. Consequently, synthetic persona generation\nwith LLMs offers a scalable, cost-effective alternative. However, current\napproaches rely on ad hoc and heuristic generation techniques that do not\nguarantee methodological rigor or simulation precision, resulting in systematic\nbiases in downstream tasks. Through extensive large-scale experiments including\npresidential election forecasts and general opinion surveys of the U.S.\npopulation, we reveal that these biases can lead to significant deviations from\nreal-world outcomes. Our findings underscore the need to develop a rigorous\nscience of persona generation and outline the methodological innovations,\norganizational and institutional support, and empirical foundations required to\nenhance the reliability and scalability of LLM-driven persona simulations. To\nsupport further research and development in this area, we have open-sourced\napproximately one million generated personas, available for public access and\nanalysis at https://huggingface.co/datasets/Tianyi-Lab/Personas.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "multi-dimensional"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16528", "pdf": "https://arxiv.org/pdf/2503.16528", "abs": "https://arxiv.org/abs/2503.16528", "authors": ["Heng Ping", "Shixuan Li", "Peiyu Zhang", "Anzhe Cheng", "Shukai Duan", "Nikos Kanakaris", "Xiongye Xiao", "Wei Yang", "Shahin Nazarian", "Andrei Irimia", "Paul Bogdan"], "title": "HDLCoRe: A Training-Free Framework for Mitigating Hallucinations in LLM-Generated HDL", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in code generation tasks. However, when applied to hardware\ndescription languages (HDL), these models exhibit significant limitations due\nto data scarcity, resulting in hallucinations and incorrect code generation. To\naddress these challenges, we propose HDLCoRe, a training-free framework that\nenhances LLMs' HDL generation capabilities through prompt engineering\ntechniques and retrieval-augmented generation (RAG). Our approach consists of\ntwo main components: (1) an HDL-aware Chain-of-Thought (CoT) prompting\ntechnique with self-verification that classifies tasks by complexity and type,\nincorporates domain-specific knowledge, and guides LLMs through step-by-step\nself-simulation for error correction; and (2) a two-stage heterogeneous RAG\nsystem that addresses formatting inconsistencies through key component\nextraction and efficiently retrieves relevant HDL examples through sequential\nfiltering and re-ranking. HDLCoRe eliminates the need for model fine-tuning\nwhile substantially improving LLMs' HDL generation capabilities. Experimental\nresults demonstrate that our framework achieves superior performance on the\nRTLLM2.0 benchmark, significantly reducing hallucinations and improving both\nsyntactic and functional correctness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-verification"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "code generation"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16536", "pdf": "https://arxiv.org/pdf/2503.16536", "abs": "https://arxiv.org/abs/2503.16536", "authors": ["Shuo Huang", "Muhammad Umair Nasir", "Steven James", "Julian Togelius"], "title": "Word2Minecraft: Generating 3D Game Levels through Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We present Word2Minecraft, a system that leverages large language models to\ngenerate playable game levels in Minecraft based on structured stories. The\nsystem transforms narrative elements-such as protagonist goals, antagonist\nchallenges, and environmental settings-into game levels with both spatial and\ngameplay constraints. We introduce a flexible framework that allows for the\ncustomization of story complexity, enabling dynamic level generation. The\nsystem employs a scaling algorithm to maintain spatial consistency while\nadapting key game elements. We evaluate Word2Minecraft using both metric-based\nand human-based methods. Our results show that GPT-4-Turbo outperforms\nGPT-4o-Mini in most areas, including story coherence and objective enjoyment,\nwhile the latter excels in aesthetic appeal. We also demonstrate the system' s\nability to generate levels with high map enjoyment, offering a promising step\nforward in the intersection of story generation and game design. We open-source\nthe code at https://github.com/JMZ-kk/Word2Minecraft/tree/word2mc_v0", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16541", "pdf": "https://arxiv.org/pdf/2503.16541", "abs": "https://arxiv.org/abs/2503.16541", "authors": ["Hanzhi Zhang", "Sumera Anjum", "Heng Fan", "Weijian Zheng", "Yan Huang", "Yunhe Feng"], "title": "Poly-FEVER: A Multilingual Fact Verification Benchmark for Hallucination Detection in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Hallucinations in generative AI, particularly in Large Language Models\n(LLMs), pose a significant challenge to the reliability of multilingual\napplications. Existing benchmarks for hallucination detection focus primarily\non English and a few widely spoken languages, lacking the breadth to assess\ninconsistencies in model performance across diverse linguistic contexts. To\naddress this gap, we introduce Poly-FEVER, a large-scale multilingual fact\nverification benchmark specifically designed for evaluating hallucination\ndetection in LLMs. Poly-FEVER comprises 77,973 labeled factual claims spanning\n11 languages, sourced from FEVER, Climate-FEVER, and SciFact. It provides the\nfirst large-scale dataset tailored for analyzing hallucination patterns across\nlanguages, enabling systematic evaluation of LLMs such as ChatGPT and the LLaMA\nseries. Our analysis reveals how topic distribution and web resource\navailability influence hallucination frequency, uncovering language-specific\nbiases that impact model accuracy. By offering a multilingual benchmark for\nfact verification, Poly-FEVER facilitates cross-linguistic comparisons of\nhallucination detection and contributes to the development of more reliable,\nlanguage-inclusive AI systems. The dataset is publicly available to advance\nresearch in responsible AI, fact-checking methodologies, and multilingual NLP,\npromoting greater transparency and robustness in LLM performance. The proposed\nPoly-FEVER is available at:\nhttps://huggingface.co/datasets/HanzhiZhang/Poly-FEVER.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16707", "pdf": "https://arxiv.org/pdf/2503.16707", "abs": "https://arxiv.org/abs/2503.16707", "authors": ["Jinlong Li", "Cristiano Saltori", "Fabio Poiesi", "Nicu Sebe"], "title": "Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "The lack of a large-scale 3D-text corpus has led recent works to distill\nopen-vocabulary knowledge from vision-language models (VLMs). owever, these\nmethods typically rely on a single VLM to align the feature spaces of 3D models\nwithin a common language space, which limits the potential of 3D models to\nleverage the diverse spatial and semantic capabilities encapsulated in various\nfoundation models. In this paper, we propose Cross-modal and Uncertainty-aware\nAgglomeration for Open-vocabulary 3D Scene Understanding dubbed CUA-O3D, the\nfirst model to integrate multiple foundation models-such as CLIP, DINOv2, and\nStable Diffusion-into 3D scene understanding. We further introduce a\ndeterministic uncertainty estimation to adaptively distill and harmonize the\nheterogeneous 2D feature embeddings from these models. Our method addresses two\nkey challenges: (1) incorporating semantic priors from VLMs alongside the\ngeometric knowledge of spatially-aware vision foundation models, and (2) using\na novel deterministic uncertainty estimation to capture model-specific\nuncertainties across diverse semantic and geometric sensitivities, helping to\nreconcile heterogeneous representations during training. Extensive experiments\non ScanNetV2 and Matterport3D demonstrate that our method not only advances\nopen-vocabulary segmentation but also achieves robust cross-domain alignment\nand competitive spatial perception capabilities. The code will be available at\n\\href{https://github.com/TyroneLi/CUA_O3D}{CUA_O3D}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16726", "pdf": "https://arxiv.org/pdf/2503.16726", "abs": "https://arxiv.org/abs/2503.16726", "authors": ["Philipp Becker", "Abhinav Mehrotra", "Ruchika Chavhan", "Malcolm Chadwick", "Luca Morreale", "Mehdi Noroozi", "Alberto Gil Ramos", "Sourav Bhattacharya"], "title": "EDiT: Efficient Diffusion Transformers with Linear Compressed Attention", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion Transformers (DiTs) have emerged as a leading architecture for\ntext-to-image synthesis, producing high-quality and photorealistic images.\nHowever, the quadratic scaling properties of the attention in DiTs hinder image\ngeneration with higher resolution or on devices with limited resources. This\nwork introduces an efficient diffusion transformer (EDiT) to alleviate these\nefficiency bottlenecks in conventional DiTs and Multimodal DiTs (MM-DiTs).\nFirst, we present a novel linear compressed attention method that uses a\nmulti-layer convolutional network to modulate queries with local information\nwhile keys and values are spatially aggregated. Second, we formulate a hybrid\nattention scheme for multi-modal inputs that combines linear attention for\nimage-to-image interactions and standard scaled dot-product attention for\ninteractions involving prompts. Merging these two approaches leads to an\nexpressive, linear-time Multimodal Efficient Diffusion Transformer (MM-EDiT).\nWe demonstrate the effectiveness of the EDiT and MM-EDiT architectures by\nintegrating them into PixArt-Sigma(conventional DiT) and Stable Diffusion\n3.5-Medium (MM-DiT), achieving up to 2.2x speedup with comparable image quality\nafter distillation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16585", "pdf": "https://arxiv.org/pdf/2503.16585", "abs": "https://arxiv.org/abs/2503.16585", "authors": ["Hadi Amini", "Md Jueal Mia", "Yasaman Saadati", "Ahmed Imteaj", "Seyedsina Nabavirazavi", "Urmish Thakker", "Md Zarif Hossain", "Awal Ahmed Fime", "S. S. Iyengar"], "title": "Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions", "categories": ["cs.CL", "cs.CV", "cs.DC", "cs.LG"], "comment": null, "summary": "Language models (LMs) are machine learning models designed to predict\nlinguistic patterns by estimating the probability of word sequences based on\nlarge-scale datasets, such as text. LMs have a wide range of applications in\nnatural language processing (NLP) tasks, including autocomplete and machine\ntranslation. Although larger datasets typically enhance LM performance,\nscalability remains a challenge due to constraints in computational power and\nresources. Distributed computing strategies offer essential solutions for\nimproving scalability and managing the growing computational demand. Further,\nthe use of sensitive datasets in training and deployment raises significant\nprivacy concerns. Recent research has focused on developing decentralized\ntechniques to enable distributed training and inference while utilizing diverse\ncomputational resources and enabling edge AI. This paper presents a survey on\ndistributed solutions for various LMs, including large language models (LLMs),\nvision language models (VLMs), multimodal LLMs (MLLMs), and small language\nmodels (SLMs). While LLMs focus on processing and generating text, MLLMs are\ndesigned to handle multiple modalities of data (e.g., text, images, and audio)\nand to integrate them for broader applications. To this end, this paper reviews\nkey advancements across the MLLM pipeline, including distributed training,\ninference, fine-tuning, and deployment, while also identifying the\ncontributions, limitations, and future areas of improvement. Further, it\ncategorizes the literature based on six primary focus areas of\ndecentralization. Our analysis describes gaps in current methodologies for\nenabling distributed solutions for LMs and outline future research directions,\nemphasizing the need for novel solutions to enhance the robustness and\napplicability of distributed LMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16776", "pdf": "https://arxiv.org/pdf/2503.16776", "abs": "https://arxiv.org/abs/2503.16776", "authors": ["Valentin Bieri", "Marco Zamboni", "Nicolas S. Blumer", "Qingxuan Chen", "Francis Engelmann"], "title": "OpenCity3D: What do Vision-Language Models know about Urban Environments?", "categories": ["cs.CV"], "comment": "Published at WACV 2025", "summary": "Vision-language models (VLMs) show great promise for 3D scene understanding\nbut are mainly applied to indoor spaces or autonomous driving, focusing on\nlow-level tasks like segmentation. This work expands their use to urban-scale\nenvironments by leveraging 3D reconstructions from multi-view aerial imagery.\nWe propose OpenCity3D, an approach that addresses high-level tasks, such as\npopulation density estimation, building age classification, property price\nprediction, crime rate assessment, and noise pollution evaluation. Our findings\nhighlight OpenCity3D's impressive zero-shot and few-shot capabilities,\nshowcasing adaptability to new contexts. This research establishes a new\nparadigm for language-driven urban analytics, enabling applications in\nplanning, policy, and environmental monitoring. See our project page:\nopencity3d.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16793", "pdf": "https://arxiv.org/pdf/2503.16793", "abs": "https://arxiv.org/abs/2503.16793", "authors": ["Haori Lu", "Xusheng Cao", "Linlan Huang", "Enguang Wang", "Fei Yang", "Xialei Liu"], "title": "Restoring Forgotten Knowledge in Non-Exemplar Class Incremental Learning through Test-Time Semantic Evolution", "categories": ["cs.CV"], "comment": null, "summary": "Continual learning aims to accumulate knowledge over a data stream while\nmitigating catastrophic forgetting. In Non-exemplar Class Incremental Learning\n(NECIL), forgetting arises during incremental optimization because old classes\nare inaccessible, hindering the retention of prior knowledge. To solve this,\nprevious methods struggle in achieving the stability-plasticity balance in the\ntraining stages. However, we note that the testing stage is rarely considered\namong them, but is promising to be a solution to forgetting. Therefore, we\npropose RoSE, which is a simple yet effective method that\n\\textbf{R}est\\textbf{o}res forgotten knowledge through test-time\n\\textbf{S}emantic \\textbf{E}volution. Specifically designed for minimizing\nforgetting, RoSE is a test-time semantic drift compensation framework that\nenables more accurate drift estimation in a self-supervised manner. Moreover,\nto avoid incomplete optimization during online testing, we derive an analytical\nsolution as an alternative to gradient descent. We evaluate RoSE on CIFAR-100,\nTinyImageNet, and ImageNet100 datasets, under both cold-start and warm-start\nsettings. Our method consistently outperforms most state-of-the-art (SOTA)\nmethods across various scenarios, validating the potential and feasibility of\ntest-time evolution in NECIL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16811", "pdf": "https://arxiv.org/pdf/2503.16811", "abs": "https://arxiv.org/abs/2503.16811", "authors": ["Maoji Zheng", "Ziyu Xu", "Qiming Xia", "Hai Wu", "Chenglu Wen", "Cheng Wang"], "title": "Seg2Box: 3D Object Detection by Point-Wise Semantics Supervision", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "LiDAR-based 3D object detection and semantic segmentation are critical tasks\nin 3D scene understanding. Traditional detection and segmentation methods\nsupervise their models through bounding box labels and semantic mask labels.\nHowever, these two independent labels inherently contain significant\nredundancy. This paper aims to eliminate the redundancy by supervising 3D\nobject detection using only semantic labels. However, the challenge arises due\nto the incomplete geometry structure and boundary ambiguity of point-cloud\ninstances, leading to inaccurate pseudo labels and poor detection results. To\naddress these challenges, we propose a novel method, named Seg2Box. We first\nintroduce a Multi-Frame Multi-Scale Clustering (MFMS-C) module, which leverages\nthe spatio-temporal consistency of point clouds to generate accurate box-level\npseudo-labels. Additionally, the Semantic?Guiding Iterative-Mining\nSelf-Training (SGIM-ST) module is proposed to enhance the performance by\nprogressively refining the pseudo-labels and mining the instances without\ngenerating pseudo-labels. Experiments on the Waymo Open Dataset and nuScenes\nDataset show that our method significantly outperforms other competitive\nmethods by 23.7\\% and 10.3\\% in mAP, respectively. The results demonstrate the\ngreat label-efficient potential and advancement of our method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16789", "pdf": "https://arxiv.org/pdf/2503.16789", "abs": "https://arxiv.org/abs/2503.16789", "authors": ["Rupak Sarkar", "Bahareh Sarrafzadeh", "Nirupama Chandrasekaran", "Nagu Rangan", "Philip Resnik", "Longqi Yang", "Sujay Kumar Jauhar"], "title": "Conversational User-AI Intervention: A Study on Prompt Rewriting for Improved LLM Response Generation", "categories": ["cs.CL"], "comment": "8 pages, ACL style", "summary": "Human-LLM conversations are increasingly becoming more pervasive in peoples'\nprofessional and personal lives, yet many users still struggle to elicit\nhelpful responses from LLM Chatbots. One of the reasons for this issue is\nusers' lack of understanding in crafting effective prompts that accurately\nconvey their information needs. Meanwhile, the existence of real-world\nconversational datasets on the one hand, and the text understanding faculties\nof LLMs on the other, present a unique opportunity to study this problem, and\nits potential solutions at scale. Thus, in this paper we present the first\nLLM-centric study of real human-AI chatbot conversations, focused on\ninvestigating aspects in which user queries fall short of expressing\ninformation needs, and the potential of using LLMs to rewrite suboptimal user\nprompts. Our findings demonstrate that rephrasing ineffective prompts can\nelicit better responses from a conversational system, while preserving the\nuser's original intent. Notably, the performance of rewrites improves in longer\nconversations, where contextual inferences about user needs can be made more\naccurately. Additionally, we observe that LLMs often need to -- and inherently\ndo -- make \\emph{plausible} assumptions about a user's intentions and goals\nwhen interpreting prompts. Our findings largely hold true across conversational\ndomains, user intents, and LLMs of varying sizes and families, indicating the\npromise of using prompt rewriting as a solution for better human-AI\ninteractions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16858", "pdf": "https://arxiv.org/pdf/2503.16858", "abs": "https://arxiv.org/abs/2503.16858", "authors": ["Jialin Chen", "Aosong Feng", "Ziyu Zhao", "Juan Garza", "Gaukhar Nurbek", "Cheng Qin", "Ali Maatouk", "Leandros Tassiulas", "Yifeng Gao", "Rex Ying"], "title": "MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages", "summary": "Understanding the relationship between textual news and time-series evolution\nis a critical yet under-explored challenge in applied data science. While\nmultimodal learning has gained traction, existing multimodal time-series\ndatasets fall short in evaluating cross-modal reasoning and complex question\nanswering, which are essential for capturing complex interactions between\nnarrative information and temporal patterns. To bridge this gap, we introduce\nMultimodal Time Series Benchmark (MTBench), a large-scale benchmark designed to\nevaluate large language models (LLMs) on time series and text understanding\nacross financial and weather domains. MTbench comprises paired time series and\ntextual data, including financial news with corresponding stock price movements\nand weather reports aligned with historical temperature records. Unlike\nexisting benchmarks that focus on isolated modalities, MTbench provides a\ncomprehensive testbed for models to jointly reason over structured numerical\ntrends and unstructured textual narratives. The richness of MTbench enables\nformulation of diverse tasks that require a deep understanding of both text and\ntime-series data, including time-series forecasting, semantic and technical\ntrend analysis, and news-driven question answering (QA). These tasks target the\nmodel's ability to capture temporal dependencies, extract key insights from\ntextual context, and integrate cross-modal information. We evaluate\nstate-of-the-art LLMs on MTbench, analyzing their effectiveness in modeling the\ncomplex relationships between news narratives and temporal patterns. Our\nfindings reveal significant challenges in current models, including\ndifficulties in capturing long-term dependencies, interpreting causality in\nfinancial and weather trends, and effectively fusing multimodal information.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "testbed", "question answering"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16835", "pdf": "https://arxiv.org/pdf/2503.16835", "abs": "https://arxiv.org/abs/2503.16835", "authors": ["Huiqiang Chen", "Tianqing Zhu", "Linlin Wang", "Xin Yu", "Longxiang Gao", "Wanlei Zhou"], "title": "Safe and Reliable Diffusion Models via Subspace Projection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Large-scale text-to-image (T2I) diffusion models have revolutionized image\ngeneration, enabling the synthesis of highly detailed visuals from textual\ndescriptions. However, these models may inadvertently generate inappropriate\ncontent, such as copyrighted works or offensive images. While existing methods\nattempt to eliminate specific unwanted concepts, they often fail to ensure\ncomplete removal, allowing the concept to reappear in subtle forms. For\ninstance, a model may successfully avoid generating images in Van Gogh's style\nwhen explicitly prompted with 'Van Gogh', yet still reproduce his signature\nartwork when given the prompt 'Starry Night'. In this paper, we propose SAFER,\na novel and efficient approach for thoroughly removing target concepts from\ndiffusion models. At a high level, SAFER is inspired by the observed\nlow-dimensional structure of the text embedding space. The method first\nidentifies a concept-specific subspace $S_c$ associated with the target concept\nc. It then projects the prompt embeddings onto the complementary subspace of\n$S_c$, effectively erasing the concept from the generated images. Since\nconcepts can be abstract and difficult to fully capture using natural language\nalone, we employ textual inversion to learn an optimized embedding of the\ntarget concept from a reference image. This enables more precise subspace\nestimation and enhances removal performance. Furthermore, we introduce a\nsubspace expansion strategy to ensure comprehensive and robust concept erasure.\nExtensive experiments demonstrate that SAFER consistently and effectively\nerases unwanted concepts from diffusion models while preserving generation\nquality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16883", "pdf": "https://arxiv.org/pdf/2503.16883", "abs": "https://arxiv.org/abs/2503.16883", "authors": ["Deniss Ruder", "Andero Uusberg", "Kairit Sirts"], "title": "Assessing the Reliability and Validity of GPT-4 in Annotating Emotion Appraisal Ratings", "categories": ["cs.CL"], "comment": null, "summary": "Appraisal theories suggest that emotions arise from subjective evaluations of\nevents, referred to as appraisals. The taxonomy of appraisals is quite diverse,\nand they are usually given ratings on a Likert scale to be annotated in an\nexperiencer-annotator or reader-annotator paradigm. This paper studies GPT-4 as\na reader-annotator of 21 specific appraisal ratings in different prompt\nsettings, aiming to evaluate and improve its performance compared to human\nannotators. We found that GPT-4 is an effective reader-annotator that performs\nclose to or even slightly better than human annotators, and its results can be\nsignificantly improved by using a majority voting of five completions. GPT-4\nalso effectively predicts appraisal ratings and emotion labels using a single\nprompt, but adding instruction complexity results in poorer performance. We\nalso found that longer event descriptions lead to more accurate annotations for\nboth model and human annotator ratings. This work contributes to the growing\nusage of LLMs in psychology and the strategies for improving GPT-4 performance\nin annotating appraisals.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16965", "pdf": "https://arxiv.org/pdf/2503.16965", "abs": "https://arxiv.org/abs/2503.16965", "authors": ["Zhe Hu", "Jing Li", "Yu Yin"], "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only Training For Human-Centered Decision Making", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17039", "pdf": "https://arxiv.org/pdf/2503.17039", "abs": "https://arxiv.org/abs/2503.17039", "authors": ["Jeremy Barnes", "Naiara Perez", "Alba Bonet-Jover", "Begoña Altuna"], "title": "Summarization Metrics for Spanish and Basque: Do Automatic Scores and LLM-Judges Correlate with Humans?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Studies on evaluation metrics and LLM-as-a-Judge models for automatic text\nsummarization have largely been focused on English, limiting our understanding\nof their effectiveness in other languages. Through our new dataset BASSE\n(BAsque and Spanish Summarization Evaluation), we address this situation by\ncollecting human judgments on 2,040 abstractive summaries in Basque and\nSpanish, generated either manually or by five LLMs with four different prompts.\nFor each summary, annotators evaluated five criteria on a 5-point Likert scale:\ncoherence, consistency, fluency, relevance, and 5W1H. We use these data to\nreevaluate traditional automatic metrics used for evaluating summaries, as well\nas several LLM-as-a-Judge models that show strong performance on this task in\nEnglish. Our results show that currently proprietary judge LLMs have the\nhighest correlation with human judgments, followed by criteria-specific\nautomatic metrics, while open-sourced judge LLMs perform poorly. We release\nBASSE and our code publicly, along with the first large-scale Basque\nsummarization dataset containing 22,525 news articles with their subheads.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation", "consistency", "summarization", "criteria"], "score": 6}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16910", "pdf": "https://arxiv.org/pdf/2503.16910", "abs": "https://arxiv.org/abs/2503.16910", "authors": ["Yu Qiu", "Yuhang Sun", "Jie Mei", "Lin Xiao", "Jing Xu"], "title": "Salient Object Detection in Traffic Scene through the TSOD10K Dataset", "categories": ["cs.CV"], "comment": "12 pages, 12 figures", "summary": "Traffic Salient Object Detection (TSOD) aims to segment the objects critical\nto driving safety by combining semantic (e.g., collision risks) and visual\nsaliency. Unlike SOD in natural scene images (NSI-SOD), which prioritizes\nvisually distinctive regions, TSOD emphasizes the objects that demand immediate\ndriver attention due to their semantic impact, even with low visual contrast.\nThis dual criterion, i.e., bridging perception and contextual risk, re-defines\nsaliency for autonomous and assisted driving systems. To address the lack of\ntask-specific benchmarks, we collect the first large-scale TSOD dataset with\npixel-wise saliency annotations, named TSOD10K. TSOD10K covers the diverse\nobject categories in various real-world traffic scenes under various\nchallenging weather/illumination variations (e.g., fog, snowstorms,\nlow-contrast, and low-light). Methodologically, we propose a Mamba-based TSOD\nmodel, termed Tramba. Considering the challenge of distinguishing inconspicuous\nvisual information from complex traffic backgrounds, Tramba introduces a novel\nDual-Frequency Visual State Space module equipped with shifted window\npartitioning and dilated scanning to enhance the perception of fine details and\nglobal structure by hierarchically decomposing high/low-frequency components.\nTo emphasize critical regions in traffic scenes, we propose a traffic-oriented\nHelix 2D-Selective-Scan (Helix-SS2D) mechanism that injects driving attention\npriors while effectively capturing global multi-direction spatial dependencies.\nWe establish a comprehensive benchmark by evaluating Tramba and 22 existing\nNSI-SOD models on TSOD10K, demonstrating Tramba's superiority. Our research\nestablishes the first foundation for safety-aware saliency analysis in\nintelligent transportation systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16929", "pdf": "https://arxiv.org/pdf/2503.16929", "abs": "https://arxiv.org/abs/2503.16929", "authors": ["Shicheng Li", "Lei Li", "Kun Ouyang", "Shuhuai Ren", "Yuanxin Liu", "Yuanxing Zhang", "Fuzheng Zhang", "Lingpeng Kong", "Qi Liu", "Xu Sun"], "title": "TEMPO: Temporal Preference Optimization of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Large Language Models (Video LLMs) have achieved significant success by\nleveraging a two-stage paradigm: pretraining on large-scale video-text data for\nvision-language alignment, followed by supervised fine-tuning (SFT) for\ntask-specific capabilities. However, existing approaches struggle with temporal\nreasoning due to weak temporal correspondence in the data and reliance on the\nnext-token prediction paradigm during training. To address these limitations,\nwe propose TEMPO (TEMporal Preference Optimization), a systematic framework\nthat enhances Video LLMs' temporal reasoning capabilities through Direct\nPreference Optimization (DPO). To facilitate this, we introduce an automated\npreference data generation pipeline that systematically constructs preference\npairs by selecting videos that are rich in temporal information, designing\nvideo-specific perturbation strategies, and finally evaluating model responses\non clean and perturbed video inputs. Our temporal alignment features two key\ninnovations: curriculum learning which that progressively increases\nperturbation difficulty to improve model robustness and adaptability; and\n``Pre-SFT Alignment'', applying preference optimization before instruction\ntuning to prioritize fine-grained temporal comprehension. Extensive experiments\ndemonstrate that our approach consistently improves Video LLM performance\nacross multiple benchmarks with a relatively small set of self-generated DPO\ndata. We further analyze the transferability of DPO data across architectures\nand the role of difficulty scheduling in optimization. Our findings highlight\nour TEMPO as a scalable and efficient complement to SFT-based methods, paving\nthe way for developing reliable Video LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17287", "pdf": "https://arxiv.org/pdf/2503.17287", "abs": "https://arxiv.org/abs/2503.17287", "authors": ["Mingyang Song", "Mao Zheng", "Zheng Li", "Wenjie Yang", "Xuan Luo", "Yue Pan", "Feng Zhang"], "title": "FastCuRL: Curriculum Reinforcement Learning with Progressive Context Extension for Efficient Training R1-like Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose \\textbf{\\textsc{FastCuRL}}, a simple yet efficient\n\\textbf{Cu}rriculum \\textbf{R}einforcement \\textbf{L}earning approach with\ncontext window extending strategy to accelerate the reinforcement learning\ntraining efficiency for R1-like reasoning models while enhancing their\nperformance in tackling complex reasoning tasks with long chain-of-thought\nrationales, particularly with a 1.5B parameter language model.\n\\textbf{\\textsc{FastCuRL}} consists of two main procedures: length-aware\ntraining data segmentation and context window extension training. Specifically,\nthe former first splits the original training data into three different levels\nby the input prompt length, and then the latter leverages segmented training\ndatasets with a progressively increasing context window length to train the\nreasoning model. Experimental results demonstrate that\n\\textbf{\\textsc{FastCuRL}}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview\nacross all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva\nMath, and OlympiadBench) while only utilizing 50\\% of training steps.\nFurthermore, all training stages for FastCuRL-1.5B-Preview are completed using\njust a single node with 8 GPUs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16945", "pdf": "https://arxiv.org/pdf/2503.16945", "abs": "https://arxiv.org/abs/2503.16945", "authors": ["Ibtissam Saadi", "Abdenour Hadid", "Douglas W. Cunningham", "Abdelmalik Taleb-Ahmed", "Yassin El Hillali"], "title": "PE-CLIP: A Parameter-Efficient Fine-Tuning of Vision Language Models for Dynamic Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) like CLIP offer promising solutions for Dynamic\nFacial Expression Recognition (DFER) but face challenges such as inefficient\nfull fine-tuning, high complexity, and poor alignment between textual and\nvisual representations. Additionally, existing methods struggle with\nineffective temporal modeling. To address these issues, we propose PE-CLIP, a\nparameter-efficient fine-tuning (PEFT) framework that adapts CLIP for DFER\nwhile significantly reducing trainable parameters while maintaining high\naccuracy. PE-CLIP introduces two specialized adapters: a Temporal Dynamic\nAdapter (TDA) and a Shared Adapter (ShA). The TDA is a GRU-based module with\ndynamic scaling that captures sequential dependencies while emphasizing\ninformative temporal features and suppressing irrelevant variations. The ShA is\na lightweight adapter that refines representations within both textual and\nvisual encoders, ensuring consistency and efficiency. Additionally, we\nintegrate Multi-modal Prompt Learning (MaPLe), introducing learnable prompts\nfor visual and action unit-based textual inputs, enhancing semantic alignment\nbetween modalities and enabling efficient CLIP adaptation for dynamic tasks. We\nevaluate PE-CLIP on two benchmark datasets, DFEW and FERV39K, achieving\ncompetitive performance compared to state-of-the-art methods while requiring\nfewer trainable parameters. By balancing efficiency and accuracy, PE-CLIP sets\na new benchmark in resource-efficient DFER. The source code of the proposed\nPE-CLIP will be publicly available at https://github.com/Ibtissam-SAADI/PE-CLIP .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16433", "pdf": "https://arxiv.org/pdf/2503.16433", "abs": "https://arxiv.org/abs/2503.16433", "authors": ["Andrew Cho", "Jason M. Woo", "Brian Shi", "Aishwaryaa Udeshi", "Jonathan S. H. Woo"], "title": "The Application of MATEC (Multi-AI Agent Team Care) Framework in Sepsis Care", "categories": ["cs.HC", "cs.CL", "cs.MA"], "comment": "15 pages", "summary": "Under-resourced or rural hospitals have limited access to medical specialists\nand healthcare professionals, which can negatively impact patient outcomes in\nsepsis. To address this gap, we developed the MATEC (Multi-AI Agent Team Care)\nframework, which integrates a team of specialized AI agents for sepsis care.\nThe sepsis AI agent team includes five doctor agents, four health professional\nagents, and a risk prediction model agent, with an additional 33 doctor agents\navailable for consultations. Ten attending physicians at a teaching hospital\nevaluated this framework, spending approximately 40 minutes on the web-based\nMATEC application and participating in the 5-point Likert scale survey (rated\nfrom 1-unfavorable to 5-favorable). The physicians found the MATEC framework\nvery useful (Median=4, P=0.01), and very accurate (Median=4, P<0.01). This\npilot study demonstrates that a Multi-AI Agent Team Care framework (MATEC) can\npotentially be useful in assisting medical professionals, particularly in\nunder-resourced hospital settings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16970", "pdf": "https://arxiv.org/pdf/2503.16970", "abs": "https://arxiv.org/abs/2503.16970", "authors": ["Yingping Liang", "Yutao Hu", "Wenqi Shao", "Ying Fu"], "title": "Distilling Monocular Foundation Model for Fine-grained Depth Completion", "categories": ["cs.CV"], "comment": null, "summary": "Depth completion involves predicting dense depth maps from sparse LiDAR\ninputs. However, sparse depth annotations from sensors limit the availability\nof dense supervision, which is necessary for learning detailed geometric\nfeatures. In this paper, we propose a two-stage knowledge distillation\nframework that leverages powerful monocular foundation models to provide dense\nsupervision for depth completion. In the first stage, we introduce a\npre-training strategy that generates diverse training data from natural images,\nwhich distills geometric knowledge to depth completion. Specifically, we\nsimulate LiDAR scans by utilizing monocular depth and mesh reconstruction,\nthereby creating training data without requiring ground-truth depth. Besides,\nmonocular depth estimation suffers from inherent scale ambiguity in real-world\nsettings. To address this, in the second stage, we employ a scale- and\nshift-invariant loss (SSI Loss) to learn real-world scales when fine-tuning on\nreal-world datasets. Our two-stage distillation framework enables depth\ncompletion models to harness the strengths of monocular foundation models.\nExperimental results demonstrate that models trained with our two-stage\ndistillation framework achieve state-of-the-art performance, ranking\n\\textbf{first place} on the KITTI benchmark. Code is available at\nhttps://github.com/Sharpiless/DMD3C", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16505", "pdf": "https://arxiv.org/pdf/2503.16505", "abs": "https://arxiv.org/abs/2503.16505", "authors": ["Dimitris Tsirmpas", "Ion Androutsopoulos", "John Pavlopoulos"], "title": "Scalable Evaluation of Online Moderation Strategies via Synthetic Simulations", "categories": ["cs.HC", "cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "25 pages, 6 tables, 9 figures", "summary": "Despite the ever-growing importance of online moderation, there has been no\nlarge-scale study evaluating the effectiveness of alternative moderation\nstrategies. This is largely due to the lack of appropriate datasets, and the\ndifficulty of getting human discussants, moderators, and evaluators involved in\nmultiple experiments. In this paper, we propose a methodology for leveraging\nsynthetic experiments performed exclusively by Large Language Models (LLMs) to\ninitially bypass the need for human participation in experiments involving\nonline moderation. We evaluate six LLM moderation configurations; two currently\nused real-life moderation strategies (guidelines issued for human moderators\nfor online moderation and real-life facilitation), two baseline strategies\n(guidelines elicited for LLM alignment work, and LLM moderation with minimal\nprompting) a baseline with no moderator at all, as well as our own proposed\nstrategy inspired by a Reinforcement Learning (RL) formulation of the problem.\nWe find that our own moderation strategy significantly outperforms established\nmoderation guidelines, as well as out-of-the-box LLM moderation. We also find\nthat smaller LLMs, with less intensive instruction-tuning, can create more\nvaried discussions than larger models. In order to run these experiments, we\ncreate and release an efficient, purpose-built, open-source Python framework,\ndubbed \"SynDisco\" to easily simulate hundreds of discussions using LLM\nuser-agents and moderators. Additionally, we release the Virtual Moderation\nDataset (VMD), a large dataset of LLM-generated and LLM-annotated discussions,\ngenerated by three families of open-source LLMs accompanied by an exploratory\nanalysis of the dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16565", "pdf": "https://arxiv.org/pdf/2503.16565", "abs": "https://arxiv.org/abs/2503.16565", "authors": ["Kirill Vishniakov", "Boulbaba Ben Amor", "Engin Tekin", "Nancy A. ElNaker", "Karthik Viswanathan", "Aleksandr Medvedev", "Aahan Singh", "Maryam Nadeem", "Mohammad Amaan Sayeed", "Praveenkumar Kanithi", "Tiago Magalhaes", "Natalia Vassilieva", "Dwarikanath Mahapatra", "Marco Pimentel", "and Shadab Khan"], "title": "Gene42: Long-Range Genomic Foundation Model With Dense Attention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Gene42, a novel family of Genomic Foundation Models (GFMs)\ndesigned to manage context lengths of up to 192,000 base pairs (bp) at a\nsingle-nucleotide resolution. Gene42 models utilize a decoder-only\n(LLaMA-style) architecture with a dense self-attention mechanism. Initially\ntrained on fixed-length sequences of 4,096 bp, our models underwent continuous\npretraining to extend the context length to 192,000 bp. This iterative\nextension allowed for the comprehensive processing of large-scale genomic data\nand the capture of intricate patterns and dependencies within the human genome.\nGene42 is the first dense attention model capable of handling such extensive\nlong context lengths in genomics, challenging state-space models that often\nrely on convolutional operators among other mechanisms. Our pretrained models\nexhibit notably low perplexity values and high reconstruction accuracy,\nhighlighting their strong ability to model genomic data. Extensive experiments\non various genomic benchmarks have demonstrated state-of-the-art performance\nacross multiple tasks, including biotype classification, regulatory region\nidentification, chromatin profiling prediction, variant pathogenicity\nprediction, and species classification. The models are publicly available at\nhuggingface.co/inceptionai.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17069", "pdf": "https://arxiv.org/pdf/2503.17069", "abs": "https://arxiv.org/abs/2503.17069", "authors": ["Yufei Shi", "Weilong Yan", "Gang Xu", "Yumeng Li", "Yuchen Li", "Zhenxi Li", "Fei Richard Yu", "Ming Li", "Si Yong Yeo"], "title": "PVChat: Personalized Video Chat with One-Shot Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video large language models (ViLLMs) excel in general video understanding,\ne.g., recognizing activities like talking and eating, but struggle with\nidentity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or\n\"Tom is discussing with Sarah\", limiting their applicability in smart\nhealthcare and smart home environments. To address this limitation, we propose\na one-shot learning framework PVChat, the first personalized ViLLM that enables\nsubject-aware question answering (QA) from a single video for each subject. Our\napproach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically\naugmented video-QA dataset, leveraging a progressive image-to-video learning\nstrategy. Specifically, we introduce an automated augmentation pipeline that\nsynthesizes identity-preserving positive samples and retrieves hard negatives\nfrom existing video corpora, generating a diverse training dataset with four QA\ntypes: existence, appearance, action, and location inquiries. To enhance\nsubject-specific learning, we propose a ReLU Routing MoH attention mechanism,\nalongside two novel objectives: (1) Smooth Proximity Regularization for\nprogressive learning through exponential distance scaling and (2) Head\nActivation Enhancement for balanced attention routing. Finally, we adopt a\ntwo-stage training strategy, transitioning from image pre-training to video\nfine-tuning, enabling a gradual learning process from static attributes to\ndynamic representations. We evaluate PVChat on diverse datasets covering\nmedical scenarios, TV series, anime, and real-world footage, demonstrating its\nsuperiority in personalized feature understanding after learning from a single\nvideo, compared to state-of-the-art ViLLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17071", "pdf": "https://arxiv.org/pdf/2503.17071", "abs": "https://arxiv.org/abs/2503.17071", "authors": ["Pablo Garcia-Fernandez", "Lorenzo Vaquero", "Mingxuan Liu", "Feng Xue", "Daniel Cores", "Nicu Sebe", "Manuel Mucientes", "Elisa Ricci"], "title": "Superpowering Open-Vocabulary Object Detectors for X-ray Vision", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary object detection (OvOD) is set to revolutionize security\nscreening by enabling systems to recognize any item in X-ray scans. However,\ndeveloping effective OvOD models for X-ray imaging presents unique challenges\ndue to data scarcity and the modality gap that prevents direct adoption of\nRGB-based solutions. To overcome these limitations, we propose RAXO, a\ntraining-free framework that repurposes off-the-shelf RGB OvOD detectors for\nrobust X-ray detection. RAXO builds high-quality X-ray class descriptors using\na dual-source retrieval strategy. It gathers relevant RGB images from the web\nand enriches them via a novel X-ray material transfer mechanism, eliminating\nthe need for labeled databases. These visual descriptors replace text-based\nclassification in OvOD, leveraging intra-modal feature distances for robust\ndetection. Extensive experiments demonstrate that RAXO consistently improves\nOvOD performance, providing an average mAP increase of up to 17.0 points over\nbase detectors. To further support research in this emerging field, we also\nintroduce DET-COMPASS, a new benchmark featuring bounding box annotations for\nover 300 object categories, enabling large-scale evaluation of OvOD in X-ray.\nCode and dataset available at: https://github.com/PAGF188/RAXO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17080", "pdf": "https://arxiv.org/pdf/2503.17080", "abs": "https://arxiv.org/abs/2503.17080", "authors": ["Gensheng Pei", "Tao Chen", "Yujia Wang", "Xinhao Cai", "Xiangbo Shu", "Tianfei Zhou", "Yazhou Yao"], "title": "Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection", "categories": ["cs.CV"], "comment": "accepted by CVPR 2025", "summary": "The CLIP model has demonstrated significant advancements in aligning visual\nand language modalities through large-scale pre-training on image-text pairs,\nenabling strong zero-shot classification and retrieval capabilities on various\ndomains. However, CLIP's training remains computationally intensive, with high\ndemands on both data processing and memory. To address these challenges, recent\nmasking strategies have emerged, focusing on the selective removal of image\npatches to improve training efficiency. Although effective, these methods often\ncompromise key semantic information, resulting in suboptimal alignment between\nvisual features and text descriptions. In this work, we present a concise yet\neffective approach called Patch Generation-to-Selection to enhance CLIP's\ntraining efficiency while preserving critical semantic content. Our method\nintroduces a gradual masking process in which a small set of candidate patches\nis first pre-selected as potential mask regions. Then, we apply Sobel edge\ndetection across the entire image to generate an edge mask that prioritizes the\nretention of the primary object areas. Finally, similarity scores between the\ncandidate mask patches and their neighboring patches are computed, with optimal\ntransport normalization refining the selection process to ensure a balanced\nsimilarity matrix. Our approach, CLIP-PGS, sets new state-of-the-art results in\nzero-shot classification and retrieval tasks, achieving superior performance in\nrobustness evaluation and language compositionality benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17110", "pdf": "https://arxiv.org/pdf/2503.17110", "abs": "https://arxiv.org/abs/2503.17110", "authors": ["Robin Hesse", "Doğukan Bağcı", "Bernt Schiele", "Simone Schaub-Meyer", "Stefan Roth"], "title": "Beyond Accuracy: What Matters in Designing Well-Behaved Models?", "categories": ["cs.CV", "cs.LG"], "comment": "Code: https://github.com/visinf/beyond-accuracy", "summary": "Deep learning has become an essential part of computer vision, with deep\nneural networks (DNNs) excelling in predictive performance. However, they often\nfall short in other critical quality dimensions, such as robustness,\ncalibration, or fairness. While existing studies have focused on a subset of\nthese quality dimensions, none have explored a more general form of\n\"well-behavedness\" of DNNs. With this work, we address this gap by\nsimultaneously studying nine different quality dimensions for image\nclassification. Through a large-scale study, we provide a bird's-eye view by\nanalyzing 326 backbone models and how different training paradigms and model\narchitectures affect the quality dimensions. We reveal various new insights\nsuch that (i) vision-language models exhibit high fairness on ImageNet-1k\nclassification and strong robustness against domain changes; (ii)\nself-supervised learning is an effective training paradigm to improve almost\nall considered quality dimensions; and (iii) the training dataset size is a\nmajor driver for most of the quality dimensions. We conclude our study by\nintroducing the QUBA score (Quality Understanding Beyond Accuracy), a novel\nmetric that ranks models across multiple dimensions of quality, enabling\ntailored recommendations based on specific user needs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17182", "pdf": "https://arxiv.org/pdf/2503.17182", "abs": "https://arxiv.org/abs/2503.17182", "authors": ["Patrick Rim", "Hyoungseob Park", "Vadim Ezhov", "Jeffrey Moon", "Alex Wong"], "title": "Radar-Guided Polynomial Fitting for Metric Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "We propose PolyRad, a novel radar-guided depth estimation method that\nintroduces polynomial fitting to transform scaleless depth predictions from\npretrained monocular depth estimation (MDE) models into metric depth maps.\nUnlike existing approaches that rely on complex architectures or expensive\nsensors, our method is grounded in a simple yet fundamental insight: using\npolynomial coefficients predicted from cheap, ubiquitous radar data to\nadaptively adjust depth predictions non-uniformly across depth ranges. Although\nMDE models often infer reasonably accurate local depth structure within each\nobject or local region, they may misalign these regions relative to one\nanother, making a linear scale-and-shift transformation insufficient given\nthree or more of these regions. In contrast, PolyRad generalizes beyond linear\ntransformations and is able to correct such misalignments by introducing\ninflection points. Importantly, our polynomial fitting framework preserves\nstructural consistency through a novel training objective that enforces\nmonotonicity via first-derivative regularization. PolyRad achieves\nstate-of-the-art performance on the nuScenes, ZJU-4DRadarCam, and View-of-Delft\ndatasets, outperforming existing methods by 30.3% in MAE and 37.2% in RMSE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17193", "pdf": "https://arxiv.org/pdf/2503.17193", "abs": "https://arxiv.org/abs/2503.17193", "authors": ["Xiaojin Lu", "Taoran yue", "Jiaxi cai", "Shibing Chu"], "title": "MSCA-Net:Multi-Scale Context Aggregation Network for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Detecting infrared small targets in complex backgrounds remains a challenging\ntask because of the low contrast and high noise levels inherent in infrared\nimages. These factors often lead to the loss of crucial details during feature\nextraction. Moreover, existing detection methods have limitations in adequately\nintegrating global and local information, which constrains the efficiency and\naccuracy of infrared small target detection. To address these challenges, this\npaper proposes a novel network architecture named MSCA-Net, which integrates\nthree key components: Multi-Scale Enhanced Detection Attention\nmechanism(MSEDA), Positional Convolutional Block Attention Module (PCBAM), and\nChannel Aggregation Block (CAB). Specifically, MSEDA employs a multi-scale\nfeature fusion attention mechanism to adaptively aggregate information across\ndifferent scales, enriching feature representation. PCBAM captures the\ncorrelation between global and local features through a correlation\nmatrix-based strategy, enabling deep feature interaction. Moreover, CAB\nredistributes input feature channels, facilitating the efficient transmission\nof beneficial features and further enhancing the model detection capability in\ncomplex backgrounds. The experimental results demonstrate that MSCA-Net\nachieves outstanding small target detection performance in complex backgrounds.\nSpecifically, it attains mIoU scores of 78.43\\%, 94.56\\%, and 67.08\\% on the\nNUAA-SIRST, NUDT-SIRST, and IRTSD-1K datasets, respectively, underscoring its\neffectiveness and strong potential for real-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17212", "pdf": "https://arxiv.org/pdf/2503.17212", "abs": "https://arxiv.org/abs/2503.17212", "authors": ["Matthew Kenely", "Dylan Seychell", "Carl James Debono", "Chris Porter"], "title": "A Deep Learning Framework for Visual Attention Prediction and Analysis of News Interfaces", "categories": ["cs.CV", "cs.HC"], "comment": "This is a preprint submitted to the 2025 IEEE Conference on\n  Artificial Intelligence (CAI)", "summary": "News outlets' competition for attention in news interfaces has highlighted\nthe need for demographically-aware saliency prediction models. Despite recent\nadvancements in saliency detection applied to user interfaces (UI), existing\ndatasets are limited in size and demographic representation. We present a deep\nlearning framework that enhances the SaRa (Saliency Ranking) model with\nDeepGaze IIE, improving Salient Object Ranking (SOR) performance by 10.7%. Our\nframework optimizes three key components: saliency map generation, grid segment\nscoring, and map normalization. Through a two-fold experiment using\neye-tracking (30 participants) and mouse-tracking (375 participants aged\n13--70), we analyze attention patterns across demographic groups. Statistical\nanalysis reveals significant age-based variations (p < 0.05, {\\epsilon^2} =\n0.042), with older users (36--70) engaging more with textual content and\nyounger users (13--35) interacting more with images. Mouse-tracking data\nclosely approximates eye-tracking behavior (sAUC = 0.86) and identifies UI\nelements that immediately stand out, validating its use in large-scale studies.\nWe conclude that saliency studies should prioritize gathering data from a\nlarger, demographically representative sample and report exact demographic\ndistributions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17221", "pdf": "https://arxiv.org/pdf/2503.17221", "abs": "https://arxiv.org/abs/2503.17221", "authors": ["Fanghua Yu", "Jinjin Gu", "Jinfan Hu", "Zheyuan Li", "Chao Dong"], "title": "UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models", "categories": ["cs.CV"], "comment": "This work has been accepted for publication at the International\n  Conference on Learning Representations (ICLR) 2025", "summary": "We introduce UniCon, a novel architecture designed to enhance control and\nefficiency in training adapters for large-scale diffusion models. Unlike\nexisting methods that rely on bidirectional interaction between the diffusion\nmodel and control adapter, UniCon implements a unidirectional flow from the\ndiffusion network to the adapter, allowing the adapter alone to generate the\nfinal output. UniCon reduces computational demands by eliminating the need for\nthe diffusion model to compute and store gradients during adapter training. Our\nresults indicate that UniCon reduces GPU memory usage by one-third and\nincreases training speed by 2.3 times, while maintaining the same adapter\nparameter size. Additionally, without requiring extra computational resources,\nUniCon enables the training of adapters with double the parameter volume of\nexisting ControlNets. In a series of image conditional generation tasks, UniCon\nhas demonstrated precise responsiveness to control inputs and exceptional\ngeneration capabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17224", "pdf": "https://arxiv.org/pdf/2503.17224", "abs": "https://arxiv.org/abs/2503.17224", "authors": ["Giacomo Savazzi", "Eugenio Lomurno", "Cristian Sbrolli", "Agnese Chiatti", "Matteo Matteucci"], "title": "Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "As machine learning models increase in scale and complexity, obtaining\nsufficient training data has become a critical bottleneck due to acquisition\ncosts, privacy constraints, and data scarcity in specialised domains. While\nsynthetic data generation has emerged as a promising alternative, a notable\nperformance gap remains compared to models trained on real data, particularly\nas task complexity grows. Concurrently, Neuro-Symbolic methods, which combine\nneural networks' learning strengths with symbolic reasoning's structured\nrepresentations, have demonstrated significant potential across various\ncognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning\nfor synthetic image dataset generation, focusing specifically on improving the\nperformance of Scene Graph Generation models. The research investigates whether\nstructured symbolic representations in the form of scene graphs can enhance\nsynthetic data quality through explicit encoding of relational constraints. The\nresults demonstrate that Neuro-Symbolic conditioning yields significant\nimprovements of up to +2.59% in standard Recall metrics and +2.83% in No Graph\nConstraint Recall metrics when used for dataset augmentation. These findings\nestablish that merging Neuro-Symbolic and generative approaches produces\nsynthetic data with complementary structural information that enhances model\nperformance when combined with real data, providing a novel approach to\novercome data scarcity limitations even for complex visual reasoning tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17238", "pdf": "https://arxiv.org/pdf/2503.17238", "abs": "https://arxiv.org/abs/2503.17238", "authors": ["Devavrat Tomar", "Guillaume Vray", "Dwarikanath Mahapatra", "Sudipta Roy", "Jean-Philippe Thiran", "Behzad Bozorgtabar"], "title": "Slide-Level Prompt Learning with Vision Language Models for Few-Shot Multiple Instance Learning in Histopathology", "categories": ["cs.CV"], "comment": "Accepted to ISBI 2025", "summary": "In this paper, we address the challenge of few-shot classification in\nhistopathology whole slide images (WSIs) by utilizing foundational\nvision-language models (VLMs) and slide-level prompt learning. Given the\ngigapixel scale of WSIs, conventional multiple instance learning (MIL) methods\nrely on aggregation functions to derive slide-level (bag-level) predictions\nfrom patch representations, which require extensive bag-level labels for\ntraining. In contrast, VLM-based approaches excel at aligning visual embeddings\nof patches with candidate class text prompts but lack essential pathological\nprior knowledge. Our method distinguishes itself by utilizing pathological\nprior knowledge from language models to identify crucial local tissue types\n(patches) for WSI classification, integrating this within a VLM-based MIL\nframework. Our approach effectively aligns patch images with tissue types, and\nwe fine-tune our model via prompt learning using only a few labeled WSIs per\ncategory. Experimentation on real-world pathological WSI datasets and ablation\nstudies highlight our method's superior performance over existing MIL- and\nVLM-based methods in few-shot WSI classification tasks. Our code is publicly\navailable at https://github.com/LTS5/SLIP.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17262", "pdf": "https://arxiv.org/pdf/2503.17262", "abs": "https://arxiv.org/abs/2503.17262", "authors": ["Shuang Guo", "Friedhelm Hamann", "Guillermo Gallego"], "title": "Unsupervised Joint Learning of Optical Flow and Intensity with Event Cameras", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "14 page, 8 figures, 9 tables. Project page:\n  https://github.com/tub-rip/e2fai", "summary": "Event cameras rely on motion to obtain information about scene appearance. In\nother words, for event cameras, motion and appearance are seen both or neither,\nwhich are encoded in the output event stream. Previous works consider\nrecovering these two visual quantities as separate tasks, which does not fit\nwith the nature of event cameras and neglects the inherent relations between\nboth tasks. In this paper, we propose an unsupervised learning framework that\njointly estimates optical flow (motion) and image intensity (appearance), with\na single network. Starting from the event generation model, we newly derive the\nevent-based photometric error as a function of optical flow and image\nintensity, which is further combined with the contrast maximization framework,\nyielding a comprehensive loss function that provides proper constraints for\nboth flow and intensity estimation. Exhaustive experiments show that our model\nachieves state-of-the-art performance for both optical flow (achieves 20% and\n25% improvement in EPE and AE respectively in the unsupervised learning\ncategory) and intensity estimation (produces competitive results with other\nbaselines, particularly in high dynamic range scenarios). Last but not least,\nour model achieves shorter inference time than all the other optical flow\nmodels and many of the image reconstruction models, while they output only one\nquantity. Project page: https://github.com/tub-rip/e2fai", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17316", "pdf": "https://arxiv.org/pdf/2503.17316", "abs": "https://arxiv.org/abs/2503.17316", "authors": ["Wonbong Jang", "Philippe Weinzaepfel", "Vincent Leroy", "Lourdes Agapito", "Jerome Revaud"], "title": "Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "We present Pow3r, a novel large 3D vision regression model that is highly\nversatile in the input modalities it accepts. Unlike previous feed-forward\nmodels that lack any mechanism to exploit known camera or scene priors at test\ntime, Pow3r incorporates any combination of auxiliary information such as\nintrinsics, relative pose, dense or sparse depth, alongside input images,\nwithin a single network. Building upon the recent DUSt3R paradigm, a\ntransformer-based architecture that leverages powerful pre-training, our\nlightweight and versatile conditioning acts as additional guidance for the\nnetwork to predict more accurate estimates when auxiliary information is\navailable. During training we feed the model with random subsets of modalities\nat each iteration, which enables the model to operate under different levels of\nknown priors at test time. This in turn opens up new capabilities, such as\nperforming inference in native image resolution, or point-cloud completion. Our\nexperiments on 3D reconstruction, depth completion, multi-view depth\nprediction, multi-view stereo, and multi-view pose estimation tasks yield\nstate-of-the-art results and confirm the effectiveness of Pow3r at exploiting\nall available information. The project webpage is\nhttps://europe.naverlabs.com/pow3r.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17347", "pdf": "https://arxiv.org/pdf/2503.17347", "abs": "https://arxiv.org/abs/2503.17347", "authors": ["Jichen Hu", "Chen Yang", "Zanwei Zhou", "Jiemin Fang", "Xiaokang Yang", "Qi Tian", "Wei Shen"], "title": "Dereflection Any Image with Diffusion Priors and Diversified Data", "categories": ["cs.CV"], "comment": null, "summary": "Reflection removal of a single image remains a highly challenging task due to\nthe complex entanglement between target scenes and unwanted reflections.\nDespite significant progress, existing methods are hindered by the scarcity of\nhigh-quality, diverse data and insufficient restoration priors, resulting in\nlimited generalization across various real-world scenarios. In this paper, we\npropose Dereflection Any Image, a comprehensive solution with an efficient data\npreparation pipeline and a generalizable model for robust reflection removal.\nFirst, we introduce a dataset named Diverse Reflection Removal (DRR) created by\nrandomly rotating reflective mediums in target scenes, enabling variation of\nreflection angles and intensities, and setting a new benchmark in scale,\nquality, and diversity. Second, we propose a diffusion-based framework with\none-step diffusion for deterministic outputs and fast inference. To ensure\nstable learning, we design a three-stage progressive training strategy,\nincluding reflection-invariant finetuning to encourage consistent outputs\nacross varying reflection patterns that characterize our dataset. Extensive\nexperiments show that our method achieves SOTA performance on both common\nbenchmarks and challenging in-the-wild images, showing superior generalization\nacross diverse real-world scenes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17358", "pdf": "https://arxiv.org/pdf/2503.17358", "abs": "https://arxiv.org/abs/2503.17358", "authors": ["Jerred Chen", "Ronald Clark"], "title": "Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image", "categories": ["cs.CV"], "comment": "Project page: https://jerredchen.github.io/image-as-imu/", "summary": "In many robotics and VR/AR applications, fast camera motions cause a high\nlevel of motion blur, causing existing camera pose estimation methods to fail.\nIn this work, we propose a novel framework that leverages motion blur as a rich\ncue for motion estimation rather than treating it as an unwanted artifact. Our\napproach works by predicting a dense motion flow field and a monocular depth\nmap directly from a single motion-blurred image. We then recover the\ninstantaneous camera velocity by solving a linear least squares problem under\nthe small motion assumption. In essence, our method produces an IMU-like\nmeasurement that robustly captures fast and aggressive camera movements. To\ntrain our model, we construct a large-scale dataset with realistic synthetic\nmotion blur derived from ScanNet++v2 and further refine our model by training\nend-to-end on real data using our fully differentiable pipeline. Extensive\nevaluations on real-world benchmarks demonstrate that our method achieves\nstate-of-the-art angular and translational velocity estimates, outperforming\ncurrent methods like MASt3R and COLMAP.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16585", "pdf": "https://arxiv.org/pdf/2503.16585", "abs": "https://arxiv.org/abs/2503.16585", "authors": ["Hadi Amini", "Md Jueal Mia", "Yasaman Saadati", "Ahmed Imteaj", "Seyedsina Nabavirazavi", "Urmish Thakker", "Md Zarif Hossain", "Awal Ahmed Fime", "S. S. Iyengar"], "title": "Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions", "categories": ["cs.CL", "cs.CV", "cs.DC", "cs.LG"], "comment": null, "summary": "Language models (LMs) are machine learning models designed to predict\nlinguistic patterns by estimating the probability of word sequences based on\nlarge-scale datasets, such as text. LMs have a wide range of applications in\nnatural language processing (NLP) tasks, including autocomplete and machine\ntranslation. Although larger datasets typically enhance LM performance,\nscalability remains a challenge due to constraints in computational power and\nresources. Distributed computing strategies offer essential solutions for\nimproving scalability and managing the growing computational demand. Further,\nthe use of sensitive datasets in training and deployment raises significant\nprivacy concerns. Recent research has focused on developing decentralized\ntechniques to enable distributed training and inference while utilizing diverse\ncomputational resources and enabling edge AI. This paper presents a survey on\ndistributed solutions for various LMs, including large language models (LLMs),\nvision language models (VLMs), multimodal LLMs (MLLMs), and small language\nmodels (SLMs). While LLMs focus on processing and generating text, MLLMs are\ndesigned to handle multiple modalities of data (e.g., text, images, and audio)\nand to integrate them for broader applications. To this end, this paper reviews\nkey advancements across the MLLM pipeline, including distributed training,\ninference, fine-tuning, and deployment, while also identifying the\ncontributions, limitations, and future areas of improvement. Further, it\ncategorizes the literature based on six primary focus areas of\ndecentralization. Our analysis describes gaps in current methodologies for\nenabling distributed solutions for LMs and outline future research directions,\nemphasizing the need for novel solutions to enhance the robustness and\napplicability of distributed LMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16711", "pdf": "https://arxiv.org/pdf/2503.16711", "abs": "https://arxiv.org/abs/2503.16711", "authors": ["Mihaela-Larisa Clement", "Mónika Farsang", "Felix Resch", "Radu Grosu"], "title": "Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Submitted to IROS 2025", "summary": "Autonomous agents that rely purely on perception to make real-time control\ndecisions require efficient and robust architectures. In this work, we\ndemonstrate that augmenting RGB input with depth information significantly\nenhances our agents' ability to predict steering commands compared to using RGB\nalone. We benchmark lightweight recurrent controllers that leverage the fused\nRGB-D features for sequential decision-making. To train our models, we collect\nhigh-quality data using a small-scale autonomous car controlled by an expert\ndriver via a physical steering wheel, capturing varying levels of steering\ndifficulty. Our models, trained under diverse configurations, were successfully\ndeployed on real hardware. Specifically, our findings reveal that the early\nfusion of depth data results in a highly robust controller, which remains\neffective even with frame drops and increased noise levels, without\ncompromising the network's focus on the task.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16848", "pdf": "https://arxiv.org/pdf/2503.16848", "abs": "https://arxiv.org/abs/2503.16848", "authors": ["Hou In Derek Pun", "Hou In Ivan Tam", "Austin T. Wang", "Xiaoliang Huo", "Angel X. Chang", "Manolis Savva"], "title": "HSM: Hierarchical Scene Motifs for Multi-Scale Indoor Scene Generation", "categories": ["cs.GR", "cs.CV"], "comment": "23 pages, 7 figures", "summary": "Despite advances in indoor 3D scene layout generation, synthesizing scenes\nwith dense object arrangements remains challenging. Existing methods primarily\nfocus on large furniture while neglecting smaller objects, resulting in\nunrealistically empty scenes. Those that place small objects typically do not\nhonor arrangement specifications, resulting in largely random placement not\nfollowing the text description. We present HSM, a hierarchical framework for\nindoor scene generation with dense object arrangements across spatial scales.\nIndoor scenes are inherently hierarchical, with surfaces supporting objects at\ndifferent scales, from large furniture on floors to smaller objects on tables\nand shelves. HSM embraces this hierarchy and exploits recurring cross-scale\nspatial patterns to generate complex and realistic indoor scenes in a unified\nmanner. Our experiments show that HSM outperforms existing methods by\ngenerating scenes that are more realistic and better conform to user input\nacross room types and spatial configurations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16965", "pdf": "https://arxiv.org/pdf/2503.16965", "abs": "https://arxiv.org/abs/2503.16965", "authors": ["Zhe Hu", "Jing Li", "Yu Yin"], "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only Training For Human-Centered Decision Making", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17117", "pdf": "https://arxiv.org/pdf/2503.17117", "abs": "https://arxiv.org/abs/2503.17117", "authors": ["Théo Bodrito", "Olivier Flasseur", "Julien Mairal", "Jean Ponce", "Maud Langlois", "Anne-Marie Lagrange"], "title": "A New Statistical Model of Star Speckles for Learning to Detect and Characterize Exoplanets in Direct Imaging Observations", "categories": ["astro-ph.IM", "astro-ph.EP", "cs.CV", "cs.LG", "stat.AP"], "comment": "Accepted to CVPR 2025", "summary": "The search for exoplanets is an active field in astronomy, with direct\nimaging as one of the most challenging methods due to faint exoplanet signals\nburied within stronger residual starlight. Successful detection requires\nadvanced image processing to separate the exoplanet signal from this nuisance\ncomponent. This paper presents a novel statistical model that captures nuisance\nfluctuations using a multi-scale approach, leveraging problem symmetries and a\njoint spectral channel representation grounded in physical principles. Our\nmodel integrates into an interpretable, end-to-end learnable framework for\nsimultaneous exoplanet detection and flux estimation. The proposed algorithm is\nevaluated against the state of the art using datasets from the SPHERE\ninstrument operating at the Very Large Telescope (VLT). It significantly\nimproves the precision-recall trade-off, notably on challenging datasets that\nare otherwise unusable by astronomers. The proposed approach is computationally\nefficient, robust to varying data quality, and well suited for large-scale\nobservational surveys.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17198", "pdf": "https://arxiv.org/pdf/2503.17198", "abs": "https://arxiv.org/abs/2503.17198", "authors": ["Yongli Xiang", "Ziming Hong", "Lina Yao", "Dadong Wang", "Tongliang Liu"], "title": "Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": "Code is released at https://github.com/tmllab/2025_CVPR_JailNTL", "summary": "Non-transferable learning (NTL) has been proposed to protect model\nintellectual property (IP) by creating a \"non-transferable barrier\" to restrict\ngeneralization from authorized to unauthorized domains. Recently, well-designed\nattack, which restores the unauthorized-domain performance by fine-tuning NTL\nmodels on few authorized samples, highlights the security risks of NTL-based\napplications. However, such attack requires modifying model weights, thus being\ninvalid in the black-box scenario. This raises a critical question: can we\ntrust the security of NTL models deployed as black-box systems? In this work,\nwe reveal the first loophole of black-box NTL models by proposing a novel\nattack method (dubbed as JailNTL) to jailbreak the non-transferable barrier\nthrough test-time data disguising. The main idea of JailNTL is to disguise\nunauthorized data so it can be identified as authorized by the NTL model,\nthereby bypassing the non-transferable barrier without modifying the NTL model\nweights. Specifically, JailNTL encourages unauthorized-domain disguising in two\nlevels, including: (i) data-intrinsic disguising (DID) for eliminating domain\ndiscrepancy and preserving class-related content at the input-level, and (ii)\nmodel-guided disguising (MGD) for mitigating output-level statistics difference\nof the NTL model. Empirically, when attacking state-of-the-art (SOTA) NTL\nmodels in the black-box scenario, JailNTL achieves an accuracy increase of up\nto 55.7% in the unauthorized domain by using only 1% authorized samples,\nlargely exceeding existing SOTA white-box attacks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17261", "pdf": "https://arxiv.org/pdf/2503.17261", "abs": "https://arxiv.org/abs/2503.17261", "authors": ["Jie Mei", "Chenyu Lin", "Yu Qiu", "Yaonan Wang", "Hui Zhang", "Ziyang Wang", "Dong Dai"], "title": "Cross-Modal Interactive Perception Network with Mamba for Lung Tumor Segmentation in PET-CT Images", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Lung cancer is a leading cause of cancer-related deaths globally. PET-CT is\ncrucial for imaging lung tumors, providing essential metabolic and anatomical\ninformation, while it faces challenges such as poor image quality, motion\nartifacts, and complex tumor morphology. Deep learning-based models are\nexpected to address these problems, however, existing small-scale and private\ndatasets limit significant performance improvements for these methods. Hence,\nwe introduce a large-scale PET-CT lung tumor segmentation dataset, termed\nPCLT20K, which comprises 21,930 pairs of PET-CT images from 605 patients.\nFurthermore, we propose a cross-modal interactive perception network with Mamba\n(CIPA) for lung tumor segmentation in PET-CT images. Specifically, we design a\nchannel-wise rectification module (CRM) that implements a channel state space\nblock across multi-modal features to learn correlated representations and helps\nfilter out modality-specific noise. A dynamic cross-modality interaction module\n(DCIM) is designed to effectively integrate position and context information,\nwhich employs PET images to learn regional position information and serves as a\nbridge to assist in modeling the relationships between local features of CT\nimages. Extensive experiments on a comprehensive benchmark demonstrate the\neffectiveness of our CIPA compared to the current state-of-the-art segmentation\nmethods. We hope our research can provide more exploration opportunities for\nmedical image segmentation. The dataset and code are available at\nhttps://github.com/mj129/CIPA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
