{"id": "2503.18942", "pdf": "https://arxiv.org/pdf/2503.18942", "abs": "https://arxiv.org/abs/2503.18942", "authors": ["Fangfu Liu", "Hanyang Wang", "Yimo Cai", "Kaiyan Zhang", "Xiaohang Zhan", "Yueqi Duan"], "title": "Video-T1: Test-Time Scaling for Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://liuff19.github.io/Video-T1", "summary": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time", "inference time", "scaling", "scale", "test-time compute"], "score": 6}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18866", "pdf": "https://arxiv.org/pdf/2503.18866", "abs": "https://arxiv.org/abs/2503.18866", "authors": ["Yangjun Ruan", "Neil Band", "Chris J. Maddison", "Tatsunori Hashimoto"], "title": "Reasoning to Learn from Latent Thoughts", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% $\\rightarrow$ 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "compute scaling", "inference compute"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18941", "pdf": "https://arxiv.org/pdf/2503.18941", "abs": "https://arxiv.org/abs/2503.18941", "authors": ["Hongru Cai", "Yongqi Li", "Ruifeng Yuan", "Wenjie Wang", "Zhen Zhang", "Wenjie Li", "Tat-Seng Chua"], "title": "Exploring Training and Inference Scaling Laws in Generative Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Generative retrieval has emerged as a novel paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers.\nAlthough promising, the mechanisms that underpin its performance and\nscalability remain largely unclear. We conduct a systematic investigation of\ntraining and inference scaling laws in generative retrieval, exploring how\nmodel size, training data scale, and inference-time compute jointly influence\nretrieval performance. To address the lack of suitable metrics, we propose a\nnovel evaluation measure inspired by contrastive entropy and generation loss,\nproviding a continuous performance signal that enables robust comparisons\nacross diverse generative retrieval methods. Our experiments show that\nn-gram-based methods demonstrate strong alignment with both training and\ninference scaling laws, especially when paired with larger LLMs. Furthermore,\nincreasing inference computation yields substantial performance gains,\nrevealing that generative retrieval can significantly benefit from higher\ncompute budgets at inference. Across these settings, LLaMA models consistently\noutperform T5 models, suggesting a particular advantage for larger decoder-only\nmodels in generative retrieval. Taken together, our findings underscore that\nmodel sizes, data availability, and inference computation interact to unlock\nthe full potential of generative retrieval, offering new insights for designing\nand optimizing future systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "scale"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18665", "pdf": "https://arxiv.org/pdf/2503.18665", "abs": "https://arxiv.org/abs/2503.18665", "authors": ["Bingchen Miao", "Yang Wu", "Minghe Gao", "Qifan Yu", "Wendong Bu", "Wenqiao Zhang", "Yunfei Li", "Siliang Tang", "Tat-Seng Chua", "Juncheng Li"], "title": "Boosting Virtual Agent Learning and Reasoning: A Step-wise, Multi-dimensional, and Generalist Reward Model with Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "The development of Generalist Virtual Agents (GVAs) powered by Multimodal\nLarge Language Models (MLLMs) has shown significant promise in autonomous task\nexecution. However, current training paradigms face critical limitations,\nincluding reliance on outcome supervision and labor-intensive human\nannotations. To address these challenges, we propose Similar, a Step-wise\nMulti-dimensional Generalist Reward Model, which offers fine-grained signals\nfor agent training and can choose better action for inference-time scaling.\nSpecifically, we begin by systematically defining five dimensions for\nevaluating agent actions. Building on this framework, we design an MCTS-P\nalgorithm to automatically collect and annotate step-wise, five-dimensional\nagent execution data. Using this data, we train Similar with the Triple-M\nstrategy. Furthermore, we introduce the first benchmark in the virtual agent\ndomain for step-wise, multi-dimensional reward model training and evaluation,\nnamed SRM. This benchmark consists of two components: SRMTrain, which serves as\nthe training set for Similar, and SRMEval, a manually selected test set for\nevaluating the reward model. Experimental results demonstrate that Similar,\nthrough its step-wise, multi-dimensional assessment and synergistic gain,\nprovides GVAs with effective intermediate signals during both training and\ninference-time scaling. The code is available at\nhttps://github.com/Galery23/Similar-v1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "MCTS"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "multi-dimensional", "fine-grained"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18923", "pdf": "https://arxiv.org/pdf/2503.18923", "abs": "https://arxiv.org/abs/2503.18923", "authors": ["Meng Cao", "Pengfei Hu", "Yingyao Wang", "Jihao Gu", "Haoran Tang", "Haoze Zhao", "Jiahua Dong", "Wangbo Yu", "Ge Zhang", "Ian Reid", "Xiaodan Liang"], "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language Models", "categories": ["cs.CV"], "comment": "24 pages", "summary": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference time", "test-time compute"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "factuality", "reliability"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17641", "pdf": "https://arxiv.org/pdf/2503.17641", "abs": "https://arxiv.org/abs/2503.17641", "authors": ["Chi Zhang", "Chengjian Feng", "Feng Yan", "Qiming Zhang", "Mingjin Zhang", "Yujie Zhong", "Jing Zhang", "Lin Ma"], "title": "InstructVEdit: A Holistic Approach for Instructional Video Editing", "categories": ["cs.CV"], "comment": "https://o937-blip.github.io/InstructVEdit", "summary": "Video editing according to instructions is a highly challenging task due to\nthe difficulty in collecting large-scale, high-quality edited video pair data.\nThis scarcity not only limits the availability of training data but also\nhinders the systematic exploration of model architectures and training\nstrategies. While prior work has improved specific aspects of video editing\n(e.g., synthesizing a video dataset using image editing techniques or\ndecomposed video editing training), a holistic framework addressing the above\nchallenges remains underexplored. In this study, we introduce InstructVEdit, a\nfull-cycle instructional video editing approach that: (1) establishes a\nreliable dataset curation workflow to initialize training, (2) incorporates two\nmodel architectural improvements to enhance edit quality while preserving\ntemporal consistency, and (3) proposes an iterative refinement strategy\nleveraging real-world data to enhance generalization and minimize train-test\ndiscrepancies. Extensive experiments show that InstructVEdit achieves\nstate-of-the-art performance in instruction-based video editing, demonstrating\nrobust adaptability to diverse real-world scenarios. Project page:\nhttps://o937-blip.github.io/InstructVEdit.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "iterative refinement"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18069", "pdf": "https://arxiv.org/pdf/2503.18069", "abs": "https://arxiv.org/abs/2503.18069", "authors": ["Si Shen", "Fei Huang", "Zhixiao Zhao", "Chang Liu", "Tiansheng Zheng", "Danhao Zhu"], "title": "Long Is More Important Than Difficult for Training Reasoning Models", "categories": ["cs.CL"], "comment": "15 pages,6 figures", "summary": "Difficult problems, which often result in long reasoning traces, are widely\nrecognized as key factors for enhancing the performance of reasoning models.\nHowever, such high-challenge problems are scarce, limiting the size of\navailable datasets. In this paper, we propose a simple method to decouple the\nreliance on problem difficulty. First, we empirically demonstrate that\nreasoning length, rather than problem difficulty, primarily influences the\nperformance of trained models. Second, we identify a scaling law on reasoning\nlength, showing that model performance increases in a log-linear fashion as the\nreasoning data length grows. Finally, we introduce a straightforward technique\nto generate reasoning data of arbitrary length, and show that synthesized data\nis effective for training reasoning models. After fine-tuning the\nQwen2.5-32B-Instruct language model on our Long1K dataset, we present our\nmodel, Long1K-32B, which achieves remarkable performance with only 1,000\ntraining samples, achieving 95.6\\% accuracy on MATH, and 71.1\\% on GPQA\noutperforming DeepSeek-R1-Distill-Qwen-32B. The model, code, and dataset are\nall open-sourced, available at https://huggingface.co/ZTss/LONG1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scaling law"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17825", "pdf": "https://arxiv.org/pdf/2503.17825", "abs": "https://arxiv.org/abs/2503.17825", "authors": ["Yawei Li", "Bin Ren", "Jingyun Liang", "Rakesh Ranjan", "Mengyuan Liu", "Nicu Sebe", "Ming-Hsuan Yang", "Luca Benini"], "title": "Fractal-IR: A Unified Framework for Efficient and Scalable Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "While vision transformers achieve significant breakthroughs in various image\nrestoration (IR) tasks, it is still challenging to efficiently scale them\nacross multiple types of degradations and resolutions. In this paper, we\npropose Fractal-IR, a fractal-based design that progressively refines degraded\nimages by repeatedly expanding local information into broader regions. This\nfractal architecture naturally captures local details at early stages and\nseamlessly transitions toward global context in deeper fractal stages, removing\nthe need for computationally heavy long-range self-attention mechanisms.\nMoveover, we observe the challenge in scaling up vision transformers for IR\ntasks. Through a series of analyses, we identify a holistic set of strategies\nto effectively guide model scaling. Extensive experimental results show that\nFractal-IR achieves state-of-the-art performance in seven common image\nrestoration tasks, including super-resolution, denoising, JPEG artifact\nremoval, IR in adverse weather conditions, motion deblurring, defocus\ndeblurring, and demosaicking. For $2\\times$ SR on Manga109, Fractal-IR achieves\na 0.21 dB PSNR gain. For grayscale image denoising on Urban100, Fractal-IR\nsurpasses the previous method by 0.2 dB for $\\sigma=50$.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17979", "pdf": "https://arxiv.org/pdf/2503.17979", "abs": "https://arxiv.org/abs/2503.17979", "authors": ["Weixiang Zhao", "Xingyu Sui", "Jiahe Guo", "Yulin Hu", "Yang Deng", "Yanyan Zhao", "Bing Qin", "Wanxiang Che", "Tat-Seng Chua", "Ting Liu"], "title": "Trade-offs in Large Reasoning Models: An Empirical Analysis of Deliberative and Adaptive Reasoning over Foundational Capabilities", "categories": ["cs.AI", "cs.CL"], "comment": "23 pages. Work in progress", "summary": "Recent advancements in Large Reasoning Models (LRMs), such as OpenAI's o1/o3\nand DeepSeek-R1, have demonstrated remarkable performance in specialized\nreasoning tasks through human-like deliberative thinking and long\nchain-of-thought reasoning. However, our systematic evaluation across various\nmodel families (DeepSeek, Qwen, and LLaMA) and scales (7B to 671B) reveals that\nacquiring these deliberative reasoning capabilities significantly reduces the\nfoundational capabilities of LRMs, including notable declines in helpfulness\nand harmlessness, alongside substantially increased inference costs.\nImportantly, we demonstrate that adaptive reasoning -- employing modes like\nZero-Thinking, Less-Thinking, and Summary-Thinking -- can effectively alleviate\nthese drawbacks. Our empirical insights underline the critical need for\ndeveloping more versatile LRMs capable of dynamically allocating inference-time\ncompute according to specific task characteristics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "o1"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "helpfulness", "harmlessness"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18334", "pdf": "https://arxiv.org/pdf/2503.18334", "abs": "https://arxiv.org/abs/2503.18334", "authors": ["Haotian Zhai", "Xinyu Chen", "Can Zhang", "Tianming Sha", "Ruirui Li"], "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild", "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n``Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17394", "pdf": "https://arxiv.org/pdf/2503.17394", "abs": "https://arxiv.org/abs/2503.17394", "authors": ["Kangrui Du", "Yuhang Wu", "Shikuang Deng", "Shi Gu"], "title": "Temporal Flexibility in Spiking Neural Networks: Towards Generalization Across Time Steps and Deployment Friendliness", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "comment": "20 pages, ICLR 2025", "summary": "Spiking Neural Networks (SNNs), models inspired by neural mechanisms in the\nbrain, allow for energy-efficient implementation on neuromorphic hardware.\nHowever, SNNs trained with current direct training approaches are constrained\nto a specific time step. This \"temporal inflexibility\" 1) hinders SNNs'\ndeployment on time-step-free fully event-driven chips and 2) prevents\nenergy-performance balance based on dynamic inference time steps. In this\nstudy, we first explore the feasibility of training SNNs that generalize across\ndifferent time steps. We then introduce Mixed Time-step Training (MTT), a novel\nmethod that improves the temporal flexibility of SNNs, making SNNs adaptive to\ndiverse temporal structures. During each iteration of MTT, random time steps\nare assigned to different SNN stages, with spikes transmitted between stages\nvia communication modules. After training, the weights are deployed and\nevaluated on both time-stepped and fully event-driven platforms. Experimental\nresults show that models trained by MTT gain remarkable temporal flexibility,\nfriendliness for both event-driven and clock-driven deployment (nearly lossless\non N-MNIST and 10.1% higher than standard methods on CIFAR10-DVS), enhanced\nnetwork generalization, and near SOTA performance. To the best of our\nknowledge, this is the first work to report the results of large-scale SNN\ndeployment on fully event-driven scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17488", "pdf": "https://arxiv.org/pdf/2503.17488", "abs": "https://arxiv.org/abs/2503.17488", "authors": ["Tianwen Zhou", "Jing Wang", "Songtao Wu", "Kuanhong Xu"], "title": "ProDehaze: Prompting Diffusion Models Toward Faithful Image Dehazing", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025", "summary": "Recent approaches using large-scale pretrained diffusion models for image\ndehazing improve perceptual quality but often suffer from hallucination issues,\nproducing unfaithful dehazed image to the original one. To mitigate this, we\npropose ProDehaze, a framework that employs internal image priors to direct\nexternal priors encoded in pretrained models. We introduce two types of\n\\textit{selective} internal priors that prompt the model to concentrate on\ncritical image areas: a Structure-Prompted Restorer in the latent space that\nemphasizes structure-rich regions, and a Haze-Aware Self-Correcting Refiner in\nthe decoding process to align distributions between clearer input regions and\nthe output. Extensive experiments on real-world datasets demonstrate that\nProDehaze achieves high-fidelity results in image dehazing, particularly in\nreducing color shifts. Our code is at https://github.com/TianwenZhou/ProDehaze.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17539", "pdf": "https://arxiv.org/pdf/2503.17539", "abs": "https://arxiv.org/abs/2503.17539", "authors": ["Bhishma Dedhia", "David Bourgin", "Krishna Kumar Singh", "Yuheng Li", "Yan Kang", "Zhan Xu", "Niraj K. Jha", "Yuchen Liu"], "title": "Generating, Fast and Slow: Scalable Parallel Video Generation with Video Interface Networks", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiTs) can generate short photorealistic videos, yet\ndirectly training and sampling longer videos with full attention across the\nvideo remains computationally challenging. Alternative methods break long\nvideos down into sequential generation of short video segments, requiring\nmultiple sampling chain iterations and specialized consistency modules. To\novercome these challenges, we introduce a new paradigm called Video Interface\nNetworks (VINs), which augment DiTs with an abstraction module to enable\nparallel inference of video chunks. At each diffusion step, VINs encode global\nsemantics from the noisy input of local chunks and the encoded representations,\nin turn, guide DiTs in denoising chunks in parallel. The coupling of VIN and\nDiT is learned end-to-end on the denoising objective. Further, the VIN\narchitecture maintains fixed-size encoding tokens that encode the input via a\nsingle cross-attention step. Disentangling the encoding tokens from the input\nthus enables VIN to scale to long videos and learn essential semantics.\nExperiments on VBench demonstrate that VINs surpass existing chunk-based\nmethods in preserving background consistency and subject coherence. We then\nshow via an optical flow analysis that our approach attains state-of-the-art\nmotion smoothness while using 25-40% fewer FLOPs than full generation. Finally,\nhuman raters favorably assessed the overall video quality and temporal\nconsistency of our method in a user study.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17860", "pdf": "https://arxiv.org/pdf/2503.17860", "abs": "https://arxiv.org/abs/2503.17860", "authors": ["Felix Faltings", "Wei Wei", "Yujia Bao"], "title": "Enhancing Retrieval Systems with Inference-Time Logical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Traditional retrieval methods rely on transforming user queries into vector\nrepresentations and retrieving documents based on cosine similarity within an\nembedding space. While efficient and scalable, this approach often fails to\nhandle complex queries involving logical constructs such as negations,\nconjunctions, and disjunctions. In this paper, we propose a novel\ninference-time logical reasoning framework that explicitly incorporates logical\nreasoning into the retrieval process. Our method extracts logical reasoning\nstructures from natural language queries and then composes the individual\ncosine similarity scores to formulate the final document scores. This approach\nenables the retrieval process to handle complex logical reasoning without\ncompromising computational efficiency. Our results on both synthetic and\nreal-world benchmarks demonstrate that the proposed method consistently\noutperforms traditional retrieval methods across different models and datasets,\nsignificantly improving retrieval performance for complex queries.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17593", "pdf": "https://arxiv.org/pdf/2503.17593", "abs": "https://arxiv.org/abs/2503.17593", "authors": ["Mehdi Noroozi", "Alberto Gil Ramos", "Luca Morreale", "Ruchika Chavhan", "Malcolm Chadwick", "Abhinav Mehrotra", "Sourav Bhattacharya"], "title": "Guidance Free Image Editing via Explicit Conditioning", "categories": ["cs.CV"], "comment": null, "summary": "Current sampling mechanisms for conditional diffusion models rely mainly on\nClassifier Free Guidance (CFG) to generate high-quality images. However, CFG\nrequires several denoising passes in each time step, e.g., up to three passes\nin image editing tasks, resulting in excessive computational costs. This paper\nintroduces a novel conditioning technique to ease the computational burden of\nthe well-established guidance techniques, thereby significantly improving the\ninference time of diffusion models. We present Explicit Conditioning (EC) of\nthe noise distribution on the input modalities to achieve this. Intuitively, we\nmodel the noise to guide the conditional diffusion model during the diffusion\nprocess. We present evaluations on image editing tasks and demonstrate that EC\noutperforms CFG in generating diverse high-quality images with significantly\nreduced computations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17633", "pdf": "https://arxiv.org/pdf/2503.17633", "abs": "https://arxiv.org/abs/2503.17633", "authors": ["Tejas Panambur", "Mario Parente"], "title": "Enhancing Martian Terrain Recognition with Deep Constrained Clustering", "categories": ["cs.CV"], "comment": null, "summary": "Martian terrain recognition is pivotal for advancing our understanding of\ntopography, geomorphology, paleoclimate, and habitability. While deep\nclustering methods have shown promise in learning semantically homogeneous\nfeature embeddings from Martian rover imagery, the natural variations in\nintensity, scale, and rotation pose significant challenges for accurate terrain\nclassification. To address these limitations, we propose Deep Constrained\nClustering with Metric Learning (DCCML), a novel algorithm that leverages\nmultiple constraint types to guide the clustering process. DCCML incorporates\nsoft must-link constraints derived from spatial and depth similarities between\nneighboring patches, alongside hard constraints from stereo camera pairs and\ntemporally adjacent images. Experimental evaluation on the Curiosity rover\ndataset (with 150 clusters) demonstrates that DCCML increases homogeneous\nclusters by 16.7 percent while reducing the Davies-Bouldin Index from 3.86 to\n1.82 and boosting retrieval accuracy from 86.71 percent to 89.86 percent. This\nimprovement enables more precise classification of Martian geological features,\nadvancing our capacity to analyze and understand the planet's landscape.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17668", "pdf": "https://arxiv.org/pdf/2503.17668", "abs": "https://arxiv.org/abs/2503.17668", "authors": ["Usha Kumari", "Shuvendu Rana"], "title": "3D Modeling: Camera Movement Estimation and path Correction for SFM Model using the Combination of Modified A-SIFT and Stereo System", "categories": ["cs.CV"], "comment": null, "summary": "Creating accurate and efficient 3D models poses significant challenges,\nparticularly in addressing large viewpoint variations, computational\ncomplexity, and alignment discrepancies. Efficient camera path generation can\nhelp resolve these issues. In this context, a modified version of the Affine\nScale-Invariant Feature Transform (ASIFT) is proposed to extract more matching\npoints with reduced computational overhead, ensuring an adequate number of\ninliers for precise camera rotation angle estimation. Additionally, a novel\ntwo-camera-based rotation correction model is introduced to mitigate small\nrotational errors, further enhancing accuracy. Furthermore, a stereo\ncamera-based translation estimation and correction model is implemented to\ndetermine camera movement in 3D space by altering the Structure From Motion\n(SFM) model. Finally, the novel combination of ASIFT and two camera-based SFM\nmodels provides an accurate camera movement trajectory in 3D space.\nExperimental results show that the proposed camera movement approach achieves\n99.9% accuracy compared to the actual camera movement path and outperforms\nstate-of-the-art camera path estimation methods. By leveraging this accurate\ncamera path, the system facilitates the creation of precise 3D models, making\nit a robust solution for applications requiring high fidelity and efficiency in\n3D reconstruction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17965", "pdf": "https://arxiv.org/pdf/2503.17965", "abs": "https://arxiv.org/abs/2503.17965", "authors": ["Beining Xu", "Arkaitz Zubiaga"], "title": "Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated Texts", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "14 pages, 3 figures", "summary": "Large Language Models (LLMs) have demonstrated exceptional performance on a\nrange of downstream NLP tasks by generating text that closely resembles human\nwriting. However, the ease of achieving this similarity raises concerns from\npotential malicious uses at scale by bad actors, as LLM-generated text becomes\nincreasingly difficult to discern from human text. Although detection methods\nhave been developed to address this issue, bad actors can further manipulate\nLLM-generated texts to make them less detectable. In this work, we study how\nfurther editing texts with Reinforcement Learning from Human Feedback (RLHF),\nwhich aligns model outputs with human preferences, affects (a) the quality of\ngenerated texts for two tasks, and (b) the performance of LLM-generated text\ndetectors, looking at both training-based and zero-shot detection methods.\nAlthough RLHF improves the quality of LLM-generated texts, we find that it also\ntends to produce more detectable, lengthy, and repetitive outputs.\nAdditionally, we observe that training-based detectors are vulnerable to short\ntexts and to texts that incorporate code, whereas zero-shot detectors exhibit\ngreater robustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17672", "pdf": "https://arxiv.org/pdf/2503.17672", "abs": "https://arxiv.org/abs/2503.17672", "authors": ["Qing Zhong", "Peng-Tao Jiang", "Wen Wang", "Guodong Ding", "Lin Wu", "Kaiqi Huang"], "title": "A Temporal Modeling Framework for Video Pre-Training on Video Instance Segmentation", "categories": ["cs.CV"], "comment": "7 pages, 5figures, 6 tables, Accepted to ICME 2025", "summary": "Contemporary Video Instance Segmentation (VIS) methods typically adhere to a\npre-train then fine-tune regime, where a segmentation model trained on images\nis fine-tuned on videos. However, the lack of temporal knowledge in the\npre-trained model introduces a domain gap which may adversely affect the VIS\nperformance. To effectively bridge this gap, we present a novel video\npre-training approach to enhance VIS models, especially for videos with\nintricate instance relationships. Our crucial innovation focuses on reducing\ndisparities between the pre-training and fine-tuning stages. Specifically, we\nfirst introduce consistent pseudo-video augmentations to create diverse\npseudo-video samples for pre-training while maintaining the instance\nconsistency across frames. Then, we incorporate a multi-scale temporal module\nto enhance the model's ability to model temporal relations through self- and\ncross-attention at short- and long-term temporal spans. Our approach does not\nset constraints on model architecture and can integrate seamlessly with various\nVIS methods. Experiment results on commonly adopted VIS benchmarks show that\nour method consistently outperforms state-of-the-art methods. Our approach\nachieves a notable 4.0% increase in average precision on the challenging OVIS\ndataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17699", "pdf": "https://arxiv.org/pdf/2503.17699", "abs": "https://arxiv.org/abs/2503.17699", "authors": ["Haolin Qin", "Tingfa Xu", "Tianhao Li", "Zhenxiang Chen", "Tao Feng", "Jianan Li"], "title": "MUST: The First Dataset and Unified Framework for Multispectral UAV Single Object Tracking", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "UAV tracking faces significant challenges in real-world scenarios, such as\nsmall-size targets and occlusions, which limit the performance of RGB-based\ntrackers. Multispectral images (MSI), which capture additional spectral\ninformation, offer a promising solution to these challenges. However, progress\nin this field has been hindered by the lack of relevant datasets. To address\nthis gap, we introduce the first large-scale Multispectral UAV Single Object\nTracking dataset (MUST), which includes 250 video sequences spanning diverse\nenvironments and challenges, providing a comprehensive data foundation for\nmultispectral UAV tracking. We also propose a novel tracking framework,\nUNTrack, which encodes unified spectral, spatial, and temporal features from\nspectrum prompts, initial templates, and sequential searches. UNTrack employs\nan asymmetric transformer with a spectral background eliminate mechanism for\noptimal relationship modeling and an encoder that continuously updates the\nspectrum prompt to refine tracking, improving both accuracy and efficiency.\nExtensive experiments show that our proposed UNTrack outperforms\nstate-of-the-art UAV trackers. We believe our dataset and framework will drive\nfuture research in this area. The dataset is available on\nhttps://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Tracking.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17716", "pdf": "https://arxiv.org/pdf/2503.17716", "abs": "https://arxiv.org/abs/2503.17716", "authors": ["Tim Alpherts", "Sennay Ghebreab", "Nanne van Noord"], "title": "EMPLACE: Self-Supervised Urban Scene Change Detection", "categories": ["cs.CV"], "comment": "7 pages, 7 figures, published at AAAI 2025", "summary": "Urban change is a constant process that influences the perception of\nneighbourhoods and the lives of the people within them. The field of Urban\nScene Change Detection (USCD) aims to capture changes in street scenes using\ncomputer vision and can help raise awareness of changes that make it possible\nto better understand the city and its residents. Traditionally, the field of\nUSCD has used supervised methods with small scale datasets. This constrains\nmethods when applied to new cities, as it requires labour-intensive labeling\nprocesses and forces a priori definitions of relevant change. In this paper we\nintroduce AC-1M the largest USCD dataset by far of over 1.1M images, together\nwith EMPLACE, a self-supervising method to train a Vision Transformer using our\nadaptive triplet loss. We show EMPLACE outperforms SOTA methods both as a\npre-training method for linear fine-tuning as well as a zero-shot setting.\nLastly, in a case study of Amsterdam, we show that we are able to detect both\nsmall and large changes throughout the city and that changes uncovered by\nEMPLACE, depending on size, correlate with housing prices - which in turn is\nindicative of inequity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18172", "pdf": "https://arxiv.org/pdf/2503.18172", "abs": "https://arxiv.org/abs/2503.18172", "authors": ["Zixin Chen", "Sicheng Song", "Kashun Shum", "Yanna Lin", "Rui Sheng", "Huamin Qu"], "title": "Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "31 pages in total. Under Review For ARR", "summary": "Misleading chart visualizations, which intentionally manipulate data\nrepresentations to support specific claims, can distort perceptions and lead to\nincorrect conclusions. Despite decades of research, misleading visualizations\nremain a widespread and pressing issue. Recent advances in multimodal large\nlanguage models (MLLMs) have demonstrated strong chart comprehension\ncapabilities, yet no existing work has systematically evaluated their ability\nto detect and interpret misleading charts. This paper introduces the Misleading\nChart Question Answering (Misleading ChartQA) Benchmark, a large-scale\nmultimodal dataset designed to assess MLLMs in identifying and reasoning about\nmisleading charts. It contains over 3,000 curated examples, covering 21 types\nof misleaders and 10 chart types. Each example includes standardized chart\ncode, CSV data, and multiple-choice questions with labeled explanations,\nvalidated through multi-round MLLM checks and exhausted expert human review. We\nbenchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitations\nin identifying visually deceptive practices. We also propose a novel pipeline\nthat detects and localizes misleaders, enhancing MLLMs' accuracy in misleading\nchart interpretation. Our work establishes a foundation for advancing\nMLLM-driven misleading chart comprehension. We publicly release the sample\ndataset to support further research in this critical area.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "question answering"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17788", "pdf": "https://arxiv.org/pdf/2503.17788", "abs": "https://arxiv.org/abs/2503.17788", "authors": ["Gaoge Han", "Yongkang Cheng", "Zhe Chen", "Shaoli Huang", "Tongliang Liu"], "title": "Aligning Foundation Model Priors and Diffusion-Based Hand Interactions for Occlusion-Resistant Two-Hand Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Two-hand reconstruction from monocular images faces persistent challenges due\nto complex and dynamic hand postures and occlusions, causing significant\ndifficulty in achieving plausible interaction alignment. Existing approaches\nstruggle with such alignment issues, often resulting in misalignment and\npenetration artifacts. To tackle this, we propose a novel framework that\nattempts to precisely align hand poses and interactions by synergistically\nintegrating foundation model-driven 2D priors with diffusion-based interaction\nrefinement for occlusion-resistant two-hand reconstruction. First, we introduce\na Fusion Alignment Encoder that learns to align fused multimodal priors\nkeypoints, segmentation maps, and depth cues from foundation models during\ntraining. This provides robust structured guidance, further enabling efficient\ninference without foundation models at test time while maintaining high\nreconstruction accuracy. Second, we employ a two-hand diffusion model\nexplicitly trained to transform interpenetrated poses into plausible,\nnon-penetrated interactions, leveraging gradient-guided denoising to correct\nartifacts and ensure realistic spatial relations. Extensive evaluations\ndemonstrate that our method achieves state-of-the-art performance on\nInterHand2.6M, FreiHAND, and HIC datasets, significantly advancing occlusion\nhandling and interaction robustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17798", "pdf": "https://arxiv.org/pdf/2503.17798", "abs": "https://arxiv.org/abs/2503.17798", "authors": ["Zexu Huang", "Min Xu", "Stuart Perry"], "title": "GaussianFocus: Constrained Attention Focus for 3D Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent developments in 3D reconstruction and neural rendering have\nsignificantly propelled the capabilities of photo-realistic 3D scene rendering\nacross various academic and industrial fields. The 3D Gaussian Splatting\ntechnique, alongside its derivatives, integrates the advantages of\nprimitive-based and volumetric representations to deliver top-tier rendering\nquality and efficiency. Despite these advancements, the method tends to\ngenerate excessive redundant noisy Gaussians overfitted to every training view,\nwhich degrades the rendering quality. Additionally, while 3D Gaussian Splatting\nexcels in small-scale and object-centric scenes, its application to larger\nscenes is hindered by constraints such as limited video memory, excessive\noptimization duration, and variable appearance across views. To address these\nchallenges, we introduce GaussianFocus, an innovative approach that\nincorporates a patch attention algorithm to refine rendering quality and\nimplements a Gaussian constraints strategy to minimize redundancy. Moreover, we\npropose a subdivision reconstruction strategy for large-scale scenes, dividing\nthem into smaller, manageable blocks for individual training. Our results\nindicate that GaussianFocus significantly reduces unnecessary Gaussians and\nenhances rendering quality, surpassing existing State-of-The-Art (SoTA)\nmethods. Furthermore, we demonstrate the capability of our approach to\neffectively manage and render large scenes, such as urban environments, whilst\nmaintaining high fidelity in the visual output.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17814", "pdf": "https://arxiv.org/pdf/2503.17814", "abs": "https://arxiv.org/abs/2503.17814", "authors": ["Wen Li", "Chen Liu", "Shangshu Yu", "Dunqiang Liu", "Yin Zhou", "Siqi Shen", "Chenglu Wen", "Cheng Wang"], "title": "LightLoc: Learning Outdoor LiDAR Localization at Light Speed", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Scene coordinate regression achieves impressive results in outdoor LiDAR\nlocalization but requires days of training. Since training needs to be repeated\nfor each new scene, long training times make these methods impractical for\ntime-sensitive applications, such as autonomous driving, drones, and robotics.\nWe identify large coverage areas and vast data in large-scale outdoor scenes as\nkey challenges that limit fast training. In this paper, we propose LightLoc,\nthe first method capable of efficiently learning localization in a new scene at\nlight speed. LightLoc introduces two novel techniques to address these\nchallenges. First, we introduce sample classification guidance to assist\nregression learning, reducing ambiguity from similar samples and improving\ntraining efficiency. Second, we propose redundant sample downsampling to remove\nwell-learned frames during training, reducing training time without\ncompromising accuracy. Additionally, the fast training and confidence\nestimation capabilities of sample classification enable its integration into\nSLAM, effectively eliminating error accumulation. Extensive experiments on\nlarge-scale outdoor datasets demonstrate that LightLoc achieves\nstate-of-the-art performance with a 50x reduction in training time than\nexisting methods. Our code is available at https://github.com/liw95/LightLoc.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17820", "pdf": "https://arxiv.org/pdf/2503.17820", "abs": "https://arxiv.org/abs/2503.17820", "authors": ["Zheng Lin", "Nan Zhou", "Chen-Xi Du", "Deng-Ping Fan", "Shi-Min Hu"], "title": "RefCut: Interactive Segmentation with Reference Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Interactive segmentation aims to segment the specified target on the image\nwith positive and negative clicks from users. Interactive ambiguity is a\ncrucial issue in this field, which refers to the possibility of multiple\ncompliant outcomes with the same clicks, such as selecting a part of an object\nversus the entire object, a single object versus a combination of multiple\nobjects, and so on. The existing methods cannot provide intuitive guidance to\nthe model, which leads to unstable output results and makes it difficult to\nmeet the large-scale and efficient annotation requirements for specific targets\nin some scenarios. To bridge this gap, we introduce RefCut, a reference-based\ninteractive segmentation framework designed to address part ambiguity and\nobject ambiguity in segmenting specific targets. Users only need to provide a\nreference image and corresponding reference masks, and the model will be\noptimized based on them, which greatly reduces the interactive burden on users\nwhen annotating a large number of such targets. In addition, to enrich these\ntwo kinds of ambiguous data, we propose a new Target Disassembly Dataset which\ncontains two subsets of part disassembly and object disassembly for evaluation.\nIn the combination evaluation of multiple datasets, our RefCut achieved\nstate-of-the-art performance. Extensive experiments and visualized results\ndemonstrate that RefCut advances the field of intuitive and controllable\ninteractive segmentation. Our code will be publicly available and the demo\nvideo is in https://www.lin-zheng.com/refcut.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18288", "pdf": "https://arxiv.org/pdf/2503.18288", "abs": "https://arxiv.org/abs/2503.18288", "authors": ["Cheng Huang", "Fan Gao", "Nyima Tashi", "Yutong Liu", "Xiangxiang Wang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Xiao Feng", "Yongbin Yu"], "title": "Sun-Shine: A Large Language Model for Tibetan Culture", "categories": ["cs.CL"], "comment": null, "summary": "Tibetan, a minority language in China, features a highly intricate\ngrammatical structure, characterized by four verb tenses and a tense system\nwith frequent irregularities, contributing to its extensive inflectional\ndiversity. Recently, advances in Large Language Models (LLMs) have transformed\nthe paradigm in many domains. Despite the success in other fields, current LLMs\noften fall short in catering to the needs of domain experts like Tibetans, and\nthe potential of LLMs for Tibetan culture is under-explored. The intrinsic\nreasons are the immense and intricate nature of Tibetan culture as well as the\nnecessity for higher granularity and richness in knowledge. Simultaneously, the\ncomplexity and uniqueness of its grammatical structure, coupled with its status\nas a minority ethnic language, contribute to data scarcity, which remains a\nfundamental challenge. To alleviate these issues, we introduce Llama-Sunshine\n(Sun-Shine), the first large language model for Tibetan culture, which is\nexpert in various Tibetan language processing tasks. Sun-Shine incorporates\nstate-of-the-art model architectures optimized for Tibetan's linguistic\nfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverse\nTibetan texts such as literature, religious scripts, news, and conversational\ndata, which is also the first large-scale dataset for Tibetan culture. Though\ncomprehensive experiments, Sun-Shine not only demonstrates a higher level of\nknowledge expertise for Tibetan culture but also gains preliminary embodied\nintelligence capabilities in Tibetan language processing tasks, like language\nmodeling, text classification, machine translation, and syntactic analysis.\nMoreover, it excels in low-resource scenarios, showcasing strong generalization\ncapabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18360", "pdf": "https://arxiv.org/pdf/2503.18360", "abs": "https://arxiv.org/abs/2503.18360", "authors": ["Yiran Hu", "Huanghai Liu", "Qingjing Chen", "Ning Zheng", "Chong Wang", "Yun Liu", "Charles L. A. Clarke", "Weixing Shen"], "title": "J&H: Evaluating the Robustness of Large Language Models Under Knowledge-Injection Attacks in Legal Domain", "categories": ["cs.CL"], "comment": "10 pages, 5 figures", "summary": "As the scale and capabilities of Large Language Models (LLMs) increase, their\napplications in knowledge-intensive fields such as legal domain have garnered\nwidespread attention. However, it remains doubtful whether these LLMs make\njudgments based on domain knowledge for reasoning. If LLMs base their judgments\nsolely on specific words or patterns, rather than on the underlying logic of\nthe language, the ''LLM-as-judges'' paradigm poses substantial risks in the\nreal-world applications. To address this question, we propose a method of legal\nknowledge injection attacks for robustness testing, thereby inferring whether\nLLMs have learned legal knowledge and reasoning logic. In this paper, we\npropose J&H: an evaluation framework for detecting the robustness of LLMs under\nknowledge injection attacks in the legal domain. The aim of the framework is to\nexplore whether LLMs perform deductive reasoning when accomplishing legal\ntasks. To further this aim, we have attacked each part of the reasoning logic\nunderlying these tasks (major premise, minor premise, and conclusion\ngeneration). We have collected mistakes that legal experts might make in\njudicial decisions in the real world, such as typos, legal synonyms, inaccurate\nexternal legal statutes retrieval. However, in real legal practice, legal\nexperts tend to overlook these mistakes and make judgments based on logic.\nHowever, when faced with these errors, LLMs are likely to be misled by\ntypographical errors and may not utilize logic in their judgments. We conducted\nknowledge injection attacks on existing general and domain-specific LLMs.\nCurrent LLMs are not robust against the attacks employed in our experiments. In\naddition we propose and compare several methods to enhance the knowledge\nrobustness of LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17934", "pdf": "https://arxiv.org/pdf/2503.17934", "abs": "https://arxiv.org/abs/2503.17934", "authors": ["Xuewei Chen", "Zhimin Chen", "Yiren Song"], "title": "TransAnimate: Taming Layer Diffusion to Generate RGBA Video", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video generative models have made remarkable advancements in recent\nyears. However, generating RGBA videos with alpha channels for transparency and\nvisual effects remains a significant challenge due to the scarcity of suitable\ndatasets and the complexity of adapting existing models for this purpose. To\naddress these limitations, we present TransAnimate, an innovative framework\nthat integrates RGBA image generation techniques with video generation modules,\nenabling the creation of dynamic and transparent videos. TransAnimate\nefficiently leverages pre-trained text-to-transparent image model weights and\ncombines them with temporal models and controllability plugins trained on RGB\nvideos, adapting them for controllable RGBA video generation tasks.\nAdditionally, we introduce an interactive motion-guided control mechanism,\nwhere directional arrows define movement and colors adjust scaling, offering\nprecise and intuitive control for designing game effects. To further alleviate\ndata scarcity, we have developed a pipeline for creating an RGBA video dataset,\nincorporating high-quality game effect videos, extracted foreground objects,\nand synthetic transparent videos. Comprehensive experiments demonstrate that\nTransAnimate generates high-quality RGBA videos, establishing it as a practical\nand effective tool for applications in gaming and visual effects.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18562", "pdf": "https://arxiv.org/pdf/2503.18562", "abs": "https://arxiv.org/abs/2503.18562", "authors": ["Nariman Naderi", "Seyed Amir Ahmad Safavi-Naini", "Thomas Savage", "Zahra Atf", "Peter Lewis", "Girish Nadkarni", "Ali Soroush"], "title": "Self-Reported Confidence of Large Language Models in Gastroenterology: Analysis of Commercial, Open-Source, and Quantized Models", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "35 pages, 5 figures, 1 table, 7 supplementary figures", "summary": "This study evaluated self-reported response certainty across several large\nlanguage models (GPT, Claude, Llama, Phi, Mistral, Gemini, Gemma, and Qwen)\nusing 300 gastroenterology board-style questions. The highest-performing models\n(GPT-o1 preview, GPT-4o, and Claude-3.5-Sonnet) achieved Brier scores of\n0.15-0.2 and AUROC of 0.6. Although newer models demonstrated improved\nperformance, all exhibited a consistent tendency towards overconfidence.\nUncertainty estimation presents a significant challenge to the safe use of LLMs\nin healthcare. Keywords: Large Language Models; Confidence Elicitation;\nArtificial Intelligence; Gastroenterology; Uncertainty Quantification", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17940", "pdf": "https://arxiv.org/pdf/2503.17940", "abs": "https://arxiv.org/abs/2503.17940", "authors": ["Dong Zhao", "Jinlong Li", "Shuang Wang", "Mengyao Wu", "Qi Zang", "Nicu Sebe", "Zhun Zhong"], "title": "FisherTune: Fisher-Guided Robust Tuning of Vision Foundation Models for Domain Generalized Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Vision Foundation Models (VFMs) excel in generalization due to large-scale\npretraining, but fine-tuning them for Domain Generalized Semantic Segmentation\n(DGSS) while maintaining this ability remains challenging. Existing approaches\neither selectively fine-tune parameters or freeze the VFMs and update only the\nadapters, both of which may underutilize the VFMs' full potential in DGSS\ntasks. We observe that domain-sensitive parameters in VFMs, arising from task\nand distribution differences, can hinder generalization. To address this, we\npropose \\textbf{FisherTune}, a robust fine-tuning method guided by the\nDomain-Related Fisher Information Matrix (DR-FIM). DR-FIM measures parameter\nsensitivity across tasks and domains, enabling selective updates that preserve\ngeneralization and enhance DGSS adaptability. FisherTune incorporates\nvariational inference to stabilize DR-FIM estimation, treating parameters as\nGaussian-distributed variables and leveraging pre-trained priors. Extensive\nexperiments show that FisherTune achieves superior cross-domain segmentation\nwhile maintaining generalization, outperforming selective-parameter and\nadapter-based methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17966", "pdf": "https://arxiv.org/pdf/2503.17966", "abs": "https://arxiv.org/abs/2503.17966", "authors": ["Zeng-Hui Zhu", "Wei Lu", "Si-Bao Chen", "Chris H. Q. Ding", "Jin Tang", "Bin Luo"], "title": "Real-World Remote Sensing Image Dehazing: Benchmark and Baseline", "categories": ["cs.CV", "eess.IV"], "comment": "11 pages, 9 figures, real-world remote sensing image dehazing dataset", "summary": "Remote Sensing Image Dehazing (RSID) poses significant challenges in\nreal-world scenarios due to the complex atmospheric conditions and severe color\ndistortions that degrade image quality. The scarcity of real-world remote\nsensing hazy image pairs has compelled existing methods to rely primarily on\nsynthetic datasets. However, these methods struggle with real-world\napplications due to the inherent domain gap between synthetic and real data. To\naddress this, we introduce Real-World Remote Sensing Hazy Image Dataset\n(RRSHID), the first large-scale dataset featuring real-world hazy and dehazed\nimage pairs across diverse atmospheric conditions. Based on this, we propose\nMCAF-Net, a novel framework tailored for real-world RSID. Its effectiveness\narises from three innovative components: Multi-branch Feature Integration Block\nAggregator (MFIBA), which enables robust feature extraction through cascaded\nintegration blocks and parallel multi-branch processing; Color-Calibrated\nSelf-Supervised Attention Module (CSAM), which mitigates complex color\ndistortions via self-supervised learning and attention-guided refinement; and\nMulti-Scale Feature Adaptive Fusion Module (MFAFM), which integrates features\neffectively while preserving local details and global context. Extensive\nexperiments validate that MCAF-Net demonstrates state-of-the-art performance in\nreal-world RSID, while maintaining competitive performance on synthetic\ndatasets. The introduction of RRSHID and MCAF-Net sets new benchmarks for\nreal-world RSID research, advancing practical solutions for this complex task.\nThe code and dataset are publicly available at\n\\url{https://github.com/lwCVer/RRSHID}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18596", "pdf": "https://arxiv.org/pdf/2503.18596", "abs": "https://arxiv.org/abs/2503.18596", "authors": ["Yihan Wang", "Peiyu Liu", "Xin Yang"], "title": "LinkAlign: Scalable Schema Linking for Real-World Large-Scale Multi-Database Text-to-SQL", "categories": ["cs.CL"], "comment": null, "summary": "Schema linking is a critical bottleneck in achieving human-level performance\nin Text-to-SQL tasks, particularly in real-world large-scale multi-database\nscenarios. Addressing schema linking faces two major challenges: (1) Database\nRetrieval: selecting the correct database from a large schema pool in\nmulti-database settings, while filtering out irrelevant ones. (2) Schema Item\nGrounding: accurately identifying the relevant tables and columns from within a\nlarge and redundant schema for SQL generation. To address this, we introduce\nLinkAlign, a novel framework that can effectively adapt existing baselines to\nreal-world environments by systematically addressing schema linking. Our\nframework comprises three key steps: multi-round semantic enhanced retrieval\nand irrelevant information isolation for Challenge 1, and schema extraction\nenhancement for Challenge 2. We evaluate our method performance of schema\nlinking on the SPIDER and BIRD benchmarks, and the ability to adapt existing\nText-to-SQL models to real-world environments on the SPIDER 2.0-lite benchmark.\nExperiments show that LinkAlign outperforms existing baselines in\nmulti-database settings, demonstrating its effectiveness and robustness. On the\nother hand, our method ranks highest among models excluding those using long\nchain-of-thought reasoning LLMs. This work bridges the gap between current\nresearch and real-world scenarios, providing a practical solution for robust\nand scalable schema linking. The codes are available at\nhttps://github.com/Satissss/LinkAlign.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18646", "pdf": "https://arxiv.org/pdf/2503.18646", "abs": "https://arxiv.org/abs/2503.18646", "authors": ["Zhen-Song Chen", "Hong-Wei Ding", "Xian-Jia Wang", "Witold Pedrycz"], "title": "ZeroLM: Data-Free Transformer Architecture Search for Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Neural architecture search (NAS) provides a systematic framework for\nautomating the design of neural network architectures, yet its widespread\nadoption is hindered by prohibitive computational requirements. Existing\nzero-cost proxy methods, while reducing search overhead, demonstrate inadequate\nperformance in architecture ranking tasks, particularly for Transformer-based\nmodels where they often underperform simple parameter counting metrics. Current\nautomated proxy discovery approaches suffer from extended search times,\nsusceptibility to data overfitting, and structural complexity. This paper\nintroduces a novel zero-cost proxy methodology that quantifies model capacity\nthrough efficient weight statistics computation while decomposing Transformer\narchitectures into functionally distinct sub-modules, thereby optimizing the\nbalance of their contributions to overall performance. Our comprehensive\nevaluation demonstrates the superiority of this approach, achieving a\nSpearman's rho of 0.76 and Kendall's tau of 0.53 on the FlexiBERT benchmark.\nThe proposed method exhibits exceptional computational efficiency while\nmaintaining robust performance across diverse NAS benchmark tasks, offering a\npractical solution for large-scale architecture search.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18010", "pdf": "https://arxiv.org/pdf/2503.18010", "abs": "https://arxiv.org/abs/2503.18010", "authors": ["Thomas Dags", "Simon Weber", "Ya-Wei Eileen Lin", "Ronen Talmon", "Daniel Cremers", "Michael Lindenbaum", "Alfred M. Bruckstein", "Ron Kimmel"], "title": "Finsler Multi-Dimensional Scaling: Manifold Learning for Asymmetric Dimensionality Reduction and Embedding", "categories": ["cs.CV"], "comment": "Accepted for publication at the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2025", "summary": "Dimensionality reduction is a fundamental task that aims to simplify complex\ndata by reducing its feature dimensionality while preserving essential\npatterns, with core applications in data analysis and visualisation. To\npreserve the underlying data structure, multi-dimensional scaling (MDS) methods\nfocus on preserving pairwise dissimilarities, such as distances. They optimise\nthe embedding to have pairwise distances as close as possible to the data\ndissimilarities. However, the current standard is limited to embedding data in\nRiemannian manifolds. Motivated by the lack of asymmetry in the Riemannian\nmetric of the embedding space, this paper extends the MDS problem to a natural\nasymmetric generalisation of Riemannian manifolds called Finsler manifolds.\nInspired by Euclidean space, we define a canonical Finsler space for embedding\nasymmetric data. Due to its simplicity with respect to geodesics, data\nrepresentation in this space is both intuitive and simple to analyse. We\ndemonstrate that our generalisation benefits from the same theoretical\nconvergence guarantees. We reveal the effectiveness of our Finsler embedding\nacross various types of non-symmetric data, highlighting its value in\napplications such as data visualisation, dimensionality reduction, directed\ngraph embedding, and link prediction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["multi-dimensional"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18035", "pdf": "https://arxiv.org/pdf/2503.18035", "abs": "https://arxiv.org/abs/2503.18035", "authors": ["Tianyi Shang", "Zhenyu Li", "Pengjie Xu", "Zhaojun Deng", "Ruirui Zhang"], "title": "Text-Driven Cross-Modal Place Recognition Method for Remote Sensing Localization", "categories": ["cs.CV"], "comment": "13 pages", "summary": "Environment description-based localization in large-scale point cloud maps\nconstructed through remote sensing is critically significant for the\nadvancement of large-scale autonomous systems, such as delivery robots\noperating in the last mile. However, current approaches encounter challenges\ndue to the inability of point cloud encoders to effectively capture local\ndetails and long-range spatial relationships, as well as a significant modality\ngap between text and point cloud representations. To address these challenges,\nwe present Des4Pos, a novel two-stage text-driven remote sensing localization\nframework. In the coarse stage, the point-cloud encoder utilizes the\nMulti-scale Fusion Attention Mechanism (MFAM) to enhance local geometric\nfeatures, followed by a bidirectional Long Short-Term Memory (LSTM) module to\nstrengthen global spatial relationships. Concurrently, the Stepped Text Encoder\n(STE) integrates cross-modal prior knowledge from CLIP [1] and aligns text and\npoint-cloud features using this prior knowledge, effectively bridging modality\ndiscrepancies. In the fine stage, we introduce a Cascaded Residual Attention\n(CRA) module to fuse cross-modal features and predict relative localization\noffsets, thereby achieving greater localization precision. Experiments on the\nKITTI360Pose test set demonstrate that Des4Pos achieves state-of-the-art\nperformance in text-to-point-cloud place recognition. Specifically, it attains\na top-1 accuracy of 40% and a top-10 accuracy of 77% under a 5-meter radius\nthreshold, surpassing the best existing methods by 7% and 7%, respectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18052", "pdf": "https://arxiv.org/pdf/2503.18052", "abs": "https://arxiv.org/abs/2503.18052", "authors": ["Yue Li", "Qi Ma", "Runyi Yang", "Huapeng Li", "Mengjiao Ma", "Bin Ren", "Nikola Popovic", "Nicu Sebe", "Ender Konukoglu", "Theo Gevers", "Luc Van Gool", "Martin R. Oswald", "Danda Pani Paudel"], "title": "SceneSplat: Gaussian Splatting-based Scene Understanding with Vision-Language Pretraining", "categories": ["cs.CV"], "comment": "Our code, model, and dataset will be released at\n  https://github.com/unique1i/SceneSplat", "summary": "Recognizing arbitrary or previously unseen categories is essential for\ncomprehensive real-world 3D scene understanding. Currently, all existing\nmethods rely on 2D or textual modalities during training, or together at\ninference. This highlights a clear absence of a model capable of processing 3D\ndata alone for learning semantics end-to-end, along with the necessary data to\ntrain such a model. Meanwhile, 3D Gaussian Splatting (3DGS) has emerged as the\nde facto standard for 3D scene representation across various vision tasks.\nHowever, effectively integrating semantic reasoning into 3DGS in a\ngeneralizable fashion remains an open challenge. To address these limitations\nwe introduce SceneSplat, to our knowledge the first large-scale 3D indoor scene\nunderstanding approach that operates natively on 3DGS. Furthermore, we propose\na self-supervised learning scheme that unlocks rich 3D feature learning from\nunlabeled scenes. In order to power the proposed methods, we introduce\nSceneSplat-7K, the first large-scale 3DGS dataset for indoor scenes, comprising\nof 6868 scenes derived from 7 established datasets like ScanNet, Matterport3D,\netc. Generating SceneSplat-7K required computational resources equivalent to\n119 GPU-days on an L4 GPU, enabling standardized benchmarking for 3DGS-based\nreasoning for indoor scenes. Our exhaustive experiments on SceneSplat-7K\ndemonstrate the significant benefit of the proposed methods over the\nestablished baselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18055", "pdf": "https://arxiv.org/pdf/2503.18055", "abs": "https://arxiv.org/abs/2503.18055", "authors": ["Mingde Yao", "Menglu Wang", "King-Man Tam", "Lingen Li", "Tianfan Xue", "Jinwei Gu"], "title": "PolarFree: Polarization-based Reflection-free Imaging", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Reflection removal is challenging due to complex light interactions, where\nreflections obscure important details and hinder scene understanding.\nPolarization naturally provides a powerful cue to distinguish between reflected\nand transmitted light, enabling more accurate reflection removal. However,\nexisting methods often rely on small-scale or synthetic datasets, which fail to\ncapture the diversity and complexity of real-world scenarios. To this end, we\nconstruct a large-scale dataset, PolaRGB, for Polarization-based reflection\nremoval of RGB images, which enables us to train models that generalize\neffectively across a wide range of real-world scenarios. The PolaRGB dataset\ncontains 6,500 well-aligned mixed-transmission image pairs, 8x larger than\nexisting polarization datasets, and is the first to include both RGB and\npolarization images captured across diverse indoor and outdoor environments\nwith varying lighting conditions. Besides, to fully exploit the potential of\npolarization cues for reflection removal, we introduce PolarFree, which\nleverages diffusion process to generate reflection-free cues for accurate\nreflection removal. Extensive experiments show that PolarFree significantly\nenhances image clarity in challenging reflective scenarios, setting a new\nbenchmark for polarized imaging and reflection removal. Code and dataset are\navailable at https://github.com/mdyao/PolarFree.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18083", "pdf": "https://arxiv.org/pdf/2503.18083", "abs": "https://arxiv.org/abs/2503.18083", "authors": ["Tianxin Huang", "Gim Hee Lee"], "title": "Unified Geometry and Color Compression Framework for Point Clouds via Generative Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "With the growth of 3D applications and the rapid increase in sensor-collected\n3D point cloud data, there is a rising demand for efficient compression\nalgorithms. Most existing learning-based compression methods handle geometry\nand color attributes separately, treating them as distinct tasks, making these\nmethods challenging to apply directly to point clouds with colors. Besides, the\nlimited capacities of training datasets also limit their generalizability\nacross points with different distributions. In this work, we introduce a\ntest-time unified geometry and color compression framework of 3D point clouds.\nInstead of training a compression model based on specific datasets, we adapt a\npre-trained generative diffusion model to compress original colored point\nclouds into sparse sets, termed 'seeds', using prompt tuning. Decompression is\nthen achieved through multiple denoising steps with separate sampling\nprocesses. Experiments on objects and indoor scenes demonstrate that our method\nhas superior performances compared to existing baselines for the compression of\ngeometry and color.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18100", "pdf": "https://arxiv.org/pdf/2503.18100", "abs": "https://arxiv.org/abs/2503.18100", "authors": ["Xuesong Chen", "Shaoshuai Shi", "Tao Ma", "Jingqiu Zhou", "Simon See", "Ka Chun Cheung", "Hongsheng Li"], "title": "M3Net: Multimodal Multi-task Learning for 3D Detection, Segmentation, and Occupancy Prediction in Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2025", "summary": "The perception system for autonomous driving generally requires to handle\nmultiple diverse sub-tasks. However, current algorithms typically tackle\nindividual sub-tasks separately, which leads to low efficiency when aiming at\nobtaining full-perception results. Some multi-task learning methods try to\nunify multiple tasks with one model, but do not solve the conflicts in\nmulti-task learning. In this paper, we introduce M3Net, a novel multimodal and\nmulti-task network that simultaneously tackles detection, segmentation, and 3D\noccupancy prediction for autonomous driving and achieves superior performance\nthan single task model. M3Net takes multimodal data as input and multiple tasks\nvia query-token interactions. To enhance the integration of multi-modal\nfeatures for multi-task learning, we first propose the Modality-Adaptive\nFeature Integration (MAFI) module, which enables single-modality features to\npredict channel-wise attention weights for their high-performing tasks,\nrespectively. Based on integrated features, we then develop task-specific query\ninitialization strategies to accommodate the needs of detection/segmentation\nand 3D occupancy prediction. Leveraging the properly initialized queries, a\nshared decoder transforms queries and BEV features layer-wise, facilitating\nmulti-task learning. Furthermore, we propose a Task-oriented Channel Scaling\n(TCS) module in the decoder to mitigate conflicts between optimizing for\ndifferent tasks. Additionally, our proposed multi-task querying and TCS module\nsupport both Transformer-based decoder and Mamba-based decoder, demonstrating\nits flexibility to different architectures. M3Net achieves state-of-the-art\nmulti-task learning performance on the nuScenes benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18107", "pdf": "https://arxiv.org/pdf/2503.18107", "abs": "https://arxiv.org/abs/2503.18107", "authors": ["Hongjia Zhai", "Hai Li", "Zhenzhe Li", "Xiaokun Pan", "Yijia He", "Guofeng Zhang"], "title": "PanoGS: Gaussian-based Panoptic Segmentation for 3D Open Vocabulary Scene Understanding", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Recently, 3D Gaussian Splatting (3DGS) has shown encouraging performance for\nopen vocabulary scene understanding tasks. However, previous methods cannot\ndistinguish 3D instance-level information, which usually predicts a heatmap\nbetween the scene feature and text query. In this paper, we propose PanoGS, a\nnovel and effective 3D panoptic open vocabulary scene understanding approach.\nTechnically, to learn accurate 3D language features that can scale to large\nindoor scenarios, we adopt the pyramid tri-plane to model the latent continuous\nparametric feature space and use a 3D feature decoder to regress the multi-view\nfused 2D feature cloud. Besides, we propose language-guided graph cuts that\nsynergistically leverage reconstructed geometry and learned language cues to\ngroup 3D Gaussian primitives into a set of super-primitives. To obtain 3D\nconsistent instance, we perform graph clustering based segmentation with\nSAM-guided edge affinity computation between different super-primitives.\nExtensive experiments on widely used datasets show better or more competitive\nperformance on 3D panoptic open vocabulary scene understanding. Project page:\n\\href{https://zju3dv.github.io/panogs}{https://zju3dv.github.io/panogs}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17955", "pdf": "https://arxiv.org/pdf/2503.17955", "abs": "https://arxiv.org/abs/2503.17955", "authors": ["Stefan Pasch", "Sun-Young Ha"], "title": "Human-AI Interaction and User Satisfaction: Empirical Evidence from Online Reviews of AI Products", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Human-AI Interaction (HAI) guidelines and design principles have become\nincreasingly important in both industry and academia to guide the development\nof AI systems that align with user needs and expectations. However, large-scale\nempirical evidence on how HAI principles shape user satisfaction in practice\nremains limited. This study addresses that gap by analyzing over 100,000 user\nreviews of AI-related products from G2.com, a leading review platform for\nbusiness software and services. Based on widely adopted industry guidelines, we\nidentify seven core HAI dimensions and examine their coverage and sentiment\nwithin the reviews. We find that the sentiment on four HAI\ndimensions-adaptability, customization, error recovery, and security-is\npositively associated with overall user satisfaction. Moreover, we show that\nengagement with HAI dimensions varies by professional background: Users with\ntechnical job roles are more likely to discuss system-focused aspects, such as\nreliability, while non-technical users emphasize interaction-focused features\nlike customization and feedback. Interestingly, the relationship between HAI\nsentiment and overall satisfaction is not moderated by job role, suggesting\nthat once an HAI dimension has been identified by users, its effect on\nsatisfaction is consistent across job roles.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "dimension"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18225", "pdf": "https://arxiv.org/pdf/2503.18225", "abs": "https://arxiv.org/abs/2503.18225", "authors": ["Massimo Bini", "Leander Girrbach", "Zeynep Akata"], "title": "Decoupling Angles and Strength in Low-rank Adaptation", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "ICLR 2025", "summary": "Parameter-Efficient FineTuning (PEFT) methods have recently gained\nsignificant popularity thanks to the widespread availability of large-scale\npretrained models. These methods allow for quick adaptation to downstream tasks\nwith minimal computational cost. However, popular finetuning methods such as\nLoRA exhibit limited robustness when it comes to hyperparameter choices or\nextended training regimes, preventing optimal out-of-the-box performance. In\ncontrast, bounded approaches, such as ETHER, provide greater robustness but are\nlimited to extremely low-rank adaptations and fixed-strength transformations,\nreducing their adaptation expressive power. In this work, we propose Decoupled\nLow-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and\nscales learnable low-rank matrices. By bounding the distance of the\ntransformation, DeLoRA effectively decouples the angular learning from the\nadaptation strength, enhancing robustness without compromising performance.\nThrough evaluations on subject-driven image generation, natural language\nunderstanding, and instruction tuning, we show that DeLoRA matches or surpasses\nperformance of competing PEFT methods, while exhibiting stronger robustness.\nCode is available at https://github.com/ExplainableML/DeLoRA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18147", "pdf": "https://arxiv.org/pdf/2503.18147", "abs": "https://arxiv.org/abs/2503.18147", "authors": ["Ke Niu", "Yuwen Chen", "Haiyang Yu", "Zhuofan Chen", "Xianghui Que", "Bin Li", "Xiangyang Xue"], "title": "PHT-CAD: Efficient CAD Parametric Primitive Analysis with Progressive Hierarchical Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing,\nyet 2D Parametric Primitive Analysis (PPA) remains underexplored due to two key\nchallenges: structural constraint reasoning and advanced semantic\nunderstanding. To tackle these challenges, we first propose an Efficient Hybrid\nParametrization (EHP) for better representing 2D engineering drawings. EHP\ncontains four types of atomic component i.e., point, line, circle, and arc).\nAdditionally, we propose PHT-CAD, a novel 2D PPA framework that harnesses the\nmodality alignment and reasoning capabilities of Vision-Language Models (VLMs)\nfor precise engineering drawing analysis. In PHT-CAD, we introduce four\ndedicated regression heads to predict corresponding atomic components. To train\nPHT-CAD, a three-stage training paradigm Progressive Hierarchical Tuning (PHT)\nis proposed to progressively enhance PHT-CAD's capability to perceive\nindividual primitives, infer structural constraints, and align annotation\nlayers with their corresponding geometric representations. Considering that\nexisting datasets lack complete annotation layers and real-world engineering\ndrawings, we introduce ParaCAD, the first large-scale benchmark that explicitly\nintegrates both the geometric and annotation layers. ParaCAD comprises over 10\nmillion annotated drawings for training and 3,000 real-world industrial\ndrawings with complex topological structures and physical constraints for test.\nExtensive experiments demonstrate the effectiveness of PHT-CAD and highlight\nthe practical significance of ParaCAD in advancing 2D PPA research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18170", "pdf": "https://arxiv.org/pdf/2503.18170", "abs": "https://arxiv.org/abs/2503.18170", "authors": ["Abderrachid Hamrani", "Anuradha Godavarty"], "title": "Self-Attention Diffusion Models for Zero-Shot Biomedical Image Segmentation: Unlocking New Frontiers in Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 5 figures", "summary": "Producing high-quality segmentation masks for medical images is a fundamental\nchallenge in biomedical image analysis. Recent research has explored\nlarge-scale supervised training to enable segmentation across various medical\nimaging modalities and unsupervised training to facilitate segmentation without\ndense annotations. However, constructing a model capable of segmenting diverse\nmedical images in a zero-shot manner without any annotations remains a\nsignificant hurdle. This paper introduces the Attention Diffusion Zero-shot\nUnsupervised System (ADZUS), a novel approach that leverages self-attention\ndiffusion models for zero-shot biomedical image segmentation. ADZUS harnesses\nthe intrinsic capabilities of pre-trained diffusion models, utilizing their\ngenerative and discriminative potentials to segment medical images without\nrequiring annotated training data or prior domain-specific knowledge. The ADZUS\narchitecture is detailed, with its integration of self-attention mechanisms\nthat facilitate context-aware and detail-sensitive segmentations being\nhighlighted. Experimental results across various medical imaging datasets,\nincluding skin lesion segmentation, chest X-ray infection segmentation, and\nwhite blood cell segmentation, reveal that ADZUS achieves state-of-the-art\nperformance. Notably, ADZUS reached Dice scores ranging from 88.7\\% to 92.9\\%\nand IoU scores from 66.3\\% to 93.3\\% across different segmentation tasks,\ndemonstrating significant improvements in handling novel, unseen medical\nimagery. It is noteworthy that while ADZUS demonstrates high effectiveness, it\ndemands substantial computational resources and extended processing times. The\nmodel's efficacy in zero-shot settings underscores its potential to reduce\nreliance on costly annotations and seamlessly adapt to new medical imaging\ntasks, thereby expanding the diagnostic capabilities of AI-driven medical\nimaging technologies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18494", "pdf": "https://arxiv.org/pdf/2503.18494", "abs": "https://arxiv.org/abs/2503.18494", "authors": ["Hao-Yuan Chen", "Cheng-Pong Huang", "Jui-Ming Yao"], "title": "Verbal Process Supervision Elicits Better Coding Agents", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The emergence of large language models and their applications as AI agents\nhave significantly advanced state-of-the-art code generation benchmarks,\ntransforming modern software engineering tasks. However, even with test-time\ncomputed reasoning models, these systems still struggle with complex software\nengineering challenges. This work introduces CURA, a code understanding and\nreasoning agent system enhanced with verbal process supervision (VPS),\nachieving a 3.65\\% improvement over baseline models on challenging benchmarks\nlike BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and\nVPS techniques, attains state-of-the-art performance. This work represents a\nstep forward in integrating reasoning-driven architectures with LLM-based code\ngeneration, enabling agentic reasoning for language models to solve complex\nsoftware engineering tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["code generation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18223", "pdf": "https://arxiv.org/pdf/2503.18223", "abs": "https://arxiv.org/abs/2503.18223", "authors": ["Valentin Gabeff", "Haozhe Qi", "Brendan Flaherty", "Gencer Sumbl", "Alexander Mathis", "Devis Tuia"], "title": "MammAlps: A multi-view video behavior monitoring dataset of wild mammals in the Swiss Alps", "categories": ["cs.CV", "cs.IR", "q-bio.NC", "q-bio.QM"], "comment": "CVPR 2025; Benchmark and code at:\n  https://github.com/eceo-epfl/MammAlps", "summary": "Monitoring wildlife is essential for ecology and ethology, especially in\nlight of the increasing human impact on ecosystems. Camera traps have emerged\nas habitat-centric sensors enabling the study of wildlife populations at scale\nwith minimal disturbance. However, the lack of annotated video datasets limits\nthe development of powerful video understanding models needed to process the\nvast amount of fieldwork data collected. To advance research in wild animal\nbehavior monitoring we present MammAlps, a multimodal and multi-view dataset of\nwildlife behavior monitoring from 9 camera-traps in the Swiss National Park.\nMammAlps contains over 14 hours of video with audio, 2D segmentation maps and\n8.5 hours of individual tracks densely labeled for species and behavior. Based\non 6135 single animal clips, we propose the first hierarchical and multimodal\nanimal behavior recognition benchmark using audio, video and reference scene\nsegmentation maps as inputs. Furthermore, we also propose a second\necology-oriented benchmark aiming at identifying activities, species, number of\nindividuals and meteorological conditions from 397 multi-view and long-term\necological events, including false positive triggers. We advocate that both\ntasks are complementary and contribute to bridging the gap between machine\nlearning and ecology. Code and data are available at:\nhttps://github.com/eceo-epfl/MammAlps", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18565", "pdf": "https://arxiv.org/pdf/2503.18565", "abs": "https://arxiv.org/abs/2503.18565", "authors": ["Abdoul Majid O. Thiombiano", "Brahim Hnich", "Ali Ben Mrad", "Mohamed Wiem Mkaouer"], "title": "Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The current era of Natural Language Processing (NLP) is dominated by\nTransformer models. However, novel architectures relying on recurrent\nmechanisms, such as xLSTM and Mamba, have been proposed as alternatives to\nattention-based models. Although computation is done differently than with the\nattention mechanism mechanism, these recurrent models yield good results and\nsometimes even outperform state-of-the-art attention-based models. In this\nwork, we propose Distil-xLSTM, an xLSTM-based Small Language Model (SLM)\ntrained by distilling knowledge from a Large Language Model (LLM) that shows\npromising results while being compute and scale efficient. Our Distil-xLSTM\nfocuses on approximating a transformer-based model attention parametrization\nusing its recurrent sequence mixing components and shows good results with\nminimal training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18254", "pdf": "https://arxiv.org/pdf/2503.18254", "abs": "https://arxiv.org/abs/2503.18254", "authors": ["Lukas Uzolas", "Elmar Eisemann", "Petr Kellnhofer"], "title": "Surface-Aware Distilled 3D Semantic Features", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Many 3D tasks such as pose alignment, animation, motion transfer, and 3D\nreconstruction rely on establishing correspondences between 3D shapes. This\nchallenge has recently been approached by matching of semantic features from\npre-trained vision models. However, despite their power, these features\nstruggle to differentiate instances of the same semantic class such as \"left\nhand\" versus \"right hand\" which leads to substantial mapping errors. To solve\nthis, we learn a surface-aware embedding space that is robust to these\nambiguities. Importantly, our approach is self-supervised and requires only a\nsmall number of unpaired training meshes to infer features for new 3D shapes at\ntest time. We achieve this by introducing a contrastive loss that preserves the\nsemantic content of the features distilled from foundational models while\ndisambiguating features located far apart on the shape's surface. We observe\nsuperior performance in correspondence matching benchmarks and enable\ndownstream applications including in-part segmentation, pose alignment, and\nmotion transfer. The project site is available at\nhttps://lukas.uzolas.com/SurfaceAware3DFeaturesSite.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18666", "pdf": "https://arxiv.org/pdf/2503.18666", "abs": "https://arxiv.org/abs/2503.18666", "authors": ["Haoyu Wang", "Christopher M. Poskitt", "Jun Sun"], "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identifying\n87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8\nscenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18267", "pdf": "https://arxiv.org/pdf/2503.18267", "abs": "https://arxiv.org/abs/2503.18267", "authors": ["Minh-Tuan Tran", "Trung Le", "Xuan-May Le", "Thanh-Toan Do", "Dinh Phung"], "title": "Enhancing Dataset Distillation via Non-Critical Region Refinement", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Dataset distillation has become a popular method for compressing large\ndatasets into smaller, more efficient representations while preserving critical\ninformation for model training. Data features are broadly categorized into two\ntypes: instance-specific features, which capture unique, fine-grained details\nof individual examples, and class-general features, which represent shared,\nbroad patterns across a class. However, previous approaches often struggle to\nbalance these features-some focus solely on class-general patterns, neglecting\nfiner instance details, while others prioritize instance-specific features,\noverlooking the shared characteristics essential for class-level understanding.\nIn this paper, we introduce the Non-Critical Region Refinement Dataset\nDistillation (NRR-DD) method, which preserves instance-specific details and\nfine-grained regions in synthetic data while enriching non-critical regions\nwith class-general information. This approach enables models to leverage all\npixel information, capturing both feature types and enhancing overall\nperformance. Additionally, we present Distance-Based Representative (DBR)\nknowledge transfer, which eliminates the need for soft labels in training by\nrelying on the distance between synthetic data predictions and one-hot encoded\nlabels. Experimental results show that NRR-DD achieves state-of-the-art\nperformance on both small- and large-scale datasets. Furthermore, by storing\nonly two distances per instance, our method delivers comparable results across\nvarious settings. The code is available at\nhttps://github.com/tmtuan1307/NRR-DD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18278", "pdf": "https://arxiv.org/pdf/2503.18278", "abs": "https://arxiv.org/abs/2503.18278", "authors": ["Cheng Yang", "Yang Sui", "Jinqi Xiao", "Lingyi Huang", "Yu Gong", "Chendi Li", "Jinghua Yan", "Yu Bai", "Ponnuswamy Sadayappan", "Xia Hu", "Bo Yuan"], "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18312", "pdf": "https://arxiv.org/pdf/2503.18312", "abs": "https://arxiv.org/abs/2503.18312", "authors": ["Jianlong Jin", "Chenglong Zhao", "Ruixin Zhang", "Sheng Shang", "Jianqing Xu", "Jingyun Zhang", "ShaoMing Wang", "Yang Zhao", "Shouhong Ding", "Wei Jia", "Yunsheng Wu"], "title": "Diff-Palm: Realistic Palmprint Generation with Polynomial Creases and Intra-Class Variation Controllable Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Palmprint recognition is significantly limited by the lack of large-scale\npublicly available datasets. Previous methods have adopted B\\'ezier curves to\nsimulate the palm creases, which then serve as input for conditional GANs to\ngenerate realistic palmprints. However, without employing real data\nfine-tuning, the performance of the recognition model trained on these\nsynthetic datasets would drastically decline, indicating a large gap between\ngenerated and real palmprints. This is primarily due to the utilization of an\ninaccurate palm crease representation and challenges in balancing intra-class\nvariation with identity consistency. To address this, we introduce a\npolynomial-based palm crease representation that provides a new palm crease\ngeneration mechanism more closely aligned with the real distribution. We also\npropose the palm creases conditioned diffusion model with a novel intra-class\nvariation control method. By applying our proposed $K$-step noise-sharing\nsampling, we are able to synthesize palmprint datasets with large intra-class\nvariation and high identity consistency. Experimental results show that, for\nthe first time, recognition models trained solely on our synthetic datasets,\nwithout any fine-tuning, outperform those trained on real datasets.\nFurthermore, our approach achieves superior recognition performance as the\nnumber of generated identities increases.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18339", "pdf": "https://arxiv.org/pdf/2503.18339", "abs": "https://arxiv.org/abs/2503.18339", "authors": ["Inpyo Hong", "Youngwan Jo", "Hyojeong Lee", "Sunghyun Ahn", "Sanghyun Park"], "title": "GranQ: Granular Zero-Shot Quantization with Unified Layer-Channel Awareness", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot quantization (ZSQ) enables neural network compression without\ntraining data, which is crucial in restricted data access environments.\nHowever, existing ZSQ methods suffer from significant activation loss in\nlow-bit environments owing to their coarse-grained scaling strategy. To address\nthis issue, we propose GranQ, a novel ZSQ approach that leverages layer-channel\nawareness to minimize the quantization error. Unlike conventional layer- or\nchannel-wise quantization, GranQ dynamically adjusts quantization granularity\nby considering both layer- and channel-level activation distributions. This\nenables fine-grained quantization while minimizing activation distortion.\nAdditionally, we introduce vectorized activation quantization, which enables\nefficient parallel computation and reduces computational overhead while\npreserving accuracy. GranQ achieves superior performance compared with those of\nstate-of-the-art ZSQ methods that employ quantization-aware training. With\nthese findings, we anticipate that GranQ will inspire novel research directions\nbeyond conventional ZSQ approaches focused on data generation and model\ntraining.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18352", "pdf": "https://arxiv.org/pdf/2503.18352", "abs": "https://arxiv.org/abs/2503.18352", "authors": ["Jinjin Zhang", "Qiuyu Huang", "Junjie Liu", "Xiefan Guo", "Di Huang"], "title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "In this paper, we present Diffusion-4K, a novel framework for direct\nultra-high-resolution image synthesis using text-to-image diffusion models. The\ncore advancements include: (1) Aesthetic-4K Benchmark: addressing the absence\nof a publicly available 4K image synthesis dataset, we construct Aesthetic-4K,\na comprehensive benchmark for ultra-high-resolution image generation. We\ncurated a high-quality 4K dataset with carefully selected images and captions\ngenerated by GPT-4o. Additionally, we introduce GLCM Score and Compression\nRatio metrics to evaluate fine details, combined with holistic measures such as\nFID, Aesthetics and CLIPScore for a comprehensive assessment of\nultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a\nwavelet-based fine-tuning approach for direct training with photorealistic 4K\nimages, applicable to various latent diffusion models, demonstrating its\neffectiveness in synthesizing highly detailed 4K images. Consequently,\nDiffusion-4K achieves impressive performance in high-quality image synthesis\nand text prompt adherence, especially when powered by modern large-scale\ndiffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results\nfrom our benchmark demonstrate the superiority of Diffusion-4K in\nultra-high-resolution image synthesis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18361", "pdf": "https://arxiv.org/pdf/2503.18361", "abs": "https://arxiv.org/abs/2503.18361", "authors": ["Wenyuan Zhang", "Emily Yue-ting Jia", "Junsheng Zhou", "Baorui Ma", "Kanle Shi", "Yu-Shen Liu"], "title": "NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene Reconstruction", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project page:\n  https://wen-yuan-zhang.github.io/NeRFPrior/", "summary": "Recently, it has shown that priors are vital for neural implicit functions to\nreconstruct high-quality surfaces from multi-view RGB images. However, current\npriors require large-scale pre-training, and merely provide geometric clues\nwithout considering the importance of color. In this paper, we present\nNeRFPrior, which adopts a neural radiance field as a prior to learn signed\ndistance fields using volume rendering for surface reconstruction. Our NeRF\nprior can provide both geometric and color clues, and also get trained fast\nunder the same scene without additional data. Based on the NeRF prior, we are\nenabled to learn a signed distance function (SDF) by explicitly imposing a\nmulti-view consistency constraint on each ray intersection for surface\ninference. Specifically, at each ray intersection, we use the density in the\nprior as a coarse geometry estimation, while using the color near the surface\nas a clue to check its visibility from another view angle. For the textureless\nareas where the multi-view consistency constraint does not work well, we\nfurther introduce a depth consistency loss with confidence weights to infer the\nSDF. Our experimental results outperform the state-of-the-art methods under the\nwidely used benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18364", "pdf": "https://arxiv.org/pdf/2503.18364", "abs": "https://arxiv.org/abs/2503.18364", "authors": ["Chenxi Xie", "Minghan Li", "Hui Zeng", "Jun Luo", "Lei Zhang"], "title": "MaSS13K: A Matting-level Semantic Segmentation Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "High-resolution semantic segmentation is essential for applications such as\nimage editing, bokeh imaging, AR/VR, etc. Unfortunately, existing datasets\noften have limited resolution and lack precise mask details and boundaries. In\nthis work, we build a large-scale, matting-level semantic segmentation dataset,\nnamed MaSS13K, which consists of 13,348 real-world images, all at 4K\nresolution. MaSS13K provides high-quality mask annotations of a number of\nobjects, which are categorized into seven categories: human, vegetation,\nground, sky, water, building, and others. MaSS13K features precise masks, with\nan average mask complexity 20-50 times higher than existing semantic\nsegmentation datasets. We consequently present a method specifically designed\nfor high-resolution semantic segmentation, namely MaSSFormer, which employs an\nefficient pixel decoder that aggregates high-level semantic features and\nlow-level texture features across three stages, aiming to produce\nhigh-resolution masks with minimal computational cost. Finally, we propose a\nnew learning paradigm, which integrates the high-quality masks of the seven\ngiven categories with pseudo labels from new classes, enabling MaSSFormer to\ntransfer its accurate segmentation capability to other classes of objects. Our\nproposed MaSSFormer is comprehensively evaluated on the MaSS13K benchmark\ntogether with 14 representative segmentation models. We expect that our\nmeticulously annotated MaSS13K dataset and the MaSSFormer model can facilitate\nthe research of high-resolution and high-quality semantic segmentation.\nDatasets and codes can be found at https://github.com/xiechenxi99/MaSS13K.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18393", "pdf": "https://arxiv.org/pdf/2503.18393", "abs": "https://arxiv.org/abs/2503.18393", "authors": ["Xinhua Xu", "Hong Liu", "Jianbing Wu", "Jinfu Liu"], "title": "PDDM: Pseudo Depth Diffusion Model for RGB-PD Semantic Segmentation Based in Complex Indoor Scenes", "categories": ["cs.CV"], "comment": null, "summary": "The integration of RGB and depth modalities significantly enhances the\naccuracy of segmenting complex indoor scenes, with depth data from RGB-D\ncameras playing a crucial role in this improvement. However, collecting an\nRGB-D dataset is more expensive than an RGB dataset due to the need for\nspecialized depth sensors. Aligning depth and RGB images also poses challenges\ndue to sensor positioning and issues like missing data and noise. In contrast,\nPseudo Depth (PD) from high-precision depth estimation algorithms can eliminate\nthe dependence on RGB-D sensors and alignment processes, as well as provide\neffective depth information and show significant potential in semantic\nsegmentation. Therefore, to explore the practicality of utilizing pseudo depth\ninstead of real depth for semantic segmentation, we design an RGB-PD\nsegmentation pipeline to integrate RGB and pseudo depth and propose a Pseudo\nDepth Aggregation Module (PDAM) for fully exploiting the informative clues\nprovided by the diverse pseudo depth maps. The PDAM aggregates multiple pseudo\ndepth maps into a single modality, making it easily adaptable to other RGB-D\nsegmentation methods. In addition, the pre-trained diffusion model serves as a\nstrong feature extractor for RGB segmentation tasks, but multi-modal\ndiffusion-based segmentation methods remain unexplored. Therefore, we present a\nPseudo Depth Diffusion Model (PDDM) that adopts a large-scale text-image\ndiffusion model as a feature extractor and a simple yet effective fusion\nstrategy to integrate pseudo depth. To verify the applicability of pseudo depth\nand our PDDM, we perform extensive experiments on the NYUv2 and SUNRGB-D\ndatasets. The experimental results demonstrate that pseudo depth can\neffectively enhance segmentation performance, and our PDDM achieves\nstate-of-the-art performance, outperforming other methods by +6.98 mIoU on\nNYUv2 and +2.11 mIoU on SUNRGB-D.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18438", "pdf": "https://arxiv.org/pdf/2503.18438", "abs": "https://arxiv.org/abs/2503.18438", "authors": ["Guosheng Zhao", "Xiaofeng Wang", "Chaojun Ni", "Zheng Zhu", "Wenkang Qin", "Guan Huang", "Xingang Wang"], "title": "ReconDreamer++: Harmonizing Generative and Reconstructive Models for Driving Scene Representation", "categories": ["cs.CV"], "comment": "Project Page: https://recondreamer-plus.github.io/", "summary": "Combining reconstruction models with generative models has emerged as a\npromising paradigm for closed-loop simulation in autonomous driving. For\nexample, ReconDreamer has demonstrated remarkable success in rendering\nlarge-scale maneuvers. However, a significant gap remains between the generated\ndata and real-world sensor observations, particularly in terms of fidelity for\nstructured elements, such as the ground surface. To address these challenges,\nwe propose ReconDreamer++, an enhanced framework that significantly improves\nthe overall rendering quality by mitigating the domain gap and refining the\nrepresentation of the ground surface. Specifically, ReconDreamer++ introduces\nthe Novel Trajectory Deformable Network (NTDNet), which leverages learnable\nspatial deformation mechanisms to bridge the domain gap between synthesized\nnovel views and original sensor observations. Moreover, for structured elements\nsuch as the ground surface, we preserve geometric prior knowledge in 3D\nGaussians, and the optimization process focuses on refining appearance\nattributes while preserving the underlying geometric structure. Experimental\nevaluations conducted on multiple datasets (Waymo, nuScenes, PandaSet, and\nEUVS) confirm the superior performance of ReconDreamer++. Specifically, on\nWaymo, ReconDreamer++ achieves performance comparable to Street Gaussians for\nthe original trajectory while significantly outperforming ReconDreamer on novel\ntrajectories. In particular, it achieves substantial improvements, including a\n6.1% increase in NTA-IoU, a 23. 0% improvement in FID, and a remarkable 4.5%\ngain in the ground surface metric NTL-IoU, highlighting its effectiveness in\naccurately reconstructing structured elements such as the road surface.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18446", "pdf": "https://arxiv.org/pdf/2503.18446", "abs": "https://arxiv.org/abs/2503.18446", "authors": ["Jinho Jeong", "Sangmin Han", "Jinwoo Kim", "Seon Joo Kim"], "title": "Latent Space Super-Resolution for Higher-Resolution Image Generation with Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "In this paper, we propose LSRNA, a novel framework for higher-resolution\n(exceeding 1K) image generation using diffusion models by leveraging\nsuper-resolution directly in the latent space. Existing diffusion models\nstruggle with scaling beyond their training resolutions, often leading to\nstructural distortions or content repetition. Reference-based methods address\nthe issues by upsampling a low-resolution reference to guide higher-resolution\ngeneration. However, they face significant challenges: upsampling in latent\nspace often causes manifold deviation, which degrades output quality. On the\nother hand, upsampling in RGB space tends to produce overly smoothed outputs.\nTo overcome these limitations, LSRNA combines Latent space Super-Resolution\n(LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance\nhigh-frequency details. Our extensive experiments demonstrate that integrating\nLSRNA outperforms state-of-the-art reference-based methods across various\nresolutions and metrics, while showing the critical role of latent space\nupsampling in preserving detail and sharpness. The code is available at\nhttps://github.com/3587jjh/LSRNA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18469", "pdf": "https://arxiv.org/pdf/2503.18469", "abs": "https://arxiv.org/abs/2503.18469", "authors": ["Hao Ni", "Lianli Gao", "Pengpeng Zeng", "Heng Tao Shen", "Jingkuan Song"], "title": "CFReID: Continual Few-shot Person Re-Identification", "categories": ["cs.CV"], "comment": "16 pages, 8 figures", "summary": "Real-world surveillance systems are dynamically evolving, requiring a person\nRe-identification model to continuously handle newly incoming data from various\ndomains. To cope with these dynamics, Lifelong ReID (LReID) has been proposed\nto learn and accumulate knowledge across multiple domains incrementally.\nHowever, LReID models need to be trained on large-scale labeled data for each\nunseen domain, which are typically inaccessible due to privacy and cost\nconcerns. In this paper, we propose a new paradigm called Continual Few-shot\nReID (CFReID), which requires models to be incrementally trained using few-shot\ndata and tested on all seen domains. Under few-shot conditions, CFREID faces\ntwo core challenges: 1) learning knowledge from few-shot data of unseen domain,\nand 2) avoiding catastrophic forgetting of seen domains. To tackle these two\nchallenges, we propose a Stable Distribution Alignment (SDA) framework from\nfeature distribution perspective. Specifically, our SDA is composed of two\nmodules, i.e., Meta Distribution Alignment (MDA) and Prototype-based Few-shot\nAdaptation (PFA). To support the study of CFReID, we establish an evaluation\nbenchmark for CFReID on five publicly available ReID datasets. Extensive\nexperiments demonstrate that our SDA can enhance the few-shot learning and\nanti-forgetting capabilities under few-shot conditions. Notably, our approach,\nusing only 5\\% of the data, i.e., 32 IDs, significantly outperforms LReID's\nstate-of-the-art performance, which requires 700 to 1,000 IDs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18470", "pdf": "https://arxiv.org/pdf/2503.18470", "abs": "https://arxiv.org/abs/2503.18470", "authors": ["Zhenyu Pan", "Han Liu"], "title": "MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse", "categories": ["cs.CV", "cs.AI"], "comment": "Working Paper", "summary": "We present MetaSpatial, the first reinforcement learning (RL)-based framework\ndesigned to enhance 3D spatial reasoning in vision-language models (VLMs),\nenabling real-time 3D scene generation without the need for hard-coded\noptimizations. MetaSpatial addresses two core challenges: (i) the lack of\ninternalized 3D spatial reasoning in VLMs, which limits their ability to\ngenerate realistic layouts, and (ii) the inefficiency of traditional supervised\nfine-tuning (SFT) for layout generation tasks, as perfect ground truth\nannotations are unavailable. Our key innovation is a multi-turn RL-based\noptimization mechanism that integrates physics-aware constraints and rendered\nimage evaluations, ensuring generated 3D layouts are coherent, physically\nplausible, and aesthetically consistent. Methodologically, MetaSpatial\nintroduces an adaptive, iterative reasoning process, where the VLM refines\nspatial arrangements over multiple turns by analyzing rendered outputs,\nimproving scene coherence progressively. Empirical evaluations demonstrate that\nMetaSpatial significantly enhances the spatial consistency and formatting\nstability of various scale models. Post-training, object placements are more\nrealistic, aligned, and functionally coherent, validating the effectiveness of\nRL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game\ndevelopment applications. Our code, data, and training pipeline are publicly\navailable at https://github.com/PzySeere/MetaSpatial.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18544", "pdf": "https://arxiv.org/pdf/2503.18544", "abs": "https://arxiv.org/abs/2503.18544", "authors": ["Rafia Rahim", "Samuel Woerz", "Andreas Zell"], "title": "Distilling Stereo Networks for Performant and Efficient Leaner Networks", "categories": ["cs.CV"], "comment": "8 pages, 3 figures. Published in: 2023 International Joint Conference\n  on Neural Networks (IJCNN)", "summary": "Knowledge distillation has been quite popular in vision for tasks like\nclassification and segmentation however not much work has been done for\ndistilling state-of-the-art stereo matching methods despite their range of\napplications. One of the reasons for its lack of use in stereo matching\nnetworks is due to the inherent complexity of these networks, where a typical\nnetwork is composed of multiple two- and three-dimensional modules. In this\nwork, we systematically combine the insights from state-of-the-art stereo\nmethods with general knowledge-distillation techniques to develop a joint\nframework for stereo networks distillation with competitive results and faster\ninference. Moreover, we show, via a detailed empirical analysis, that\ndistilling knowledge from the stereo network requires careful design of the\ncomplete distillation pipeline starting from backbone to the right selection of\ndistillation points and corresponding loss functions. This results in the\nstudent networks that are not only leaner and faster but give excellent\nperformance . For instance, our student network while performing better than\nthe performance oriented methods like PSMNet [1], CFNet [2], and LEAStereo [3])\non benchmark SceneFlow dataset, is 8x, 5x, and 8x faster respectively.\nFurthermore, compared to speed oriented methods having inference time less than\n100ms, our student networks perform better than all the tested methods. In\naddition, our student network also shows better generalization capabilities\nwhen tested on unseen datasets like ETH3D and Middlebury.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18557", "pdf": "https://arxiv.org/pdf/2503.18557", "abs": "https://arxiv.org/abs/2503.18557", "authors": ["Rafia Rahim", "Samuel Woerz", "Andreas Zell"], "title": "LeanStereo: A Leaner Backbone based Stereo Network", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Recently, end-to-end deep networks based stereo matching methods, mainly\nbecause of their performance, have gained popularity. However, this improvement\nin performance comes at the cost of increased computational and memory\nbandwidth requirements, thus necessitating specialized hardware (GPUs); even\nthen, these methods have large inference times compared to classical methods.\nThis limits their applicability in real-world applications. Although we desire\nhigh accuracy stereo methods albeit with reasonable inference time. To this\nend, we propose a fast end-to-end stereo matching method. Majority of this\nspeedup comes from integrating a leaner backbone. To recover the performance\nlost because of a leaner backbone, we propose to use learned attention weights\nbased cost volume combined with LogL1 loss for stereo matching. Using LogL1\nloss not only improves the overall performance of the proposed network but also\nleads to faster convergence. We do a detailed empirical evaluation of different\ndesign choices and show that our method requires 4x less operations and is also\nabout 9 to 14x faster compared to the state of the art methods like ACVNet [1],\nLEAStereo [2] and CFNet [3] while giving comparable performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18567", "pdf": "https://arxiv.org/pdf/2503.18567", "abs": "https://arxiv.org/abs/2503.18567", "authors": ["Biwen Meng", "Xi Long", "Wanrong Yang", "Ruochen Liu", "Yi Tian", "Yalin Zheng", "Jingxin Liu"], "title": "Advancing Cross-Organ Domain Generalization with Test-Time Style Transfer and Diversity Enhancement", "categories": ["cs.CV"], "comment": "2025 IEEE International Symposium on Biomedical Imaging (ISBI)", "summary": "Deep learning has made significant progress in addressing challenges in\nvarious fields including computational pathology (CPath). However, due to the\ncomplexity of the domain shift problem, the performance of existing models will\ndegrade, especially when it comes to multi-domain or cross-domain tasks. In\nthis paper, we propose a Test-time style transfer (T3s) that uses a\nbidirectional mapping mechanism to project the features of the source and\ntarget domains into a unified feature space, enhancing the generalization\nability of the model. To further increase the style expression space, we\nintroduce a Cross-domain style diversification module (CSDM) to ensure the\northogonality between style bases. In addition, data augmentation and low-rank\nadaptation techniques are used to improve feature alignment and sensitivity,\nenabling the model to adapt to multi-domain inputs effectively. Our method has\ndemonstrated effectiveness on three unseen datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18673", "pdf": "https://arxiv.org/pdf/2503.18673", "abs": "https://arxiv.org/abs/2503.18673", "authors": ["Taeyeop Lee", "Bowen Wen", "Minjun Kang", "Gyuree Kang", "In So Kweon", "Kuk-Jin Yoon"], "title": "Any6D: Model-free 6D Pose Estimation of Novel Objects", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "CVPR 2025, Project Page: https://taeyeop.com/any6d", "summary": "We introduce Any6D, a model-free framework for 6D object pose estimation that\nrequires only a single RGB-D anchor image to estimate both the 6D pose and size\nof unknown objects in novel scenes. Unlike existing methods that rely on\ntextured 3D models or multiple viewpoints, Any6D leverages a joint object\nalignment process to enhance 2D-3D alignment and metric scale estimation for\nimproved pose accuracy. Our approach integrates a render-and-compare strategy\nto generate and refine pose hypotheses, enabling robust performance in\nscenarios with occlusions, non-overlapping views, diverse lighting conditions,\nand large cross-environment variations. We evaluate our method on five\nchallenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,\ndemonstrating its effectiveness in significantly outperforming state-of-the-art\nmethods for novel object pose estimation. Project page:\nhttps://taeyeop.com/any6d", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18695", "pdf": "https://arxiv.org/pdf/2503.18695", "abs": "https://arxiv.org/abs/2503.18695", "authors": ["Luyao Tang", "Yuxuan Yuan", "Chaoqi Chen", "Zeyu Zhang", "Yue Huang", "Kun Zhang"], "title": "OCRT: Boosting Foundation Models in the Open World with Object-Concept-Relation Triad", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by CVPR 2025", "summary": "Although foundation models (FMs) claim to be powerful, their generalization\nability significantly decreases when faced with distribution shifts, weak\nsupervision, or malicious attacks in the open world. On the other hand, most\ndomain generalization or adversarial fine-tuning methods are task-related or\nmodel-specific, ignoring the universality in practical applications and the\ntransferability between FMs. This paper delves into the problem of generalizing\nFMs to the out-of-domain data. We propose a novel framework, the\nObject-Concept-Relation Triad (OCRT), that enables FMs to extract sparse,\nhigh-level concepts and intricate relational structures from raw visual inputs.\nThe key idea is to bind objects in visual scenes and a set of object-centric\nrepresentations through unsupervised decoupling and iterative refinement. To be\nspecific, we project the object-centric representations onto a semantic concept\nspace that the model can readily interpret and estimate their importance to\nfilter out irrelevant elements. Then, a concept-based graph, which has a\nflexible degree, is constructed to incorporate the set of concepts and their\ncorresponding importance, enabling the extraction of high-order factors from\ninformative concepts and facilitating relational reasoning among these\nconcepts. Extensive experiments demonstrate that OCRT can substantially boost\nthe generalizability and robustness of SAM and CLIP across multiple downstream\ntasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18709", "pdf": "https://arxiv.org/pdf/2503.18709", "abs": "https://arxiv.org/abs/2503.18709", "authors": ["Boqi Chen", "Cdric Vincent-Cuaz", "Lydia A. Schoenpflug", "Manuel Madeira", "Lisa Fournier", "Vaishnavi Subramanian", "Sonali Andani", "Samuel Ruiperez-Campillo", "Julia E. Vogt", "Raphalle Luisier", "Dorina Thanou", "Viktor H. Koelzer", "Pascal Frossard", "Gabriele Campanella", "Gunnar Rtsch"], "title": "Revisiting Automatic Data Curation for Vision Foundation Models in Digital Pathology", "categories": ["cs.CV"], "comment": "MICCAI 2025", "summary": "Vision foundation models (FMs) are accelerating the development of digital\npathology algorithms and transforming biomedical research. These models learn,\nin a self-supervised manner, to represent histological features in highly\nheterogeneous tiles extracted from whole-slide images (WSIs) of real-world\npatient samples. The performance of these FMs is significantly influenced by\nthe size, diversity, and balance of the pre-training data. However, data\nselection has been primarily guided by expert knowledge at the WSI level,\nfocusing on factors such as disease classification and tissue types, while\nlargely overlooking the granular details available at the tile level. In this\npaper, we investigate the potential of unsupervised automatic data curation at\nthe tile-level, taking into account 350 million tiles. Specifically, we apply\nhierarchical clustering trees to pre-extracted tile embeddings, allowing us to\nsample balanced datasets uniformly across the embedding space of the pretrained\nFM. We further identify these datasets are subject to a trade-off between size\nand balance, potentially compromising the quality of representations learned by\nFMs, and propose tailored batch sampling strategies to mitigate this effect. We\ndemonstrate the effectiveness of our method through improved performance on a\ndiverse range of clinically relevant downstream tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18719", "pdf": "https://arxiv.org/pdf/2503.18719", "abs": "https://arxiv.org/abs/2503.18719", "authors": ["Cong Liu", "Liang Hou", "Mingwu Zheng", "Xin Tao", "Pengfei Wan", "Di Zhang", "Kun Gai"], "title": "Boosting Resolution Generalization of Diffusion Transformers with Randomized Positional Encodings", "categories": ["cs.CV"], "comment": null, "summary": "Resolution generalization in image generation tasks enables the production of\nhigher-resolution images with lower training resolution overhead. However, a\nsignificant challenge in resolution generalization, particularly in the widely\nused Diffusion Transformers, lies in the mismatch between the positional\nencodings encountered during testing and those used during training. While\nexisting methods have employed techniques such as interpolation, extrapolation,\nor their combinations, none have fully resolved this issue. In this paper, we\npropose a novel two-dimensional randomized positional encodings (RPE-2D)\nframework that focuses on learning positional order of image patches instead of\nthe specific distances between them, enabling seamless high- and low-resolution\nimage generation without requiring high- and low-resolution image training.\nSpecifically, RPE-2D independently selects positions over a broader range along\nboth the horizontal and vertical axes, ensuring that all position encodings are\ntrained during the inference phase, thus improving resolution generalization.\nAdditionally, we propose a random data augmentation technique to enhance the\nmodeling of position order. To address the issue of image cropping caused by\nthe augmentation, we introduce corresponding micro-conditioning to enable the\nmodel to perceive the specific cropping patterns. On the ImageNet dataset, our\nproposed RPE-2D achieves state-of-the-art resolution generalization\nperformance, outperforming existing competitive methods when trained at a\nresolution of $256 \\times 256$ and inferred at $384 \\times 384$ and $512 \\times\n512$, as well as when scaling from $512 \\times 512$ to $768 \\times 768$ and\n$1024 \\times 1024$. And it also exhibits outstanding capabilities in\nlow-resolution image generation, multi-stage training acceleration and\nmulti-resolution inheritance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18742", "pdf": "https://arxiv.org/pdf/2503.18742", "abs": "https://arxiv.org/abs/2503.18742", "authors": ["Sebastian Tewes", "Yufan Chen", "Omar Moured", "Jiaming Zhang", "Rainer Stiefelhagen"], "title": "SFDLA: Source-Free Document Layout Analysis", "categories": ["cs.CV"], "comment": "The benchmark, models, and code will be publicly available at\n  https://github.com/s3setewe/sfdla-DLAdapter", "summary": "Document Layout Analysis (DLA) is a fundamental task in document\nunderstanding. However, existing DLA and adaptation methods often require\naccess to large-scale source data and target labels. This requirements severely\nlimiting their real-world applicability, particularly in privacy-sensitive and\nresource-constrained domains, such as financial statements, medical records,\nand proprietary business documents. According to our observation, directly\ntransferring source-domain fine-tuned models on target domains often results in\na significant performance drop (Avg. -32.64%). In this work, we introduce\nSource-Free Document Layout Analysis (SFDLA), aiming for adapting a pre-trained\nsource DLA models to an unlabeled target domain, without access to any source\ndata. To address this challenge, we establish the first SFDLA benchmark,\ncovering three major DLA datasets for geometric- and content-aware adaptation.\nFurthermore, we propose Document Layout Analysis Adapter (DLAdapter), a novel\nframework that is designed to improve source-free adaptation across document\ndomains. Our method achieves a +4.21% improvement over the source-only baseline\nand a +2.26% gain over existing source-free methods from PubLayNet to\nDocLayNet. We believe this work will inspire the DLA community to further\ninvestigate source-free document understanding. To support future research of\nthe community, the benchmark, models, and code will be publicly available at\nhttps://github.com/s3setewe/sfdla-DLAdapter.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18746", "pdf": "https://arxiv.org/pdf/2503.18746", "abs": "https://arxiv.org/abs/2503.18746", "authors": ["Yifei Zhang", "Chang Liu", "Jin Wei", "Xiaomeng Yang", "Yu Zhou", "Can Ma", "Xiangyang Ji"], "title": "Linguistics-aware Masked Image Modeling for Self-supervised Scene Text Recognition", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Text images are unique in their dual nature, encompassing both visual and\nlinguistic information. The visual component encompasses structural and\nappearance-based features, while the linguistic dimension incorporates\ncontextual and semantic elements. In scenarios with degraded visual quality,\nlinguistic patterns serve as crucial supplements for comprehension,\nhighlighting the necessity of integrating both aspects for robust scene text\nrecognition (STR). Contemporary STR approaches often use language models or\nsemantic reasoning modules to capture linguistic features, typically requiring\nlarge-scale annotated datasets. Self-supervised learning, which lacks\nannotations, presents challenges in disentangling linguistic features related\nto the global context. Typically, sequence contrastive learning emphasizes the\nalignment of local features, while masked image modeling (MIM) tends to exploit\nlocal structures to reconstruct visual patterns, resulting in limited\nlinguistic knowledge. In this paper, we propose a Linguistics-aware Masked\nImage Modeling (LMIM) approach, which channels the linguistic information into\nthe decoding process of MIM through a separate branch. Specifically, we design\na linguistics alignment module to extract vision-independent features as\nlinguistic guidance using inputs with different visual appearances. As features\nextend beyond mere visual structures, LMIM must consider the global context to\nachieve reconstruction. Extensive experiments on various benchmarks\nquantitatively demonstrate our state-of-the-art performance, and attention\nvisualizations qualitatively show the simultaneous capture of both visual and\nlinguistic information.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18784", "pdf": "https://arxiv.org/pdf/2503.18784", "abs": "https://arxiv.org/abs/2503.18784", "authors": ["Wenxi Chen", "Raymond A. Yeh", "Shaoshuai Mou", "Yan Gu"], "title": "Leveraging Perturbation Robustness to Enhance Out-of-Distribution Detection", "categories": ["cs.CV"], "comment": null, "summary": "Out-of-distribution (OOD) detection is the task of identifying inputs that\ndeviate from the training data distribution. This capability is essential for\nsafely deploying deep computer vision models in open-world environments. In\nthis work, we propose a post-hoc method, Perturbation-Rectified OOD detection\n(PRO), based on the insight that prediction confidence for OOD inputs is more\nsusceptible to reduction under perturbation than in-distribution (IND) inputs.\nBased on the observation, we propose an adversarial score function that\nsearches for the local minimum scores near the original inputs by applying\ngradient descent. This procedure enhances the separability between IND and OOD\nsamples. Importantly, the approach improves OOD detection performance without\ncomplex modifications to the underlying model architectures. We conduct\nextensive experiments using the OpenOOD benchmark~\\cite{yang2022openood}. Our\napproach further pushes the limit of softmax-based OOD detection and is the\nleading post-hoc method for small-scale models. On a CIFAR-10 model with\nadversarial training, PRO effectively detects near-OOD inputs, achieving a\nreduction of more than 10\\% on FPR@95 compared to state-of-the-art methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18817", "pdf": "https://arxiv.org/pdf/2503.18817", "abs": "https://arxiv.org/abs/2503.18817", "authors": ["Jeonghyeon Kim", "Sangheum Hwang"], "title": "Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Prior research on out-of-distribution detection (OoDD) has primarily focused\non single-modality models. Recently, with the advent of large-scale pretrained\nvision-language models such as CLIP, OoDD methods utilizing such multi-modal\nrepresentations through zero-shot and prompt learning strategies have emerged.\nHowever, these methods typically involve either freezing the pretrained weights\nor only partially tuning them, which can be suboptimal for downstream datasets.\nIn this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve\nnotable OoDD performance. Despite some recent works demonstrating the impact of\nfine-tuning methods for OoDD, there remains significant potential for\nperformance improvement. We investigate the limitation of na\\\"ive fine-tuning\nmethods, examining why they fail to fully leverage the pretrained knowledge.\nOur empirical analysis suggests that this issue could stem from the modality\ngap within in-distribution (ID) embeddings. To address this, we propose a\ntraining objective that enhances cross-modal alignment by regularizing the\ndistances between image and text embeddings of ID data. This adjustment helps\nin better utilizing pretrained textual information by aligning similar\nsemantics from different modalities (i.e., text and image) more closely in the\nhyperspherical representation space. We theoretically demonstrate that the\nproposed regularization corresponds to the maximum likelihood estimation of an\nenergy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark\ndatasets, we show that our method, combined with post-hoc OoDD approaches\nleveraging pretrained knowledge (e.g., NegLabel), significantly outperforms\nexisting methods, achieving state-of-the-art OoDD performance and leading ID\naccuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18886", "pdf": "https://arxiv.org/pdf/2503.18886", "abs": "https://arxiv.org/abs/2503.18886", "authors": ["Weichen Fan", "Amber Yijia Zheng", "Raymond A. Yeh", "Ziwei Liu"], "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models", "categories": ["cs.CV"], "comment": null, "summary": "Classifier-Free Guidance (CFG) is a widely adopted technique in\ndiffusion/flow models to improve image fidelity and controllability. In this\nwork, we first analytically study the effect of CFG on flow matching models\ntrained on Gaussian mixtures where the ground-truth flow can be derived. We\nobserve that in the early stages of training, when the flow estimation is\ninaccurate, CFG directs samples toward incorrect trajectories. Building on this\nobservation, we propose CFG-Zero*, an improved CFG with two contributions: (a)\noptimized scale, where a scalar is optimized to correct for the inaccuracies in\nthe estimated velocity, hence the * in the name; and (b) zero-init, which\ninvolves zeroing out the first few steps of the ODE solver. Experiments on both\ntext-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video\n(Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG,\nhighlighting its effectiveness in guiding Flow Matching models. (Code is\navailable at github.com/WeichenFan/CFG-Zero-star)", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18944", "pdf": "https://arxiv.org/pdf/2503.18944", "abs": "https://arxiv.org/abs/2503.18944", "authors": ["Karim Abou Zeid", "Kadir Yilmaz", "Daan de Geus", "Alexander Hermans", "David Adrian", "Timm Linder", "Bastian Leibe"], "title": "DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation", "categories": ["cs.CV"], "comment": "Project page at https://vision.rwth-aachen.de/DITR", "summary": "Vision foundation models (VFMs) trained on large-scale image datasets provide\nhigh-quality features that have significantly advanced 2D visual recognition.\nHowever, their potential in 3D vision remains largely untapped, despite the\ncommon availability of 2D images alongside 3D point cloud datasets. While\nsignificant research has been dedicated to 2D-3D fusion, recent\nstate-of-the-art 3D methods predominantly focus on 3D data, leaving the\nintegration of VFMs into 3D models underexplored. In this work, we challenge\nthis trend by introducing DITR, a simple yet effective approach that extracts\n2D foundation model features, projects them to 3D, and finally injects them\ninto a 3D point cloud segmentation model. DITR achieves state-of-the-art\nresults on both indoor and outdoor 3D semantic segmentation benchmarks. To\nenable the use of VFMs even when images are unavailable during inference, we\nfurther propose to distill 2D foundation models into a 3D backbone as a\npretraining task. By initializing the 3D backbone with knowledge distilled from\n2D VFMs, we create a strong basis for downstream 3D segmentation tasks,\nultimately boosting performance across various datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18947", "pdf": "https://arxiv.org/pdf/2503.18947", "abs": "https://arxiv.org/abs/2503.18947", "authors": ["Jae Joong Lee", "Bedrich Benes", "Raymond A. Yeh"], "title": "Tuning-Free Amodal Segmentation via the Occlusion-Free Bias of Inpainting Models", "categories": ["cs.CV"], "comment": null, "summary": "Amodal segmentation aims to predict segmentation masks for both the visible\nand occluded regions of an object. Most existing works formulate this as a\nsupervised learning problem, requiring manually annotated amodal masks or\nsynthetic training data. Consequently, their performance depends on the quality\nof the datasets, which often lack diversity and scale. This work introduces a\ntuning-free approach that repurposes pretrained diffusion-based inpainting\nmodels for amodal segmentation. Our approach is motivated by the\n\"occlusion-free bias\" of inpainting models, i.e., the inpainted objects tend to\nbe complete objects without occlusions. Specifically, we reconstruct the\noccluded regions of an object via inpainting and then apply segmentation, all\nwithout additional training or fine-tuning. Experiments on five datasets\ndemonstrate the generalizability and robustness of our approach. On average,\nour approach achieves 5.3% more accurate masks over the state-of-the-art.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17477", "pdf": "https://arxiv.org/pdf/2503.17477", "abs": "https://arxiv.org/abs/2503.17477", "authors": ["Miguel Lpez-Prez", "Marco Miani", "Valery Naranjo", "Sren Hauberg", "Aasa Feragen"], "title": "Bayesian generative models can flag performance loss, bias, and out-of-distribution image content", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "Under review", "summary": "Generative models are popular for medical imaging tasks such as anomaly\ndetection, feature extraction, data visualization, or image generation. Since\nthey are parameterized by deep learning models, they are often sensitive to\ndistribution shifts and unreliable when applied to out-of-distribution data,\ncreating a risk of, e.g. underrepresentation bias. This behavior can be flagged\nusing uncertainty quantification methods for generative models, but their\navailability remains limited. We propose SLUG: A new UQ method for VAEs that\ncombines recent advances in Laplace approximations with stochastic trace\nestimators to scale gracefully with image dimensionality. We show that our UQ\nscore -- unlike the VAE's encoder variances -- correlates strongly with\nreconstruction error and racial underrepresentation bias for dermatological\nimages. We also show how pixel-wise uncertainty can detect out-of-distribution\nimage content such as ink, rulers, and patches, which is known to induce\nlearning shortcuts in predictive models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17482", "pdf": "https://arxiv.org/pdf/2503.17482", "abs": "https://arxiv.org/abs/2503.17482", "authors": ["Keyon Vafa", "Sarah Bentley", "Jon Kleinberg", "Sendhil Mullainathan"], "title": "What's Producible May Not Be Reachable: Measuring the Steerability of Generative Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "How should we evaluate the quality of generative models? Many existing\nmetrics focus on a model's producibility, i.e. the quality and breadth of\noutputs it can generate. However, the actual value from using a generative\nmodel stems not just from what it can produce but whether a user with a\nspecific goal can produce an output that satisfies that goal. We refer to this\nproperty as steerability. In this paper, we first introduce a mathematical\nframework for evaluating steerability independently from producibility.\nSteerability is more challenging to evaluate than producibility because it\nrequires knowing a user's goals. We address this issue by creating a benchmark\ntask that relies on one key idea: sample an output from a generative model and\nask users to reproduce it. We implement this benchmark in a large-scale user\nstudy of text-to-image models and large language models. Despite the ability of\nthese models to produce high-quality outputs, they all perform poorly on\nsteerabilty. This suggests that we need to focus on improving the steerability\nof generative models. We show such improvements are indeed possible: through\nreinforcement learning techniques, we create an alternative steering mechanism\nfor image models that achieves more than 2x improvement on this benchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17551", "pdf": "https://arxiv.org/pdf/2503.17551", "abs": "https://arxiv.org/abs/2503.17551", "authors": ["Yu Sun", "Yin Li", "Ruixiao Sun", "Chunhui Liu", "Fangming Zhou", "Ze Jin", "Linjie Wang", "Xiang Shen", "Zhuolin Hao", "Hongyu Xiong"], "title": "Audio-Enhanced Vision-Language Modeling with Latent Space Broadening for High Quality Data Expansion", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Transformer-based multimodal models are widely used in industrial-scale\nrecommendation, search, and advertising systems for content understanding and\nrelevance ranking. Enhancing labeled training data quality and cross-modal\nfusion significantly improves model performance, influencing key metrics such\nas quality view rates and ad revenue. High-quality annotations are crucial for\nadvancing content modeling, yet traditional statistical-based active learning\n(AL) methods face limitations: they struggle to detect overconfident\nmisclassifications and are less effective in distinguishing semantically\nsimilar items in deep neural networks. Additionally, audio information plays an\nincreasing role, especially in short-video platforms, yet most pre-trained\nmultimodal architectures primarily focus on text and images. While training\nfrom scratch across all three modalities is possible, it sacrifices the\nbenefits of leveraging existing pre-trained visual-language (VL) and audio\nmodels. To address these challenges, we propose kNN-based Latent Space\nBroadening (LSB) to enhance AL efficiency and Vision-Language Modeling with\nAudio Enhancement (VLMAE), a mid-fusion approach integrating audio into VL\nmodels. This system deployed in production systems, leading to significant\nbusiness gains.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17831", "pdf": "https://arxiv.org/pdf/2503.17831", "abs": "https://arxiv.org/abs/2503.17831", "authors": ["Qingshan Hou", "Meng Wang", "Peng Cao", "Zou Ke", "Xiaoli Liu", "Huazhu Fu", "Osmar R. Zaiane"], "title": "FundusGAN: A Hierarchical Feature-Aware Generative Framework for High-Fidelity Fundus Image Generation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent advancements in ophthalmology foundation models such as RetFound have\ndemonstrated remarkable diagnostic capabilities but require massive datasets\nfor effective pre-training, creating significant barriers for development and\ndeployment. To address this critical challenge, we propose FundusGAN, a novel\nhierarchical feature-aware generative framework specifically designed for\nhigh-fidelity fundus image synthesis. Our approach leverages a Feature Pyramid\nNetwork within its encoder to comprehensively extract multi-scale information,\ncapturing both large anatomical structures and subtle pathological features.\nThe framework incorporates a modified StyleGAN-based generator with dilated\nconvolutions and strategic upsampling adjustments to preserve critical retinal\nstructures while enhancing pathological detail representation. Comprehensive\nevaluations on the DDR, DRIVE, and IDRiD datasets demonstrate that FundusGAN\nconsistently outperforms state-of-the-art methods across multiple metrics\n(SSIM: 0.8863, FID: 54.2, KID: 0.0436 on DDR). Furthermore, disease\nclassification experiments reveal that augmenting training data with\nFundusGAN-generated images significantly improves diagnostic accuracy across\nmultiple CNN architectures (up to 6.49\\% improvement with ResNet50). These\nresults establish FundusGAN as a valuable foundation model component that\neffectively addresses data scarcity challenges in ophthalmological AI research,\nenabling more robust and generalizable diagnostic systems while reducing\ndependency on large-scale clinical data collection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18032", "pdf": "https://arxiv.org/pdf/2503.18032", "abs": "https://arxiv.org/abs/2503.18032", "authors": ["Emma Coletta", "Davide Salvi", "Viola Negroni", "Daniele Ugo Leonzio", "Paolo Bestagini"], "title": "Anomaly Detection and Localization for Speech Deepfakes via Feature Pyramid Matching", "categories": ["cs.SD", "cs.CV", "cs.MM"], "comment": null, "summary": "The rise of AI-driven generative models has enabled the creation of highly\nrealistic speech deepfakes - synthetic audio signals that can imitate target\nspeakers' voices - raising critical security concerns. Existing methods for\ndetecting speech deepfakes primarily rely on supervised learning, which suffers\nfrom two critical limitations: limited generalization to unseen synthesis\ntechniques and a lack of explainability. In this paper, we address these issues\nby introducing a novel interpretable one-class detection framework, which\nreframes speech deepfake detection as an anomaly detection task. Our model is\ntrained exclusively on real speech to characterize its distribution, enabling\nthe classification of out-of-distribution samples as synthetically generated.\nAdditionally, our framework produces interpretable anomaly maps during\ninference, highlighting anomalous regions across both time and frequency\ndomains. This is done through a Student-Teacher Feature Pyramid Matching\nsystem, enhanced with Discrepancy Scaling to improve generalization\ncapabilities across unseen data distributions. Extensive evaluations\ndemonstrate the superior performance of our approach compared to the considered\nbaselines, validating the effectiveness of framing speech deepfake detection as\nan anomaly detection problem.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18108", "pdf": "https://arxiv.org/pdf/2503.18108", "abs": "https://arxiv.org/abs/2503.18108", "authors": ["Junhao Ge", "Zuhong Liu", "Longteng Fan", "Yifan Jiang", "Jiaqi Su", "Yiming Li", "Zhejun Zhang", "Siheng Chen"], "title": "Unraveling the Effects of Synthetic Data on End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "End-to-end (E2E) autonomous driving (AD) models require diverse, high-quality\ndata to perform well across various driving scenarios. However, collecting\nlarge-scale real-world data is expensive and time-consuming, making\nhigh-fidelity synthetic data essential for enhancing data diversity and model\nrobustness. Existing driving simulators for synthetic data generation have\nsignificant limitations: game-engine-based simulators struggle to produce\nrealistic sensor data, while NeRF-based and diffusion-based methods face\nefficiency challenges. Additionally, recent simulators designed for closed-loop\nevaluation provide limited interaction with other vehicles, failing to simulate\ncomplex real-world traffic dynamics. To address these issues, we introduce\nSceneCrafter, a realistic, interactive, and efficient AD simulator based on 3D\nGaussian Splatting (3DGS). SceneCrafter not only efficiently generates\nrealistic driving logs across diverse traffic scenarios but also enables robust\nclosed-loop evaluation of end-to-end models. Experimental results demonstrate\nthat SceneCrafter serves as both a reliable evaluation platform and a efficient\ndata generator that significantly improves end-to-end model generalization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18225", "pdf": "https://arxiv.org/pdf/2503.18225", "abs": "https://arxiv.org/abs/2503.18225", "authors": ["Massimo Bini", "Leander Girrbach", "Zeynep Akata"], "title": "Decoupling Angles and Strength in Low-rank Adaptation", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "ICLR 2025", "summary": "Parameter-Efficient FineTuning (PEFT) methods have recently gained\nsignificant popularity thanks to the widespread availability of large-scale\npretrained models. These methods allow for quick adaptation to downstream tasks\nwith minimal computational cost. However, popular finetuning methods such as\nLoRA exhibit limited robustness when it comes to hyperparameter choices or\nextended training regimes, preventing optimal out-of-the-box performance. In\ncontrast, bounded approaches, such as ETHER, provide greater robustness but are\nlimited to extremely low-rank adaptations and fixed-strength transformations,\nreducing their adaptation expressive power. In this work, we propose Decoupled\nLow-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and\nscales learnable low-rank matrices. By bounding the distance of the\ntransformation, DeLoRA effectively decouples the angular learning from the\nadaptation strength, enhancing robustness without compromising performance.\nThrough evaluations on subject-driven image generation, natural language\nunderstanding, and instruction tuning, we show that DeLoRA matches or surpasses\nperformance of competing PEFT methods, while exhibiting stronger robustness.\nCode is available at https://github.com/ExplainableML/DeLoRA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18246", "pdf": "https://arxiv.org/pdf/2503.18246", "abs": "https://arxiv.org/abs/2503.18246", "authors": ["Feiran Wang", "Bin Duan", "Jiachen Tao", "Nikhil Sharma", "Dawen Cai", "Yan Yan"], "title": "ZECO: ZeroFusion Guided 3D MRI Conditional Generation", "categories": ["eess.IV", "cs.CV"], "comment": "Project page: \\url{https://brack-wang.github.io/ZECO_web/}; Github\n  Code: \\url{https://github.com/Brack-Wang/ZECO}", "summary": "Medical image segmentation is crucial for enhancing diagnostic accuracy and\ntreatment planning in Magnetic Resonance Imaging (MRI). However, acquiring\nprecise lesion masks for segmentation model training demands specialized\nexpertise and significant time investment, leading to a small dataset scale in\nclinical practice. In this paper, we present ZECO, a ZeroFusion guided 3D MRI\nconditional generation framework that extracts, compresses, and generates\nhigh-fidelity MRI images with corresponding 3D segmentation masks to mitigate\ndata scarcity. To effectively capture inter-slice relationships within volumes,\nwe introduce a Spatial Transformation Module that encodes MRI images into a\ncompact latent space for the diffusion process. Moving beyond unconditional\ngeneration, our novel ZeroFusion method progressively maps 3D masks to MRI\nimages in latent space, enabling robust training on limited datasets while\navoiding overfitting. ZECO outperforms state-of-the-art models in both\nquantitative and qualitative evaluations on Brain MRI datasets across various\nmodalities, showcasing its exceptional capability in synthesizing high-quality\nMRI images conditioned on segmentation masks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18462", "pdf": "https://arxiv.org/pdf/2503.18462", "abs": "https://arxiv.org/abs/2503.18462", "authors": ["Tadeusz Dziarmaga", "Marcin Kdzioka", "Artur Kasymov", "Marcin Mazur"], "title": "PALATE: Peculiar Application of the Law of Total Expectation to Enhance the Evaluation of Deep Generative Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep generative models (DGMs) have caused a paradigm shift in the field of\nmachine learning, yielding noteworthy advancements in domains such as image\nsynthesis, natural language processing, and other related areas. However, a\ncomprehensive evaluation of these models that accounts for the trichotomy\nbetween fidelity, diversity, and novelty in generated samples remains a\nformidable challenge. A recently introduced solution that has emerged as a\npromising approach in this regard is the Feature Likelihood Divergence (FLD), a\nmethod that offers a theoretically motivated practical tool, yet also exhibits\nsome computational challenges. In this paper, we propose PALATE, a novel\nenhancement to the evaluation of DGMs that addresses limitations of existing\nmetrics. Our approach is based on a peculiar application of the law of total\nexpectation to random variables representing accessible real data. When\ncombined with the MMD baseline metric and DINOv2 feature extractor, PALATE\noffers a holistic evaluation framework that matches or surpasses\nstate-of-the-art solutions while providing superior computational efficiency\nand scalability to large-scale datasets. Through a series of experiments, we\ndemonstrate the effectiveness of the PALATE enhancement, contributing a\ncomputationally efficient, holistic evaluation approach that advances the field\nof DGMs assessment, especially in detecting sample memorization and evaluating\ngeneralization capabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18578", "pdf": "https://arxiv.org/pdf/2503.18578", "abs": "https://arxiv.org/abs/2503.18578", "authors": ["Tianyu Chen", "Xingcheng Fu", "Yisen Gao", "Haodong Qian", "Yuecen Wei", "Kun Yan", "Haoyi Zhou", "Jianxin Li"], "title": "Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Modern vision-language models (VLMs) develop patch embedding and convolution\nbackbone within vector space, especially Euclidean ones, at the very founding.\nWhen expanding VLMs to a galaxy scale for understanding astronomical phenomena,\nthe integration of spherical space for planetary orbits and hyperbolic spaces\nfor black holes raises two formidable challenges. a) The current pre-training\nmodel is confined to Euclidean space rather than a comprehensive geometric\nembedding. b) The predominant architecture lacks suitable backbones for\nanisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a\ngeometry-aware VLM, for the universe-level vision understanding tasks. We\nproposed the geometry prompt that generates geometry tokens by random walks\nacross diverse spaces on a multi-scale physical graph, along with a geometry\nadapter that compresses and reshapes the space anisotropy in a\nmixture-of-experts manner. Extensive experiments demonstrate the effectiveness\nof our approach, with Galaxy-Walker achieving state-of-the-art performance in\nboth galaxy property estimation ($R^2$ scores up to $0.91$) and morphology\nclassification tasks (up to $+0.17$ F1 improvement in challenging features),\nsignificantly outperforming both domain-specific models and general-purpose\nVLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
