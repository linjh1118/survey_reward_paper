{"id": "2503.19309", "pdf": "https://arxiv.org/pdf/2503.19309", "abs": "https://arxiv.org/abs/2503.19309", "authors": ["Gollam Rabby", "Diyana Muhammed", "Prasenjit Mitra", "Sören Auer"], "title": "Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees", "categories": ["cs.CL"], "comment": null, "summary": "Scientific hypothesis generation is a fundamentally challenging task in\nresearch, requiring the synthesis of novel and empirically grounded insights.\nTraditional approaches rely on human intuition and domain expertise, while\npurely large language model (LLM) based methods often struggle to produce\nhypotheses that are both innovative and reliable. To address these limitations,\nwe propose the Monte Carlo Nash Equilibrium Self-Refine Tree (MC-NEST), a novel\nframework that integrates Monte Carlo Tree Search with Nash Equilibrium\nstrategies to iteratively refine and validate hypotheses. MC-NEST dynamically\nbalances exploration and exploitation through adaptive sampling strategies,\nwhich prioritize high-potential hypotheses while maintaining diversity in the\nsearch space. We demonstrate the effectiveness of MC-NEST through comprehensive\nexperiments across multiple domains, including biomedicine, social science, and\ncomputer science. MC-NEST achieves average scores of 2.65, 2.74, and 2.80 (on a\n1-3 scale) for novelty, clarity, significance, and verifiability metrics on the\nsocial science, computer science, and biomedicine datasets, respectively,\noutperforming state-of-the-art prompt-based methods, which achieve 2.36, 2.51,\nand 2.52 on the same datasets. These results underscore MC-NEST's ability to\ngenerate high-quality, empirically grounded hypotheses across diverse domains.\nFurthermore, MC-NEST facilitates structured human-AI collaboration, ensuring\nthat LLMs augment human creativity rather than replace it. By addressing key\nchallenges such as iterative refinement and the exploration-exploitation\nbalance, MC-NEST sets a new benchmark in automated hypothesis generation.\nAdditionally, MC-NEST's ethical design enables responsible AI use, emphasizing\ntransparency and human supervision in hypothesis generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "sampling strategies", "monte carlo tree search", "iterative refinement"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19855", "pdf": "https://arxiv.org/pdf/2503.19855", "abs": "https://arxiv.org/abs/2503.19855", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yunjie Ji", "Yiping Peng", "Han Zhao", "Xiangang Li"], "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "o1"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19877", "pdf": "https://arxiv.org/pdf/2503.19877", "abs": "https://arxiv.org/abs/2503.19877", "authors": ["Seungone Kim", "Ian Wu", "Jinu Lee", "Xiang Yue", "Seongyun Lee", "Mingyeong Moon", "Kiril Gashteovski", "Carolin Lawrence", "Julia Hockenmaier", "Graham Neubig", "Sean Welleck"], "title": "Scaling Evaluation-time Compute with Reasoning Models as Process Evaluators", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "As language model (LM) outputs get more and more natural, it is becoming more\ndifficult than ever to evaluate their quality. Simultaneously, increasing LMs'\n\"thinking\" time through scaling test-time compute has proven an effective\ntechnique to solve challenging problems in domains such as math and code. This\nraises a natural question: can an LM's evaluation capability also be improved\nby spending more test-time compute? To answer this, we investigate employing\nreasoning models-LMs that natively generate long chain-of-thought reasoning-as\nevaluators. Specifically, we examine methods to leverage more test-time compute\nby (1) using reasoning models, and (2) prompting these models to evaluate not\nonly the response as a whole (i.e., outcome evaluation) but also assess each\nstep in the response separately (i.e., process evaluation). In experiments, we\nobserve that the evaluator's performance improves monotonically when generating\nmore reasoning tokens, similar to the trends observed in LM-based generation.\nFurthermore, we use these more accurate evaluators to rerank multiple\ngenerations, and demonstrate that spending more compute at evaluation time can\nbe as effective as using more compute at generation time in improving an LM's\nproblem-solving capability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "test-time compute"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19312", "pdf": "https://arxiv.org/pdf/2503.19312", "abs": "https://arxiv.org/abs/2503.19312", "authors": ["Jiaqi Liao", "Zhengyuan Yang", "Linjie Li", "Dianqi Li", "Kevin Lin", "Yu Cheng", "Lijuan Wang"], "title": "ImageGen-CoT: Enhancing Text-to-Image In-context Learning with Chain-of-Thought Reasoning", "categories": ["cs.CV"], "comment": "Project Page: https://ImageGen-CoT.github.io/", "summary": "In this work, we study the problem of Text-to-Image In-Context Learning\n(T2I-ICL). While Unified Multimodal LLMs (MLLMs) have advanced rapidly in\nrecent years, they struggle with contextual reasoning in T2I-ICL scenarios. To\naddress this limitation, we propose a novel framework that incorporates a\nthought process called ImageGen-CoT prior to image generation. To avoid\ngenerating unstructured ineffective reasoning steps, we develop an automatic\npipeline to curate a high-quality ImageGen-CoT dataset. We then fine-tune MLLMs\nusing this dataset to enhance their contextual reasoning capabilities. To\nfurther enhance performance, we explore test-time scale-up strategies and\npropose a novel hybrid scaling approach. This approach first generates multiple\nImageGen-CoT chains and then produces multiple images for each chain via\nsampling. Extensive experiments demonstrate the effectiveness of our proposed\nmethod. Notably, fine-tuning with the ImageGen-CoT dataset leads to a\nsubstantial 80\\% performance gain for SEED-X on T2I-ICL tasks. See our project\npage at https://ImageGen-CoT.github.io/. Code and model weights will be\nopen-sourced.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19903", "pdf": "https://arxiv.org/pdf/2503.19903", "abs": "https://arxiv.org/abs/2503.19903", "authors": ["Baifeng Shi", "Boyi Li", "Han Cai", "Yao Lu", "Sifei Liu", "Marco Pavone", "Jan Kautz", "Song Han", "Trevor Darrell", "Pavlo Molchanov", "Hongxu Yin"], "title": "Scaling Vision Pre-Training to 4K Resolution", "categories": ["cs.CV"], "comment": "CVPR 2025. Project Page: https://nvlabs.github.io/PS3", "summary": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "test-time compute"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19551", "pdf": "https://arxiv.org/pdf/2503.19551", "abs": "https://arxiv.org/abs/2503.19551", "authors": ["Zeyu Qin", "Qingxiu Dong", "Xingxing Zhang", "Li Dong", "Xiaolong Huang", "Ziyi Yang", "Mahmoud Khademi", "Dongdong Zhang", "Hany Hassan Awadalla", "Yi R. Fung", "Weizhu Chen", "Minhao Cheng", "Furu Wei"], "title": "Scaling Laws of Synthetic Data for Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the \\emph{rectified scaling law} across various model\nsizes; (2) Performance improvements plateau near 300B tokens; and (3) Larger\nmodels approach optimal performance with fewer training tokens. For instance,\nan 8B model peaks at 1T tokens, while a 3B model requires 4T. Moreover,\ncomparisons with existing synthetic data generation and augmentation methods\ndemonstrate that SynthLLM achieves superior performance and scalability. Our\nfindings highlight synthetic data as a scalable and reliable alternative to\norganic pre-training corpora, offering a viable path toward continued\nimprovement in model performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scaling law"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19108", "pdf": "https://arxiv.org/pdf/2503.19108", "abs": "https://arxiv.org/abs/2503.19108", "authors": ["Tommie Kerssies", "Niccolò Cavagnero", "Alexander Hermans", "Narges Norouzi", "Giuseppe Averta", "Bastian Leibe", "Gijs Dubbelman", "Daan de Geus"], "title": "Your ViT is Secretly an Image Segmentation Model", "categories": ["cs.CV"], "comment": "CVPR 2025. Code: https://www.tue-mps.org/eomt/", "summary": "Vision Transformers (ViTs) have shown remarkable performance and scalability\nacross various computer vision tasks. To apply single-scale ViTs to image\nsegmentation, existing methods adopt a convolutional adapter to generate\nmulti-scale features, a pixel decoder to fuse these features, and a Transformer\ndecoder that uses the fused features to make predictions. In this paper, we\nshow that the inductive biases introduced by these task-specific components can\ninstead be learned by the ViT itself, given sufficiently large models and\nextensive pre-training. Based on these findings, we introduce the Encoder-only\nMask Transformer (EoMT), which repurposes the plain ViT architecture to conduct\nimage segmentation. With large-scale models and pre-training, EoMT obtains a\nsegmentation accuracy similar to state-of-the-art models that use task-specific\ncomponents. At the same time, EoMT is significantly faster than these methods\ndue to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a\nrange of model sizes, EoMT demonstrates an optimal balance between segmentation\naccuracy and prediction speed, suggesting that compute resources are better\nspent on scaling the ViT itself rather than adding architectural complexity.\nCode: https://www.tue-mps.org/eomt/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19145", "pdf": "https://arxiv.org/pdf/2503.19145", "abs": "https://arxiv.org/abs/2503.19145", "authors": ["Marco Garosi", "Alessandro Conti", "Gaowen Liu", "Elisa Ricci", "Massimiliano Mancini"], "title": "Compositional Caching for Training-free Open-vocabulary Attribute Detection", "categories": ["cs.CV"], "comment": "CVPR 2025. Project website at https://comca-attributes.github.io/", "summary": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19470", "pdf": "https://arxiv.org/pdf/2503.19470", "abs": "https://arxiv.org/abs/2503.19470", "authors": ["Mingyang Chen", "Tianpeng Li", "Haoze Sun", "Yijie Zhou", "Chenzheng Zhu", "Fan Yang", "Zenan Zhou", "Weipeng Chen", "Haofen Wang", "Jeff Z. Pan", "Wen Zhang", "Huajun Chen"], "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1", "self-correction"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19355", "pdf": "https://arxiv.org/pdf/2503.19355", "abs": "https://arxiv.org/abs/2503.19355", "authors": ["Dohwan Ko", "Sihyeon Kim", "Yumin Suh", "Vijay Kumar B. G", "Minseo Yoon", "Manmohan Chandraker", "Hyunwoo J. Kim"], "title": "ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Spatio-temporal reasoning is essential in understanding real-world\nenvironments in various fields, eg, autonomous driving and sports analytics.\nRecent advances have improved the spatial reasoning ability of Vision-Language\nModels (VLMs) by introducing large-scale data, but these models still struggle\nto analyze kinematic elements like traveled distance and speed of moving\nobjects. To bridge this gap, we construct a spatio-temporal reasoning dataset\nand benchmark involving kinematic instruction tuning, referred to as STKit and\nSTKit-Bench. They consist of real-world videos with 3D annotations, detailing\nobject motion dynamics: traveled distance, speed, movement direction,\ninter-object distance comparisons, and relative movement direction. To further\nscale such data construction to videos without 3D labels, we propose an\nautomatic pipeline to generate pseudo-labels using 4D reconstruction in\nreal-world scale. With our kinematic instruction tuning data for\nspatio-temporal reasoning, we present ST-VLM, a VLM enhanced for\nspatio-temporal reasoning, which exhibits outstanding performance on\nSTKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across\ndiverse domains and tasks, outperforming baselines on other spatio-temporal\nbenchmarks (eg, ActivityNet, TVQA+). Finally, by integrating learned\nspatio-temporal reasoning with existing abilities, ST-VLM enables complex\nmulti-step reasoning. Project page: https://ikodoh.github.io/ST-VLM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "multi-step reasoning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19385", "pdf": "https://arxiv.org/pdf/2503.19385", "abs": "https://arxiv.org/abs/2503.19385", "authors": ["Jaihoon Kim", "Taehoon Yoon", "Jisung Hwang", "Minhyuk Sung"], "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://flow-inference-time-scaling.github.io/", "summary": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19257", "pdf": "https://arxiv.org/pdf/2503.19257", "abs": "https://arxiv.org/abs/2503.19257", "authors": ["Farhana Keya", "Gollam Rabby", "Prasenjit Mitra", "Sahar Vahdati", "Sören Auer", "Yaser Jaradeh"], "title": "SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings", "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "Every scientific discovery starts with an idea inspired by prior work,\ninterdisciplinary concepts, and emerging challenges. Recent advancements in\nlarge language models (LLMs) trained on scientific corpora have driven interest\nin AI-supported idea generation. However, generating context-aware,\nhigh-quality, and innovative ideas remains challenging. We introduce SCI-IDEA,\na framework that uses LLM prompting strategies and Aha Moment detection for\niterative idea refinement. SCI-IDEA extracts essential facets from research\npublications, assessing generated ideas on novelty, excitement, feasibility,\nand effectiveness. Comprehensive experiments validate SCI-IDEA's effectiveness,\nachieving average scores of 6.84, 6.86, 6.89, and 6.84 (on a 1-10 scale) across\nnovelty, excitement, feasibility, and effectiveness, respectively. Evaluations\nemployed GPT-4o, GPT-4.5, DeepSeek-32B (each under 2-shot prompting), and\nDeepSeek-70B (3-shot prompting), with token-level embeddings used for Aha\nMoment detection. Similarly, it achieves scores of 6.87, 6.86, 6.83, and 6.87\nusing GPT-4o under 5-shot prompting, GPT-4.5 under 3-shot prompting,\nDeepSeek-32B under zero-shot chain-of-thought prompting, and DeepSeek-70B under\n5-shot prompting with sentence-level embeddings. We also address ethical\nconsiderations such as intellectual credit, potential misuse, and balancing\nhuman creativity with AI-driven ideation. Our results highlight SCI-IDEA's\npotential to facilitate the structured and flexible exploration of\ncontext-aware scientific ideas, supporting innovation while maintaining ethical\nstandards.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19633", "pdf": "https://arxiv.org/pdf/2503.19633", "abs": "https://arxiv.org/abs/2503.19633", "authors": ["Han Zhao", "Haotian Wang", "Yiping Peng", "Sitong Zhao", "Xiaoyu Tian", "Shuaiting Chen", "Yunjie Ji", "Xiangang Li"], "title": "1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large Language Model Training", "categories": ["cs.CL"], "comment": null, "summary": "The AM-DeepSeek-R1-Distilled is a large-scale dataset with thinking traces\nfor general reasoning tasks, composed of high-quality and challenging reasoning\nproblems. These problems are collected from a multitude of open-source\ndatasets, subjected to semantic deduplication and meticulous cleaning to\neliminate test set contamination. All responses within the dataset are\ndistilled from reasoning models (predominantly DeepSeek-R1) and have undergone\nrigorous verification procedures. Mathematical problems are validated by\nchecking against reference answers, code problems are verified using test\ncases, and other tasks are evaluated with the aid of a reward model. The\nAM-Distill-Qwen-32B model, which was trained through only simple Supervised\nFine-Tuning (SFT) using this batch of data, outperformed the\nDeepSeek-R1-Distill-Qwen-32B model on four benchmarks: AIME2024, MATH-500,\nGPQA-Diamond, and LiveCodeBench. Additionally, the AM-Distill-Qwen-72B model\nsurpassed the DeepSeek-R1-Distill-Llama-70B model on all benchmarks as well. We\nare releasing these 1.4 million problems and their corresponding responses to\nthe research community with the objective of fostering the development of\npowerful reasoning-oriented Large Language Models (LLMs). The dataset was\npublished in\n\\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19786", "pdf": "https://arxiv.org/pdf/2503.19786", "abs": "https://arxiv.org/abs/2503.19786", "authors": ["Gemma Team", "Aishwarya Kamath", "Johan Ferret", "Shreya Pathak", "Nino Vieillard", "Ramona Merhej", "Sarah Perrin", "Tatiana Matejovicova", "Alexandre Ramé", "Morgane Rivière", "Louis Rouillard", "Thomas Mesnard", "Geoffrey Cideron", "Jean-bastien Grill", "Sabela Ramos", "Edouard Yvinec", "Michelle Casbon", "Etienne Pot", "Ivo Penchev", "Gaël Liu", "Francesco Visin", "Kathleen Kenealy", "Lucas Beyer", "Xiaohai Zhai", "Anton Tsitsulin", "Robert Busa-Fekete", "Alex Feng", "Noveen Sachdeva", "Benjamin Coleman", "Yi Gao", "Basil Mustafa", "Iain Barr", "Emilio Parisotto", "David Tian", "Matan Eyal", "Colin Cherry", "Jan-Thorsten Peter", "Danila Sinopalnikov", "Surya Bhupatiraju", "Rishabh Agarwal", "Mehran Kazemi", "Dan Malkin", "Ravin Kumar", "David Vilar", "Idan Brusilovsky", "Jiaming Luo", "Andreas Steiner", "Abe Friesen", "Abhanshu Sharma", "Abheesht Sharma", "Adi Mayrav Gilady", "Adrian Goedeckemeyer", "Alaa Saade", "Alex Feng", "Alexander Kolesnikov", "Alexei Bendebury", "Alvin Abdagic", "Amit Vadi", "András György", "André Susano Pinto", "Anil Das", "Ankur Bapna", "Antoine Miech", "Antoine Yang", "Antonia Paterson", "Ashish Shenoy", "Ayan Chakrabarti", "Bilal Piot", "Bo Wu", "Bobak Shahriari", "Bryce Petrini", "Charlie Chen", "Charline Le Lan", "Christopher A. Choquette-Choo", "CJ Carey", "Cormac Brick", "Daniel Deutsch", "Danielle Eisenbud", "Dee Cattle", "Derek Cheng", "Dimitris Paparas", "Divyashree Shivakumar Sreepathihalli", "Doug Reid", "Dustin Tran", "Dustin Zelle", "Eric Noland", "Erwin Huizenga", "Eugene Kharitonov", "Frederick Liu", "Gagik Amirkhanyan", "Glenn Cameron", "Hadi Hashemi", "Hanna Klimczak-Plucińska", "Harman Singh", "Harsh Mehta", "Harshal Tushar Lehri", "Hussein Hazimeh", "Ian Ballantyne", "Idan Szpektor", "Ivan Nardini", "Jean Pouget-Abadie", "Jetha Chan", "Joe Stanton", "John Wieting", "Jonathan Lai", "Jordi Orbay", "Joseph Fernandez", "Josh Newlan", "Ju-yeong Ji", "Jyotinder Singh", "Kat Black", "Kathy Yu", "Kevin Hui", "Kiran Vodrahalli", "Klaus Greff", "Linhai Qiu", "Marcella Valentine", "Marina Coelho", "Marvin Ritter", "Matt Hoffman", "Matthew Watson", "Mayank Chaturvedi", "Michael Moynihan", "Min Ma", "Nabila Babar", "Natasha Noy", "Nathan Byrd", "Nick Roy", "Nikola Momchev", "Nilay Chauhan", "Noveen Sachdeva", "Oskar Bunyan", "Pankil Botarda", "Paul Caron", "Paul Kishan Rubenstein", "Phil Culliton", "Philipp Schmid", "Pier Giuseppe Sessa", "Pingmei Xu", "Piotr Stanczyk", "Pouya Tafti", "Rakesh Shivanna", "Renjie Wu", "Renke Pan", "Reza Rokni", "Rob Willoughby", "Rohith Vallu", "Ryan Mullins", "Sammy Jerome", "Sara Smoot", "Sertan Girgin", "Shariq Iqbal", "Shashir Reddy", "Shruti Sheth", "Siim Põder", "Sijal Bhatnagar", "Sindhu Raghuram Panyam", "Sivan Eiger", "Susan Zhang", "Tianqi Liu", "Trevor Yacovone", "Tyler Liechty", "Uday Kalra", "Utku Evci", "Vedant Misra", "Vincent Roseberry", "Vlad Feinberg", "Vlad Kolesnikov", "Woohyun Han", "Woosuk Kwon", "Xi Chen", "Yinlam Chow", "Yuvein Zhu", "Zichuan Wei", "Zoltan Egyed", "Victor Cotruta", "Minh Giang", "Phoebe Kirk", "Anand Rao", "Kat Black", "Nabila Babar", "Jessica Lo", "Erica Moreira", "Luiz Gustavo Martins", "Omar Sanseviero", "Lucas Gonzalez", "Zach Gleicher", "Tris Warkentin", "Vahab Mirrokni", "Evan Senter", "Eli Collins", "Joelle Barral", "Zoubin Ghahramani", "Raia Hadsell", "Yossi Matias", "D. Sculley", "Slav Petrov", "Noah Fiedel", "Noam Shazeer", "Oriol Vinyals", "Jeff Dean", "Demis Hassabis", "Koray Kavukcuoglu", "Clement Farabet", "Elena Buchatskaya", "Jean-Baptiste Alayrac", "Rohan Anil", "Dmitry", "Lepikhin", "Sebastian Borgeaud", "Olivier Bachem", "Armand Joulin", "Alek Andreev", "Cassidy Hardin", "Robert Dadashi", "Léonard Hussenot"], "title": "Gemma 3 Technical Report", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19207", "pdf": "https://arxiv.org/pdf/2503.19207", "abs": "https://arxiv.org/abs/2503.19207", "authors": ["Rong Wang", "Fabian Prada", "Ziyan Wang", "Zhongshi Jiang", "Chengxiang Yin", "Junxuan Li", "Shunsuke Saito", "Igor Santesteban", "Javier Romero", "Rohan Joshi", "Hongdong Li", "Jason Saragih", "Yaser Sheikh"], "title": "FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from Few Images", "categories": ["cs.CV"], "comment": "Published in CVPR 2025", "summary": "We present a novel method for reconstructing personalized 3D human avatars\nwith realistic animation from only a few images. Due to the large variations in\nbody shapes, poses, and cloth types, existing methods mostly require hours of\nper-subject optimization during inference, which limits their practical\napplications. In contrast, we learn a universal prior from over a thousand\nclothed humans to achieve instant feedforward generation and zero-shot\ngeneralization. Specifically, instead of rigging the avatar with shared\nskinning weights, we jointly infer personalized avatar shape, skinning weights,\nand pose-dependent deformations, which effectively improves overall geometric\nfidelity and reduces deformation artifacts. Moreover, to normalize pose\nvariations and resolve coupled ambiguity between canonical shapes and skinning\nweights, we design a 3D canonicalization process to produce pixel-aligned\ninitial conditions, which helps to reconstruct fine-grained geometric details.\nWe then propose a multi-frame feature aggregation to robustly reduce artifacts\nintroduced in canonicalization and fuse a plausible avatar preserving\nperson-specific identities. Finally, we train the model in an end-to-end\nframework on a large-scale capture dataset, which contains diverse human\nsubjects paired with high-quality 3D scans. Extensive experiments show that our\nmethod generates more authentic reconstruction and animation than\nstate-of-the-arts, and can be directly generalized to inputs from casually\ntaken phone photos. Project page and code is available at\nhttps://github.com/rongakowang/FRESA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19262", "pdf": "https://arxiv.org/pdf/2503.19262", "abs": "https://arxiv.org/abs/2503.19262", "authors": ["Ruiyi Wang", "Yushuo Zheng", "Zicheng Zhang", "Chunyi Li", "Shuaicheng Liu", "Guangtao Zhai", "Xiaohong Liu"], "title": "Learning Hazing to Dehazing: Towards Realistic Haze Generation for Real-World Image Dehazing", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Existing real-world image dehazing methods primarily attempt to fine-tune\npre-trained models or adapt their inference procedures, thus heavily relying on\nthe pre-trained models and associated training data. Moreover, restoring\nheavily distorted information under dense haze requires generative diffusion\nmodels, whose potential in dehazing remains underutilized partly due to their\nlengthy sampling processes. To address these limitations, we introduce a novel\nhazing-dehazing pipeline consisting of a Realistic Hazy Image Generation\nframework (HazeGen) and a Diffusion-based Dehazing framework (DiffDehaze).\nSpecifically, HazeGen harnesses robust generative diffusion priors of\nreal-world hazy images embedded in a pre-trained text-to-image diffusion model.\nBy employing specialized hybrid training and blended sampling strategies,\nHazeGen produces realistic and diverse hazy images as high-quality training\ndata for DiffDehaze. To alleviate the inefficiency and fidelity concerns\nassociated with diffusion-based methods, DiffDehaze adopts an Accelerated\nFidelity-Preserving Sampling process (AccSamp). The core of AccSamp is the\nTiled Statistical Alignment Operation (AlignOp), which can provide a clean and\nfaithful dehazing estimate within a small fraction of sampling steps to reduce\ncomplexity and enable effective fidelity guidance. Extensive experiments\ndemonstrate the superior dehazing performance and visual quality of our\napproach over existing methods. The code is available at\nhttps://github.com/ruiyi-w/Learning-Hazing-to-Dehazing.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19353", "pdf": "https://arxiv.org/pdf/2503.19353", "abs": "https://arxiv.org/abs/2503.19353", "authors": ["Yuxuan Hu", "Xiaodong Chen", "Cuiping Li", "Hong Chen", "Jing Zhang"], "title": "QUAD: Quantization and Parameter-Efficient Tuning of LLM with Activation Decomposition", "categories": ["cs.LG", "cs.CL", "I.2.7"], "comment": "18 pages, 8 figures, 8 tables", "summary": "Large Language Models (LLMs) excel in diverse applications but suffer\ninefficiency due to massive scale. While quantization reduces computational\ncosts, existing methods degrade accuracy in medium-sized LLMs (e.g.,\nLlama-3-8B) due to activation outliers. To address this, we propose QUAD\n(Quantization with Activation Decomposition), a framework leveraging Singular\nValue Decomposition (SVD) to suppress activation outliers for effective 4-bit\nquantization. QUAD estimates activation singular vectors offline using\ncalibration data to construct an orthogonal transformation matrix P, shifting\noutliers to additional dimensions in full precision while quantizing rest\ncomponents to 4-bit. Additionally, QUAD enables parameter-efficient fine-tuning\nvia adaptable full-precision outlier weights, narrowing the accuracy gap\nbetween quantized and full-precision models. Experiments demonstrate that QUAD\nachieves 94% ~ 96% accuracy under W4A4 quantization and 98% accuracy with\nW4A4/A8 and parameter-efficient fine-tuning for Llama-3 and Qwen-2.5 models.\nOur code is available at \\href{https://github.com/hyx1999/Quad}{repository}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19584", "pdf": "https://arxiv.org/pdf/2503.19584", "abs": "https://arxiv.org/abs/2503.19584", "authors": ["Songtao Sun", "Jingyi Li", "Yuanfei Dong", "Haoguang Liu", "Chenxin Xu", "Fuyang Li", "Qiang Liu"], "title": "Multi-agent Application System in Office Collaboration Scenarios", "categories": ["cs.AI", "cs.CL", "cs.SE"], "comment": "Technical report", "summary": "This paper introduces a multi-agent application system designed to enhance\noffice collaboration efficiency and work quality. The system integrates\nartificial intelligence, machine learning, and natural language processing\ntechnologies, achieving functionalities such as task allocation, progress\nmonitoring, and information sharing. The agents within the system are capable\nof providing personalized collaboration support based on team members' needs\nand incorporate data analysis tools to improve decision-making quality. The\npaper also proposes an intelligent agent architecture that separates Plan and\nSolver, and through techniques such as multi-turn query rewriting and business\ntool retrieval, it enhances the agent's multi-intent and multi-turn dialogue\ncapabilities. Furthermore, the paper details the design of tools and multi-turn\ndialogue in the context of office collaboration scenarios, and validates the\nsystem's effectiveness through experiments and evaluations. Ultimately, the\nsystem has demonstrated outstanding performance in real business applications,\nparticularly in query understanding, task planning, and tool calling. Looking\nforward, the system is expected to play a more significant role in addressing\ncomplex interaction issues within dynamic environments and large-scale\nmulti-agent systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19278", "pdf": "https://arxiv.org/pdf/2503.19278", "abs": "https://arxiv.org/abs/2503.19278", "authors": ["Junle Liu", "Yun Zhang", "Zixi Guo"], "title": "Multiscale Feature Importance-based Bit Allocation for End-to-End Feature Coding for Machines", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Feature Coding for Machines (FCM) aims to compress intermediate features\neffectively for remote intelligent analytics, which is crucial for future\nintelligent visual applications. In this paper, we propose a Multiscale Feature\nImportance-based Bit Allocation (MFIBA) for end-to-end FCM. First, we find that\nthe importance of features for machine vision tasks varies with the scales,\nobject size, and image instances. Based on this finding, we propose a\nMultiscale Feature Importance Prediction (MFIP) module to predict the\nimportance weight for each scale of features. Secondly, we propose a task\nloss-rate model to establish the relationship between the task accuracy losses\nof using compressed features and the bitrate of encoding these features.\nFinally, we develop a MFIBA for end-to-end FCM, which is able to assign coding\nbits of multiscale features more reasonably based on their importance.\nExperimental results demonstrate that when combined with a retained Efficient\nLearned Image Compression (ELIC), the proposed MFIBA achieves an average of\n38.202% bitrate savings in object detection compared to the anchor ELIC.\nMoreover, the proposed MFIBA achieves an average of 17.212% and 36.492% feature\nbitrate savings for instance segmentation and keypoint detection, respectively.\nWhen the proposed MFIBA is applied to the LIC-TCM, it achieves an average of\n18.103%, 19.866% and 19.597% bit rate savings on three machine vision tasks,\nrespectively, which validates the proposed MFIBA has good generalizability and\nadaptability to different machine vision tasks and FCM base codecs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19308", "pdf": "https://arxiv.org/pdf/2503.19308", "abs": "https://arxiv.org/abs/2503.19308", "authors": ["Chaohan Wang", "Yutong Xie", "Qi Chen", "Yuyin Zhou", "Qi Wu"], "title": "A Comprehensive Analysis of Mamba for 3D Volumetric Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Mamba, with its selective State Space Models (SSMs), offers a more\ncomputationally efficient solution than Transformers for long-range dependency\nmodeling. However, there is still a debate about its effectiveness in\nhigh-resolution 3D medical image segmentation. In this study, we present a\ncomprehensive investigation into Mamba's capabilities in 3D medical image\nsegmentation by tackling three pivotal questions: Can Mamba replace\nTransformers? Can it elevate multi-scale representation learning? Is complex\nscanning necessary to unlock its full potential? We evaluate Mamba's\nperformance across three large public benchmarks-AMOS, TotalSegmentator, and\nBraTS. Our findings reveal that UlikeMamba, a U-shape Mamba-based network,\nconsistently surpasses UlikeTrans, a U-shape Transformer-based network,\nparticularly when enhanced with custom-designed 3D depthwise convolutions,\nboosting accuracy and computational efficiency. Further, our proposed\nmulti-scale Mamba block demonstrates superior performance in capturing both\nfine-grained details and global context, especially in complex segmentation\ntasks, surpassing Transformer-based counterparts. We also critically assess\ncomplex scanning strategies, finding that simpler methods often suffice, while\nour Tri-scan approach delivers notable advantages in the most challenging\nscenarios. By integrating these advancements, we introduce a new network for 3D\nmedical image segmentation, positioning Mamba as a transformative force that\noutperforms leading models such as nnUNet, CoTr, and U-Mamba, offering\ncompetitive accuracy with superior computational efficiency. This study\nprovides key insights into Mamba's unique advantages, paving the way for more\nefficient and accurate approaches to 3D medical imaging.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19325", "pdf": "https://arxiv.org/pdf/2503.19325", "abs": "https://arxiv.org/abs/2503.19325", "authors": ["Yuchao Gu", "Weijia Mao", "Mike Zheng Shou"], "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction", "categories": ["cs.CV"], "comment": "Project page at https://farlongctx.github.io/", "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context vision modeling\nfaces challenges due to visual redundancy. Existing RoPE lacks effective\ntemporal decay for remote context and fails to extrapolate well to long video\nsequences. Additionally, training on long videos is computationally expensive,\nas vision tokens grow much faster than language tokens. To tackle these issues,\nwe propose balancing locality and long-range dependency. We introduce FlexRoPE,\nan test-time technique that adds flexible temporal decay to RoPE, enabling\nextrapolation to 16x longer vision contexts. Furthermore, we propose long\nshort-term context modeling, where a high-resolution short-term context window\nensures fine-grained temporal consistency, while an unlimited long-term context\nwindow encodes long-range information using fewer tokens. With this approach,\nwe can train on long video sequences with a manageable token context length. We\ndemonstrate that FAR achieves state-of-the-art performance in both short- and\nlong-video generation, providing a simple yet effective baseline for video\nautoregressive modeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19361", "pdf": "https://arxiv.org/pdf/2503.19361", "abs": "https://arxiv.org/abs/2503.19361", "authors": ["Piera Riccio", "Francesco Galati", "Kajetan Schweighofer", "Noa Garcia", "Nuria Oliver"], "title": "ImageSet2Text: Describing Sets of Images through Text", "categories": ["cs.CV"], "comment": null, "summary": "We introduce ImageSet2Text, a novel approach that leverages vision-language\nfoundation models to automatically create natural language descriptions of\nimage sets. Inspired by concept bottleneck models (CBMs) and based on\nvisual-question answering (VQA) chains, ImageSet2Text iteratively extracts key\nconcepts from image subsets, encodes them into a structured graph, and refines\ninsights using an external knowledge graph and CLIP-based validation. This\niterative process enhances interpretability and enables accurate and detailed\nset-level summarization. Through extensive experiments, we evaluate\nImageSet2Text's descriptions on accuracy, completeness, readability and overall\nquality, benchmarking it against existing vision-language models and\nintroducing new datasets for large-scale group image captioning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "summarization", "question answering"], "score": 3}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19369", "pdf": "https://arxiv.org/pdf/2503.19369", "abs": "https://arxiv.org/abs/2503.19369", "authors": ["Yufei Cai", "Hu Han", "Yuxiang Wei", "Shiguang Shan", "Xilin Chen"], "title": "EfficientMT: Efficient Temporal Adaptation for Motion Transfer in Text-to-Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "The progress on generative models has led to significant advances on\ntext-to-video (T2V) generation, yet the motion controllability of generated\nvideos remains limited. Existing motion transfer methods explored the motion\nrepresentations of reference videos to guide generation. Nevertheless, these\nmethods typically rely on sample-specific optimization strategy, resulting in\nhigh computational burdens. In this paper, we propose \\textbf{EfficientMT}, a\nnovel and efficient end-to-end framework for video motion transfer. By\nleveraging a small set of synthetic paired motion transfer samples, EfficientMT\neffectively adapts a pretrained T2V model into a general motion transfer\nframework that can accurately capture and reproduce diverse motion patterns.\nSpecifically, we repurpose the backbone of the T2V model to extract temporal\ninformation from reference videos, and further propose a scaler module to\ndistill motion-related information. Subsequently, we introduce a temporal\nintegration mechanism that seamlessly incorporates reference motion features\ninto the video generation process. After training on our self-collected\nsynthetic paired samples, EfficientMT enables general video motion transfer\nwithout requiring test-time optimization. Extensive experiments demonstrate\nthat our EfficientMT outperforms existing methods in efficiency while\nmaintaining flexible motion controllability. Our code will be available\nhttps://github.com/PrototypeNx/EfficientMT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19377", "pdf": "https://arxiv.org/pdf/2503.19377", "abs": "https://arxiv.org/abs/2503.19377", "authors": ["Akshay Kulkarni", "Ge Yan", "Chung-En Sun", "Tuomas Oikarinen", "Tsui-Wei Weng"], "title": "Interpretable Generative Models through Post-hoc Concept Bottlenecks", "categories": ["cs.CV", "cs.LG"], "comment": "CVPR 2025. Project Page:\n  https://lilywenglab.github.io/posthoc-generative-cbm/", "summary": "Concept bottleneck models (CBM) aim to produce inherently interpretable\nmodels that rely on human-understandable concepts for their predictions.\nHowever, existing approaches to design interpretable generative models based on\nCBMs are not yet efficient and scalable, as they require expensive generative\nmodel training from scratch as well as real images with labor-intensive concept\nsupervision. To address these challenges, we present two novel and low-cost\nmethods to build interpretable generative models through post-hoc techniques\nand we name our approaches: concept-bottleneck autoencoder (CB-AE) and concept\ncontroller (CC). Our proposed approaches enable efficient and scalable training\nwithout the need of real data and require only minimal to no concept\nsupervision. Additionally, our methods generalize across modern generative\nmodel families including generative adversarial networks and diffusion models.\nWe demonstrate the superior interpretability and steerability of our methods on\nnumerous standard datasets like CelebA, CelebA-HQ, and CUB with large\nimprovements (average ~25%) over the prior work, while being 4-15x faster to\ntrain. Finally, a large-scale user study is performed to validate the\ninterpretability and steerability of our methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19452", "pdf": "https://arxiv.org/pdf/2503.19452", "abs": "https://arxiv.org/abs/2503.19452", "authors": ["Yiqing Li", "Xuan Wang", "Jiawei Wu", "Yikun Ma", "Zhi Jin"], "title": "SparseGS-W: Sparse-View 3D Gaussian Splatting in the Wild with Generative Priors", "categories": ["cs.CV"], "comment": null, "summary": "Synthesizing novel views of large-scale scenes from unconstrained in-the-wild\nimages is an important but challenging task in computer vision. Existing\nmethods, which optimize per-image appearance and transient occlusion through\nimplicit neural networks from dense training views (approximately 1000 images),\nstruggle to perform effectively under sparse input conditions, resulting in\nnoticeable artifacts. To this end, we propose SparseGS-W, a novel framework\nbased on 3D Gaussian Splatting that enables the reconstruction of complex\noutdoor scenes and handles occlusions and appearance changes with as few as\nfive training images. We leverage geometric priors and constrained diffusion\npriors to compensate for the lack of multi-view information from extremely\nsparse input. Specifically, we propose a plug-and-play Constrained Novel-View\nEnhancement module to iteratively improve the quality of rendered novel views\nduring the Gaussian optimization process. Furthermore, we propose an Occlusion\nHandling module, which flexibly removes occlusions utilizing the inherent\nhigh-quality inpainting capability of constrained diffusion priors. Both\nmodules are capable of extracting appearance features from any user-provided\nreference image, enabling flexible modeling of illumination-consistent scenes.\nExtensive experiments on the PhotoTourism and Tanks and Temples datasets\ndemonstrate that SparseGS-W achieves state-of-the-art performance not only in\nfull-reference metrics, but also in commonly used non-reference metrics such as\nFID, ClipIQA, and MUSIQ.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19543", "pdf": "https://arxiv.org/pdf/2503.19543", "abs": "https://arxiv.org/abs/2503.19543", "authors": ["Junwei Zheng", "Ruiping Liu", "Yufan Chen", "Zhenfang Chen", "Kailun Yang", "Jiaming Zhang", "Rainer Stiefelhagen"], "title": "Scene-agnostic Pose Regression for Visual Localization", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project page:\n  https://junweizheng93.github.io/publications/SPR/SPR.html", "summary": "Absolute Pose Regression (APR) predicts 6D camera poses but lacks the\nadaptability to unknown environments without retraining, while Relative Pose\nRegression (RPR) generalizes better yet requires a large image retrieval\ndatabase. Visual Odometry (VO) generalizes well in unseen environments but\nsuffers from accumulated error in open trajectories. To address this dilemma,\nwe introduce a new task, Scene-agnostic Pose Regression (SPR), which can\nachieve accurate pose regression in a flexible way while eliminating the need\nfor retraining or databases. To benchmark SPR, we created a large-scale\ndataset, 360SPR, with over 200K photorealistic panoramas, 3.6M pinhole images\nand camera poses in 270 scenes at three different sensor heights. Furthermore,\na SPR-Mamba model is initially proposed to address SPR in a dual-branch manner.\nExtensive experiments and studies demonstrate the effectiveness of our SPR\nparadigm, dataset, and model. In the unknown scenes of both 360SPR and 360Loc\ndatasets, our method consistently outperforms APR, RPR and VO. The dataset and\ncode are available at\nhttps://junweizheng93.github.io/publications/SPR/SPR.html.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19546", "pdf": "https://arxiv.org/pdf/2503.19546", "abs": "https://arxiv.org/abs/2503.19546", "authors": ["Jan Kohút", "Michal Hradiš"], "title": "Practical Fine-Tuning of Autoregressive Models on Limited Handwritten Texts", "categories": ["cs.CV"], "comment": "Submitted to ICDAR2025 conference", "summary": "A common use case for OCR applications involves users uploading documents and\nprogressively correcting automatic recognition to obtain the final transcript.\nThis correction phase presents an opportunity for progressive adaptation of the\nOCR model, making it crucial to adapt early, while ensuring stability and\nreliability. We demonstrate that state-of-the-art transformer-based models can\neffectively support this adaptation, gradually reducing the annotator's\nworkload. Our results show that fine-tuning can reliably start with just 16\nlines, yielding a 10% relative improvement in CER, and scale up to 40% with 256\nlines. We further investigate the impact of model components, clarifying the\nroles of the encoder and decoder in the fine-tuning process. To guide\nadaptation, we propose reliable stopping criteria, considering both direct\napproaches and global trend analysis. Additionally, we show that OCR models can\nbe leveraged to cut annotation costs by half through confidence-based selection\nof informative lines, achieving the same performance with fewer annotations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "reliability", "criteria"], "score": 3}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19592", "pdf": "https://arxiv.org/pdf/2503.19592", "abs": "https://arxiv.org/abs/2503.19592", "authors": ["Xinxing Cheng", "Tianyang Zhang", "Wenqi Lu", "Qingjie Meng", "Alejandro F. Frangi", "Jinming Duan"], "title": "SACB-Net: Spatial-awareness Convolutions for Medical Image Registration", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Deep learning-based image registration methods have shown state-of-the-art\nperformance and rapid inference speeds. Despite these advances, many existing\napproaches fall short in capturing spatially varying information in non-local\nregions of feature maps due to the reliance on spatially-shared convolution\nkernels. This limitation leads to suboptimal estimation of deformation fields.\nIn this paper, we propose a 3D Spatial-Awareness Convolution Block (SACB) to\nenhance the spatial information within feature representations. Our SACB\nestimates the spatial clusters within feature maps by leveraging feature\nsimilarity and subsequently parameterizes the adaptive convolution kernels\nacross diverse regions. This adaptive mechanism generates the convolution\nkernels (weights and biases) tailored to spatial variations, thereby enabling\nthe network to effectively capture spatially varying information. Building on\nSACB, we introduce a pyramid flow estimator (named SACB-Net) that integrates\nSACBs to facilitate multi-scale flow composition, particularly addressing large\ndeformations. Experimental results on the brain IXI and LPBA datasets as well\nas Abdomen CT datasets demonstrate the effectiveness of SACB and the\nsuperiority of SACB-Net over the state-of-the-art learning-based registration\nmethods. The code is available at https://github.com/x-xc/SACB_Net .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19622", "pdf": "https://arxiv.org/pdf/2503.19622", "abs": "https://arxiv.org/abs/2503.19622", "authors": ["Hongcheng Gao", "Jiashu Qu", "Jingyi Tang", "Baolong Bi", "Yue Liu", "Hongyu Chen", "Li Liang", "Li Su", "Qingming Huang"], "title": "Exploring Hallucination of Large Multimodal Models in Video Understanding: Benchmark, Analysis and Mitigation", "categories": ["cs.CV"], "comment": null, "summary": "The hallucination of large multimodal models (LMMs), providing responses that\nappear correct but are actually incorrect, limits their reliability and\napplicability. This paper aims to study the hallucination problem of LMMs in\nvideo modality, which is dynamic and more challenging compared to static\nmodalities like images and text. From this motivation, we first present a\ncomprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in\nvideo understanding tasks. It is built upon three dimensions, i.e.,\nhallucination causes, hallucination aspects, and question formats, resulting in\n6K questions. Then, we quantitatively study 7 influential factors on\nhallucinations, e.g., duration time of videos, model sizes, and model\nreasoning, via experiments of 16 LMMs on the presented benchmark. In addition,\ninspired by recent thinking models like OpenAI o1, we propose a video-thinking\nmodel to mitigate the hallucinations of LMMs via supervised reasoning\nfine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT\nenhances reasoning capabilities while TDPO reduces hallucinations in the\nthinking process. Extensive experiments and analyses demonstrate the\neffectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on\nhallucination evaluation and reduces the bias score by 4.5%. The code and data\nare public at https://github.com/Hongcheng-Gao/HAVEN.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19654", "pdf": "https://arxiv.org/pdf/2503.19654", "abs": "https://arxiv.org/abs/2503.19654", "authors": ["Mehdi Moshtaghi", "Siavash H. Khajavi", "Joni Pajarinen"], "title": "RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce RGB-Th-Bench, the first benchmark designed to evaluate the\nability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.\nWhile VLMs have demonstrated remarkable progress in visual reasoning and\nmultimodal understanding, their evaluation has been predominantly limited to\nRGB-based benchmarks, leaving a critical gap in assessing their capabilities in\ninfrared vision tasks. Existing visible-infrared datasets are either\ntask-specific or lack high-quality annotations necessary for rigorous model\nevaluation. To address these limitations, RGB-Th-Bench provides a comprehensive\nevaluation framework covering 14 distinct skill dimensions, with a total of\n1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracy\nmetrics: a standard question-level accuracy and a stricter skill-level\naccuracy, which evaluates model robustness across multiple questions within\neach skill dimension. This design ensures a thorough assessment of model\nperformance, including resilience to adversarial and hallucinated responses. We\nconduct extensive evaluations on 19 state-of-the-art VLMs, revealing\nsignificant performance gaps in RGB-Thermal understanding. Our results show\nthat even the strongest models struggle with thermal image comprehension, with\nperformance heavily constrained by their RGB-based capabilities. Additionally,\nthe lack of large-scale application-specific and expert-annotated\nthermal-caption-pair datasets in pre-training is an important reason of the\nobserved performance gap. RGB-Th-Bench highlights the urgent need for further\nadvancements in multimodal learning to bridge the gap between visible and\nthermal image understanding. The dataset is available through this link, and\nthe evaluation code will also be made publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "dimension"], "score": 5}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19703", "pdf": "https://arxiv.org/pdf/2503.19703", "abs": "https://arxiv.org/abs/2503.19703", "authors": ["Qian Wang", "Zhihao Zhan", "Jialei He", "Zhituo Tu", "Xiang Zhu", "Jie Yuan"], "title": "High-Quality Spatial Reconstruction and Orthoimage Generation Using Efficient 2D Gaussian Splatting", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Highly accurate geometric precision and dense image features characterize\nTrue Digital Orthophoto Maps (TDOMs), which are in great demand for\napplications such as urban planning, infrastructure management, and\nenvironmental monitoring. Traditional TDOM generation methods need\nsophisticated processes, such as Digital Surface Models (DSM) and occlusion\ndetection, which are computationally expensive and prone to errors. This work\npresents an alternative technique rooted in 2D Gaussian Splatting (2DGS), free\nof explicit DSM and occlusion detection. With depth map generation, spatial\ninformation for every pixel within the TDOM is retrieved and can reconstruct\nthe scene with high precision. Divide-and-conquer strategy achieves excellent\nGS training and rendering with high-resolution TDOMs at a lower resource cost,\nwhich preserves higher quality of rendering on complex terrain and thin\nstructure without a decrease in efficiency. Experimental results demonstrate\nthe efficiency of large-scale scene reconstruction and high-precision terrain\nmodeling. This approach provides accurate spatial data, which assists users in\nbetter planning and decision-making based on maps.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19897", "pdf": "https://arxiv.org/pdf/2503.19897", "abs": "https://arxiv.org/abs/2503.19897", "authors": ["Lifu Wang", "Daqing Liu", "Xinchen Liu", "Xiaodong He"], "title": "Scaling Down Text Encoders of Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": "accepted by CVPR 2025", "summary": "Text encoders in diffusion models have rapidly evolved, transitioning from\nCLIP to T5-XXL. Although this evolution has significantly enhanced the models'\nability to understand complex prompts and generate text, it also leads to a\nsubstantial increase in the number of parameters. Despite T5 series encoders\nbeing trained on the C4 natural language corpus, which includes a significant\namount of non-visual data, diffusion models with T5 encoder do not respond to\nthose non-visual prompts, indicating redundancy in representational power.\nTherefore, it raises an important question: \"Do we really need such a large\ntext encoder?\" In pursuit of an answer, we employ vision-based knowledge\ndistillation to train a series of T5 encoder models. To fully inherit its\ncapabilities, we constructed our dataset based on three criteria: image\nquality, semantic understanding, and text-rendering. Our results demonstrate\nthe scaling down pattern that the distilled T5-base model can generate images\nof comparable quality to those produced by T5-XXL, while being 50 times smaller\nin size. This reduction in model size significantly lowers the GPU requirements\nfor running state-of-the-art models such as FLUX and SD3, making high-quality\ntext-to-image generation more accessible.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "criteria"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19910", "pdf": "https://arxiv.org/pdf/2503.19910", "abs": "https://arxiv.org/abs/2503.19910", "authors": ["Chuong Huynh", "Jinyu Yang", "Ashish Tawari", "Mubarak Shah", "Son Tran", "Raffay Hamid", "Trishul Chilimbi", "Abhinav Shrivastava"], "title": "CoLLM: A Large Language Model for Composed Image Retrieval", "categories": ["cs.CV", "cs.IR"], "comment": "CVPR 2025. Project page: https://collm-cvpr25.github.io/", "summary": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation", "reliability"], "score": 4}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19912", "pdf": "https://arxiv.org/pdf/2503.19912", "abs": "https://arxiv.org/abs/2503.19912", "authors": ["Xiang Xu", "Lingdong Kong", "Hui Shuai", "Wenwei Zhang", "Liang Pan", "Kai Chen", "Ziwei Liu", "Qingshan Liu"], "title": "SuperFlow++: Enhanced Spatiotemporal Consistency for Cross-Modal Data Pretraining", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Preprint; 15 pages, 6 figures, 10 tables; Code at\n  https://github.com/Xiangxu-0103/SuperFlow", "summary": "LiDAR representation learning has emerged as a promising approach to reducing\nreliance on costly and labor-intensive human annotations. While existing\nmethods primarily focus on spatial alignment between LiDAR and camera sensors,\nthey often overlook the temporal dynamics critical for capturing motion and\nscene continuity in driving scenarios. To address this limitation, we propose\nSuperFlow++, a novel framework that integrates spatiotemporal cues in both\npretraining and downstream tasks using consecutive LiDAR-camera pairs.\nSuperFlow++ introduces four key components: (1) a view consistency alignment\nmodule to unify semantic information across camera views, (2) a dense-to-sparse\nconsistency regularization mechanism to enhance feature robustness across\nvarying point cloud densities, (3) a flow-based contrastive learning approach\nthat models temporal relationships for improved scene understanding, and (4) a\ntemporal voting strategy that propagates semantic information across LiDAR\nscans to improve prediction consistency. Extensive evaluations on 11\nheterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms\nstate-of-the-art methods across diverse tasks and driving conditions.\nFurthermore, by scaling both 2D and 3D backbones during pretraining, we uncover\nemergent properties that provide deeper insights into developing scalable 3D\nfoundation models. With strong generalizability and computational efficiency,\nSuperFlow++ establishes a new benchmark for data-efficient LiDAR-based\nperception in autonomous driving. The code is publicly available at\nhttps://github.com/Xiangxu-0103/SuperFlow", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19913", "pdf": "https://arxiv.org/pdf/2503.19913", "abs": "https://arxiv.org/abs/2503.19913", "authors": ["Mingju Gao", "Yike Pan", "Huan-ang Gao", "Zongzheng Zhang", "Wenyi Li", "Hao Dong", "Hao Tang", "Li Yi", "Hao Zhao"], "title": "PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project Page: https://partrm.c7w.tech/", "summary": "As interest grows in world models that predict future states from current\nobservations and actions, accurately modeling part-level dynamics has become\nincreasingly relevant for various applications. Existing approaches, such as\nPuppet-Master, rely on fine-tuning large-scale pre-trained video diffusion\nmodels, which are impractical for real-world use due to the limitations of 2D\nvideo representation and slow processing times. To overcome these challenges,\nwe present PartRM, a novel 4D reconstruction framework that simultaneously\nmodels appearance, geometry, and part-level motion from multi-view images of a\nstatic object. PartRM builds upon large 3D Gaussian reconstruction models,\nleveraging their extensive knowledge of appearance and geometry in static\nobjects. To address data scarcity in 4D, we introduce the PartDrag-4D dataset,\nproviding multi-view observations of part-level dynamics across over 20,000\nstates. We enhance the model's understanding of interaction conditions with a\nmulti-scale drag embedding module that captures dynamics at varying\ngranularities. To prevent catastrophic forgetting during fine-tuning, we\nimplement a two-stage training process that focuses sequentially on motion and\nappearance learning. Experimental results show that PartRM establishes a new\nstate-of-the-art in part-level motion learning and can be applied in\nmanipulation tasks in robotics. Our code, data, and models are publicly\navailable to facilitate future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19916", "pdf": "https://arxiv.org/pdf/2503.19916", "abs": "https://arxiv.org/abs/2503.19916", "authors": ["Lingdong Kong", "Dongyue Lu", "Xiang Xu", "Lai Xing Ng", "Wei Tsang Ooi", "Benoit R. Cottereau"], "title": "EventFly: Event Camera Perception from Ground to the Sky", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025; 30 pages, 8 figures, 16 tables; Project Page at\n  https://event-fly.github.io/", "summary": "Cross-platform adaptation in event-based dense perception is crucial for\ndeploying event cameras across diverse settings, such as vehicles, drones, and\nquadrupeds, each with unique motion dynamics, viewpoints, and class\ndistributions. In this work, we introduce EventFly, a framework for robust\ncross-platform adaptation in event camera perception. Our approach comprises\nthree key components: i) Event Activation Prior (EAP), which identifies\nhigh-activation regions in the target domain to minimize prediction entropy,\nfostering confident, domain-adaptive predictions; ii) EventBlend, a data-mixing\nstrategy that integrates source and target event voxel grids based on\nEAP-driven similarity and density maps, enhancing feature alignment; and iii)\nEventMatch, a dual-discriminator technique that aligns features from source,\ntarget, and blended domains for better domain-invariant learning. To\nholistically assess cross-platform adaptation abilities, we introduce EXPo, a\nlarge-scale benchmark with diverse samples across vehicle, drone, and quadruped\nplatforms. Extensive experiments validate our effectiveness, demonstrating\nsubstantial gains over popular adaptation methods. We hope this work can pave\nthe way for more adaptive, high-performing event perception across diverse and\ncomplex environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19005", "pdf": "https://arxiv.org/pdf/2503.19005", "abs": "https://arxiv.org/abs/2503.19005", "authors": ["Abdul Qayyum", "Moona Mazher", "Devran Ugurlu", "Jose Alonso Solis Lemus", "Cristobal Rodero", "Steven A Niederer"], "title": "Foundation Model for Whole-Heart Segmentation: Leveraging Student-Teacher Learning in Multi-Modal Medical Imaging", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Whole-heart segmentation from CT and MRI scans is crucial for cardiovascular\ndisease analysis, yet existing methods struggle with modality-specific biases\nand the need for extensive labeled datasets. To address these challenges, we\npropose a foundation model for whole-heart segmentation using a self-supervised\nlearning (SSL) framework based on a student-teacher architecture. Our model is\npretrained on a large, unlabeled dataset of CT and MRI scans, leveraging the\nxLSTM backbone to capture long-range spatial dependencies and complex\nanatomical structures in 3D medical images. By incorporating multi-modal\npretraining, our approach ensures strong generalization across both CT and MRI\nmodalities, mitigating modality-specific variations and improving segmentation\naccuracy in diverse clinical settings. The use of large-scale unlabeled data\nsignificantly reduces the dependency on manual annotations, enabling robust\nperformance even with limited labeled data. We further introduce an\nxLSTM-UNet-based architecture for downstream whole-heart segmentation tasks,\ndemonstrating its effectiveness on few-label CT and MRI datasets. Our results\nvalidate the robustness and adaptability of the proposed model, highlighting\nits potential for advancing automated whole-heart segmentation in medical\nimaging.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19589", "pdf": "https://arxiv.org/pdf/2503.19589", "abs": "https://arxiv.org/abs/2503.19589", "authors": ["Shaolei Zhang", "Jinyan Liu", "Tianyi Qian", "Xuesong Li"], "title": "Prompt-Guided Dual-Path UNet with Mamba for Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Convolutional neural networks (CNNs) and transformers are widely employed in\nconstructing UNet architectures for medical image segmentation tasks. However,\nCNNs struggle to model long-range dependencies, while transformers suffer from\nquadratic computational complexity. Recently, Mamba, a type of State Space\nModels, has gained attention for its exceptional ability to model long-range\ninteractions while maintaining linear computational complexity. Despite the\nemergence of several Mamba-based methods, they still present the following\nlimitations: first, their network designs generally lack perceptual\ncapabilities for the original input data; second, they primarily focus on\ncapturing global information, while often neglecting local details. To address\nthese challenges, we propose a prompt-guided CNN-Mamba dual-path UNet, termed\nPGM-UNet, for medical image segmentation. Specifically, we introduce a\nprompt-guided residual Mamba module that adaptively extracts dynamic visual\nprompts from the original input data, effectively guiding Mamba in capturing\nglobal information. Additionally, we design a local-global information fusion\nnetwork, comprising a local information extraction module, a prompt-guided\nresidual Mamba module, and a multi-focus attention fusion module, which\neffectively integrates local and global information. Furthermore, inspired by\nKolmogorov-Arnold Networks (KANs), we develop a multi-scale information\nextraction module to capture richer contextual information without altering the\nresolution. We conduct extensive experiments on the ISIC-2017, ISIC-2018, DIAS,\nand DRIVE. The results demonstrate that the proposed method significantly\noutperforms state-of-the-art approaches in multiple medical image segmentation\ntasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19604", "pdf": "https://arxiv.org/pdf/2503.19604", "abs": "https://arxiv.org/abs/2503.19604", "authors": ["Ge Gao", "Siyue Teng", "Tianhao Peng", "Fan Zhang", "David Bull"], "title": "GIViC: Generative Implicit Video Compression", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "While video compression based on implicit neural representations (INRs) has\nrecently demonstrated great potential, existing INR-based video codecs still\ncannot achieve state-of-the-art (SOTA) performance compared to their\nconventional or autoencoder-based counterparts given the same coding\nconfiguration. In this context, we propose a Generative Implicit Video\nCompression framework, GIViC, aiming at advancing the performance limits of\nthis type of coding methods. GIViC is inspired by the characteristics that INRs\nshare with large language and diffusion models in exploiting long-term\ndependencies. Through the newly designed implicit diffusion process, GIViC\nperforms diffusive sampling across coarse-to-fine spatiotemporal\ndecompositions, gradually progressing from coarser-grained full-sequence\ndiffusion to finer-grained per-token diffusion. A novel Hierarchical Gated\nLinear Attention-based transformer (HGLA), is also integrated into the\nframework, which dual-factorizes global dependency modeling along scale and\nsequential axes. The proposed GIViC model has been benchmarked against SOTA\nconventional and neural codecs using a Random Access (RA) configuration (YUV\n4:2:0, GOPSize=32), and yields BD-rate savings of 15.94%, 22.46% and 8.52% over\nVVC VTM, DCVC-FM and NVRC, respectively. As far as we are aware, GIViC is the\nfirst INR-based video codec that outperforms VTM based on the RA coding\nconfiguration. The source code will be made available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19713", "pdf": "https://arxiv.org/pdf/2503.19713", "abs": "https://arxiv.org/abs/2503.19713", "authors": ["Yusen Xie", "Zhengmin Huang", "Shaojie Shen", "Jun Ma"], "title": "Semi-SD: Semi-Supervised Metric Depth Estimation via Surrounding Cameras for Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "In this paper, we introduce Semi-SD, a novel metric depth estimation\nframework tailored for surrounding cameras equipment in autonomous driving. In\nthis work, the input data consists of adjacent surrounding frames and camera\nparameters. We propose a unified spatial-temporal-semantic fusion module to\nconstruct the visual fused features. Cross-attention components for surrounding\ncameras and adjacent frames are utilized to focus on metric scale information\nrefinement and temporal feature matching. Building on this, we propose a pose\nestimation framework using surrounding cameras, their corresponding estimated\ndepths, and extrinsic parameters, which effectively address the scale ambiguity\nin multi-camera setups. Moreover, semantic world model and monocular depth\nestimation world model are integrated to supervised the depth estimation, which\nimprove the quality of depth estimation. We evaluate our algorithm on DDAD and\nnuScenes datasets, and the results demonstrate that our method achieves\nstate-of-the-art performance in terms of surrounding camera based depth\nestimation quality. The source code will be available on\nhttps://github.com/xieyuser/Semi-SD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19757", "pdf": "https://arxiv.org/pdf/2503.19757", "abs": "https://arxiv.org/abs/2503.19757", "authors": ["Zhi Hou", "Tianyi Zhang", "Yuwen Xiong", "Haonan Duan", "Hengjun Pu", "Ronglei Tong", "Chengyang Zhao", "Xizhou Zhu", "Yu Qiao", "Jifeng Dai", "Yuntao Chen"], "title": "Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy", "categories": ["cs.RO", "cs.CV"], "comment": "Preprint; https://robodita.github.io;", "summary": "While recent vision-language-action models trained on diverse robot datasets\nexhibit promising generalization capabilities with limited in-domain data,\ntheir reliance on compact action heads to predict discretized or continuous\nactions constrains adaptability to heterogeneous action spaces. We present\nDita, a scalable framework that leverages Transformer architectures to directly\ndenoise continuous action sequences through a unified multimodal diffusion\nprocess. Departing from prior methods that condition denoising on fused\nembeddings via shallow networks, Dita employs in-context conditioning --\nenabling fine-grained alignment between denoised actions and raw visual tokens\nfrom historical observations. This design explicitly models action deltas and\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\nTransformer's scalability, Dita effectively integrates cross-embodiment\ndatasets across diverse camera perspectives, observation scenes, tasks, and\naction spaces. Such synergy enhances robustness against various variances and\nfacilitates the successful execution of long-horizon tasks. Evaluations across\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\nsimulation. Notably, Dita achieves robust real-world adaptation to\nenvironmental variances and complex long-horizon tasks through 10-shot\nfinetuning, using only third-person camera inputs. The architecture establishes\na versatile, lightweight and open-source baseline for generalist robot policy\nlearning. Project Page: https://robodita.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19823", "pdf": "https://arxiv.org/pdf/2503.19823", "abs": "https://arxiv.org/abs/2503.19823", "authors": ["Yan Zhuang", "Minheng Chen", "Chao Cao", "Tong Chen", "Jing Zhang", "Xiaowei Yu", "Yanjun Lyu", "Lu Zhang", "Tianming Liu", "Dajiang Zhu"], "title": "GyralNet Subnetwork Partitioning via Differentiable Spectral Modularity Optimization", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": "10 pages, 3 figures", "summary": "Understanding the structural and functional organization of the human brain\nrequires a detailed examination of cortical folding patterns, among which the\nthree-hinge gyrus (3HG) has been identified as a key structural landmark.\nGyralNet, a network representation of cortical folding, models 3HGs as nodes\nand gyral crests as edges, highlighting their role as critical hubs in\ncortico-cortical connectivity. However, existing methods for analyzing 3HGs\nface significant challenges, including the sub-voxel scale of 3HGs at typical\nneuroimaging resolutions, the computational complexity of establishing\ncross-subject correspondences, and the oversimplification of treating 3HGs as\nindependent nodes without considering their community-level relationships. To\naddress these limitations, we propose a fully differentiable subnetwork\npartitioning framework that employs a spectral modularity maximization\noptimization strategy to modularize the organization of 3HGs within GyralNet.\nBy incorporating topological structural similarity and DTI-derived connectivity\npatterns as attribute features, our approach provides a biologically meaningful\nrepresentation of cortical organization. Extensive experiments on the Human\nConnectome Project (HCP) dataset demonstrate that our method effectively\npartitions GyralNet at the individual level while preserving the\ncommunity-level consistency of 3HGs across subjects, offering a robust\nfoundation for understanding brain connectivity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
