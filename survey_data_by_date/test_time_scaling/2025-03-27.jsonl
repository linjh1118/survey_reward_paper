{"id": "2503.20757", "pdf": "https://arxiv.org/pdf/2503.20757", "abs": "https://arxiv.org/abs/2503.20757", "authors": ["Yunhai Hu", "Yilun Zhao", "Chen Zhao", "Arman Cohan"], "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search", "categories": ["cs.CL"], "comment": null, "summary": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "scale", "monte carlo tree search", "MCTS"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.19948", "pdf": "https://arxiv.org/pdf/2503.19948", "abs": "https://arxiv.org/abs/2503.19948", "authors": ["Alexander Gambashidze", "Konstantin Sobolev", "Andrey Kuznetsov", "Ivan Oseledets"], "title": "Test-Time Reasoning Through Visual Human Preferences with VLMs and Soft Rewards", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Can Visual Language Models (VLMs) effectively capture human visual\npreferences? This work addresses this question by training VLMs to think about\npreferences at test time, employing reinforcement learning methods inspired by\nDeepSeek R1 and OpenAI O1. Using datasets such as ImageReward and Human\nPreference Score v2 (HPSv2), our models achieve accuracies of 64.9% on the\nImageReward test set (trained on ImageReward official split) and 65.4% on HPSv2\n(trained on approximately 25% of its data). These results match traditional\nencoder-based models while providing transparent reasoning and enhanced\ngeneralization. This approach allows to use not only rich VLM world knowledge,\nbut also its potential to think, yielding interpretable outcomes that help\ndecision-making processes. By demonstrating that human visual preferences\nreasonable by current VLMs, we introduce efficient soft-reward strategies for\nimage ranking, outperforming simplistic selection or scoring methods. This\nreasoning capability enables VLMs to rank arbitrary images-regardless of aspect\nratio or complexity-thereby potentially amplifying the effectiveness of visual\nPreference Optimization. By reducing the need for extensive markup while\nimproving reward generalization and explainability, our findings can be a\nstrong mile-stone that will enhance text-to-vision models even further.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time", "o1"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "ranking"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20314", "pdf": "https://arxiv.org/pdf/2503.20314", "abs": "https://arxiv.org/abs/2503.20314", "authors": ["WanTeam", ":", "Ang Wang", "Baole Ai", "Bin Wen", "Chaojie Mao", "Chen-Wei Xie", "Di Chen", "Feiwu Yu", "Haiming Zhao", "Jianxiao Yang", "Jianyuan Zeng", "Jiayu Wang", "Jingfeng Zhang", "Jingren Zhou", "Jinkai Wang", "Jixuan Chen", "Kai Zhu", "Kang Zhao", "Keyu Yan", "Lianghua Huang", "Mengyang Feng", "Ningyi Zhang", "Pandeng Li", "Pingyu Wu", "Ruihang Chu", "Ruili Feng", "Shiwei Zhang", "Siyang Sun", "Tao Fang", "Tianxing Wang", "Tianyi Gui", "Tingyu Weng", "Tong Shen", "Wei Lin", "Wei Wang", "Wei Wang", "Wenmeng Zhou", "Wente Wang", "Wenting Shen", "Wenyuan Yu", "Xianzhong Shi", "Xiaoming Huang", "Xin Xu", "Yan Kou", "Yangyu Lv", "Yifei Li", "Yijing Liu", "Yiming Wang", "Yingya Zhang", "Yitong Huang", "Yong Li", "You Wu", "Yu Liu", "Yulin Pan", "Yun Zheng", "Yuntao Hong", "Yupeng Shi", "Yutong Feng", "Zeyinzi Jiang", "Zhen Han", "Zhi-Fan Wu", "Ziyu Liu"], "title": "Wan: Open and Advanced Large-Scale Video Generative Models", "categories": ["cs.CV"], "comment": "60 pages, 33 figures", "summary": "This report presents Wan, a comprehensive and open suite of video foundation\nmodels designed to push the boundaries of video generation. Built upon the\nmainstream diffusion transformer paradigm, Wan achieves significant\nadvancements in generative capabilities through a series of innovations,\nincluding our novel VAE, scalable pre-training strategies, large-scale data\ncuration, and automated evaluation metrics. These contributions collectively\nenhance the model's performance and versatility. Specifically, Wan is\ncharacterized by four key features: Leading Performance: The 14B model of Wan,\ntrained on a vast dataset comprising billions of images and videos,\ndemonstrates the scaling laws of video generation with respect to both data and\nmodel size. It consistently outperforms the existing open-source models as well\nas state-of-the-art commercial solutions across multiple internal and external\nbenchmarks, demonstrating a clear and significant performance superiority.\nComprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B\nparameters, for efficiency and effectiveness respectively. It also covers\nmultiple downstream applications, including image-to-video, instruction-guided\nvideo editing, and personal video generation, encompassing up to eight tasks.\nConsumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource\nefficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range\nof consumer-grade GPUs. Openness: We open-source the entire series of Wan,\nincluding source code and all models, with the goal of fostering the growth of\nthe video generation community. This openness seeks to significantly expand the\ncreative possibilities of video production in the industry and provide academia\nwith high-quality video foundation models. All the code and models are\navailable at https://github.com/Wan-Video/Wan2.1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20354", "pdf": "https://arxiv.org/pdf/2503.20354", "abs": "https://arxiv.org/abs/2503.20354", "authors": ["Ke Ma", "Jiaqi Tang", "Bin Guo", "Fan Dang", "Sicong Liu", "Zhui Zhu", "Lei Wu", "Cheng Fang", "Ying-Cong Chen", "Zhiwen Yu", "Yunhao Liu"], "title": "SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic Activation Sparsity", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "Despite the growing integration of deep models into mobile terminals, the\naccuracy of these models declines significantly due to various deployment\ninterferences. Test-time adaptation (TTA) has emerged to improve the\nperformance of deep models by adapting them to unlabeled target data online.\nYet, the significant memory cost, particularly in resource-constrained\nterminals, impedes the effective deployment of most backward-propagation-based\nTTA methods. To tackle memory constraints, we introduce SURGEON, a method that\nsubstantially reduces memory cost while preserving comparable accuracy\nimprovements during fully test-time adaptation (FTTA) without relying on\nspecific network architectures or modifications to the original training\nprocedure. Specifically, we propose a novel dynamic activation sparsity\nstrategy that directly prunes activations at layer-specific dynamic ratios\nduring adaptation, allowing for flexible control of learning ability and memory\ncost in a data-sensitive manner. Among this, two metrics, Gradient Importance\nand Layer Activation Memory, are considered to determine the layer-wise pruning\nratios, reflecting accuracy contribution and memory efficiency, respectively.\nExperimentally, our method surpasses the baselines by not only reducing memory\nusage but also achieving superior accuracy, delivering SOTA performance across\ndiverse datasets, architectures, and tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20519", "pdf": "https://arxiv.org/pdf/2503.20519", "abs": "https://arxiv.org/abs/2503.20519", "authors": ["Jinnan Chen", "Lingting Zhu", "Zeyu Hu", "Shengju Qian", "Yugang Chen", "Xin Wang", "Gim Hee Lee"], "title": "MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D Generation", "categories": ["cs.CV"], "comment": "Aceepted to CVPR 2025", "summary": "Recent advances in auto-regressive transformers have revolutionized\ngenerative modeling across different domains, from language processing to\nvisual generation, demonstrating remarkable capabilities. However, applying\nthese advances to 3D generation presents three key challenges: the unordered\nnature of 3D data conflicts with sequential next-token prediction paradigm,\nconventional vector quantization approaches incur substantial compression loss\nwhen applied to 3D meshes, and the lack of efficient scaling strategies for\nhigher resolution latent prediction. To address these challenges, we introduce\nMAR-3D, which integrates a pyramid variational autoencoder with a cascaded\nmasked auto-regressive transformer (Cascaded MAR) for progressive latent\nupscaling in the continuous space. Our architecture employs random masking\nduring training and auto-regressive denoising in random order during inference,\nnaturally accommodating the unordered property of 3D latent tokens.\nAdditionally, we propose a cascaded training strategy with condition\naugmentation that enables efficiently up-scale the latent token resolution with\nfast convergence. Extensive experiments demonstrate that MAR-3D not only\nachieves superior performance and generalization capabilities compared to\nexisting methods but also exhibits enhanced scaling capabilities compared to\njoint distribution modeling approaches (e.g., diffusion transformers).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.19953", "pdf": "https://arxiv.org/pdf/2503.19953", "abs": "https://arxiv.org/abs/2503.19953", "authors": ["Stefan Stojanov", "David Wendt", "Seungwoo Kim", "Rahul Venkatesh", "Kevin Feigelis", "Jiajun Wu", "Daniel LK Yamins"], "title": "Self-Supervised Learning of Motion Concepts by Optimizing Counterfactuals", "categories": ["cs.CV"], "comment": "Project webpage: https://neuroailab.github.io/opt_cwm_page/", "summary": "Estimating motion in videos is an essential computer vision problem with many\ndownstream applications, including controllable video generation and robotics.\nCurrent solutions are primarily trained using synthetic data or require tuning\nof situation-specific heuristics, which inherently limits these models'\ncapabilities in real-world contexts. Despite recent developments in large-scale\nself-supervised learning from videos, leveraging such representations for\nmotion estimation remains relatively underexplored. In this work, we develop\nOpt-CWM, a self-supervised technique for flow and occlusion estimation from a\npre-trained next-frame prediction model. Opt-CWM works by learning to optimize\ncounterfactual probes that extract motion information from a base video model,\navoiding the need for fixed heuristics while training on unrestricted video\ninputs. We achieve state-of-the-art performance for motion estimation on\nreal-world videos while requiring no labeled data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20212", "pdf": "https://arxiv.org/pdf/2503.20212", "abs": "https://arxiv.org/abs/2503.20212", "authors": ["Yangyang Meng", "Jinpeng Li", "Guodong Lin", "Yu Pu", "Guanbo Wang", "Hu Du", "Zhiming Shao", "Yukai Huang", "Ke Li", "Wei-Qiang Zhang"], "title": "Dolphin: A Large-Scale Automatic Speech Recognition Model for Eastern Languages", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "This report introduces Dolphin, a large-scale multilingual automatic speech\nrecognition (ASR) model that extends the Whisper architecture to support a\nwider range of languages. Our approach integrates in-house proprietary and\nopen-source datasets to refine and optimize Dolphin's performance. The model is\nspecifically designed to achieve notable recognition accuracy for 40 Eastern\nlanguages across East Asia, South Asia, Southeast Asia, and the Middle East,\nwhile also supporting 22 Chinese dialects. Experimental evaluations show that\nDolphin significantly outperforms current state-of-the-art open-source models\nacross various languages. To promote reproducibility and community-driven\ninnovation, we are making our trained models and inference source code publicly\navailable.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20068", "pdf": "https://arxiv.org/pdf/2503.20068", "abs": "https://arxiv.org/abs/2503.20068", "authors": ["Naitik Jain", "Amogh Joshi", "Mason Earles"], "title": "iNatAg: Multi-Class Classification Models Enabled by a Large-Scale Benchmark Dataset with 4.7M Images of 2,959 Crop and Weed Species", "categories": ["cs.CV"], "comment": null, "summary": "Accurate identification of crop and weed species is critical for precision\nagriculture and sustainable farming. However, it remains a challenging task due\nto a variety of factors -- a high degree of visual similarity among species,\nenvironmental variability, and a continued lack of large, agriculture-specific\nimage data. We introduce iNatAg, a large-scale image dataset which contains\nover 4.7 million images of 2,959 distinct crop and weed species, with precise\nannotations along the taxonomic hierarchy from binary crop/weed labels to\nspecific species labels. Curated from the broader iNaturalist database, iNatAg\ncontains data from every continent and accurately reflects the variability of\nnatural image captures and environments. Enabled by this data, we train\nbenchmark models built upon the Swin Transformer architecture and evaluate the\nimpact of various modifications such as the incorporation of geospatial data\nand LoRA finetuning. Our best models achieve state-of-the-art performance\nacross all taxonomic classification tasks, achieving 92.38\\% on crop and weed\nclassification. Furthermore, the scale of our dataset enables us to explore\nincorrect misclassifications and unlock new analytic possiblities for plant\nspecies. By combining large-scale species coverage, multi-task labels, and\ngeographic diversity, iNatAg provides a new foundation for building robust,\ngeolocation-aware agricultural classification systems. We release the iNatAg\ndataset publicly through AgML (https://github.com/Project-AgML/AgML), enabling\ndirect access and integration into agricultural machine learning workflows.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20279", "pdf": "https://arxiv.org/pdf/2503.20279", "abs": "https://arxiv.org/abs/2503.20279", "authors": ["Sejin Lee", "Jian Kim", "Haon Park", "Ashkan Yousefpour", "Sangyoon Yu", "Min Song"], "title": "sudo rm -rf agentic_security", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed as computer-use\nagents, autonomously performing tasks within real desktop or web environments.\nWhile this evolution greatly expands practical use cases for humans, it also\ncreates serious security exposures. We present SUDO (Screen-based Universal\nDetox2Tox Offense), a novel attack framework that systematically bypasses\nrefusal trained safeguards in commercial computer-use agents, such as Claude\nComputer Use. The core mechanism, Detox2Tox, transforms harmful requests (that\nagents initially reject) into seemingly benign requests via detoxification,\nsecures detailed instructions from advanced vision language models (VLMs), and\nthen reintroduces malicious content via toxification just before execution.\nUnlike conventional jailbreaks, SUDO iteratively refines its attacks based on a\nbuilt-in refusal feedback, making it increasingly effective against robust\npolicy filters. In extensive tests spanning 50 real-world tasks and multiple\nstate-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with\nno refinement), and up to 41% (by its iterative refinement) in Claude Computer\nUse. By revealing these vulnerabilities and demonstrating the ease with which\nthey can be exploited in real-world computing environments, this paper\nhighlights an immediate need for robust, context-aware safeguards. WARNING:\nThis paper includes harmful or offensive model outputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20084", "pdf": "https://arxiv.org/pdf/2503.20084", "abs": "https://arxiv.org/abs/2503.20084", "authors": ["Simiao Ren", "Yao Yao", "Kidus Zewde", "Zisheng Liang", "Tsang", "Ng", "Ning-Yau Cheng", "Xiaoou Zhan", "Qinzhe Liu", "Yifei Chen", "Hengwei Xu"], "title": "Can Multi-modal (reasoning) LLMs work as deepfake detectors?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deepfake detection remains a critical challenge in the era of advanced\ngenerative models, particularly as synthetic media becomes more sophisticated.\nIn this study, we explore the potential of state of the art multi-modal\n(reasoning) large language models (LLMs) for deepfake image detection such as\n(OpenAI O1/4o, Gemini thinking Flash 2, Deepseek Janus, Grok 3, llama 3.2, Qwen\n2/2.5 VL, Mistral Pixtral, Claude 3.5/3.7 sonnet) . We benchmark 12 latest\nmulti-modal LLMs against traditional deepfake detection methods across multiple\ndatasets, including recently published real-world deepfake imagery. To enhance\nperformance, we employ prompt tuning and conduct an in-depth analysis of the\nmodels' reasoning pathways to identify key contributing factors in their\ndecision-making process. Our findings indicate that best multi-modal LLMs\nachieve competitive performance with promising generalization ability with zero\nshot, even surpass traditional deepfake detection pipelines in\nout-of-distribution datasets while the rest of the LLM families performs\nextremely disappointing with some worse than random guess. Furthermore, we\nfound newer model version and reasoning capabilities does not contribute to\nperformance in such niche tasks of deepfake detection while model size do help\nin some cases. This study highlights the potential of integrating multi-modal\nreasoning in future deepfake detection frameworks and provides insights into\nmodel interpretability for robustness in real-world scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20302", "pdf": "https://arxiv.org/pdf/2503.20302", "abs": "https://arxiv.org/abs/2503.20302", "authors": ["Sunayana Sitaram", "Adrian de Wynter", "Isobel McCrum", "Qilong Gu", "Si-Qing Chen"], "title": "A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Misgendering is the act of referring to someone by a gender that does not\nmatch their chosen identity. It marginalizes and undermines a person's sense of\nself, causing significant harm. English-based approaches have clear-cut\napproaches to avoiding misgendering, such as the use of the pronoun ``they''.\nHowever, other languages pose unique challenges due to both grammatical and\ncultural constructs. In this work we develop methodologies to assess and\nmitigate misgendering across 42 languages and dialects using a\nparticipatory-design approach to design effective and appropriate guardrails\nacross all languages. We test these guardrails in a standard large language\nmodel-based application (meeting transcript summarization), where both the data\ngeneration and the annotation steps followed a human-in-the-loop approach. We\nfind that the proposed guardrails are very effective in reducing misgendering\nrates across all languages in the summaries generated, and without incurring\nloss of quality. Our human-in-the-loop approach demonstrates a method to\nfeasibly scale inclusive and responsible AI-based solutions across multiple\nlanguages and cultures.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "summarization"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20588", "pdf": "https://arxiv.org/pdf/2503.20588", "abs": "https://arxiv.org/abs/2503.20588", "authors": ["Frances Yung", "Varsha Suresh", "Zaynab Reza", "Mansoor Ahmad", "Vera Demberg"], "title": "Synthetic Data Augmentation for Cross-domain Implicit Discourse Relation Recognition", "categories": ["cs.CL"], "comment": null, "summary": "Implicit discourse relation recognition (IDRR) -- the task of identifying the\nimplicit coherence relation between two text spans -- requires deep semantic\nunderstanding. Recent studies have shown that zero- or few-shot approaches\nsignificantly lag behind supervised models, but LLMs may be useful for\nsynthetic data augmentation, where LLMs generate a second argument following a\nspecified coherence relation. We applied this approach in a cross-domain\nsetting, generating discourse continuations using unlabelled target-domain data\nto adapt a base model which was trained on source-domain labelled data.\nEvaluations conducted on a large-scale test set revealed that different\nvariations of the approach did not result in any significant improvements. We\nconclude that LLMs often fail to generate useful samples for IDRR, and\nemphasize the importance of considering both statistical significance and\ncomparability when evaluating IDRR models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20641", "pdf": "https://arxiv.org/pdf/2503.20641", "abs": "https://arxiv.org/abs/2503.20641", "authors": ["Han Wu", "Yuxuan Yao", "Shuqi Liu", "Zehua Liu", "Xiaojin Fu", "Xiongwei Han", "Xing Li", "Hui-Ling Zhen", "Tao Zhong", "Mingxuan Yuan"], "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging", "categories": ["cs.CL"], "comment": "Work in progress; technical report", "summary": "The transition from System 1 to System 2 reasoning in large language models\n(LLMs) has marked significant advancements in handling complex tasks through\ndeliberate, iterative thinking. However, this progress often comes at the cost\nof efficiency, as models tend to overthink, generating redundant reasoning\nsteps without proportional improvements in output quality. Long-to-Short (L2S)\nreasoning has emerged as a promising solution to this challenge, aiming to\nbalance reasoning depth with practical efficiency. While existing approaches,\nsuch as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt\nengineering, have shown potential, they are either computationally expensive or\nunstable. Model merging, on the other hand, offers a cost-effective and robust\nalternative by integrating the quick-thinking capabilities of System 1 models\nwith the methodical reasoning of System 2 models. In this work, we present a\ncomprehensive empirical study on model merging for L2S reasoning, exploring\ndiverse methodologies, including task-vector-based, SVD-based, and\nactivation-informed merging. Our experiments reveal that model merging can\nreduce average response length by up to 55% while preserving or even improving\nbaseline performance. We also identify a strong correlation between model scale\nand merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models.\nFurthermore, we investigate the merged model's ability to self-critique and\nself-correct, as well as its adaptive response length based on task complexity.\nOur findings highlight model merging as a highly efficient and effective\nparadigm for L2S reasoning, offering a practical solution to the overthinking\nproblem while maintaining the robustness of System 2 reasoning. This work can\nbe found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20648", "pdf": "https://arxiv.org/pdf/2503.20648", "abs": "https://arxiv.org/abs/2503.20648", "authors": ["Raj Sanjay Shah", "Lei Xu", "Qianchu Liu", "Jon Burnsky", "Drew Bertagnolli", "Chaitanya Shivade"], "title": "TN-Eval: Rubric and Evaluation Protocols for Measuring the Quality of Behavioral Therapy Notes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Behavioral therapy notes are important for both legal compliance and patient\ncare. Unlike progress notes in physical health, quality standards for\nbehavioral therapy notes remain underdeveloped. To address this gap, we\ncollaborated with licensed therapists to design a comprehensive rubric for\nevaluating therapy notes across key dimensions: completeness, conciseness, and\nfaithfulness. Further, we extend a public dataset of behavioral health\nconversations with therapist-written notes and LLM-generated notes, and apply\nour evaluation framework to measure their quality. We find that: (1) A\nrubric-based manual evaluation protocol offers more reliable and interpretable\nresults than traditional Likert-scale annotations. (2) LLMs can mimic human\nevaluators in assessing completeness and conciseness but struggle with\nfaithfulness. (3) Therapist-written notes often lack completeness and\nconciseness, while LLM-generated notes contain hallucination. Surprisingly, in\na blind test, therapists prefer and judge LLM-generated notes to be superior to\ntherapist-written notes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "rubric"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20220", "pdf": "https://arxiv.org/pdf/2503.20220", "abs": "https://arxiv.org/abs/2503.20220", "authors": ["Weijie Guo", "Guofeng Zhang", "Wufei Ma", "Alan Yuille"], "title": "DINeMo: Learning Neural Mesh Models with no 3D Annotations", "categories": ["cs.CV"], "comment": "Technical report", "summary": "Category-level 3D/6D pose estimation is a crucial step towards comprehensive\n3D scene understanding, which would enable a broad range of applications in\nrobotics and embodied AI. Recent works explored neural mesh models that\napproach a range of 2D and 3D tasks from an analysis-by-synthesis perspective.\nDespite the largely enhanced robustness to partial occlusion and domain shifts,\nthese methods depended heavily on 3D annotations for part-contrastive learning,\nwhich confines them to a narrow set of categories and hinders efficient\nscaling. In this work, we present DINeMo, a novel neural mesh model that is\ntrained with no 3D annotations by leveraging pseudo-correspondence obtained\nfrom large visual foundation models. We adopt a bidirectional\npseudo-correspondence generation method, which produce pseudo correspondence\nutilize both local appearance features and global context information.\nExperimental results on car datasets demonstrate that our DINeMo outperforms\nprevious zero- and few-shot 3D pose estimation by a wide margin, narrowing the\ngap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively\nand efficiently when incorporating more unlabeled images during training, which\ndemonstrate the advantages over supervised learning methods that rely on 3D\nannotations. Our project page is available at\nhttps://analysis-by-synthesis.github.io/DINeMo/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20701", "pdf": "https://arxiv.org/pdf/2503.20701", "abs": "https://arxiv.org/abs/2503.20701", "authors": ["Zhendong Chu", "Jian Xie", "Shen Wang", "Zichao Wang", "Qingsong Wen"], "title": "UniEDU: A Unified Language and Vision Assistant for Education Applications", "categories": ["cs.CL"], "comment": null, "summary": "Education materials for K-12 students often consist of multiple modalities,\nsuch as text and images, posing challenges for models to fully understand\nnuanced information in these materials. In this paper, we propose a unified\nlanguage and vision assistant UniEDU designed for various educational\napplications, including knowledge recommendation, knowledge tracing, time cost\nprediction, and user answer prediction, all within a single model. Unlike\nconventional task-specific models, UniEDU offers a unified solution that excels\nacross multiple educational tasks while maintaining strong generalization\ncapabilities. Its adaptability makes it well-suited for real-world deployment\nin diverse learning environments. Furthermore, UniEDU is optimized for\nindustry-scale deployment by significantly reducing computational\noverhead-achieving approximately a 300\\% increase in efficiency-while\nmaintaining competitive performance with minimal degradation compared to fully\nfine-tuned models. This work represents a significant step toward creating\nversatile AI systems tailored to the evolving demands of education.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20737", "pdf": "https://arxiv.org/pdf/2503.20737", "abs": "https://arxiv.org/abs/2503.20737", "authors": ["Jeffery L Painter", "François Haguinet", "Gregory E Powell", "Andrew Bate"], "title": "Ontology-based Semantic Similarity Measures for Clustering Medical Concepts in Drug Safety", "categories": ["cs.CL", "I.2.4; G.3; H.3.3"], "comment": null, "summary": "Semantic similarity measures (SSMs) are widely used in biomedical research\nbut remain underutilized in pharmacovigilance. This study evaluates six\nontology-based SSMs for clustering MedDRA Preferred Terms (PTs) in drug safety\ndata. Using the Unified Medical Language System (UMLS), we assess each method's\nability to group PTs around medically meaningful centroids. A high-throughput\nframework was developed with a Java API and Python and R interfaces support\nlarge-scale similarity computations. Results show that while path-based methods\nperform moderately with F1 scores of 0.36 for WUPALMER and 0.28 for LCH,\nintrinsic information content (IC)-based measures, especially INTRINSIC-LIN and\nSOKAL, consistently yield better clustering accuracy (F1 score of 0.403).\nValidated against expert review and standard MedDRA queries (SMQs), our\nfindings highlight the promise of IC-based SSMs in enhancing pharmacovigilance\nworkflows by improving early signal detection and reducing manual review.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20749", "pdf": "https://arxiv.org/pdf/2503.20749", "abs": "https://arxiv.org/abs/2503.20749", "authors": ["Yuxuan Lu", "Jing Huang", "Yan Han", "Bennet Bei", "Yaochen Xie", "Dakuo Wang", "Jessie Wang", "Qi He"], "title": "Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Recent research shows that LLMs can simulate ``believable'' human behaviors\nto power LLM agents via prompt-only methods. In this work, we focus on\nevaluating and improving LLM's objective ``accuracy'' rather than the\nsubjective ``believability'' in the web action generation task, leveraging a\nlarge-scale, real-world dataset collected from online shopping human actions.\nWe present the first comprehensive quantitative evaluation of state-of-the-art\nLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action\ngeneration. Our results show that fine-tuning LLMs on real-world behavioral\ndata substantially improves their ability to generate actions compared to\nprompt-only methods. Furthermore, incorporating synthesized reasoning traces\ninto model training leads to additional performance gains, demonstrating the\nvalue of explicit rationale in behavior modeling. This work establishes a new\nbenchmark for evaluating LLMs in behavior simulation and offers actionable\ninsights into how real-world action data and reasoning augmentation can enhance\nthe fidelity of LLM agents.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20248", "pdf": "https://arxiv.org/pdf/2503.20248", "abs": "https://arxiv.org/abs/2503.20248", "authors": ["Mingfu Liang", "Jiahuan Zhou", "Xu Zou", "Ying Wu"], "title": "Incremental Object Keypoint Learning", "categories": ["cs.CV", "cs.LG"], "comment": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "summary": "Existing progress in object keypoint estimation primarily benefits from the\nconventional supervised learning paradigm based on numerous data labeled with\npre-defined keypoints. However, these well-trained models can hardly detect the\nundefined new keypoints in test time, which largely hinders their feasibility\nfor diverse downstream tasks. To handle this, various solutions are explored\nbut still suffer from either limited generalizability or transferability.\nTherefore, in this paper, we explore a novel keypoint learning paradigm in that\nwe only annotate new keypoints in the new data and incrementally train the\nmodel, without retaining any old data, called Incremental object Keypoint\nLearning (IKL). A two-stage learning scheme as a novel baseline tailored to IKL\nis developed. In the first Knowledge Association stage, given the data labeled\nwith only new keypoints, an auxiliary KA-Net is trained to automatically\nassociate the old keypoints to these new ones based on their spatial and\nintrinsic anatomical relations. In the second Mutual Promotion stage, based on\na keypoint-oriented spatial distillation loss, we jointly leverage the\nauxiliary KA-Net and the old model for knowledge consolidation to mutually\npromote the estimation of all old and new keypoints. Owing to the investigation\nof the correlations between new and old keypoints, our proposed method can not\njust effectively mitigate the catastrophic forgetting of old keypoints, but may\neven further improve the estimation of the old ones and achieve a positive\ntransfer beyond anti-forgetting. Such an observation has been solidly verified\nby extensive experiments on different keypoint datasets, where our method\nexhibits superiority in alleviating the forgetting issue and boosting\nperformance while enjoying labeling efficiency even under the low-shot data\nregime.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20786", "pdf": "https://arxiv.org/pdf/2503.20786", "abs": "https://arxiv.org/abs/2503.20786", "authors": ["Sondos Mahmoud Bsharat", "Mukul Ranjan", "Aidar Myrzakhan", "Jiacheng Liu", "Bowei Guo", "Shengkun Tang", "Zhuang Liu", "Yuanzhi Li", "Zhiqiang Shen"], "title": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": "An order-invariant and mobile-centric benchmark. Code and data are\n  available at: https://github.com/VILA-Lab/Mobile-MMLU", "summary": "Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20271", "pdf": "https://arxiv.org/pdf/2503.20271", "abs": "https://arxiv.org/abs/2503.20271", "authors": ["Haoqin Tu", "Weitao Feng", "Hardy Chen", "Hui Liu", "Xianfeng Tang", "Cihang Xie"], "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\nand data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20228", "pdf": "https://arxiv.org/pdf/2503.20228", "abs": "https://arxiv.org/abs/2503.20228", "authors": ["Xiao Lin", "Manoj Acharya", "Anirban Roy", "Susmit Jha"], "title": "TeleLoRA: Teleporting Model-Specific Alignment Across LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Mitigating Trojans in Large Language Models (LLMs) is one of many tasks where\nalignment data is LLM specific, as different LLMs have different Trojan\ntriggers and trigger behaviors to be removed. In this paper, we introduce\nTeleLoRA (Teleporting Low-Rank Adaptation), a novel framework that synergizes\nmodel-specific alignment data across multiple LLMs to enable zero-shot Trojan\nmitigation on unseen LLMs without alignment data. TeleLoRA learns a unified\ngenerator of LoRA adapter weights by leveraging local activation information\nacross multiple LLMs. This generator is designed to be permutation symmetric to\ngeneralize across models with different architectures and sizes. We optimize\nthe model design for memory efficiency, making it feasible to learn with\nlarge-scale LLMs with minimal computational resources. Experiments on LLM\nTrojan mitigation benchmarks demonstrate that TeleLoRA effectively reduces\nattack success rates while preserving the benign performance of the models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20271", "pdf": "https://arxiv.org/pdf/2503.20271", "abs": "https://arxiv.org/abs/2503.20271", "authors": ["Haoqin Tu", "Weitao Feng", "Hardy Chen", "Hui Liu", "Xianfeng Tang", "Cihang Xie"], "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\nand data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20783", "pdf": "https://arxiv.org/pdf/2503.20783", "abs": "https://arxiv.org/abs/2503.20783", "authors": ["Zichen Liu", "Changyu Chen", "Wenjun Li", "Penghui Qi", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Understanding R1-Zero-Like Training: A Critical Perspective", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20382", "pdf": "https://arxiv.org/pdf/2503.20382", "abs": "https://arxiv.org/abs/2503.20382", "authors": ["Chunshan Li", "Rong Wang", "Xiaofei Yang", "Dianhui Chu"], "title": "RSRWKV: A Linear-Complexity 2D Attention Mechanism for Efficient Remote Sensing Vision Task", "categories": ["cs.CV"], "comment": null, "summary": "High-resolution remote sensing analysis faces challenges in global context\nmodeling due to scene complexity and scale diversity. While CNNs excel at local\nfeature extraction via parameter sharing, their fixed receptive fields\nfundamentally restrict long-range dependency modeling. Vision Transformers\n(ViTs) effectively capture global semantic relationships through self-attention\nmechanisms but suffer from quadratic computational complexity relative to image\nresolution, creating critical efficiency bottlenecks for high-resolution\nimagery. The RWKV model's linear-complexity sequence modeling achieves\nbreakthroughs in NLP but exhibits anisotropic limitations in vision tasks due\nto its 1D scanning mechanism. To address these challenges, we propose RSRWKV,\nfeaturing a novel 2D-WKV scanning mechanism that bridges sequential processing\nand 2D spatial reasoning while maintaining linear complexity. This enables\nisotropic context aggregation across multiple directions. The MVC-Shift module\nenhances multi-scale receptive field coverage, while the ECA module strengthens\ncross-channel feature interaction and semantic saliency modeling. Experimental\nresults demonstrate RSRWKV's superior performance over CNN and Transformer\nbaselines in classification, detection, and segmentation tasks on NWPU\nRESISC45, VHR-10.v2, and GLH-Water datasets, offering a scalable solution for\nhigh-resolution remote sensing analysis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20428", "pdf": "https://arxiv.org/pdf/2503.20428", "abs": "https://arxiv.org/abs/2503.20428", "authors": ["F. Xavier Gaya-Morey", "Cristina Manresa-Yee", "Célia Martinie", "Jose M. Buades-Rubio"], "title": "Evaluating Facial Expression Recognition Datasets for Deep Learning: A Benchmark Study with Novel Similarity Metrics", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This study investigates the key characteristics and suitability of widely\nused Facial Expression Recognition (FER) datasets for training deep learning\nmodels. In the field of affective computing, FER is essential for interpreting\nhuman emotions, yet the performance of FER systems is highly contingent on the\nquality and diversity of the underlying datasets. To address this issue, we\ncompiled and analyzed 24 FER datasets, including those targeting specific age\ngroups such as children, adults, and the elderly, and processed them through a\ncomprehensive normalization pipeline. In addition, we enriched the datasets\nwith automatic annotations for age and gender, enabling a more nuanced\nevaluation of their demographic properties. To further assess dataset efficacy,\nwe introduce three novel metricsLocal, Global, and Paired Similarity, which\nquantitatively measure dataset difficulty, generalization capability, and\ncross-dataset transferability. Benchmark experiments using state-of-the-art\nneural networks reveal that large-scale, automatically collected datasets\n(e.g., AffectNet, FER2013) tend to generalize better, despite issues with\nlabeling noise and demographic biases, whereas controlled datasets offer higher\nannotation quality but limited variability. Our findings provide actionable\nrecommendations for dataset selection and design, advancing the development of\nmore robust, fair, and effective FER systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "annotation"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20429", "pdf": "https://arxiv.org/pdf/2503.20429", "abs": "https://arxiv.org/abs/2503.20429", "authors": ["Guilherme Fernandes", "Vasco Ramos", "Regev Cohen", "Idan Szpektor", "João Magalhães"], "title": "Latent Beam Diffusion Models for Decoding Image Sequences", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models excel at generating high-quality images from text\nprompts, they struggle with visual consistency in image sequences. Existing\nmethods generate each image independently, leading to disjointed narratives - a\nchallenge further exacerbated in non-linear storytelling, where scenes must\nconnect beyond adjacent frames. We introduce a novel beam search strategy for\nlatent space exploration, enabling conditional generation of full image\nsequences with beam search decoding. Unlike prior approaches that use fixed\nlatent priors, our method dynamically searches for an optimal sequence of\nlatent representations, ensuring coherent visual transitions. To address beam\nsearch's quadratic complexity, we integrate a cross-attention mechanism that\nefficiently scores search paths and enables pruning, prioritizing alignment\nwith both textual prompts and visual context. Human evaluations confirm that\nour approach outperforms baseline methods, producing full sequences with\nsuperior coherence, visual continuity, and textual alignment. By bridging\nadvances in search optimization and latent space refinement, this work sets a\nnew standard for structured image sequence generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["beam search"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20472", "pdf": "https://arxiv.org/pdf/2503.20472", "abs": "https://arxiv.org/abs/2503.20472", "authors": ["Yucheng Suo", "Fan Ma", "Linchao Zhu", "Tianyi Wang", "Fengyun Rao", "Yi Yang"], "title": "From Trial to Triumph: Advancing Long Video Understanding via Visual Context Sample Scaling and Self-reward Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-modal Large language models (MLLMs) show remarkable ability in video\nunderstanding. Nevertheless, understanding long videos remains challenging as\nthe models can only process a finite number of frames in a single inference,\npotentially omitting crucial visual information. To address the challenge, we\npropose generating multiple predictions through visual context sampling,\nfollowed by a scoring mechanism to select the final prediction. Specifically,\nwe devise a bin-wise sampling strategy that enables MLLMs to generate diverse\nanswers based on various combinations of keyframes, thereby enriching the\nvisual context. To determine the final prediction from the sampled answers, we\nemploy a self-reward by linearly combining three scores: (1) a frequency score\nindicating the prevalence of each option, (2) a marginal confidence score\nreflecting the inter-intra sample certainty of MLLM predictions, and (3) a\nreasoning score for different question types, including clue-guided answering\nfor global questions and temporal self-refocusing for local questions. The\nfrequency score ensures robustness through majority correctness, the\nconfidence-aligned score reflects prediction certainty, and the typed-reasoning\nscore addresses cases with sparse key visual information using tailored\nstrategies. Experiments show that this approach covers the correct answer for a\nhigh percentage of long video questions, on seven datasets show that our method\nimproves the performance of three MLLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20492", "pdf": "https://arxiv.org/pdf/2503.20492", "abs": "https://arxiv.org/abs/2503.20492", "authors": ["Fanhu Zeng", "Zhen Cheng", "Fei Zhu", "Xu-Yao Zhang"], "title": "Towards Efficient and General-Purpose Few-Shot Misclassification Detection for Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "preprint", "summary": "Reliable prediction by classifiers is crucial for their deployment in high\nsecurity and dynamically changing situations. However, modern neural networks\noften exhibit overconfidence for misclassified predictions, highlighting the\nneed for confidence estimation to detect errors. Despite the achievements\nobtained by existing methods on small-scale datasets, they all require training\nfrom scratch and there are no efficient and effective misclassification\ndetection (MisD) methods, hindering practical application towards large-scale\nand ever-changing datasets. In this paper, we pave the way to exploit vision\nlanguage model (VLM) leveraging text information to establish an efficient and\ngeneral-purpose misclassification detection framework. By harnessing the power\nof VLM, we construct FSMisD, a Few-Shot prompt learning framework for MisD to\nrefrain from training from scratch and therefore improve tuning efficiency. To\nenhance misclassification detection ability, we use adaptive pseudo sample\ngeneration and a novel negative loss to mitigate the issue of overconfidence by\npushing category prompts away from pseudo features. We conduct comprehensive\nexperiments with prompt learning methods and validate the generalization\nability across various datasets with domain shift. Significant and consistent\nimprovement demonstrates the effectiveness, efficiency and generalizability of\nour approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20516", "pdf": "https://arxiv.org/pdf/2503.20516", "abs": "https://arxiv.org/abs/2503.20516", "authors": ["Mahya Nikouei", "Bita Baroutian", "Shahabedin Nabavi", "Fateme Taraghi", "Atefe Aghaei", "Ayoob Sajedi", "Mohsen Ebrahimi Moghaddam"], "title": "Small Object Detection: A Comprehensive Survey on Challenges, Techniques and Real-World Applications", "categories": ["cs.CV"], "comment": null, "summary": "Small object detection (SOD) is a critical yet challenging task in computer\nvision, with applications like spanning surveillance, autonomous systems,\nmedical imaging, and remote sensing. Unlike larger objects, small objects\ncontain limited spatial and contextual information, making accurate detection\ndifficult. Challenges such as low resolution, occlusion, background\ninterference, and class imbalance further complicate the problem. This survey\nprovides a comprehensive review of recent advancements in SOD using deep\nlearning, focusing on articles published in Q1 journals during 2024-2025. We\nanalyzed challenges, state-of-the-art techniques, datasets, evaluation metrics,\nand real-world applications. Recent advancements in deep learning have\nintroduced innovative solutions, including multi-scale feature extraction,\nSuper-Resolution (SR) techniques, attention mechanisms, and transformer-based\narchitectures. Additionally, improvements in data augmentation, synthetic data\ngeneration, and transfer learning have addressed data scarcity and domain\nadaptation issues. Furthermore, emerging trends such as lightweight neural\nnetworks, knowledge distillation (KD), and self-supervised learning offer\npromising directions for improving detection efficiency, particularly in\nresource-constrained environments like Unmanned Aerial Vehicles (UAV)-based\nsurveillance and edge computing. We also review widely used datasets, along\nwith standard evaluation metrics such as mean Average Precision (mAP) and\nsize-specific AP scores. The survey highlights real-world applications,\nincluding traffic monitoring, maritime surveillance, industrial defect\ndetection, and precision agriculture. Finally, we discuss open research\nchallenges and future directions, emphasizing the need for robust domain\nadaptation techniques, better feature fusion strategies, and real-time\nperformance optimization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20662", "pdf": "https://arxiv.org/pdf/2503.20662", "abs": "https://arxiv.org/abs/2503.20662", "authors": ["Sadaf Khademi", "Mehran Shabanpour", "Reza Taleei", "Anastasia Oikonomou", "Arash Mohammadi"], "title": "AutoRad-Lung: A Radiomic-Guided Prompting Autoregressive Vision-Language Model for Lung Nodule Malignancy Prediction", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Lung cancer remains one of the leading causes of cancer-related mortality\nworldwide. A crucial challenge for early diagnosis is differentiating uncertain\ncases with similar visual characteristics and closely annotation scores. In\nclinical practice, radiologists rely on quantitative, hand-crafted Radiomic\nfeatures extracted from Computed Tomography (CT) images, while recent research\nhas primarily focused on deep learning solutions. More recently,\nVision-Language Models (VLMs), particularly Contrastive Language-Image\nPre-Training (CLIP)-based models, have gained attention for their ability to\nintegrate textual knowledge into lung cancer diagnosis. While CLIP-Lung models\nhave shown promising results, we identified the following potential\nlimitations: (a) dependence on radiologists' annotated attributes, which are\ninherently subjective and error-prone, (b) use of textual information only\nduring training, limiting direct applicability at inference, and (c)\nConvolutional-based vision encoder with randomly initialized weights, which\ndisregards prior knowledge. To address these limitations, we introduce\nAutoRad-Lung, which couples an autoregressively pre-trained VLM, with prompts\ngenerated from hand-crafted Radiomics. AutoRad-Lung uses the vision encoder of\nthe Large-Scale Autoregressive Image Model (AIMv2), pre-trained using a\nmulti-modal autoregressive objective. Given that lung tumors are typically\nsmall, irregularly shaped, and visually similar to healthy tissue, AutoRad-Lung\noffers significant advantages over its CLIP-based counterparts by capturing\npixel-level differences. Additionally, we introduce conditional context\noptimization, which dynamically generates context-specific prompts based on\ninput Radiomics, improving cross-modal alignment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20663", "pdf": "https://arxiv.org/pdf/2503.20663", "abs": "https://arxiv.org/abs/2503.20663", "authors": ["Mingze Sun", "Shiwei Mao", "Keyi Chen", "Yurun Chen", "Shunlin Lu", "Jingbo Wang", "Junting Dong", "Ruqi Huang"], "title": "ARMO: Autoregressive Rigging for Multi-Category Objects", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in large-scale generative models have significantly\nimproved the quality and diversity of 3D shape generation. However, most\nexisting methods focus primarily on generating static 3D models, overlooking\nthe potentially dynamic nature of certain shapes, such as humanoids, animals,\nand insects. To address this gap, we focus on rigging, a fundamental task in\nanimation that establishes skeletal structures and skinning for 3D models. In\nthis paper, we introduce OmniRig, the first large-scale rigging dataset,\ncomprising 79,499 meshes with detailed skeleton and skinning information.\nUnlike traditional benchmarks that rely on predefined standard poses (e.g.,\nA-pose, T-pose), our dataset embraces diverse shape categories, styles, and\nposes. Leveraging this rich dataset, we propose ARMO, a novel rigging framework\nthat utilizes an autoregressive model to predict both joint positions and\nconnectivity relationships in a unified manner. By treating the skeletal\nstructure as a complete graph and discretizing it into tokens, we encode the\njoints using an auto-encoder to obtain a latent embedding and an autoregressive\nmodel to predict the tokens. A mesh-conditioned latent diffusion model is used\nto predict the latent embedding for conditional skeleton generation. Our method\naddresses the limitations of regression-based approaches, which often suffer\nfrom error accumulation and suboptimal connectivity estimation. Through\nextensive experiments on the OmniRig dataset, our approach achieves\nstate-of-the-art performance in skeleton prediction, demonstrating improved\ngeneralization across diverse object categories. The code and dataset will be\nmade public for academic use upon acceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20734", "pdf": "https://arxiv.org/pdf/2503.20734", "abs": "https://arxiv.org/abs/2503.20734", "authors": ["Ziyu Zhou", "Keyan Hu", "Yutian Fang", "Xiaoping Rui"], "title": "SChanger: Change Detection from a Semantic Change and Spatial Consistency Perspective", "categories": ["cs.CV"], "comment": null, "summary": "Change detection is a key task in Earth observation applications. Recently,\ndeep learning methods have demonstrated strong performance and widespread\napplication. However, change detection faces data scarcity due to the\nlabor-intensive process of accurately aligning remote sensing images of the\nsame area, which limits the performance of deep learning algorithms. To address\nthe data scarcity issue, we develop a fine-tuning strategy called the Semantic\nChange Network (SCN). We initially pre-train the model on single-temporal\nsupervised tasks to acquire prior knowledge of instance feature extraction. The\nmodel then employs a shared-weight Siamese architecture and extended Temporal\nFusion Module (TFM) to preserve this prior knowledge and is fine-tuned on\nchange detection tasks. The learned semantics for identifying all instances is\nchanged to focus on identifying only the changes. Meanwhile, we observe that\nthe locations of changes between the two images are spatially identical, a\nconcept we refer to as spatial consistency. We introduce this inductive bias\nthrough an attention map that is generated by large-kernel convolutions and\napplied to the features from both time points. This enhances the modeling of\nmulti-scale changes and helps capture underlying relationships in change\ndetection semantics. We develop a binary change detection model utilizing these\ntwo strategies. The model is validated against state-of-the-art methods on six\ndatasets, surpassing all benchmark methods and achieving F1 scores of 92.87%,\n86.43%, 68.95%, 97.62%, 84.58%, and 93.20% on the LEVIR-CD, LEVIR-CD+,\nS2Looking, CDD, SYSU-CD, and WHU-CD datasets, respectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20748", "pdf": "https://arxiv.org/pdf/2503.20748", "abs": "https://arxiv.org/abs/2503.20748", "authors": ["Chen Tang", "Xinzhu Ma", "Encheng Su", "Xiufeng Song", "Xiaohong Liu", "Wei-Hong Li", "Lei Bai", "Wanli Ouyang", "Xiangyu Yue"], "title": "UniSTD: Towards Unified Spatio-Temporal Learning across Diverse Disciplines", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Traditional spatiotemporal models generally rely on task-specific\narchitectures, which limit their generalizability and scalability across\ndiverse tasks due to domain-specific design requirements. In this paper, we\nintroduce \\textbf{UniSTD}, a unified Transformer-based framework for\nspatiotemporal modeling, which is inspired by advances in recent foundation\nmodels with the two-stage pretraining-then-adaption paradigm. Specifically, our\nwork demonstrates that task-agnostic pretraining on 2D vision and vision-text\ndatasets can build a generalizable model foundation for spatiotemporal\nlearning, followed by specialized joint training on spatiotemporal datasets to\nenhance task-specific adaptability. To improve the learning capabilities across\ndomains, our framework employs a rank-adaptive mixture-of-expert adaptation by\nusing fractional interpolation to relax the discrete variables so that can be\noptimized in the continuous space. Additionally, we introduce a temporal module\nto incorporate temporal dynamics explicitly. We evaluate our approach on a\nlarge-scale dataset covering 10 tasks across 4 disciplines, demonstrating that\na unified spatiotemporal model can achieve scalable, cross-task learning and\nsupport up to 10 tasks simultaneously within one model while reducing training\ncosts in multi-domain applications. Code will be available at\nhttps://github.com/1hunters/UniSTD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20776", "pdf": "https://arxiv.org/pdf/2503.20776", "abs": "https://arxiv.org/abs/2503.20776", "authors": ["Shijie Zhou", "Hui Ren", "Yijia Weng", "Shuwang Zhang", "Zhen Wang", "Dejia Xu", "Zhiwen Fan", "Suya You", "Zhangyang Wang", "Leonidas Guibas", "Achuta Kadambi"], "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20781", "pdf": "https://arxiv.org/pdf/2503.20781", "abs": "https://arxiv.org/abs/2503.20781", "authors": ["Yulu Pan", "Ce Zhang", "Gedas Bertasius"], "title": "BASKET: A Large-Scale Video Dataset for Fine-Grained Skill Estimation", "categories": ["cs.CV"], "comment": null, "summary": "We present BASKET, a large-scale basketball video dataset for fine-grained\nskill estimation. BASKET contains 4,477 hours of video capturing 32,232\nbasketball players from all over the world. Compared to prior skill estimation\ndatasets, our dataset includes a massive number of skilled participants with\nunprecedented diversity in terms of gender, age, skill level, geographical\nlocation, etc. BASKET includes 20 fine-grained basketball skills, challenging\nmodern video recognition models to capture the intricate nuances of player\nskill through in-depth video analysis. Given a long highlight video (8-10\nminutes) of a particular player, the model needs to predict the skill level\n(e.g., excellent, good, average, fair, poor) for each of the 20 basketball\nskills. Our empirical analysis reveals that the current state-of-the-art video\nmodels struggle with this task, significantly lagging behind the human\nbaseline. We believe that BASKET could be a useful resource for developing new\nvideo models with advanced long-range, fine-grained recognition capabilities.\nIn addition, we hope that our dataset will be useful for domain-specific\napplications such as fair basketball scouting, personalized player development,\nand many others. Dataset and code are available at\nhttps://github.com/yulupan00/BASKET.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20785", "pdf": "https://arxiv.org/pdf/2503.20785", "abs": "https://arxiv.org/abs/2503.20785", "authors": ["Tianqi Liu", "Zihao Huang", "Zhaoxi Chen", "Guangcong Wang", "Shoukang Hu", "Liao Shen", "Huiqiang Sun", "Zhiguo Cao", "Wei Li", "Ziwei Liu"], "title": "Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal Consistency", "categories": ["cs.CV"], "comment": "Project Page: https://free4d.github.io/ , Code:\n  https://github.com/TQTQliu/Free4D", "summary": "We present Free4D, a novel tuning-free framework for 4D scene generation from\na single image. Existing methods either focus on object-level generation,\nmaking scene-level generation infeasible, or rely on large-scale multi-view\nvideo datasets for expensive training, with limited generalization ability due\nto the scarcity of 4D scene data. In contrast, our key insight is to distill\npre-trained foundation models for consistent 4D scene representation, which\noffers promising advantages such as efficiency and generalizability. 1) To\nachieve this, we first animate the input image using image-to-video diffusion\nmodels followed by 4D geometric structure initialization. 2) To turn this\ncoarse structure into spatial-temporal consistent multiview videos, we design\nan adaptive guidance mechanism with a point-guided denoising strategy for\nspatial consistency and a novel latent replacement strategy for temporal\ncoherence. 3) To lift these generated observations into consistent 4D\nrepresentation, we propose a modulation-based refinement to mitigate\ninconsistencies while fully leveraging the generated information. The resulting\n4D representation enables real-time, controllable rendering, marking a\nsignificant advancement in single-image-based 4D scene generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20047", "pdf": "https://arxiv.org/pdf/2503.20047", "abs": "https://arxiv.org/abs/2503.20047", "authors": ["Yu Xin", "Gorkem Can Ates", "Kuang Gong", "Wei Shao"], "title": "Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image Analysis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have shown promise in 2D medical image\nanalysis, but extending them to 3D remains challenging due to the high\ncomputational demands of volumetric data and the difficulty of aligning 3D\nspatial features with clinical text. We present Med3DVLM, a 3D VLM designed to\naddress these challenges through three key innovations: (1) DCFormer, an\nefficient encoder that uses decomposed 3D convolutions to capture fine-grained\nspatial features at scale; (2) SigLIP, a contrastive learning strategy with\npairwise sigmoid loss that improves image-text alignment without relying on\nlarge negative batches; and (3) a dual-stream MLP-Mixer projector that fuses\nlow- and high-level image features with text embeddings for richer multi-modal\nrepresentations. We evaluate our model on the M3D dataset, which includes\nradiology reports and VQA data for 120,084 3D medical images. Results show that\nMed3DVLM achieves superior performance across multiple benchmarks. For\nimage-text retrieval, it reaches 61.00% R@1 on 2,000 samples, significantly\noutperforming the current state-of-the-art M3D model (19.10%). For report\ngeneration, it achieves a METEOR score of 36.42% (vs. 14.38%). In open-ended\nvisual question answering (VQA), it scores 36.76% METEOR (vs. 33.58%), and in\nclosed-ended VQA, it achieves 79.95% accuracy (vs. 75.78%). These results\nhighlight Med3DVLM's ability to bridge the gap between 3D imaging and language,\nenabling scalable, multi-task reasoning across clinical applications. Our code\nis publicly available at https://github.com/mirthAI/Med3DVLM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "question answering", "fine-grained"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20653", "pdf": "https://arxiv.org/pdf/2503.20653", "abs": "https://arxiv.org/abs/2503.20653", "authors": ["Antoine Schieb", "Bilal Hadjadji", "Daniel Tshokola Mweze", "Natalia Fernanda Valderrama", "Valentin Derangère", "Laurent Arnould", "Sylvain Ladoire", "Alain Lalande", "Louis-Oscar Morel", "Nathan Vinçon"], "title": "UWarp: A Whole Slide Image Registration Pipeline to Characterize Scanner-Induced Local Domain Shift", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Histopathology slide digitization introduces scanner-induced domain shift\nthat can significantly impact computational pathology models based on deep\nlearning methods. In the state-of-the-art, this shift is often characterized at\na broad scale (slide-level or dataset-level) but not patch-level, which limits\nour comprehension of the impact of localized tissue characteristics on the\naccuracy of the deep learning models. To address this challenge, we present a\ndomain shift analysis framework based on UWarp, a novel registration tool\ndesigned to accurately align histological slides scanned under varying\nconditions. UWarp employs a hierarchical registration approach, combining\nglobal affine transformations with fine-grained local corrections to achieve\nrobust tissue patch alignment. We evaluate UWarp using two private datasets,\nCypathLung and BosomShieldBreast, containing whole slide images scanned by\nmultiple devices. Our experiments demonstrate that UWarp outperforms existing\nopen-source registration methods, achieving a median target registration error\n(TRE) of less than 4 pixels (<1 micrometer at 40x magnification) while\nsignificantly reducing computational time. Additionally, we apply UWarp to\ncharacterize scanner-induced local domain shift in the predictions of\nBreast-NEOprAIdict, a deep learning model for breast cancer pathological\nresponse prediction. We find that prediction variability is strongly correlated\nwith tissue density on a given patch. Our findings highlight the importance of\nlocalized domain shift analysis and suggest that UWarp can serve as a valuable\ntool for improving model robustness and domain adaptation strategies in\ncomputational pathology.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
