{"id": "2503.20853", "pdf": "https://arxiv.org/pdf/2503.20853", "abs": "https://arxiv.org/abs/2503.20853", "authors": ["Alexander Swerdlow", "Mihir Prabhudesai", "Siddharth Gandhi", "Deepak Pathak", "Katerina Fragkiadaki"], "title": "Unified Multimodal Discrete Diffusion", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project Website: https://unidisc.github.io", "summary": "Multimodal generative models that can understand and generate across multiple\nmodalities are dominated by autoregressive (AR) approaches, which process\ntokens sequentially from left to right, or top to bottom. These models jointly\nhandle images, text, video, and audio for various tasks such as image\ncaptioning, question answering, and image generation. In this work, we explore\ndiscrete diffusion models as a unified generative formulation in the joint text\nand image domain, building upon their recent success in text generation.\nDiscrete diffusion models offer several advantages over AR models, including\nimproved control over quality versus diversity of generated samples, the\nability to perform joint multimodal inpainting (across both text and image\ndomains), and greater controllability in generation through guidance.\nLeveraging these benefits, we present the first Unified Multimodal Discrete\nDiffusion (UniDisc) model which is capable of jointly understanding and\ngenerating text and images for a variety of downstream tasks. We compare\nUniDisc to multimodal AR models, performing a scaling analysis and\ndemonstrating that UniDisc outperforms them in terms of both performance and\ninference-time compute, enhanced controllability, editability, inpainting, and\nflexible trade-off between inference time and generation quality. Code and\nadditional visualizations are available at https://unidisc.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "inference time", "scaling"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21720", "pdf": "https://arxiv.org/pdf/2503.21720", "abs": "https://arxiv.org/abs/2503.21720", "authors": ["Souradip Chakraborty", "Sujay Bhatt", "Udari Madhushani Sehwag", "Soumya Suvra Ghosal", "Jiahao Qiu", "Mengdi Wang", "Dinesh Manocha", "Furong Huang", "Alec Koppel", "Sumitra Ganesh"], "title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICLR 2025", "summary": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy\ndeployment in applications. Reinforcement learning from human feedback (RLHF)\nhas emerged as an effective technique to align LLMs to human preferences and\nbroader utilities, but it requires updating billions of model parameters, which\nis computationally expensive. Controlled Decoding, by contrast, provides a\nmechanism for aligning a model at inference time without retraining. However,\nsingle-agent decoding approaches often struggle to adapt to diverse tasks due\nto the complexity and variability inherent in these tasks. To strengthen the\ntest-time performance w.r.t the target task, we propose a mixture of\nagent-based decoding strategies leveraging the existing off-the-shelf aligned\nLLM policies. Treating each prior policy as an agent in the spirit of mixture\nof agent collaboration, we develop a decoding method that allows for\ninference-time alignment through a token-level selection strategy among\nmultiple agents. For each token, the most suitable LLM is dynamically chosen\nfrom a pool of models based on a long-term utility metric. This\npolicy-switching mechanism ensures optimal model selection at each step,\nenabling efficient collaboration and alignment among LLMs during decoding.\nTheoretical analysis of our proposed algorithm establishes optimal performance\nwith respect to the target task represented via a target reward for the given\noff-the-shelf models. We conduct comprehensive empirical evaluations with\nopen-source aligned models on diverse tasks and preferences, which demonstrates\nthe merits of this approach over single-agent decoding baselines. Notably,\nCollab surpasses the current SoTA decoding strategy, achieving an improvement\nof up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time", "inference time"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "alignment"], "score": 5}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21295", "pdf": "https://arxiv.org/pdf/2503.21295", "abs": "https://arxiv.org/abs/2503.21295", "authors": ["Shuaijie She", "Junxiao Liu", "Yifeng Liu", "Jiajun Chen", "Xin Huang", "Shujian Huang"], "title": "R-PRM: Reasoning-Driven Process Reward Modeling", "categories": ["cs.CL"], "comment": "The project is available at https://github.com/NJUNLP/R-PRM", "summary": "Large language models (LLMs) inevitably make mistakes when performing\nstep-by-step mathematical reasoning. Process Reward Models (PRMs) have emerged\nas a promising solution by evaluating each reasoning step. However, existing\nPRMs typically output evaluation scores directly, limiting both learning\nefficiency and evaluation accuracy, which is further exacerbated by the\nscarcity of annotated data. To address these issues, we propose\nReasoning-Driven Process Reward Modeling (R-PRM). First, we leverage stronger\nLLMs to generate seed data from limited annotations, effectively bootstrapping\nour model's reasoning capabilities and enabling comprehensive step-by-step\nevaluation. Second, we further enhance performance through preference\noptimization, without requiring additional annotated data. Third, we introduce\ninference-time scaling to fully harness the model's reasoning potential.\nExtensive experiments demonstrate R-PRM's effectiveness: on ProcessBench and\nPRMBench, it surpasses strong baselines by 11.9 and 8.5 points in F1 scores,\nrespectively. When applied to guide mathematical reasoning, R-PRM achieves\nconsistent accuracy improvements of over 8.5 points across six challenging\ndatasets. Further analysis reveals that R-PRM exhibits more comprehensive\nevaluation and stronger generalization capabilities, thereby highlighting its\nsignificant potential.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "mathematical reasoning"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21208", "pdf": "https://arxiv.org/pdf/2503.21208", "abs": "https://arxiv.org/abs/2503.21208", "authors": ["Wenxuan Qiu", "Chengxin Xie", "Jingui Huang"], "title": "An improved EfficientNetV2 for garbage classification", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents an enhanced waste classification framework based on\nEfficientNetV2 to address challenges in data acquisition cost, generalization,\nand real-time performance. We propose a Channel-Efficient Attention\n(CE-Attention) module that mitigates feature loss during global pooling without\nintroducing dimensional scaling, effectively enhancing critical feature\nextraction. Additionally, a lightweight multi-scale spatial feature extraction\nmodule (SAFM) is developed by integrating depthwise separable convolutions,\nsignificantly reducing model complexity. Comprehensive data augmentation\nstrategies are further employed to improve generalization. Experiments on the\nHuawei Cloud waste classification dataset demonstrate that our method achieves\na classification accuracy of 95.4\\%, surpassing the baseline by 3.2\\% and\noutperforming mainstream models. The results validate the effectiveness of our\napproach in balancing accuracy and efficiency for practical waste\nclassification scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21614", "pdf": "https://arxiv.org/pdf/2503.21614", "abs": "https://arxiv.org/abs/2503.21614", "authors": ["Xiaoye Qu", "Yafu Li", "Zhaochen Su", "Weigao Sun", "Jianhao Yan", "Dongrui Liu", "Ganqu Cui", "Daizong Liu", "Shuxian Liang", "Junxian He", "Peng Li", "Wei Wei", "Jing Shao", "Chaochao Lu", "Yue Zhang", "Xian-Sheng Hua", "Bowen Zhou", "Yu Cheng"], "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond", "categories": ["cs.CL"], "comment": "Survey, 32 pages, Large Reasoning Models, Efficient Reasoning for\n  Language, Multimodality, and Beyond", "summary": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have\ndemonstrated strong performance gains by scaling up the length of\nChain-of-Thought (CoT) reasoning during inference. However, a growing concern\nlies in their tendency to produce excessively long reasoning traces, which are\noften filled with redundant content (e.g., repeated definitions), over-analysis\nof simple problems, and superficial exploration of multiple reasoning paths for\nharder tasks. This inefficiency introduces significant challenges for training,\ninference, and real-world deployment (e.g., in agent-based systems), where\ntoken economy is critical. In this survey, we provide a comprehensive overview\nof recent efforts aimed at improving reasoning efficiency in LRMs, with a\nparticular focus on the unique challenges that arise in this new paradigm. We\nidentify common patterns of inefficiency, examine methods proposed across the\nLRM lifecycle, i.e., from pretraining to inference, and discuss promising\nfuture directions for research. To support ongoing development, we also\nmaintain a real-time GitHub repository tracking recent progress in the field.\nWe hope this survey serves as a foundation for further exploration and inspires\ninnovation in this rapidly evolving area.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "o1"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21696", "pdf": "https://arxiv.org/pdf/2503.21696", "abs": "https://arxiv.org/abs/2503.21696", "authors": ["Wenqi Zhang", "Mengna Wang", "Gangao Liu", "Xu Huixin", "Yiwei Jiang", "Yongliang Shen", "Guiyang Hou", "Zhe Zheng", "Hang Zhang", "Xin Li", "Weiming Lu", "Peng Li", "Yueting Zhuang"], "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks", "categories": ["cs.CL", "cs.CV"], "comment": "Code: https://github.com/zwq2018/embodied_reasoner Dataset:\n  https://huggingface.co/datasets/zwq2018/embodied_reasoner", "summary": "Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1", "self-correction"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "mathematical reasoning"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21777", "pdf": "https://arxiv.org/pdf/2503.21777", "abs": "https://arxiv.org/abs/2503.21777", "authors": ["Jiahao Xie", "Alessio Tonioni", "Nathalie Rauschmayr", "Federico Tombari", "Bernt Schiele"], "title": "Test-Time Visual In-Context Tuning", "categories": ["cs.CV", "cs.LG"], "comment": "CVPR 2025. Code: https://github.com/Jiahao000/VICT", "summary": "Visual in-context learning (VICL), as a new paradigm in computer vision,\nallows the model to rapidly adapt to various tasks with only a handful of\nprompts and examples. While effective, the existing VICL paradigm exhibits poor\ngeneralizability under distribution shifts. In this work, we propose test-time\nVisual In-Context Tuning (VICT), a method that can adapt VICL models on the fly\nwith a single test sample. Specifically, we flip the role between the task\nprompts and the test sample and use a cycle consistency loss to reconstruct the\noriginal task prompt output. Our key insight is that a model should be aware of\na new test distribution if it can successfully recover the original task\nprompts. Extensive experiments on six representative vision tasks ranging from\nhigh-level visual understanding to low-level image processing, with 15 common\ncorruptions, demonstrate that our VICT can improve the generalizability of VICL\nto unseen new domains. In addition, we show the potential of applying VICT for\nunseen tasks at test time. Code: https://github.com/Jiahao000/VICT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21696", "pdf": "https://arxiv.org/pdf/2503.21696", "abs": "https://arxiv.org/abs/2503.21696", "authors": ["Wenqi Zhang", "Mengna Wang", "Gangao Liu", "Xu Huixin", "Yiwei Jiang", "Yongliang Shen", "Guiyang Hou", "Zhe Zheng", "Hang Zhang", "Xin Li", "Weiming Lu", "Peng Li", "Yueting Zhuang"], "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks", "categories": ["cs.CL", "cs.CV"], "comment": "Code: https://github.com/zwq2018/embodied_reasoner Dataset:\n  https://huggingface.co/datasets/zwq2018/embodied_reasoner", "summary": "Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1", "self-correction"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "mathematical reasoning"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20801", "pdf": "https://arxiv.org/pdf/2503.20801", "abs": "https://arxiv.org/abs/2503.20801", "authors": ["Tao Meng", "Shuo Shan", "Hongen Shao", "Yuntao Shou", "Wei Ai", "Keqin Li"], "title": "SE-GNN: Seed Expanded-Aware Graph Neural Network with Iterative Optimization for Semi-supervised Entity Alignment", "categories": ["cs.CL"], "comment": "15 pages", "summary": "Entity alignment aims to use pre-aligned seed pairs to find other equivalent\nentities from different knowledge graphs (KGs) and is widely used in graph\nfusion-related fields. However, as the scale of KGs increases, manually\nannotating pre-aligned seed pairs becomes difficult. Existing research utilizes\nentity embeddings obtained by aggregating single structural information to\nidentify potential seed pairs, thus reducing the reliance on pre-aligned seed\npairs. However, due to the structural heterogeneity of KGs, the quality of\npotential seed pairs obtained using only a single structural information is not\nideal. In addition, although existing research improves the quality of\npotential seed pairs through semi-supervised iteration, they underestimate the\nimpact of embedding distortion produced by noisy seed pairs on the alignment\neffect. In order to solve the above problems, we propose a seed expanded-aware\ngraph neural network with iterative optimization for semi-supervised entity\nalignment, named SE-GNN. First, we utilize the semantic attributes and\nstructural features of entities, combined with a conditional filtering\nmechanism, to obtain high-quality initial potential seed pairs. Next, we\ndesigned a local and global awareness mechanism. It introduces initial\npotential seed pairs and combines local and global information to obtain a more\ncomprehensive entity embedding representation, which alleviates the impact of\nKGs structural heterogeneity and lays the foundation for the optimization of\ninitial potential seed pairs. Then, we designed the threshold nearest neighbor\nembedding correction strategy. It combines the similarity threshold and the\nbidirectional nearest neighbor method as a filtering mechanism to select\niterative potential seed pairs and also uses an embedding correction strategy\nto eliminate the embedding distortion.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20835", "pdf": "https://arxiv.org/pdf/2503.20835", "abs": "https://arxiv.org/abs/2503.20835", "authors": ["Qichen Sun", "Yuxing Lu", "Kun Xia", "Li Chen", "He Sun", "Jinzhuo Wang"], "title": "Comprehensive Manuscript Assessment with Text Summarization Using 69707 articles", "categories": ["cs.CL"], "comment": null, "summary": "Rapid and efficient assessment of the future impact of research articles is a\nsignificant concern for both authors and reviewers. The most common standard\nfor measuring the impact of academic papers is the number of citations. In\nrecent years, numerous efforts have been undertaken to predict citation counts\nwithin various citation windows. However, most of these studies focus solely on\na specific academic field or require early citation counts for prediction,\nrendering them impractical for the early-stage evaluation of papers. In this\nwork, we harness Scopus to curate a significantly comprehensive and large-scale\ndataset of information from 69707 scientific articles sourced from 99 journals\nspanning multiple disciplines. We propose a deep learning methodology for the\nimpact-based classification tasks, which leverages semantic features extracted\nfrom the manuscripts and paper metadata. To summarize the semantic features,\nsuch as titles and abstracts, we employ a Transformer-based language model to\nencode semantic features and design a text fusion layer to capture shared\ninformation between titles and abstracts. We specifically focus on the\nfollowing impact-based prediction tasks using information of scientific\nmanuscripts in pre-publication stage: (1) The impact of journals in which the\nmanuscripts will be published. (2) The future impact of manuscripts themselves.\nExtensive experiments on our datasets demonstrate the superiority of our\nproposed model for impact-based prediction tasks. We also demonstrate\npotentials in generating manuscript's feedback and improvement suggestions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "summarization"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20897", "pdf": "https://arxiv.org/pdf/2503.20897", "abs": "https://arxiv.org/abs/2503.20897", "authors": ["Venuri Amarasinghe", "Asini Jayakody", "Isun Randila", "Kalinga Bandara", "Chamuditha Jayanga Galappaththige", "Ranga Rodrigo"], "title": "Feature Modulation for Semi-Supervised Domain Generalization without Domain Labels", "categories": ["cs.CV"], "comment": null, "summary": "Semi-supervised domain generalization (SSDG) leverages a small fraction of\nlabeled data alongside unlabeled data to enhance model generalization. Most of\nthe existing SSDG methods rely on pseudo-labeling (PL) for unlabeled data,\noften assuming access to domain labels-a privilege not always available.\nHowever, domain shifts introduce domain noise, leading to inconsistent PLs that\ndegrade model performance. Methods derived from FixMatch suffer particularly\nfrom lower PL accuracy, reducing the effectiveness of unlabeled data. To\naddress this, we tackle the more challenging domain-label agnostic SSDG, where\ndomain labels for unlabeled data are not available during training. First, we\npropose a feature modulation strategy that enhances class-discriminative\nfeatures while suppressing domain-specific information. This modulation shifts\nfeatures toward Similar Average Representations-a modified version of class\nprototypes-that are robust across domains, encouraging the classifier to\ndistinguish between closely related classes and feature extractor to form\ntightly clustered, domain-invariant representations. Second, to mitigate domain\nnoise and improve pseudo-label accuracy, we introduce a loss-scaling function\nthat dynamically lowers the fixed confidence threshold for pseudo-labels,\noptimizing the use of unlabeled data. With these key innovations, our approach\nachieves significant improvements on four major domain generalization\nbenchmarks-even without domain labels. We will make the code available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20960", "pdf": "https://arxiv.org/pdf/2503.20960", "abs": "https://arxiv.org/abs/2503.20960", "authors": ["Arnav Arora", "Srishti Yadav", "Maria Antoniak", "Serge Belongie", "Isabelle Augenstein"], "title": "Multi-Modal Framing Analysis of News", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Automated frame analysis of political communication is a popular task in\ncomputational social science that is used to study how authors select aspects\nof a topic to frame its reception. So far, such studies have been narrow, in\nthat they use a fixed set of pre-defined frames and focus only on the text,\nignoring the visual contexts in which those texts appear. Especially for\nframing in the news, this leaves out valuable information about editorial\nchoices, which include not just the written article but also accompanying\nphotographs. To overcome such limitations, we present a method for conducting\nmulti-modal, multi-label framing analysis at scale using large\n(vision-)language models. Grounding our work in framing theory, we extract\nlatent meaning embedded in images used to convey a certain point and contrast\nthat to the text by comparing the respective frames used. We also identify\nhighly partisan framing of topics with issue-specific frame analysis found in\nprior qualitative work. We demonstrate a method for doing scalable integrative\nframing analysis of both text and image in news, providing a more complete\npicture for understanding media bias.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20991", "pdf": "https://arxiv.org/pdf/2503.20991", "abs": "https://arxiv.org/abs/2503.20991", "authors": ["Tai D. Nguyen", "Matthew C. Stamm"], "title": "MVFNet: Multipurpose Video Forensics Network using Multiple Forms of Forensic Evidence", "categories": ["cs.CV"], "comment": "Proceedings of the Winter Conference on Applications of Computer\n  Vision (WACV) 2025", "summary": "While videos can be falsified in many different ways, most existing forensic\nnetworks are specialized to detect only a single manipulation type (e.g.\ndeepfake, inpainting). This poses a significant issue as the manipulation used\nto falsify a video is not known a priori. To address this problem, we propose\nMVFNet - a multipurpose video forensics network capable of detecting multiple\ntypes of manipulations including inpainting, deepfakes, splicing, and editing.\nOur network does this by extracting and jointly analyzing a broad set of\nforensic feature modalities that capture both spatial and temporal anomalies in\nfalsified videos. To reliably detect and localize fake content of all shapes\nand sizes, our network employs a novel Multi-Scale Hierarchical Transformer\nmodule to identify forensic inconsistencies across multiple spatial scales.\nExperimental results show that our network obtains state-of-the-art performance\nin general scenarios where multiple different manipulations are possible, and\nrivals specialized detectors in targeted scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20988", "pdf": "https://arxiv.org/pdf/2503.20988", "abs": "https://arxiv.org/abs/2503.20988", "authors": ["Hannah Kim", "Sofia Martinez", "Jason Lee"], "title": "Cross-Modal State-Space Graph Reasoning for Structured Summarization", "categories": ["cs.CL", "cs.GR"], "comment": null, "summary": "The ability to extract compact, meaningful summaries from large-scale and\nmultimodal data is critical for numerous applications, ranging from video\nanalytics to medical reports. Prior methods in cross-modal summarization have\noften suffered from high computational overheads and limited interpretability.\nIn this paper, we propose a \\textit{Cross-Modal State-Space Graph Reasoning}\n(\\textbf{CSS-GR}) framework that incorporates a state-space model with\ngraph-based message passing, inspired by prior work on efficient state-space\nmodels. Unlike existing approaches relying on purely sequential models, our\nmethod constructs a graph that captures inter- and intra-modal relationships,\nallowing more holistic reasoning over both textual and visual streams. We\ndemonstrate that our approach significantly improves summarization quality and\ninterpretability while maintaining computational efficiency, as validated on\nstandard multimodal summarization benchmarks. We also provide a thorough\nablation study to highlight the contributions of each component.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21056", "pdf": "https://arxiv.org/pdf/2503.21056", "abs": "https://arxiv.org/abs/2503.21056", "authors": ["Yiqing Shen", "Bohan Liu", "Chenjia Li", "Lalithkumar Seenivasan", "Mathias Unberath"], "title": "Online Reasoning Video Segmentation with Just-in-Time Digital Twins", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Reasoning segmentation (RS) aims to identify and segment objects of interest\nbased on implicit text queries. As such, RS is a catalyst for embodied AI\nagents, enabling them to interpret high-level commands without requiring\nexplicit step-by-step guidance. However, current RS approaches rely heavily on\nthe visual perception capabilities of multimodal large language models (LLMs),\nleading to several major limitations. First, they struggle with queries that\nrequire multiple steps of reasoning or those that involve complex\nspatial/temporal relationships. Second, they necessitate LLM fine-tuning, which\nmay require frequent updates to maintain compatibility with contemporary LLMs\nand may increase risks of catastrophic forgetting during fine-tuning. Finally,\nbeing primarily designed for static images or offline video processing, they\nscale poorly to online video data. To address these limitations, we propose an\nagent framework that disentangles perception and reasoning for online video RS\nwithout LLM fine-tuning. Our innovation is the introduction of a just-in-time\ndigital twin concept, where -- given an implicit query -- a LLM plans the\nconstruction of a low-level scene representation from high-level video using\nspecialist vision models. We refer to this approach to creating a digital twin\nas \"just-in-time\" because the LLM planner will anticipate the need for specific\ninformation and only request this limited subset instead of always evaluating\nevery specialist model. The LLM then performs reasoning on this digital twin\nrepresentation to identify target objects. To evaluate our approach, we\nintroduce a new comprehensive video reasoning segmentation benchmark comprising\n200 videos with 895 implicit text queries. The benchmark spans three reasoning\ncategories (semantic, spatial, and temporal) with three different reasoning\nchain complexity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21061", "pdf": "https://arxiv.org/pdf/2503.21061", "abs": "https://arxiv.org/abs/2503.21061", "authors": ["Mehraveh Javan Roshtkhari", "Matthew Toews", "Marco Pedersoli"], "title": "Neural Architecture Search by Learning a Hierarchical Search Space", "categories": ["cs.CV"], "comment": null, "summary": "Monte-Carlo Tree Search (MCTS) is a powerful tool for many non-differentiable\nsearch related problems such as adversarial games. However, the performance of\nsuch approach highly depends on the order of the nodes that are considered at\neach branching of the tree. If the first branches cannot distinguish between\npromising and deceiving configurations for the final task, the efficiency of\nthe search is exponentially reduced. In Neural Architecture Search (NAS), as\nonly the final architecture matters, the visiting order of the branching can be\noptimized to improve learning. In this paper, we study the application of MCTS\nto NAS for image classification. We analyze several sampling methods and\nbranching alternatives for MCTS and propose to learn the branching by\nhierarchical clustering of architectures based on their similarity. The\nsimilarity is measured by the pairwise distance of output vectors of\narchitectures. Extensive experiments on two challenging benchmarks on CIFAR10\nand ImageNet show that MCTS, if provided with a good branching hierarchy, can\nyield promising solutions more efficiently than other approaches for NAS\nproblems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["MCTS"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21082", "pdf": "https://arxiv.org/pdf/2503.21082", "abs": "https://arxiv.org/abs/2503.21082", "authors": ["Jinjie Mai", "Wenxuan Zhu", "Haozhe Liu", "Bing Li", "Cheng Zheng", "JÃ¼rgen Schmidhuber", "Bernard Ghanem"], "title": "Can Video Diffusion Model Reconstruct 4D Geometry?", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing dynamic 3D scenes (i.e., 4D geometry) from monocular video is\nan important yet challenging problem. Conventional multiview geometry-based\napproaches often struggle with dynamic motion, whereas recent learning-based\nmethods either require specialized 4D representation or sophisticated\noptimization. In this paper, we present Sora3R, a novel framework that taps\ninto the rich spatiotemporal priors of large-scale video diffusion models to\ndirectly infer 4D pointmaps from casual videos. Sora3R follows a two-stage\npipeline: (1) we adapt a pointmap VAE from a pretrained video VAE, ensuring\ncompatibility between the geometry and video latent spaces; (2) we finetune a\ndiffusion backbone in combined video and pointmap latent space to generate\ncoherent 4D pointmaps for every frame. Sora3R operates in a fully feedforward\nmanner, requiring no external modules (e.g., depth, optical flow, or\nsegmentation) or iterative global alignment. Extensive experiments demonstrate\nthat Sora3R reliably recovers both camera poses and detailed scene geometry,\nachieving performance on par with state-of-the-art methods for dynamic 4D\nreconstruction across diverse scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21248", "pdf": "https://arxiv.org/pdf/2503.21248", "abs": "https://arxiv.org/abs/2503.21248", "authors": ["Yujie Liu", "Zonglin Yang", "Tong Xie", "Jinjie Ni", "Ben Gao", "Yuqiang Li", "Shixiang Tang", "Wanli Ouyang", "Erik Cambria", "Dongzhan Zhou"], "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition", "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21140", "pdf": "https://arxiv.org/pdf/2503.21140", "abs": "https://arxiv.org/abs/2503.21140", "authors": ["Junjie Chen", "Weilong Chen", "Yifan Zuo", "Yuming Fang"], "title": "Recurrent Feature Mining and Keypoint Mixup Padding for Category-Agnostic Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Category-agnostic pose estimation aims to locate keypoints on query images\naccording to a few annotated support images for arbitrary novel classes.\nExisting methods generally extract support features via heatmap pooling, and\nobtain interacted features from support and query via cross-attention. Hence,\nthese works neglect to mine fine-grained and structure-aware (FGSA) features\nfrom both support and query images, which are crucial for pixel-level keypoint\nlocalization. To this end, we propose a novel yet concise framework, which\nrecurrently mines FGSA features from both support and query images.\nSpecifically, we design a FGSA mining module based on deformable attention\nmechanism. On the one hand, we mine fine-grained features by applying\ndeformable attention head over multi-scale feature maps. On the other hand, we\nmine structure-aware features by offsetting the reference points of keypoints\nto their linked keypoints. By means of above module, we recurrently mine FGSA\nfeatures from support and query images, and thus obtain better support features\nand query estimations. In addition, we propose to use mixup keypoints to pad\nvarious classes to a unified keypoint number, which could provide richer\nsupervision than the zero padding used in existing works. We conduct extensive\nexperiments and in-depth studies on large-scale MP-100 dataset, and outperform\nSOTA method dramatically (+3.2\\%PCK@0.05). Code is avaiable at\nhttps://github.com/chenbys/FMMP.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21332", "pdf": "https://arxiv.org/pdf/2503.21332", "abs": "https://arxiv.org/abs/2503.21332", "authors": ["Taewon Yun", "Jihwan Oh", "Hyangsuk Min", "Yuho Lee", "Jihwan Bang", "Jason Cai", "Hwanjun Song"], "title": "ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Summarization refinement faces challenges when extending to multi-dimension.\nIn this paper, we introduce ReFeed, a powerful summarization refinement\npipeline that enhances multiple dimensions through reflective reasoning on\nfeedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based\ndataset optimized for training a lightweight model with reflective reasoning.\nOur experiments reveal how the number of dimensions, feedback exposure, and\nreasoning policy influence refinement performance, highlighting reflective\nreasoning and simultaneously addressing multiple feedback is crucial to\nmitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy\nfeedback and feedback order. Lastly, our finding emphasizes that creating data\nwith a proper goal and guideline constitutes a fundamental pillar of effective\nreasoning. The dataset and model will be released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "summarization", "multi-dimensional", "dimension"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21150", "pdf": "https://arxiv.org/pdf/2503.21150", "abs": "https://arxiv.org/abs/2503.21150", "authors": ["Yuhan Liu", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "title": "The Devil is in Low-Level Features for Cross-Domain Few-Shot Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Cross-Domain Few-Shot Segmentation (CDFSS) is proposed to transfer the\npixel-level segmentation capabilities learned from large-scale source-domain\ndatasets to downstream target-domain datasets, with only a few annotated images\nper class. In this paper, we focus on a well-observed but unresolved phenomenon\nin CDFSS: for target domains, particularly those distant from the source\ndomain, segmentation performance peaks at the very early epochs, and declines\nsharply as the source-domain training proceeds. We delve into this phenomenon\nfor an interpretation: low-level features are vulnerable to domain shifts,\nleading to sharper loss landscapes during the source-domain training, which is\nthe devil of CDFSS. Based on this phenomenon and interpretation, we further\npropose a method that includes two plug-and-play modules: one to flatten the\nloss landscapes for low-level features during source-domain training as a novel\nsharpness-aware minimization method, and the other to directly supplement\ntarget-domain information to the model during target-domain testing by\nlow-level-based calibration. Extensive experiments on four target datasets\nvalidate our rationale and demonstrate that our method surpasses the\nstate-of-the-art method in CDFSS signifcantly by 3.71% and 5.34% average MIoU\nin 1-shot and 5-shot scenarios, respectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21187", "pdf": "https://arxiv.org/pdf/2503.21187", "abs": "https://arxiv.org/abs/2503.21187", "authors": ["Yimin Xu"], "title": "DGSUnet: An Improved Unet Model with DINO-Guided SAM2 for Multi-Scale Feature Collaboration", "categories": ["cs.CV"], "comment": null, "summary": "Despite the significant advancements in general image segmentation achieved\nby large-scale pre-trained foundation models (such as Meta's Segment Any-thing\nModel (SAM) series and DINOv2), their performance in specialized fields remains\nlimited by two critical issues: the excessive training costs due to large model\nparameters, and the insufficient ability to represent specific domain\ncharacteristics. This paper proposes a multi-scale feature collabora-tion\nframework guided by DINOv2 for SAM2, with core innovations in three aspects:\n(1) Establishing a feature collaboration mechanism between DINOv2 and SAM2\nbackbones, where high-dimensional semantic features extracted by the\nself-supervised model guide multi-scale feature fusion; (2) Designing\nlightweight adapter modules and cross-modal, cross-layer feature fusion units\nto inject cross-domain knowledge while freezing the base model parameters; (3)\nConstructing a U-shaped network structure based on U-net, which utilizes\nattention mechanisms to achieve adaptive aggregation decoding of\nmulti-granularity features. This framework surpasses existing state-of-the-art\nmeth-ods in downstream tasks such as camouflage target detection and salient\nob-ject detection, without requiring costly training processes. It provides a\ntech-nical pathway for efficient deployment of visual image segmentation,\ndemon-strating significant application value in a wide range of downstream\ntasks and specialized fields within image segmentation.Project page:\nhttps://github.com/CheneyXuYiMin/SAM2DINO-Seg", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21210", "pdf": "https://arxiv.org/pdf/2503.21210", "abs": "https://arxiv.org/abs/2503.21210", "authors": ["Yueying Gao", "Dongliang Chang", "Bingyao Yu", "Haotian Qin", "Lei Chen", "Kongming Liang", "Zhanyu Ma"], "title": "FakeReasoning: Towards Generalizable Forgery Detection and Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Accurate and interpretable detection of AI-generated images is essential for\nmitigating risks associated with AI misuse. However, the substantial domain gap\namong generative models makes it challenging to develop a generalizable forgery\ndetection model. Moreover, since every pixel in an AI-generated image is\nsynthesized, traditional saliency-based forgery explanation methods are not\nwell suited for this task. To address these challenges, we propose modeling\nAI-generated image detection and explanation as a Forgery Detection and\nReasoning task (FDR-Task), leveraging vision-language models (VLMs) to provide\naccurate detection through structured and reliable reasoning over forgery\nattributes. To facilitate this task, we introduce the Multi-Modal Forgery\nReasoning dataset (MMFR-Dataset), a large-scale dataset containing 100K images\nacross 10 generative models, with 10 types of forgery reasoning annotations,\nenabling comprehensive evaluation of FDR-Task. Additionally, we propose\nFakeReasoning, a forgery detection and reasoning framework with two key\ncomponents. First, Forgery-Aligned Contrastive Learning enhances VLMs'\nunderstanding of forgery-related semantics through both cross-modal and\nintra-modal contrastive learning between images and forgery attribute\nreasoning. Second, a Classification Probability Mapper bridges the optimization\ngap between forgery detection and language modeling by mapping the output\nlogits of VLMs to calibrated binary classification probabilities. Experiments\nacross multiple generative models demonstrate that FakeReasoning not only\nachieves robust generalization but also outperforms state-of-the-art methods on\nboth detection and reasoning tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21505", "pdf": "https://arxiv.org/pdf/2503.21505", "abs": "https://arxiv.org/abs/2503.21505", "authors": ["Yue Li", "Meng Tian", "Zhenyu Lin", "Jiangtong Zhu", "Dechang Zhu", "Haiqiang Liu", "Zining Wang", "Yueyi Zhang", "Zhiwei Xiong", "Xinhai Zhao"], "title": "Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Existing benchmarks for Vision-Language Model (VLM) on autonomous driving\n(AD) primarily assess interpretability through open-form visual question\nanswering (QA) within coarse-grained tasks, which remain insufficient to assess\ncapabilities in complex driving scenarios. To this end, we introduce\n$\\textbf{VLADBench}$, a challenging and fine-grained dataset featuring\nclose-form QAs that progress from static foundational knowledge and elements to\nadvanced reasoning for dynamic on-road situations. The elaborate\n$\\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,\nGeneral Element Recognition, Traffic Graph Generation, Target Attribute\nComprehension, and Ego Decision-Making and Planning. These domains are further\nbroken down into 11 secondary aspects and 29 tertiary tasks for a granular\nevaluation. A thorough assessment of general and domain-specific (DS) VLMs on\nthis benchmark reveals both their strengths and critical limitations in AD\ncontexts. To further exploit the cognitive and reasoning interactions among the\n5 domains for AD understanding, we start from a small-scale VLM and train the\nDS models on individual domain datasets (collected from 1.4M DS QAs across\npublic sources). The experimental results demonstrate that the proposed\nbenchmark provides a crucial step toward a more comprehensive assessment of\nVLMs in AD, paving the way for the development of more cognitively\nsophisticated and reasoning-capable AD systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21236", "pdf": "https://arxiv.org/pdf/2503.21236", "abs": "https://arxiv.org/abs/2503.21236", "authors": ["Shuai Li", "Jie Zhang", "Yuang Qi", "Kejiang Chen", "Tianwei Zhang", "Weiming Zhang", "Nenghai Yu"], "title": "Clean Image May be Dangerous: Data Poisoning Attacks Against Deep Hashing", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by TMM", "summary": "Large-scale image retrieval using deep hashing has become increasingly\npopular due to the exponential growth of image data and the remarkable feature\nextraction capabilities of deep neural networks (DNNs). However, deep hashing\nmethods are vulnerable to malicious attacks, including adversarial and backdoor\nattacks. It is worth noting that these attacks typically involve altering the\nquery images, which is not a practical concern in real-world scenarios. In this\npaper, we point out that even clean query images can be dangerous, inducing\nmalicious target retrieval results, like undesired or illegal images. To the\nbest of our knowledge, we are the first to study data \\textbf{p}oisoning\n\\textbf{a}ttacks against \\textbf{d}eep \\textbf{hash}ing\n\\textbf{(\\textit{PADHASH})}. Specifically, we first train a surrogate model to\nsimulate the behavior of the target deep hashing model. Then, a strict gradient\nmatching strategy is proposed to generate the poisoned images. Extensive\nexperiments on different models, datasets, hash methods, and hash code lengths\ndemonstrate the effectiveness and generality of our attack method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21670", "pdf": "https://arxiv.org/pdf/2503.21670", "abs": "https://arxiv.org/abs/2503.21670", "authors": ["Rajvee Sheth", "Himanshu Beniwal", "Mayank Singh"], "title": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid growth of digital communication has driven the widespread use of\ncode-mixing, particularly Hindi-English, in multilingual communities. Existing\ndatasets often focus on romanized text, have limited scope, or rely on\nsynthetic data, which fails to capture realworld language nuances. Human\nannotations are crucial for assessing the naturalness and acceptability of\ncode-mixed text. To address these challenges, We introduce COMI-LINGUA, the\nlargest manually annotated dataset for code-mixed text, comprising 100,970\ninstances evaluated by three expert annotators in both Devanagari and Roman\nscripts. The dataset supports five fundamental NLP tasks: Language\nIdentification, Matrix Language Identification, Part-of-Speech Tagging, Named\nEntity Recognition, and Translation. We evaluate LLMs on these tasks using\nCOMILINGUA, revealing limitations in current multilingual modeling strategies\nand emphasizing the need for improved code-mixed text processing capabilities.\nCOMI-LINGUA is publically availabe at:\nhttps://huggingface.co/datasets/LingoIITGN/COMI-LINGUA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21268", "pdf": "https://arxiv.org/pdf/2503.21268", "abs": "https://arxiv.org/abs/2503.21268", "authors": ["Ming Yan", "Xincheng Lin", "Yuhua Luo", "Shuqi Fan", "Yudi Dai", "Qixin Zhong", "Lincai Zhong", "Yuexin Ma", "Lan Xu", "Chenglu Wen", "Siqi Shen", "Cheng Wang"], "title": "ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate", "categories": ["cs.CV"], "comment": "CVPR2025, project in \\href{this\n  link}{http://www.lidarhumanmotion.net/climbingcap/}", "summary": "Human Motion Recovery (HMR) research mainly focuses on ground-based motions\nsuch as running. The study on capturing climbing motion, an off-ground motion,\nis sparse. This is partly due to the limited availability of climbing motion\ndatasets, especially large-scale and challenging 3D labeled datasets. To\naddress the insufficiency of climbing motion datasets, we collect AscendMotion,\na large-scale well-annotated, and challenging climbing motion dataset. It\nconsists of 412k RGB, LiDAR frames, and IMU measurements, including the\nchallenging climbing motions of 22 skilled climbing coaches across 12 different\nrock walls. Capturing the climbing motions is challenging as it requires\nprecise recovery of not only the complex pose but also the global position of\nclimbers. Although multiple global HMR methods have been proposed, they cannot\nfaithfully capture climbing motions. To address the limitations of HMR methods\nfor climbing, we propose ClimbingCap, a motion recovery method that\nreconstructs continuous 3D human climbing motion in a global coordinate system.\nOne key insight is to use the RGB and LiDAR modalities to separately\nreconstruct motions in camera coordinates and global coordinates and to\noptimize them jointly. We demonstrate the quality of the AscendMotion dataset\nand present promising results from ClimbingCap. The AscendMotion dataset and\nsource code release publicly at \\href{this\nlink}{http://www.lidarhumanmotion.net/climbingcap/}", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21284", "pdf": "https://arxiv.org/pdf/2503.21284", "abs": "https://arxiv.org/abs/2503.21284", "authors": ["Hanyue Tu", "Siqi Wu", "Li Li", "Wengang Zhou", "Houqiang Li"], "title": "Multi-Scale Invertible Neural Network for Wide-Range Variable-Rate Learned Image Compression", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to IEEE Transactions on Multimedia 2025", "summary": "Autoencoder-based structures have dominated recent learned image compression\nmethods. However, the inherent information loss associated with autoencoders\nlimits their rate-distortion performance at high bit rates and restricts their\nflexibility of rate adaptation. In this paper, we present a variable-rate image\ncompression model based on invertible transform to overcome these limitations.\nSpecifically, we design a lightweight multi-scale invertible neural network,\nwhich bijectively maps the input image into multi-scale latent representations.\nTo improve the compression efficiency, a multi-scale spatial-channel context\nmodel with extended gain units is devised to estimate the entropy of the latent\nrepresentation from high to low levels. Experimental results demonstrate that\nthe proposed method achieves state-of-the-art performance compared to existing\nvariable-rate methods, and remains competitive with recent multi-model\napproaches. Notably, our method is the first learned image compression solution\nthat outperforms VVC across a very wide range of bit rates using a single\nmodel, especially at high bit rates.The source code is available at\n\\href{https://github.com/hytu99/MSINN-VRLIC}{https://github.com/hytu99/MSINN-VRLIC}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21729", "pdf": "https://arxiv.org/pdf/2503.21729", "abs": "https://arxiv.org/abs/2503.21729", "authors": ["Zhicheng Lee", "Shulin Cao", "Jinxin Liu", "Jiajie Zhang", "Weichuan Liu", "Xiaoyin Che", "Lei Hou", "Juanzi Li"], "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21364", "pdf": "https://arxiv.org/pdf/2503.21364", "abs": "https://arxiv.org/abs/2503.21364", "authors": ["Zhenxiang Ma", "Zhenyu Yang", "Miao Tao", "Yuanzhen Zhou", "Zeyu He", "Yuchang Zhang", "Rong Fu", "Hengjie Li"], "title": "LandMarkSystem Technical Report", "categories": ["cs.CV"], "comment": null, "summary": "3D reconstruction is vital for applications in autonomous driving, virtual\nreality, augmented reality, and the metaverse. Recent advancements such as\nNeural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformed\nthe field, yet traditional deep learning frameworks struggle to meet the\nincreasing demands for scene quality and scale. This paper introduces\nLandMarkSystem, a novel computing framework designed to enhance multi-scale\nscene reconstruction and rendering. By leveraging a componentized model\nadaptation layer, LandMarkSystem supports various NeRF and 3DGS structures\nwhile optimizing computational efficiency through distributed parallel\ncomputing and model parameter offloading. Our system addresses the limitations\nof existing frameworks, providing dedicated operators for complex 3D sparse\ncomputations, thus facilitating efficient training and rapid inference over\nextensive scenes. Key contributions include a modular architecture, a dynamic\nloading strategy for limited resources, and proven capabilities across multiple\nrepresentative algorithms.This comprehensive solution aims to advance the\nefficiency and effectiveness of 3D reconstruction tasks.To facilitate further\nresearch and collaboration, the source code and documentation for the\nLandMarkSystem project are publicly available in an open-source repository,\naccessing the repository at: https://github.com/InternLandMark/LandMarkSystem.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21408", "pdf": "https://arxiv.org/pdf/2503.21408", "abs": "https://arxiv.org/abs/2503.21408", "authors": ["Marshall Thomas", "Edward Fish", "Richard Bowden"], "title": "VALLR: Visual ASR Language Model for Lip Reading", "categories": ["cs.CV"], "comment": null, "summary": "Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex\ntask requiring the interpretation of spoken language exclusively from visual\ncues, primarily lip movements and facial expressions. This task is especially\nchallenging due to the absence of auditory information and the inherent\nambiguity when visually distinguishing phonemes that have overlapping visemes\nwhere different phonemes appear identical on the lips. Current methods\ntypically attempt to predict words or characters directly from these visual\ncues, but this approach frequently encounters high error rates due to\ncoarticulation effects and viseme ambiguity. We propose a novel two-stage,\nphoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that\naddresses these longstanding challenges. First, our model predicts a compact\nsequence of phonemes from visual inputs using a Video Transformer with a CTC\nhead, thereby reducing the task complexity and achieving robust speaker\ninvariance. This phoneme output then serves as the input to a fine-tuned Large\nLanguage Model (LLM), which reconstructs coherent words and sentences by\nleveraging broader linguistic context. Unlike existing methods that either\npredict words directly-often faltering on visually similar phonemes-or rely on\nlarge-scale multimodal pre-training, our approach explicitly encodes\nintermediate linguistic structure while remaining highly data efficient. We\ndemonstrate state-of-the-art performance on two challenging datasets, LRS2 and\nLRS3, where our method achieves significant reductions in Word Error Rate (WER)\nachieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data\nthan the next best approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21438", "pdf": "https://arxiv.org/pdf/2503.21438", "abs": "https://arxiv.org/abs/2503.21438", "authors": ["Anis Ur Rahman", "Einari Heinaro", "Mete Ahishali", "Samuli Junttila"], "title": "Dual-Task Learning for Dead Tree Detection and Segmentation with Hybrid Self-Attention U-Nets in Aerial Imagery", "categories": ["cs.CV"], "comment": "11 pages, 4 figures, 4 tables", "summary": "Mapping standing dead trees is critical for assessing forest health,\nmonitoring biodiversity, and mitigating wildfire risks, for which aerial\nimagery has proven useful. However, dense canopy structures, spectral overlaps\nbetween living and dead vegetation, and over-segmentation errors limit the\nreliability of existing methods. This study introduces a hybrid postprocessing\nframework that refines deep learning-based tree segmentation by integrating\nwatershed algorithms with adaptive filtering, enhancing boundary delineation,\nand reducing false positives in complex forest environments. Tested on\nhigh-resolution aerial imagery from boreal forests, the framework improved\ninstance-level segmentation accuracy by 41.5% and reduced positional errors by\n57%, demonstrating robust performance in densely vegetated regions. By\nbalancing detection accuracy and over-segmentation artifacts, the method\nenabled the precise identification of individual dead trees, which is critical\nfor ecological monitoring. The framework's computational efficiency supports\nscalable applications, such as wall-to-wall tree mortality mapping over large\ngeographic regions using aerial or satellite imagery. These capabilities\ndirectly benefit wildfire risk assessment (identifying fuel accumulations),\ncarbon stock estimation (tracking emissions from decaying biomass), and\nprecision forestry (targeting salvage loggings). By bridging advanced remote\nsensing techniques with practical forest management needs, this work advances\ntools for large-scale ecological conservation and climate resilience planning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20992", "pdf": "https://arxiv.org/pdf/2503.20992", "abs": "https://arxiv.org/abs/2503.20992", "authors": ["Michael Brown", "Sofia Martinez", "Priya Singh"], "title": "ReverBERT: A State Space Model for Efficient Text-Driven Speech Style Transfer", "categories": ["cs.GR", "cs.CL"], "comment": null, "summary": "Text-driven speech style transfer aims to mold the intonation, pace, and\ntimbre of a spoken utterance to match stylistic cues from text descriptions.\nWhile existing methods leverage large-scale neural architectures or pre-trained\nlanguage models, the computational costs often remain high. In this paper, we\npresent \\emph{ReverBERT}, an efficient framework for text-driven speech style\ntransfer that draws inspiration from a state space model (SSM) paradigm,\nloosely motivated by the image-based method of Wang and\nLiu~\\cite{wang2024stylemamba}. Unlike image domain techniques, our method\noperates in the speech space and integrates a discrete Fourier transform of\nlatent speech features to enable smooth and continuous style modulation. We\nalso propose a novel \\emph{Transformer-based SSM} layer for bridging textual\nstyle descriptors with acoustic attributes, dramatically reducing inference\ntime while preserving high-quality speech characteristics. Extensive\nexperiments on benchmark speech corpora demonstrate that \\emph{ReverBERT}\nsignificantly outperforms baselines in terms of naturalness, expressiveness,\nand computational efficiency. We release our model and code publicly to foster\nfurther research in text-driven speech style transfer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21449", "pdf": "https://arxiv.org/pdf/2503.21449", "abs": "https://arxiv.org/abs/2503.21449", "authors": ["Lucas Nunes", "Rodrigo Marcuzzi", "Jens Behley", "Cyrill Stachniss"], "title": "Towards Generating Realistic 3D Semantic Training Data for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Semantic scene understanding is crucial for robotics and computer vision\napplications. In autonomous driving, 3D semantic segmentation plays an\nimportant role for enabling safe navigation. Despite significant advances in\nthe field, the complexity of collecting and annotating 3D data is a bottleneck\nin this developments. To overcome that data annotation limitation, synthetic\nsimulated data has been used to generate annotated data on demand. There is\nstill however a domain gap between real and simulated data. More recently,\ndiffusion models have been in the spotlight, enabling close-to-real data\nsynthesis. Those generative models have been recently applied to the 3D data\ndomain for generating scene-scale data with semantic annotations. Still, those\nmethods either rely on image projection or decoupled models trained with\ndifferent resolutions in a coarse-to-fine manner. Such intermediary\nrepresentations impact the generated data quality due to errors added in those\ntransformations. In this work, we propose a novel approach able to generate 3D\nsemantic scene-scale data without relying on any projection or decoupled\ntrained multi-resolution models, achieving more realistic semantic scene data\ngeneration compared to previous state-of-the-art methods. Besides improving 3D\nsemantic scene-scale data synthesis, we thoroughly evaluate the use of the\nsynthetic scene samples as labeled data to train a semantic segmentation\nnetwork. In our experiments, we show that using the synthetic annotated data\ngenerated by our method as training data together with the real semantic\nsegmentation labels, leads to an improvement in the semantic segmentation model\nperformance. Our results show the potential of generated scene-scale point\nclouds to generate more training data to extend existing datasets, reducing the\ndata annotation effort. Our code is available at\nhttps://github.com/PRBonn/3DiSS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21459", "pdf": "https://arxiv.org/pdf/2503.21459", "abs": "https://arxiv.org/abs/2503.21459", "authors": ["Chirag Parikh", "Deepti Rawat", "Rakshitha R. T.", "Tathagata Ghosh", "Ravi Kiran Sarvadevabhatla"], "title": "RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event Understanding from Social Video Narratives", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025; Project Page: https://roadsocial.github.io/", "summary": "We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for\ngeneric road event understanding from social media narratives. Unlike existing\ndatasets limited by regional bias, viewpoint bias and expert-driven\nannotations, RoadSocial captures the global complexity of road events with\nvaried geographies, camera viewpoints (CCTV, handheld, drones) and rich social\ndiscourse. Our scalable semi-automatic annotation framework leverages Text LLMs\nand Video LLMs to generate comprehensive question-answer pairs across 12\nchallenging QA tasks, pushing the boundaries of road event understanding.\nRoadSocial is derived from social media videos spanning 14M frames and 414K\nsocial comments, resulting in a dataset with 13.2K videos, 674 tags and 260K\nhigh-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary,\ndriving-specific and general-purpose) on our road event understanding\nbenchmark. We also demonstrate RoadSocial's utility in improving road event\nunderstanding capabilities of general-purpose Video LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21483", "pdf": "https://arxiv.org/pdf/2503.21483", "abs": "https://arxiv.org/abs/2503.21483", "authors": ["Shuming Liu", "Chen Zhao", "Tianqi Xu", "Bernard Ghanem"], "title": "BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Large video-language models (VLMs) have demonstrated promising progress in\nvarious video understanding tasks. However, their effectiveness in long-form\nvideo analysis is constrained by limited context windows. Traditional\napproaches, such as uniform frame sampling, often inevitably allocate resources\nto irrelevant content, diminishing their effectiveness in real-world scenarios.\nIn this paper, we introduce BOLT, a method to BOost Large VLMs without\nadditional Training through a comprehensive study of frame selection\nstrategies. First, to enable a more realistic evaluation of VLMs in long-form\nvideo understanding, we propose a multi-source retrieval evaluation setting.\nOur findings reveal that uniform sampling performs poorly in noisy contexts,\nunderscoring the importance of selecting the right frames. Second, we explore\nseveral frame selection strategies based on query-frame similarity and analyze\ntheir effectiveness at inference time. Our results show that inverse transform\nsampling yields the most significant performance improvement, increasing\naccuracy on the Video-MME benchmark from 53.8% to 56.1% and MLVU benchmark from\n58.9% to 63.4%. Our code is available at https://github.com/sming256/BOLT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21566", "pdf": "https://arxiv.org/pdf/2503.21566", "abs": "https://arxiv.org/abs/2503.21566", "authors": ["Tongchao Luo", "Mingquan Qiu", "Zhenyu Wu", "Zebo Zhao", "Dingyou Zhang"], "title": "Bearing fault diagnosis based on multi-scale spectral images and convolutional neural network", "categories": ["cs.CV"], "comment": "12pages, 10 figures and 8 tables", "summary": "To address the challenges of low diagnostic accuracy in traditional bearing\nfault diagnosis methods, this paper proposes a novel fault diagnosis approach\nbased on multi-scale spectrum feature images and deep learning. Firstly, the\nvibration signal are preprocessed through mean removal and then converted to\nmulti-length spectrum with fast Fourier transforms (FFT). Secondly, a novel\nfeature called multi-scale spectral image (MSSI) is constructed by multi-length\nspectrum paving scheme. Finally, a deep learning framework, convolutional\nneural network (CNN), is formulated to diagnose the bearing faults. Two\nexperimental cases are utilized to verify the effectiveness of the proposed\nmethod. Experimental results demonstrate that the proposed method significantly\nimproves the accuracy of fault diagnosis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21745", "pdf": "https://arxiv.org/pdf/2503.21745", "abs": "https://arxiv.org/abs/2503.21745", "authors": ["Yuhan Zhang", "Mengchen Zhang", "Tong Wu", "Tengfei Wang", "Gordon Wetzstein", "Dahua Lin", "Ziwei Liu"], "title": "3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "3D generation is experiencing rapid advancements, while the development of 3D\nevaluation has not kept pace. How to keep automatic evaluation equitably\naligned with human perception has become a well-recognized challenge. Recent\nadvances in the field of language and image generation have explored human\npreferences and showcased respectable fitting ability. However, the 3D domain\nstill lacks such a comprehensive preference dataset over generative models. To\nmitigate this absence, we develop 3DGen-Arena, an integrated platform in a\nbattle manner. Then, we carefully design diverse text and image prompts and\nleverage the arena platform to gather human preferences from both public users\nand expert annotators, resulting in a large-scale multi-dimension human\npreference dataset 3DGen-Bench. Using this dataset, we further train a\nCLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator,\n3DGen-Eval. These two models innovatively unify the quality evaluation of\ntext-to-3D and image-to-3D generation, and jointly form our automated\nevaluation system with their respective strengths. Extensive experiments\ndemonstrate the efficacy of our scoring model in predicting human preferences,\nexhibiting a superior correlation with human ranks compared to existing\nmetrics. We believe that our 3DGen-Bench dataset and automated evaluation\nsystem will foster a more equitable evaluation in the field of 3D generation,\nfurther promoting the development of 3D generative models and their downstream\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "preference dataset", "correlation", "dimension"], "score": 6}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21771", "pdf": "https://arxiv.org/pdf/2503.21771", "abs": "https://arxiv.org/abs/2503.21771", "authors": ["Hongkai Lin", "Dingkang Liang", "Zhenghao Qi", "Xiang Bai"], "title": "A Unified Image-Dense Annotation Generation Model for Underwater Scenes", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. The code is available at https:\n  //github.com/HongkLin/TIDE", "summary": "Underwater dense prediction, especially depth estimation and semantic\nsegmentation, is crucial for gaining a comprehensive understanding of\nunderwater scenes. Nevertheless, high-quality and large-scale underwater\ndatasets with dense annotations remain scarce because of the complex\nenvironment and the exorbitant data collection costs. This paper proposes a\nunified Text-to-Image and DEnse annotation generation method (TIDE) for\nunderwater scenes. It relies solely on text as input to simultaneously generate\nrealistic underwater images and multiple highly consistent dense annotations.\nSpecifically, we unify the generation of text-to-image and text-to-dense\nannotations within a single model. The Implicit Layout Sharing mechanism (ILS)\nand cross-modal interaction method called Time Adaptive Normalization (TAN) are\nintroduced to jointly optimize the consistency between image and dense\nannotations. We synthesize a large-scale underwater dataset using TIDE to\nvalidate the effectiveness of our method in underwater dense prediction tasks.\nThe results demonstrate that our method effectively improves the performance of\nexisting underwater dense prediction models and mitigates the scarcity of\nunderwater data with dense annotations. We hope our method can offer new\nperspectives on alleviating data scarcity issues in other fields. The code is\navailable at https: //github.com/HongkLin/TIDE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "consistency"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21780", "pdf": "https://arxiv.org/pdf/2503.21780", "abs": "https://arxiv.org/abs/2503.21780", "authors": ["Reza Qorbani", "Gianluca Villani", "Theodoros Panagiotakopoulos", "Marc Botet Colomer", "Linus HÃ¤renstam-Nielsen", "Mattia Segu", "Pier Luigi Dovesi", "Jussi Karlgren", "Daniel Cremers", "Federico Tombari", "Matteo Poggi"], "title": "Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": "CVPR 2025. Project page: https://thegoodailab.org/semla Code:\n  https://github.com/rezaqorbani/SemLA", "summary": "Open-vocabulary semantic segmentation models associate vision and text to\nlabel pixels from an undefined set of classes using textual queries, providing\nversatile performance on novel datasets. However, large shifts between training\nand test domains degrade their performance, requiring fine-tuning for effective\nreal-world applications. We introduce Semantic Library Adaptation (SemLA), a\nnovel framework for training-free, test-time domain adaptation. SemLA leverages\na library of LoRA-based adapters indexed with CLIP embeddings, dynamically\nmerging the most relevant adapters based on proximity to the target domain in\nthe embedding space. This approach constructs an ad-hoc model tailored to each\nspecific input without additional training. Our method scales efficiently,\nenhances explainability by tracking adapter contributions, and inherently\nprotects data privacy, making it ideal for sensitive applications.\nComprehensive experiments on a 20-domain benchmark built over 10 standard\ndatasets demonstrate SemLA's superior adaptability and performance across\ndiverse settings, establishing a new standard in domain adaptation for\nopen-vocabulary semantic segmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21242", "pdf": "https://arxiv.org/pdf/2503.21242", "abs": "https://arxiv.org/abs/2503.21242", "authors": ["Bashar Tahir", "Philipp Svoboda", "Markus Rupp"], "title": "PLAIN: Scalable Estimation Architecture for Integrated Sensing and Communication", "categories": ["eess.SP", "cs.CV"], "comment": "Submitted to the IEEE Transactions on Wireless Communications. Code\n  available at GitHub: https://github.com/bashar-tahir/plain", "summary": "Integrated sensing and communication (ISAC) is envisioned be to one of the\nparadigms upon which next-generation mobile networks will be built, extending\nlocalization and tracking capabilities, as well as giving birth to\nenvironment-aware wireless access. A key aspect of sensing integration is\nparameter estimation, which involves extracting information about the\nsurrounding environment, such as the direction, distance, and velocity of\nvarious objects within. This is typically of a high-dimensional nature, which\nleads to significant computational complexity, if performed jointly across\nmultiple sensing dimensions, such as space, frequency, and time. Additionally,\ndue to the incorporation of sensing on top of the data transmission, the time\nwindow available for sensing is likely to be short, resulting in an estimation\nproblem where only a single snapshot is accessible. In this work, we propose\nPLAIN, a tensor-based estimation architecture that flexibly scales with\nmultiple sensing dimensions and can handle high dimensionality, limited\nmeasurement time, and super-resolution requirements. It consists of three\nstages: a compression stage, where the high dimensional input is converted into\nlower dimensionality, without sacrificing resolution; a decoupled estimation\nstage, where the parameters across the different dimensions are estimated in\nparallel with low complexity; an input-based fusion stage, where the decoupled\nparameters are fused together to form a paired multidimensional estimate. We\ninvestigate the performance of the architecture for different configurations\nand compare it against practical sequential and joint estimation baselines, as\nwell as theoretical bounds. Our results show that PLAIN, using tools from\ntensor algebra, subspace-based processing, and compressed sensing, can scale\nflexibly with dimensionality, while operating with low complexity and\nmaintaining super-resolution.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21442", "pdf": "https://arxiv.org/pdf/2503.21442", "abs": "https://arxiv.org/abs/2503.21442", "authors": ["Qiyu Dai", "Xingyu Ni", "Qianfan Shen", "Wenzheng Chen", "Baoquan Chen", "Mengyu Chu"], "title": "RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We consider the problem of adding dynamic rain effects to in-the-wild scenes\nin a physically-correct manner. Recent advances in scene modeling have made\nsignificant progress, with NeRF and 3DGS techniques emerging as powerful tools\nfor reconstructing complex scenes. However, while effective for novel view\nsynthesis, these methods typically struggle with challenging scene editing\ntasks, such as physics-based rain simulation. In contrast, traditional\nphysics-based simulations can generate realistic rain effects, such as\nraindrops and splashes, but they often rely on skilled artists to carefully set\nup high-fidelity scenes. This process lacks flexibility and scalability,\nlimiting its applicability to broader, open-world environments. In this work,\nwe introduce RainyGS, a novel approach that leverages the strengths of both\nphysics-based modeling and 3DGS to generate photorealistic, dynamic rain\neffects in open-world scenes with physical accuracy. At the core of our method\nis the integration of physically-based raindrop and shallow water simulation\ntechniques within the fast 3DGS rendering framework, enabling realistic and\nefficient simulations of raindrop behavior, splashes, and reflections. Our\nmethod supports synthesizing rain effects at over 30 fps, offering users\nflexible control over rain intensity -- from light drizzles to heavy downpours.\nWe demonstrate that RainyGS performs effectively for both real-world outdoor\nscenes and large-scale driving scenarios, delivering more photorealistic and\nphysically-accurate rain effects compared to state-of-the-art methods. Project\npage can be found at https://pku-vcl-geometry.github.io/RainyGS/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21505", "pdf": "https://arxiv.org/pdf/2503.21505", "abs": "https://arxiv.org/abs/2503.21505", "authors": ["Yue Li", "Meng Tian", "Zhenyu Lin", "Jiangtong Zhu", "Dechang Zhu", "Haiqiang Liu", "Zining Wang", "Yueyi Zhang", "Zhiwei Xiong", "Xinhai Zhao"], "title": "Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Existing benchmarks for Vision-Language Model (VLM) on autonomous driving\n(AD) primarily assess interpretability through open-form visual question\nanswering (QA) within coarse-grained tasks, which remain insufficient to assess\ncapabilities in complex driving scenarios. To this end, we introduce\n$\\textbf{VLADBench}$, a challenging and fine-grained dataset featuring\nclose-form QAs that progress from static foundational knowledge and elements to\nadvanced reasoning for dynamic on-road situations. The elaborate\n$\\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,\nGeneral Element Recognition, Traffic Graph Generation, Target Attribute\nComprehension, and Ego Decision-Making and Planning. These domains are further\nbroken down into 11 secondary aspects and 29 tertiary tasks for a granular\nevaluation. A thorough assessment of general and domain-specific (DS) VLMs on\nthis benchmark reveals both their strengths and critical limitations in AD\ncontexts. To further exploit the cognitive and reasoning interactions among the\n5 domains for AD understanding, we start from a small-scale VLM and train the\nDS models on individual domain datasets (collected from 1.4M DS QAs across\npublic sources). The experimental results demonstrate that the proposed\nbenchmark provides a crucial step toward a more comprehensive assessment of\nVLMs in AD, paving the way for the development of more cognitively\nsophisticated and reasoning-capable AD systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21694", "pdf": "https://arxiv.org/pdf/2503.21694", "abs": "https://arxiv.org/abs/2503.21694", "authors": ["Zhiyuan Ma", "Xinyue Liang", "Rongyuan Wu", "Xiangyu Zhu", "Zhen Lei", "Lei Zhang"], "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Accepted to CVPR 2025.\n  Code:https://github.com/theEricMa/TriplaneTurbo.\n  Demo:https://huggingface.co/spaces/ZhiyuanthePony/TriplaneTurbo", "summary": "It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only $2.5\\%$\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21699", "pdf": "https://arxiv.org/pdf/2503.21699", "abs": "https://arxiv.org/abs/2503.21699", "authors": ["Liuyue Xie", "George Z. Wei", "Avik Kuthiala", "Ce Zheng", "Ananya Bal", "Mosam Dabhi", "Liting Wen", "Taru Rustagi", "Ethan Lai", "Sushil Khyalia", "Rohan Choudhury", "Morteza Ziyadi", "Xu Zhang", "Hao Yang", "LÃ¡szlÃ³ A. Jeni"], "title": "MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX", "categories": ["cs.SD", "cs.AI", "cs.CV"], "comment": null, "summary": "Frontier models have either been language-only or have primarily focused on\nvision and language modalities. Although recent advancements in models with\nvision and audio understanding capabilities have shown substantial progress,\nthe field lacks a standardized evaluation framework for thoroughly assessing\ntheir cross-modality perception performance. We introduce MAVERIX~(Multimodal\nAudio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and\n2,556 questions explicitly designed to evaluate multimodal models through tasks\nthat necessitate close integration of video and audio information. MAVERIX\nuniquely provides models with audiovisual tasks, closely mimicking the\nmultimodal perceptual experiences available to humans during inference and\ndecision-making processes. To our knowledge, MAVERIX is the first benchmark\naimed explicitly at assessing comprehensive audiovisual integration.\nExperiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show\nperformance approaching human levels (around 70% accuracy), while human experts\nreach near-ceiling performance (95.1%). With standardized evaluation protocols,\na rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a\nchallenging testbed for advancing audiovisual multimodal intelligence.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "testbed", "accuracy"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
