{"id": "2503.22351", "pdf": "https://arxiv.org/pdf/2503.22351", "abs": "https://arxiv.org/abs/2503.22351", "authors": ["Byeongjun Kwon", "Munchurl Kim"], "title": "One Look is Enough: A Novel Seamless Patchwise Refinement for Zero-Shot Monocular Depth Estimation Models on High-Resolution Images", "categories": ["cs.CV"], "comment": "Please visit our project page this\n  https://kaist-viclab.github.io/One-Look-is-Enough_site", "summary": "Zero-shot depth estimation (DE) models exhibit strong generalization\nperformance as they are trained on large-scale datasets. However, existing\nmodels struggle with high-resolution images due to the discrepancy in image\nresolutions of training (with smaller resolutions) and inference (for high\nresolutions). Processing them at full resolution leads to decreased estimation\naccuracy on depth with tremendous memory consumption, while downsampling to the\ntraining resolution results in blurred edges in the estimated depth images.\nPrevailing high-resolution depth estimation methods adopt a patch-based\napproach, which introduces depth discontinuity issues when reassembling the\nestimated depth patches and results in test-time inefficiency. Additionally, to\nobtain fine-grained depth details, these methods rely on synthetic datasets due\nto the real-world sparse ground truth depth, leading to poor generalizability.\nTo tackle these limitations, we propose Patch Refine Once (PRO), an efficient\nand generalizable tile-based framework. Our PRO consists of two key components:\n(i) Grouped Patch Consistency Training that enhances test-time efficiency while\nmitigating the depth discontinuity problem by jointly processing four\noverlapping patches and enforcing a consistency loss on their overlapping\nregions within a single backpropagation step, and (ii) Bias Free Masking that\nprevents the DE models from overfitting to dataset-specific biases, enabling\nbetter generalization to real-world datasets even after training on synthetic\ndata. Zero-shot evaluation on Booster, ETH3D, Middlebury 2014, and NuScenes\ndemonstrates into which our PRO can be well harmonized, making their DE\ncapabilities still effective for the grid input of high-resolution images with\nlittle depth discontinuities at the grid boundaries. Our PRO runs fast at\ninference time.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference time", "scale"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21893", "pdf": "https://arxiv.org/pdf/2503.21893", "abs": "https://arxiv.org/abs/2503.21893", "authors": ["Taufiq Ahmed", "Abhishek Kumar", "Constantino Álvarez Casado", "Anlan Zhang", "Tuomo Hänninen", "Lauri Loven", "Miguel Bordallo López", "Sasu Tarkoma"], "title": "Exponentially Weighted Instance-Aware Repeat Factor Sampling for Long-Tailed Object Detection Model Training in Unmanned Aerial Vehicles Surveillance Scenarios", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages, 2 figures, 9 tables, 6 formulas, conference paper", "summary": "Object detection models often struggle with class imbalance, where rare\ncategories appear significantly less frequently than common ones. Existing\nsampling-based rebalancing strategies, such as Repeat Factor Sampling (RFS) and\nInstance-Aware Repeat Factor Sampling (IRFS), mitigate this issue by adjusting\nsample frequencies based on image and instance counts. However, these methods\nare based on linear adjustments, which limit their effectiveness in long-tailed\ndistributions. This work introduces Exponentially Weighted Instance-Aware\nRepeat Factor Sampling (E-IRFS), an extension of IRFS that applies exponential\nscaling to better differentiate between rare and frequent classes. E-IRFS\nadjusts sampling probabilities using an exponential function applied to the\ngeometric mean of image and instance frequencies, ensuring a more adaptive\nrebalancing strategy. We evaluate E-IRFS on a dataset derived from the\nFireman-UAV-RGBT Dataset and four additional public datasets, using YOLOv11\nobject detection models to identify fire, smoke, people and lakes in emergency\nscenarios. The results show that E-IRFS improves detection performance by 22\\%\nover the baseline and outperforms RFS and IRFS, particularly for rare\ncategories. The analysis also highlights that E-IRFS has a stronger effect on\nlightweight models with limited capacity, as these models rely more on data\nsampling strategies to address class imbalance. The findings demonstrate that\nE-IRFS improves rare object detection in resource-constrained environments,\nmaking it a suitable solution for real-time applications such as UAV-based\nemergency monitoring.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "sampling strategies"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22675", "pdf": "https://arxiv.org/pdf/2503.22675", "abs": "https://arxiv.org/abs/2503.22675", "authors": ["Jiakai Tang", "Sunhao Dai", "Teng Shi", "Jun Xu", "Xu Chen", "Wen Chen", "Wu Jian", "Yuning Jiang"], "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "multi-step reasoning"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21806", "pdf": "https://arxiv.org/pdf/2503.21806", "abs": "https://arxiv.org/abs/2503.21806", "authors": ["Heqing Zou", "Fengmao Lv", "Desheng Zheng", "Eng Siong Chng", "Deepu Rajan"], "title": "Large Language Models Meet Contrastive Learning: Zero-Shot Emotion Recognition Across Languages", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICME 2025", "summary": "Multilingual speech emotion recognition aims to estimate a speaker's\nemotional state using a contactless method across different languages. However,\nvariability in voice characteristics and linguistic diversity poses significant\nchallenges for zero-shot speech emotion recognition, especially with\nmultilingual datasets. In this paper, we propose leveraging contrastive\nlearning to refine multilingual speech features and extend large language\nmodels for zero-shot multilingual speech emotion estimation. Specifically, we\nemploy a novel two-stage training framework to align speech signals with\nlinguistic features in the emotional space, capturing both emotion-aware and\nlanguage-agnostic speech representations. To advance research in this field, we\nintroduce a large-scale synthetic multilingual speech emotion dataset, M5SER.\nOur experiments demonstrate the effectiveness of the proposed method in both\nspeech emotion recognition and zero-shot multilingual speech emotion\nrecognition, including previously unseen datasets and languages.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21817", "pdf": "https://arxiv.org/pdf/2503.21817", "abs": "https://arxiv.org/abs/2503.21817", "authors": ["Weili Zeng", "Ziyuan Huang", "Kaixiang Ji", "Yichao Yan"], "title": "Skip-Vision: A Comprehensive Framework for Accelerating Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21838", "pdf": "https://arxiv.org/pdf/2503.21838", "abs": "https://arxiv.org/abs/2503.21838", "authors": ["Jiancheng Zhao", "Xingda Yu", "Zhen Yang"], "title": "MSPLoRA: A Multi-Scale Pyramid Low-Rank Adaptation for Efficient Model Fine-Tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) has become an essential approach for\nadapting large-scale pre-trained models while reducing computational costs.\nAmong PEFT methods, LoRA significantly reduces trainable parameters by\ndecomposing weight updates into low-rank matrices. However, traditional LoRA\napplies a fixed rank across all layers, failing to account for the varying\ncomplexity of hierarchical information, which leads to inefficient adaptation\nand redundancy. To address this, we propose MSPLoRA (Multi-Scale Pyramid LoRA),\nwhich introduces Global Shared LoRA, Mid-Level Shared LoRA, and Layer-Specific\nLoRA to capture global patterns, mid-level features, and fine-grained\ninformation, respectively. This hierarchical structure reduces inter-layer\nredundancy while maintaining strong adaptation capability. Experiments on\nvarious NLP tasks demonstrate that MSPLoRA achieves more efficient adaptation\nand better performance while significantly reducing the number of trainable\nparameters. Furthermore, additional analyses based on Singular Value\nDecomposition validate its information decoupling ability, highlighting MSPLoRA\nas a scalable and effective optimization strategy for parameter-efficient\nfine-tuning in large language models. Our code is available at\nhttps://github.com/Oblivioniss/MSPLoRA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21827", "pdf": "https://arxiv.org/pdf/2503.21827", "abs": "https://arxiv.org/abs/2503.21827", "authors": ["Mark Phil Pacot", "Jayno Juventud", "Gleen Dalaorao"], "title": "Hybrid Multi-Stage Learning Framework for Edge Detection: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Edge detection remains a fundamental yet challenging task in computer vision,\nespecially under varying illumination, noise, and complex scene conditions.\nThis paper introduces a Hybrid Multi-Stage Learning Framework that integrates\nConvolutional Neural Network (CNN) feature extraction with a Support Vector\nMachine (SVM) classifier to improve edge localization and structural accuracy.\nUnlike conventional end-to-end deep learning models, our approach decouples\nfeature representation and classification stages, enhancing robustness and\ninterpretability. Extensive experiments conducted on benchmark datasets such as\nBSDS500 and NYUDv2 demonstrate that the proposed framework outperforms\ntraditional edge detectors and even recent learning-based methods in terms of\nOptimal Dataset Scale (ODS) and Optimal Image Scale (OIS), while maintaining\ncompetitive Average Precision (AP). Both qualitative and quantitative results\nhighlight enhanced performance on edge continuity, noise suppression, and\nperceptual clarity achieved by our method. This work not only bridges classical\nand deep learning paradigms but also sets a new direction for scalable,\ninterpretable, and high-quality edge detection solutions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21836", "pdf": "https://arxiv.org/pdf/2503.21836", "abs": "https://arxiv.org/abs/2503.21836", "authors": ["Ran Wei", "ZhiXiong Lan", "Qing Yan", "Ning Song", "Ming Lv", "LongQing Ye"], "title": "iMedImage Technical Report", "categories": ["cs.CV"], "comment": null, "summary": "Background: Chromosome karyotype analysis is crucial for diagnosing\nhereditary diseases, yet detecting structural abnormalities remains\nchallenging. While AI has shown promise in medical imaging, its effectiveness\nvaries across modalities. Leveraging advances in Foundation Models that\nintegrate multimodal medical imaging for robust feature extraction and accurate\ndiagnosis, we developed iMedImage, an end-to-end model for general medical\nimage recognition, demonstrating strong performance across multiple imaging\ntasks, including chromosome abnormality detection. Materials and Methods: We\nconstructed a comprehensive medical image dataset encompassing multiple\nmodalities from common medical domains, including chromosome, cell, pathology,\nultrasound, X-ray, CT, and MRI images. Based on this dataset, we developed the\niMedImage model, which incorporates the following key features: (1) a unified\nrepresentation method for diverse modality inputs and medical imaging tasks;\n(2) multi-level (case-level, image-level, patch-level) image recognition\ncapabilities enhanced by Chain of Thought (CoT) embedding and Mixture of\nExperts (MoE) strategies. Results: The test set comprised data from 12\ninstitutions across six regions in China, covering three mainstream scanning\ndevices, and included naturally distributed, unscreened abnormal cases. On this\ndiverse dataset, the model achieved a fully automated chromosome analysis\nworkflow, including segmentation, karyotyping, and abnormality detection,\nreaching a sensitivity of 92.75% and a specificity of 91.5%. Conclusion: We\npropose iMedImage, an end-to-end foundation model for medical image analysis,\ndemonstrating its superior performance across various medical imaging tasks.\niMedImage provides clinicians with a precise imaging analysis tool and\ncontributes to improving diagnostic accuracy and disease screening.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21841", "pdf": "https://arxiv.org/pdf/2503.21841", "abs": "https://arxiv.org/abs/2503.21841", "authors": ["Jingtao Li", "Yingyi Liu", "Xinyu Wang", "Yunning Peng", "Chen Sun", "Shaoyu Wang", "Zhendong Sun", "Tian Ke", "Xiao Jiang", "Tangwei Lu", "Anran Zhao", "Yanfei Zhong"], "title": "HyperFree: A Channel-adaptive and Tuning-free Foundation Model for Hyperspectral Remote Sensing Imagery", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Advanced interpretation of hyperspectral remote sensing images benefits many\nprecise Earth observation tasks. Recently, visual foundation models have\npromoted the remote sensing interpretation but concentrating on RGB and\nmultispectral images. Due to the varied hyperspectral channels,existing\nfoundation models would face image-by-image tuning situation, imposing great\npressure on hardware and time resources. In this paper, we propose a\ntuning-free hyperspectral foundation model called HyperFree, by adapting the\nexisting visual prompt engineering. To process varied channel numbers, we\ndesign a learned weight dictionary covering full-spectrum from $0.4 \\sim 2.5 \\,\n\\mu\\text{m}$, supporting to build the embedding layer dynamically. To make the\nprompt design more tractable, HyperFree can generate multiple semantic-aware\nmasks for one prompt by treating feature distance as semantic-similarity. After\npre-training HyperFree on constructed large-scale high-resolution hyperspectral\nimages, HyperFree (1 prompt) has shown comparable results with specialized\nmodels (5 shots) on 5 tasks and 11 datasets.Code and dataset are accessible at\nhttps://rsidea.whu.edu.cn/hyperfree.htm.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21904", "pdf": "https://arxiv.org/pdf/2503.21904", "abs": "https://arxiv.org/abs/2503.21904", "authors": ["Zhiwei Yang", "Chen Gao", "Jing Liu", "Peng Wu", "Guansong Pang", "Mike Zheng Shou"], "title": "AssistPDA: An Online Video Surveillance Assistant for Video Anomaly Prediction, Detection, and Analysis", "categories": ["cs.CV"], "comment": "13 pages", "summary": "The rapid advancements in large language models (LLMs) have spurred growing\ninterest in LLM-based video anomaly detection (VAD). However, existing\napproaches predominantly focus on video-level anomaly question answering or\noffline detection, ignoring the real-time nature essential for practical VAD\napplications. To bridge this gap and facilitate the practical deployment of\nLLM-based VAD, we introduce AssistPDA, the first online video anomaly\nsurveillance assistant that unifies video anomaly prediction, detection, and\nanalysis (VAPDA) within a single framework. AssistPDA enables real-time\ninference on streaming videos while supporting interactive user engagement.\nNotably, we introduce a novel event-level anomaly prediction task, enabling\nproactive anomaly forecasting before anomalies fully unfold. To enhance the\nability to model intricate spatiotemporal relationships in anomaly events, we\npropose a Spatio-Temporal Relation Distillation (STRD) module. STRD transfers\nthe long-term spatiotemporal modeling capabilities of vision-language models\n(VLMs) from offline settings to real-time scenarios. Thus it equips AssistPDA\nwith a robust understanding of complex temporal dependencies and long-sequence\nmemory. Additionally, we construct VAPDA-127K, the first large-scale benchmark\ndesigned for VLM-based online VAPDA. Extensive experiments demonstrate that\nAssistPDA outperforms existing offline VLM-based approaches, setting a new\nstate-of-the-art for real-time VAPDA. Our dataset and code will be open-sourced\nto facilitate further research in the community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21907", "pdf": "https://arxiv.org/pdf/2503.21907", "abs": "https://arxiv.org/abs/2503.21907", "authors": ["Oliver Heinimann", "Assaf Shocher", "Tal Zimbalist", "Michal Irani"], "title": "KernelFusion: Assumption-Free Blind Super-Resolution via Patch Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Traditional super-resolution (SR) methods assume an ``ideal'' downscaling\nSR-kernel (e.g., bicubic downscaling) between the high-resolution (HR) image\nand the low-resolution (LR) image. Such methods fail once the LR images are\ngenerated differently. Current blind-SR methods aim to remove this assumption,\nbut are still fundamentally restricted to rather simplistic downscaling\nSR-kernels (e.g., anisotropic Gaussian kernels), and fail on more complex (out\nof distribution) downscaling degradations. However, using the correct SR-kernel\nis often more important than using a sophisticated SR algorithm. In\n``KernelFusion'' we introduce a zero-shot diffusion-based method that makes no\nassumptions about the kernel. Our method recovers the unique image-specific\nSR-kernel directly from the LR input image, while simultaneously recovering its\ncorresponding HR image. KernelFusion exploits the principle that the correct\nSR-kernel is the one that maximizes patch similarity across different scales of\nthe LR image. We first train an image-specific patch-based diffusion model on\nthe single LR input image, capturing its unique internal patch statistics. We\nthen reconstruct a larger HR image with the same learned patch distribution,\nwhile simultaneously recovering the correct downscaling SR-kernel that\nmaintains this cross-scale relation between the HR and LR images. Empirical\nresults show that KernelFusion vastly outperforms all SR baselines on complex\ndownscaling degradations, where existing SotA Blind-SR methods fail miserably.\nBy breaking free from predefined kernel assumptions, KernelFusion pushes\nBlind-SR into a new assumption-free paradigm, handling downscaling kernels\npreviously thought impossible.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21970", "pdf": "https://arxiv.org/pdf/2503.21970", "abs": "https://arxiv.org/abs/2503.21970", "authors": ["Yujie Chen", "Haotong Qin", "Zhang Zhang", "Michelo Magno", "Luca Benini", "Yawei Li"], "title": "Q-MambaIR: Accurate Quantized Mamba for Efficient Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "State-Space Models (SSMs) have attracted considerable attention in Image\nRestoration (IR) due to their ability to scale linearly sequence length while\neffectively capturing long-distance dependencies. However, deploying SSMs to\nedge devices is challenging due to the constraints in memory, computing\ncapacity, and power consumption, underscoring the need for efficient\ncompression strategies. While low-bit quantization is an efficient model\ncompression strategy for reducing size and accelerating IR tasks, SSM suffers\nsubstantial performance drops at ultra-low bit-widths (2-4 bits), primarily due\nto outliers that exacerbate quantization error. To address this challenge, we\npropose Q-MambaIR, an accurate, efficient, and flexible Quantized Mamba for IR\ntasks. Specifically, we introduce a Statistical Dynamic-balancing Learnable\nScalar (DLS) to dynamically adjust the quantization mapping range, thereby\nmitigating the peak truncation loss caused by extreme values. Furthermore, we\ndesign a Range-floating Flexible Allocator (RFA) with an adaptive threshold to\nflexibly round values. This approach preserves high-frequency details and\nmaintains the SSM's feature extraction capability. Notably, RFA also enables\npre-deployment weight quantization, striking a balance between computational\nefficiency and model accuracy. Extensive experiments on IR tasks demonstrate\nthat Q-MambaIR consistently outperforms existing quantized SSMs, achieving much\nhigher state-of-the-art (SOTA) accuracy results with only a negligible increase\nin training computation and storage saving.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21990", "pdf": "https://arxiv.org/pdf/2503.21990", "abs": "https://arxiv.org/abs/2503.21990", "authors": ["Isaac Kazuo Uyehara", "Heesup Yun", "Earl Ranario", "Mason Earles"], "title": "AgRowStitch: A High-fidelity Image Stitching Pipeline for Ground-based Agricultural Images", "categories": ["cs.CV"], "comment": null, "summary": "Agricultural imaging often requires individual images to be stitched together\ninto a final mosaic for analysis. However, agricultural images can be\nparticularly challenging to stitch because feature matching across images is\ndifficult due to repeated textures, plants are non-planar, and mosaics built\nfrom many images can accumulate errors that cause drift. Although these issues\ncan be mitigated by using georeferenced images or taking images at high\naltitude, there is no general solution for images taken close to the crop. To\naddress this, we created a user-friendly and open source pipeline for stitching\nground-based images of a linear row of crops that does not rely on additional\ndata. First, we use SuperPoint and LightGlue to extract and match features\nwithin small batches of images. Then we stitch the images in each batch in\nseries while imposing constraints on the camera movement. After straightening\nand rescaling each batch mosaic, all batch mosaics are stitched together in\nseries and then straightened into a final mosaic. We tested the pipeline on\nimages collected along 72 m long rows of crops using two different agricultural\nrobots and a camera manually carried over the row. In all three cases, the\npipeline produced high-quality mosaics that could be used to georeference real\nworld positions with a mean absolute error of 20 cm. This approach provides\naccessible leaf-scale stitching to users who need to coarsely georeference\npositions within a row, but do not have access to accurate positional data or\nsophisticated imaging systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22020", "pdf": "https://arxiv.org/pdf/2503.22020", "abs": "https://arxiv.org/abs/2503.22020", "authors": ["Qingqing Zhao", "Yao Lu", "Moo Jin Kim", "Zipeng Fu", "Zhuoyang Zhang", "Yecheng Wu", "Zhaoshuo Li", "Qianli Ma", "Song Han", "Chelsea Finn", "Ankur Handa", "Ming-Yu Liu", "Donglai Xiang", "Gordon Wetzstein", "Tsung-Yi Lin"], "title": "CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project website: https://cot-vla.github.io/", "summary": "Vision-language-action models (VLAs) have shown potential in leveraging\npretrained vision-language models and diverse robot demonstrations for learning\ngeneralizable sensorimotor control. While this paradigm effectively utilizes\nlarge-scale data from both robotic and non-robotic sources, current VLAs\nprimarily focus on direct input--output mappings, lacking the intermediate\nreasoning steps crucial for complex manipulation tasks. As a result, existing\nVLAs lack temporal planning or reasoning capabilities. In this paper, we\nintroduce a method that incorporates explicit visual chain-of-thought (CoT)\nreasoning into vision-language-action models (VLAs) by predicting future image\nframes autoregressively as visual goals before generating a short action\nsequence to achieve these goals. We introduce CoT-VLA, a state-of-the-art 7B\nVLA that can understand and generate visual and action tokens. Our experimental\nresults demonstrate that CoT-VLA achieves strong performance, outperforming the\nstate-of-the-art VLA model by 17% in real-world manipulation tasks and 6% in\nsimulation benchmarks. Project website: https://cot-vla.github.io/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22050", "pdf": "https://arxiv.org/pdf/2503.22050", "abs": "https://arxiv.org/abs/2503.22050", "authors": ["Tai An", "Weiqiang Huang", "Da Xu", "Qingyuan He", "Jiacheng Hu", "Yujia Lou"], "title": "A Deep Learning Framework for Boundary-Aware Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "As a fundamental task in computer vision, semantic segmentation is widely\napplied in fields such as autonomous driving, remote sensing image analysis,\nand medical image processing. In recent years, Transformer-based segmentation\nmethods have demonstrated strong performance in global feature modeling.\nHowever, they still struggle with blurred target boundaries and insufficient\nrecognition of small targets. To address these issues, this study proposes a\nMask2Former-based semantic segmentation algorithm incorporating a boundary\nenhancement feature bridging module (BEFBM). The goal is to improve target\nboundary accuracy and segmentation consistency. Built upon the Mask2Former\nframework, this method constructs a boundary-aware feature map and introduces a\nfeature bridging mechanism. This enables effective cross-scale feature fusion,\nenhancing the model's ability to focus on target boundaries. Experiments on the\nCityscapes dataset demonstrate that, compared to mainstream segmentation\nmethods, the proposed approach achieves significant improvements in metrics\nsuch as mIOU, mDICE, and mRecall. It also exhibits superior boundary retention\nin complex scenes. Visual analysis further confirms the model's advantages in\nfine-grained regions. Future research will focus on optimizing computational\nefficiency and exploring its potential in other high-precision segmentation\ntasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22060", "pdf": "https://arxiv.org/pdf/2503.22060", "abs": "https://arxiv.org/abs/2503.22060", "authors": ["Ukcheol Shin", "Jinsun Park"], "title": "Deep Depth Estimation from Thermal Image: Dataset, Benchmark, and Challenges", "categories": ["cs.CV", "cs.RO"], "comment": "MS^2 dataset:\n  https://sites.google.com/view/multi-spectral-stereo-dataset, Source code:\n  https://github.com/UkcheolShin/SupDepth4Thermal", "summary": "Achieving robust and accurate spatial perception under adverse weather and\nlighting conditions is crucial for the high-level autonomy of self-driving\nvehicles and robots. However, existing perception algorithms relying on the\nvisible spectrum are highly affected by weather and lighting conditions. A\nlong-wave infrared camera (i.e., thermal imaging camera) can be a potential\nsolution to achieve high-level robustness. However, the absence of large-scale\ndatasets and standardized benchmarks remains a significant bottleneck to\nprogress in active research for robust visual perception from thermal images.\nTo this end, this manuscript provides a large-scale Multi-Spectral Stereo\n(MS$^2$) dataset that consists of stereo RGB, stereo NIR, stereo thermal,\nstereo LiDAR data, and GNSS/IMU information along with semi-dense depth ground\ntruth. MS$^2$ dataset includes 162K synchronized multi-modal data pairs\ncaptured across diverse locations (e.g., urban city, residential area, campus,\nand high-way road) at different times (e.g., morning, daytime, and nighttime)\nand under various weather conditions (e.g., clear-sky, cloudy, and rainy).\nSecondly, we conduct a thorough evaluation of monocular and stereo depth\nestimation networks across RGB, NIR, and thermal modalities to establish\nstandardized benchmark results on MS$^2$ depth test sets (e.g., day, night, and\nrainy). Lastly, we provide in-depth analyses and discuss the challenges\nrevealed by the benchmark results, such as the performance variability for each\nmodality under adverse conditions, domain shift between different sensor\nmodalities, and potential research direction for thermal perception. Our\ndataset and source code are publicly available at\nhttps://sites.google.com/view/multi-spectral-stereo-dataset and\nhttps://github.com/UkcheolShin/SupDepth4Thermal.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22444", "pdf": "https://arxiv.org/pdf/2503.22444", "abs": "https://arxiv.org/abs/2503.22444", "authors": ["Pengsong Zhang", "Heng Zhang", "Huazhe Xu", "Renjun Xu", "Zhenting Wang", "Cong Wang", "Animesh Garg", "Zhibin Li", "Arash Ajoudani", "Xinyu Liu"], "title": "Scaling Laws of Scientific Discovery with AI and Robot Scientists", "categories": ["cs.CL", "cs.RO"], "comment": "22 pages, 7 figures", "summary": "The rapid evolution of scientific inquiry highlights an urgent need for\ngroundbreaking methodologies that transcend the limitations of traditional\nresearch. Conventional approaches, bogged down by manual processes and siloed\nexpertise, struggle to keep pace with the demands of modern discovery. We\nenvision an autonomous generalist scientist (AGS) system-a fusion of agentic AI\nand embodied robotics-that redefines the research lifecycle. This system\npromises to autonomously navigate physical and digital realms, weaving together\ninsights from disparate disciplines with unprecedented efficiency. By embedding\nadvanced AI and robot technologies into every phase-from hypothesis formulation\nto peer-ready manuscripts-AGS could slash the time and resources needed for\nscientific research in diverse field. We foresee a future where scientific\ndiscovery follows new scaling laws, driven by the proliferation and\nsophistication of such systems. As these autonomous agents and robots adapt to\nextreme environments and leverage a growing reservoir of knowledge, they could\nspark a paradigm shift, pushing the boundaries of what's possible and ushering\nin an era of relentless innovation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22079", "pdf": "https://arxiv.org/pdf/2503.22079", "abs": "https://arxiv.org/abs/2503.22079", "authors": ["Kunshan Yang", "Wenwei Luo", "Yuguo Hu", "Jiafu Yan", "Mengmeng Jing", "Lin Zuo"], "title": "A Semantic-Enhanced Heterogeneous Graph Learning Method for Flexible Objects Recognition", "categories": ["cs.CV"], "comment": "Accepted by ICME 2025", "summary": "Flexible objects recognition remains a significant challenge due to its\ninherently diverse shapes and sizes, translucent attributes, and subtle\ninter-class differences. Graph-based models, such as graph convolution networks\nand graph vision models, are promising in flexible objects recognition due to\ntheir ability of capturing variable relations within the flexible objects.\nThese methods, however, often focus on global visual relationships or fail to\nalign semantic and visual information. To alleviate these limitations, we\npropose a semantic-enhanced heterogeneous graph learning method. First, an\nadaptive scanning module is employed to extract discriminative semantic\ncontext, facilitating the matching of flexible objects with varying shapes and\nsizes while aligning semantic and visual nodes to enhance cross-modal feature\ncorrelation. Second, a heterogeneous graph generation module aggregates global\nvisual and local semantic node features, improving the recognition of flexible\nobjects. Additionally, We introduce the FSCW, a large-scale flexible dataset\ncurated from existing sources. We validate our method through extensive\nexperiments on flexible datasets (FDA and FSCW), and challenge benchmarks\n(CIFAR-100 and ImageNet-Hard), demonstrating competitive performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22081", "pdf": "https://arxiv.org/pdf/2503.22081", "abs": "https://arxiv.org/abs/2503.22081", "authors": ["Ziyue Huang", "Hongxi Yan", "Qiqi Zhan", "Shuai Yang", "Mingming Zhang", "Chenkai Zhang", "YiMing Lei", "Zeming Liu", "Qingjie Liu", "Yunhong Wang"], "title": "A Survey on Remote Sensing Foundation Models: From Vision to Multimodality", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of remote sensing foundation models, particularly\nvision and multimodal models, has significantly enhanced the capabilities of\nintelligent geospatial data interpretation. These models combine various data\nmodalities, such as optical, radar, and LiDAR imagery, with textual and\ngeographic information, enabling more comprehensive analysis and understanding\nof remote sensing data. The integration of multiple modalities allows for\nimproved performance in tasks like object detection, land cover classification,\nand change detection, which are often challenged by the complex and\nheterogeneous nature of remote sensing data. However, despite these\nadvancements, several challenges remain. The diversity in data types, the need\nfor large-scale annotated datasets, and the complexity of multimodal fusion\ntechniques pose significant obstacles to the effective deployment of these\nmodels. Moreover, the computational demands of training and fine-tuning\nmultimodal models require significant resources, further complicating their\npractical application in remote sensing image interpretation tasks. This paper\nprovides a comprehensive review of the state-of-the-art in vision and\nmultimodal foundation models for remote sensing, focusing on their\narchitecture, training methods, datasets and application scenarios. We discuss\nthe key challenges these models face, such as data alignment, cross-modal\ntransfer learning, and scalability, while also identifying emerging research\ndirections aimed at overcoming these limitations. Our goal is to provide a\nclear understanding of the current landscape of remote sensing foundation\nmodels and inspire future research that can push the boundaries of what these\nmodels can achieve in real-world applications. The list of resources collected\nby the paper can be found in the\nhttps://github.com/IRIP-BUAA/A-Review-for-remote-sensing-vision-language-models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21902", "pdf": "https://arxiv.org/pdf/2503.21902", "abs": "https://arxiv.org/abs/2503.21902", "authors": ["Hamed Babaei Giglou", "Jennifer D'Souza", "Oliver Karras", "Sören Auer"], "title": "OntoAligner: A Comprehensive Modular and Robust Python Toolkit for Ontology Alignment", "categories": ["cs.AI", "cs.CL"], "comment": "18 pages, 3 figures. Accepted for the ESWC 2025 Resource Track", "summary": "Ontology Alignment (OA) is fundamental for achieving semantic\ninteroperability across diverse knowledge systems. We present OntoAligner, a\ncomprehensive, modular, and robust Python toolkit for ontology alignment,\ndesigned to address current limitations with existing tools faced by\npractitioners. Existing tools are limited in scalability, modularity, and ease\nof integration with recent AI advances. OntoAligner provides a flexible\narchitecture integrating existing lightweight OA techniques such as fuzzy\nmatching but goes beyond by supporting contemporary methods with\nretrieval-augmented generation and large language models for OA. The framework\nprioritizes extensibility, enabling researchers to integrate custom alignment\nalgorithms and datasets. This paper details the design principles,\narchitecture, and implementation of the OntoAligner, demonstrating its utility\nthrough benchmarks on standard OA tasks. Our evaluation highlights\nOntoAligner's ability to handle large-scale ontologies efficiently with few\nlines of code while delivering high alignment quality. By making OntoAligner\nopen-source, we aim to provide a resource that fosters innovation and\ncollaboration within the OA community, empowering researchers and practitioners\nwith a toolkit for reproducible OA research and real-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22171", "pdf": "https://arxiv.org/pdf/2503.22171", "abs": "https://arxiv.org/abs/2503.22171", "authors": ["Min Cao", "ZiYin Zeng", "YuXin Lu", "Mang Ye", "Dong Yi", "Jinqiao Wang"], "title": "An Empirical Study of Validating Synthetic Data for Text-Based Person Retrieval", "categories": ["cs.CV"], "comment": "20 pages,13 figures", "summary": "Data plays a pivotal role in Text-Based Person Retrieval (TBPR) research.\nMainstream research paradigm necessitates real-world person images with manual\ntextual annotations for training models, posing privacy-sensitive and\nlabor-intensive issues. Several pioneering efforts explore synthetic data for\nTBPR but still rely on real data, keeping the aforementioned issues and also\nresulting in diversity-deficient issue in synthetic datasets, thus impacting\nTBPR performance. Moreover, these works tend to explore synthetic data for TBPR\nthrough limited perspectives, leading to exploration-restricted issue. In this\npaper, we conduct an empirical study to explore the potential of synthetic data\nfor TBPR, highlighting three key aspects. (1) We propose an inter-class image\ngeneration pipeline, in which an automatic prompt construction strategy is\nintroduced to guide generative Artificial Intelligence (AI) models in\ngenerating various inter-class images without reliance on original data. (2) We\ndevelop an intra-class image augmentation pipeline, in which the generative AI\nmodels are applied to further edit the images for obtaining various intra-class\nimages. (3) Building upon the proposed pipelines and an automatic text\ngeneration pipeline, we explore the effectiveness of synthetic data in diverse\nscenarios through extensive experiments. Additionally, we experimentally\ninvestigate various noise-robust learning strategies to mitigate the inherent\nnoise in synthetic data. We will release the code, along with the synthetic\nlarge-scale dataset generated by our pipelines, which are expected to advance\npractical TBPR research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22194", "pdf": "https://arxiv.org/pdf/2503.22194", "abs": "https://arxiv.org/abs/2503.22194", "authors": ["Yunhong Min", "Daehyeon Choi", "Kyeongmin Yeo", "Jihyun Lee", "Minhyuk Sung"], "title": "ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation", "categories": ["cs.CV", "cs.LG"], "comment": "Project Page: https://origen2025.github.io", "summary": "We introduce ORIGEN, the first zero-shot method for 3D orientation grounding\nin text-to-image generation across multiple objects and diverse categories.\nWhile previous work on spatial grounding in image generation has mainly focused\non 2D positioning, it lacks control over 3D orientation. To address this, we\npropose a reward-guided sampling approach using a pretrained discriminative\nmodel for 3D orientation estimation and a one-step text-to-image generative\nflow model. While gradient-ascent-based optimization is a natural choice for\nreward-based guidance, it struggles to maintain image realism. Instead, we\nadopt a sampling-based approach using Langevin dynamics, which extends gradient\nascent by simply injecting random noise--requiring just a single additional\nline of code. Additionally, we introduce adaptive time rescaling based on the\nreward function to accelerate convergence. Our experiments show that ORIGEN\noutperforms both training-based and test-time guidance methods across\nquantitative metrics and user studies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22262", "pdf": "https://arxiv.org/pdf/2503.22262", "abs": "https://arxiv.org/abs/2503.22262", "authors": ["Songsong Yu", "Yuxin Chen", "Zhongang Qi", "Zeke Xie", "Yifan Wang", "Lijun Wang", "Ying Shan", "Huchuan Lu"], "title": "Mono2Stereo: A Benchmark and Empirical Study for Stereo Conversion", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 Project webpage:\n  https://mono2stereo-bench.github.io/", "summary": "With the rapid proliferation of 3D devices and the shortage of 3D content,\nstereo conversion is attracting increasing attention. Recent works introduce\npretrained Diffusion Models (DMs) into this task. However, due to the scarcity\nof large-scale training data and comprehensive benchmarks, the optimal\nmethodologies for employing DMs in stereo conversion and the accurate\nevaluation of stereo effects remain largely unexplored. In this work, we\nintroduce the Mono2Stereo dataset, providing high-quality training data and\nbenchmark to support in-depth exploration of stereo conversion. With this\ndataset, we conduct an empirical study that yields two primary findings. 1) The\ndifferences between the left and right views are subtle, yet existing metrics\nconsider overall pixels, failing to concentrate on regions critical to stereo\neffects. 2) Mainstream methods adopt either one-stage left-to-right generation\nor warp-and-inpaint pipeline, facing challenges of degraded stereo effect and\nimage distortion respectively. Based on these findings, we introduce a new\nevaluation metric, Stereo Intersection-over-Union, which prioritizes disparity\nand achieves a high correlation with human judgments on stereo effect.\nMoreover, we propose a strong baseline model, harmonizing the stereo effect and\nimage quality simultaneously, and notably surpassing current mainstream\nmethods. Our code and data will be open-sourced to promote further research in\nstereo conversion. Our models are available at mono2stereo-bench.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "correlation"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22346", "pdf": "https://arxiv.org/pdf/2503.22346", "abs": "https://arxiv.org/abs/2503.22346", "authors": ["Ruifeng Luo", "Zhengjie Liu", "Tianxiao Cheng", "Jie Wang", "Tongjie Wang", "Xingguang Wei", "Haomin Wang", "YanPeng Li", "Fu Chai", "Fei Cheng", "Shenglong Ye", "Wenhai Wang", "Yanting Zhang", "Yu Qiao", "Hongjie Zhang", "Xianzhong Zhao"], "title": "ArchCAD-400K: An Open Large-Scale Architectural CAD Dataset and New Baseline for Panoptic Symbol Spotting", "categories": ["cs.CV"], "comment": null, "summary": "Recognizing symbols in architectural CAD drawings is critical for various\nadvanced engineering applications. In this paper, we propose a novel CAD data\nannotation engine that leverages intrinsic attributes from systematically\narchived CAD drawings to automatically generate high-quality annotations, thus\nsignificantly reducing manual labeling efforts. Utilizing this engine, we\nconstruct ArchCAD-400K, a large-scale CAD dataset consisting of 413,062 chunks\nfrom 5538 highly standardized drawings, making it over 26 times larger than the\nlargest existing CAD dataset. ArchCAD-400K boasts an extended drawing diversity\nand broader categories, offering line-grained annotations. Furthermore, we\npresent a new baseline model for panoptic symbol spotting, termed Dual-Pathway\nSymbol Spotter (DPSS). It incorporates an adaptive fusion module to enhance\nprimitive features with complementary image features, achieving\nstate-of-the-art performance and enhanced robustness. Extensive experiments\nvalidate the effectiveness of DPSS, demonstrating the value of ArchCAD-400K and\nits potential to drive innovation in architectural design and construction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22357", "pdf": "https://arxiv.org/pdf/2503.22357", "abs": "https://arxiv.org/abs/2503.22357", "authors": ["Hadrien Reynaud", "Alberto Gomez", "Paul Leeson", "Qingjie Meng", "Bernhard Kainz"], "title": "EchoFlow: A Foundation Model for Cardiac Ultrasound Image and Video Generation", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Advances in deep learning have significantly enhanced medical image analysis,\nyet the availability of large-scale medical datasets remains constrained by\npatient privacy concerns. We present EchoFlow, a novel framework designed to\ngenerate high-quality, privacy-preserving synthetic echocardiogram images and\nvideos. EchoFlow comprises four key components: an adversarial variational\nautoencoder for defining an efficient latent representation of cardiac\nultrasound images, a latent image flow matching model for generating accurate\nlatent echocardiogram images, a latent re-identification model to ensure\nprivacy by filtering images anatomically, and a latent video flow matching\nmodel for animating latent images into realistic echocardiogram videos\nconditioned on ejection fraction. We rigorously evaluate our synthetic datasets\non the clinically relevant task of ejection fraction regression and\ndemonstrate, for the first time, that downstream models trained exclusively on\nEchoFlow-generated synthetic datasets achieve performance parity with models\ntrained on real datasets. We release our models and synthetic datasets,\nenabling broader, privacy-compliant research in medical ultrasound imaging at\nhttps://huggingface.co/spaces/HReynaud/EchoFlow.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22374", "pdf": "https://arxiv.org/pdf/2503.22374", "abs": "https://arxiv.org/abs/2503.22374", "authors": ["Giulio Federico", "Giuseppe Amato", "Fabio Carrara", "Claudio Gennaro", "Marco Di Benedetto"], "title": "ViSketch-GPT: Collaborative Multi-Scale Feature Extraction for Sketch Recognition and Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding the nature of human sketches is challenging because of the wide\nvariation in how they are created. Recognizing complex structural patterns\nimproves both the accuracy in recognizing sketches and the fidelity of the\ngenerated sketches. In this work, we introduce ViSketch-GPT, a novel algorithm\ndesigned to address these challenges through a multi-scale context extraction\napproach. The model captures intricate details at multiple scales and combines\nthem using an ensemble-like mechanism, where the extracted features work\ncollaboratively to enhance the recognition and generation of key details\ncrucial for classification and generation tasks.\n  The effectiveness of ViSketch-GPT is validated through extensive experiments\non the QuickDraw dataset. Our model establishes a new benchmark, significantly\noutperforming existing methods in both classification and generation tasks,\nwith substantial improvements in accuracy and the fidelity of generated\nsketches.\n  The proposed algorithm offers a robust framework for understanding complex\nstructures by extracting features that collaborate to recognize intricate\ndetails, enhancing the understanding of structures like sketches and making it\na versatile tool for various applications in computer vision and machine\nlearning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22394", "pdf": "https://arxiv.org/pdf/2503.22394", "abs": "https://arxiv.org/abs/2503.22394", "authors": ["Rulin Zhou", "Wenlong He", "An Wang", "Qiqi Yao", "Haijun Hu", "Jiankun Wang", "Xi Zhang an Hongliang Ren"], "title": "Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate tissue point tracking in endoscopic videos is critical for\nrobotic-assisted surgical navigation and scene understanding, but remains\nchallenging due to complex deformations, instrument occlusion, and the scarcity\nof dense trajectory annotations. Existing methods struggle with long-term\ntracking under these conditions due to limited feature utilization and\nannotation dependence. We present Endo-TTAP, a novel framework addressing these\nchallenges through: (1) A Multi-Facet Guided Attention (MFGA) module that\nsynergizes multi-scale flow dynamics, DINOv2 semantic embeddings, and explicit\nmotion patterns to jointly predict point positions with uncertainty and\nocclusion awareness; (2) A two-stage curriculum learning strategy employing an\nAuxiliary Curriculum Adapter (ACA) for progressive initialization and hybrid\nsupervision. Stage I utilizes synthetic data with optical flow ground truth for\nuncertainty-occlusion regularization, while Stage II combines unsupervised flow\nconsistency and semi-supervised learning with refined pseudo-labels from\noff-the-shelf trackers. Extensive validation on two MICCAI Challenge datasets\nand our collected dataset demonstrates that Endo-TTAP achieves state-of-the-art\nperformance in tissue point tracking, particularly in scenarios characterized\nby complex endoscopic conditions. The source code and dataset will be available\nat https://anonymous.4open.science/r/Endo-TTAP-36E5.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "consistency"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22397", "pdf": "https://arxiv.org/pdf/2503.22397", "abs": "https://arxiv.org/abs/2503.22397", "authors": ["Vida Adeli", "Soroush Mehraban", "Majid Mirmehdi", "Alan Whone", "Benjamin Filtjens", "Amirhossein Dadashzadeh", "Alfonso Fasano", "Andrea Iaboni Babak Taati"], "title": "GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain", "categories": ["cs.CV"], "comment": null, "summary": "Gait analysis is crucial for the diagnosis and monitoring of movement\ndisorders like Parkinson's Disease. While computer vision models have shown\npotential for objectively evaluating parkinsonian gait, their effectiveness is\nlimited by scarce clinical datasets and the challenge of collecting large and\nwell-labelled data, impacting model accuracy and risk of bias. To address these\ngaps, we propose GAITGen, a novel framework that generates realistic gait\nsequences conditioned on specified pathology severity levels. GAITGen employs a\nConditional Residual Vector Quantized Variational Autoencoder to learn\ndisentangled representations of motion dynamics and pathology-specific factors,\ncoupled with Mask and Residual Transformers for conditioned sequence\ngeneration. GAITGen generates realistic, diverse gait sequences across severity\nlevels, enriching datasets and enabling large-scale model training in\nparkinsonian gait analysis. Experiments on our new PD-GaM (real) dataset\ndemonstrate that GAITGen outperforms adapted state-of-the-art models in both\nreconstruction fidelity and generation quality, accurately capturing critical\npathology-specific gait features. A clinical user study confirms the realism\nand clinical relevance of our generated sequences. Moreover, incorporating\nGAITGen-generated data into downstream tasks improves parkinsonian gait\nseverity estimation, highlighting its potential for advancing clinical gait\nanalysis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22430", "pdf": "https://arxiv.org/pdf/2503.22430", "abs": "https://arxiv.org/abs/2503.22430", "authors": ["Sergio Izquierdo", "Mohamed Sayed", "Michael Firman", "Guillermo Garcia-Hernando", "Daniyar Turmukhambetov", "Javier Civera", "Oisin Mac Aodha", "Gabriel Brostow", "Jamie Watson"], "title": "MVSAnywhere: Zero-Shot Multi-View Stereo", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Computing accurate depth from multiple views is a fundamental and\nlongstanding challenge in computer vision. However, most existing approaches do\nnot generalize well across different domains and scene types (e.g. indoor vs.\noutdoor). Training a general-purpose multi-view stereo model is challenging and\nraises several questions, e.g. how to best make use of transformer-based\narchitectures, how to incorporate additional metadata when there is a variable\nnumber of input views, and how to estimate the range of valid depths which can\nvary considerably across different scenes and is typically not known a priori?\nTo address these issues, we introduce MVSA, a novel and versatile Multi-View\nStereo architecture that aims to work Anywhere by generalizing across diverse\ndomains and depth ranges. MVSA combines monocular and multi-view cues with an\nadaptive cost volume to deal with scale-related issues. We demonstrate\nstate-of-the-art zero-shot depth estimation on the Robust Multi-View Depth\nBenchmark, surpassing existing multi-view stereo and monocular baselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22436", "pdf": "https://arxiv.org/pdf/2503.22436", "abs": "https://arxiv.org/abs/2503.22436", "authors": ["Fuhao Li", "Huan Jin", "Bin Gao", "Liaoyuan Fan", "Lihui Jiang", "Long Zeng"], "title": "NuGrounding: A Multi-View 3D Visual Grounding Framework in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view 3D visual grounding is critical for autonomous driving vehicles to\ninterpret natural languages and localize target objects in complex\nenvironments. However, existing datasets and methods suffer from coarse-grained\nlanguage instructions, and inadequate integration of 3D geometric reasoning\nwith linguistic comprehension. To this end, we introduce NuGrounding, the first\nlarge-scale benchmark for multi-view 3D visual grounding in autonomous driving.\nWe present a Hierarchy of Grounding (HoG) method to construct NuGrounding to\ngenerate hierarchical multi-level instructions, ensuring comprehensive coverage\nof human instruction patterns. To tackle this challenging dataset, we propose a\nnovel paradigm that seamlessly combines instruction comprehension abilities of\nmulti-modal LLMs (MLLMs) with precise localization abilities of specialist\ndetection models. Our approach introduces two decoupled task tokens and a\ncontext query to aggregate 3D geometric information and semantic instructions,\nfollowed by a fusion decoder to refine spatial-semantic feature fusion for\nprecise localization. Extensive experiments demonstrate that our method\nsignificantly outperforms the baselines adapted from representative 3D scene\nunderstanding methods by a significant margin and achieves 0.59 in precision\nand 0.64 in recall, with improvements of 50.8% and 54.7%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22437", "pdf": "https://arxiv.org/pdf/2503.22437", "abs": "https://arxiv.org/abs/2503.22437", "authors": ["Xu Wang", "Shuai Zhang", "Baoru Huang", "Danail Stoyanov", "Evangelos B. Mazomenos"], "title": "EndoLRMGS: Complete Endoscopic Scene Reconstruction combining Large Reconstruction Modelling and Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Complete reconstruction of surgical scenes is crucial for robot-assisted\nsurgery (RAS). Deep depth estimation is promising but existing works struggle\nwith depth discontinuities, resulting in noisy predictions at object boundaries\nand do not achieve complete reconstruction omitting occluded surfaces. To\naddress these issues we propose EndoLRMGS, that combines Large Reconstruction\nModelling (LRM) and Gaussian Splatting (GS), for complete surgical scene\nreconstruction. GS reconstructs deformable tissues and LRM generates 3D models\nfor surgical tools while position and scale are subsequently optimized by\nintroducing orthogonal perspective joint projection optimization (OPjPO) to\nenhance accuracy. In experiments on four surgical videos from three public\ndatasets, our method improves the Intersection-over-union (IoU) of tool 3D\nmodels in 2D projections by>40%. Additionally, EndoLRMGS improves the PSNR of\nthe tools projection from 3.82% to 11.07%. Tissue rendering quality also\nimproves, with PSNR increasing from 0.46% to 49.87%, and SSIM from 1.53% to\n29.21% across all test videos.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22513", "pdf": "https://arxiv.org/pdf/2503.22513", "abs": "https://arxiv.org/abs/2503.22513", "authors": ["Martin Kišš", "Michal Hradiš"], "title": "Masked Self-Supervised Pre-Training for Text Recognition Transformers on Large-Scale Datasets", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "18 pages, 7 tables, 6 figures; Submitted to ICDAR25", "summary": "Self-supervised learning has emerged as a powerful approach for leveraging\nlarge-scale unlabeled data to improve model performance in various domains. In\nthis paper, we explore masked self-supervised pre-training for text recognition\ntransformers. Specifically, we propose two modifications to the pre-training\nphase: progressively increasing the masking probability, and modifying the loss\nfunction to incorporate both masked and non-masked patches. We conduct\nextensive experiments using a dataset of 50M unlabeled text lines for\npre-training and four differently sized annotated datasets for fine-tuning.\nFurthermore, we compare our pre-trained models against those trained with\ntransfer learning, demonstrating the effectiveness of the self-supervised\npre-training. In particular, pre-training consistently improves the character\nerror rate of models, in some cases up to 30 % relatively. It is also on par\nwith transfer learning but without relying on extra annotated text lines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22677", "pdf": "https://arxiv.org/pdf/2503.22677", "abs": "https://arxiv.org/abs/2503.22677", "authors": ["Ruining Li", "Chuanxia Zheng", "Christian Rupprecht", "Andrea Vedaldi"], "title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://ruiningli.com/dso", "summary": "Most 3D object generators focus on aesthetic quality, often neglecting\nphysical constraints necessary in applications. One such constraint is that the\n3D object should be self-supporting, i.e., remains balanced under gravity.\nPrior approaches to generating stable 3D objects used differentiable physics\nsimulators to optimize geometry at test-time, which is slow, unstable, and\nprone to local optima. Inspired by the literature on aligning generative models\nto external feedback, we propose Direct Simulation Optimization (DSO), a\nframework to use the feedback from a (non-differentiable) simulator to increase\nthe likelihood that the 3D generator outputs stable 3D objects directly. We\nconstruct a dataset of 3D objects labeled with a stability score obtained from\nthe physics simulator. We can then fine-tune the 3D generator using the\nstability score as the alignment metric, via direct preference optimization\n(DPO) or direct reward optimization (DRO), a novel objective, which we\nintroduce, to align diffusion models without requiring pairwise preferences.\nOur experiments show that the fine-tuned feed-forward generator, using either\nDPO or DRO objective, is much faster and more likely to produce stable objects\nthan test-time optimization. Notably, the DSO framework works even without any\nground-truth 3D objects for training, allowing the 3D generator to self-improve\nby automatically collecting simulation feedback on its own outputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise", "alignment", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22679", "pdf": "https://arxiv.org/pdf/2503.22679", "abs": "https://arxiv.org/abs/2503.22679", "authors": ["Weiqi Li", "Xuanyu Zhang", "Shijie Zhao", "Yabin Zhang", "Junlin Li", "Li Zhang", "Jian Zhang"], "title": "Q-Insight: Understanding Image Quality via Visual Reinforcement Learning", "categories": ["cs.CV"], "comment": "Technical report", "summary": "Image quality assessment (IQA) focuses on the perceptual visual quality of\nimages, playing a crucial role in downstream tasks such as image\nreconstruction, compression, and generation. The rapid advancement of\nmulti-modal large language models (MLLMs) has significantly broadened the scope\nof IQA, moving toward comprehensive image quality understanding that\nincorporates content analysis, degradation perception, and comparison reasoning\nbeyond mere numerical scoring. Previous MLLM-based methods typically either\ngenerate numerical scores lacking interpretability or heavily rely on\nsupervised fine-tuning (SFT) using large-scale annotated datasets to provide\ndescriptive assessments, limiting their flexibility and applicability. In this\npaper, we propose Q-Insight, a reinforcement learning-based model built upon\ngroup relative policy optimization (GRPO), which demonstrates strong visual\nreasoning capability for image quality understanding while requiring only a\nlimited amount of rating scores and degradation labels. By jointly optimizing\nscore regression and degradation perception tasks with carefully designed\nreward functions, our approach effectively exploits their mutual benefits for\nenhanced performance. Extensive experiments demonstrate that Q-Insight\nsubstantially outperforms existing state-of-the-art methods in both score\nregression and degradation perception tasks, while exhibiting impressive\nzero-shot generalization to comparison reasoning tasks. Code will be available\nat https://github.com/lwq20020127/Q-Insight.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization", "comparison"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21860", "pdf": "https://arxiv.org/pdf/2503.21860", "abs": "https://arxiv.org/abs/2503.21860", "authors": ["Kailin Li", "Puhao Li", "Tengyu Liu", "Yuyang Li", "Siyuan Huang"], "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Human hands play a central role in interacting, motivating increasing\nresearch in dexterous robotic manipulation. Data-driven embodied AI algorithms\ndemand precise, large-scale, human-like manipulation sequences, which are\nchallenging to obtain with conventional reinforcement learning or real-world\nteleoperation. To address this, we introduce ManipTrans, a novel two-stage\nmethod for efficiently transferring human bimanual skills to dexterous robotic\nhands in simulation. ManipTrans first pre-trains a generalist trajectory\nimitator to mimic hand motion, then fine-tunes a specific residual module under\ninteraction constraints, enabling efficient learning and accurate execution of\ncomplex bimanual tasks. Experiments show that ManipTrans surpasses\nstate-of-the-art methods in success rate, fidelity, and efficiency. Leveraging\nManipTrans, we transfer multiple hand-object datasets to robotic hands,\ncreating DexManipNet, a large-scale dataset featuring previously unexplored\ntasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K\nepisodes of robotic manipulation and is easily extensible, facilitating further\npolicy training for dexterous hands and enabling real-world deployments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22159", "pdf": "https://arxiv.org/pdf/2503.22159", "abs": "https://arxiv.org/abs/2503.22159", "authors": ["Hao Feng", "Hao Sun", "Wei Xie"], "title": "Disentangled 4D Gaussian Splatting: Towards Faster and More Efficient Dynamic Scene Rendering", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Novel-view synthesis (NVS) for dynamic scenes from 2D images presents\nsignificant challenges due to the spatial complexity and temporal variability\nof such scenes. Recently, inspired by the remarkable success of NVS using 3D\nGaussian Splatting (3DGS), researchers have sought to extend 3D Gaussian models\nto four dimensions (4D) for dynamic novel-view synthesis. However, methods\nbased on 4D rotation and scaling introduce spatiotemporal deformation into the\n4D covariance matrix, necessitating the slicing of 4D Gaussians into 3D\nGaussians. This process increases redundant computations as timestamps\nchange-an inherent characteristic of dynamic scene rendering. Additionally,\nperforming calculations on a four-dimensional matrix is computationally\nintensive. In this paper, we introduce Disentangled 4D Gaussian Splatting\n(Disentangled4DGS), a novel representation and rendering approach that\ndisentangles temporal and spatial deformations, thereby eliminating the\nreliance on 4D matrix computations. We extend the 3DGS rendering process to 4D,\nenabling the projection of temporal and spatial deformations into dynamic 2D\nGaussians in ray space. Consequently, our method facilitates faster dynamic\nscene synthesis. Moreover, it reduces storage requirements by at least 4.5\\%\ndue to our efficient presentation method. Our approach achieves an\nunprecedented average rendering speed of 343 FPS at a resolution of\n$1352\\times1014$ on an RTX 3090 GPU, with experiments across multiple\nbenchmarks demonstrating its competitive performance in both monocular and\nmulti-view scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22178", "pdf": "https://arxiv.org/pdf/2503.22178", "abs": "https://arxiv.org/abs/2503.22178", "authors": ["Chanhyuk Lee", "Jiho Choi", "Chanryeol Lee", "Donggyun Kim", "Seunghoon Hong"], "title": "AdaRank: Adaptive Rank Pruning for Enhanced Model Merging", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Code Available at: https://github.com/david3684/AdaRank", "summary": "Model merging has emerged as a promising approach for unifying independently\nfine-tuned models into an integrated framework, significantly enhancing\ncomputational efficiency in multi-task learning. Recently, several SVD-based\ntechniques have been introduced to exploit low-rank structures for enhanced\nmerging, but their reliance on such manually designed rank selection often\nleads to cross-task interference and suboptimal performance. In this paper, we\npropose AdaRank, a novel model merging framework that adaptively selects the\nmost beneficial singular directions of task vectors to merge multiple models.\nWe empirically show that the dominant singular components of task vectors can\ncause critical interference with other tasks, and that naive truncation across\ntasks and layers degrades performance. In contrast, AdaRank dynamically prunes\nthe singular components that cause interference and offers an optimal amount of\ninformation to each task vector by learning to prune ranks during test-time via\nentropy minimization. Our analysis demonstrates that such method mitigates\ndetrimental overlaps among tasks, while empirical results show that AdaRank\nconsistently achieves state-of-the-art performance with various backbones and\nnumber of tasks, reducing the performance gap between fine-tuned models to\nnearly 1%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22589", "pdf": "https://arxiv.org/pdf/2503.22589", "abs": "https://arxiv.org/abs/2503.22589", "authors": ["Adam Breuer", "Bryce J. Dietrich", "Michael H. Crespin", "Matthew Butler", "J. A. Pyrse", "Kosuke Imai"], "title": "Using AI to Summarize US Presidential Campaign TV Advertisement Videos, 1952-2012", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.LG"], "comment": "17 pages, 7 tables, 4 figures, and linked datasets", "summary": "This paper introduces the largest and most comprehensive dataset of US\npresidential campaign television advertisements, available in digital format.\nThe dataset also includes machine-searchable transcripts and high-quality\nsummaries designed to facilitate a variety of academic research. To date, there\nhas been great interest in collecting and analyzing US presidential campaign\nadvertisements, but the need for manual procurement and annotation led many to\nrely on smaller subsets. We design a large-scale parallelized, AI-based\nanalysis pipeline that automates the laborious process of preparing,\ntranscribing, and summarizing videos. We then apply this methodology to the\n9,707 presidential ads from the Julian P. Kanter Political Commercial Archive.\nWe conduct extensive human evaluations to show that these transcripts and\nsummaries match the quality of manually generated alternatives. We illustrate\nthe value of this data by including an application that tracks the genesis and\nevolution of current focal issue areas over seven decades of presidential\nelections. Our analysis pipeline and codebase also show how to use LLM-based\ntools to obtain high-quality summaries for other video datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22655", "pdf": "https://arxiv.org/pdf/2503.22655", "abs": "https://arxiv.org/abs/2503.22655", "authors": ["Xiaomin Yu", "Pengxiang Ding", "Wenjie Zhang", "Siteng Huang", "Songyang Gao", "Chengwei Qin", "Kejian Wu", "Zhaoxin Fan", "Ziyue Qiao", "Donglin Wang"], "title": "Unicorn: Text-Only Data Synthesis for Vision Language Model Training", "categories": ["cs.AI", "cs.CV", "cs.MM"], "comment": null, "summary": "Training vision-language models (VLMs) typically requires large-scale,\nhigh-quality image-text pairs, but collecting or synthesizing such data is\ncostly. In contrast, text data is abundant and inexpensive, prompting the\nquestion: can high-quality multimodal training data be synthesized purely from\ntext? To tackle this, we propose a cross-integrated three-stage multimodal data\nsynthesis framework, which generates two datasets: Unicorn-1.2M and\nUnicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we\nconstruct 1.2M semantically diverse high-quality captions by expanding sparse\ncaption seeds using large language models (LLMs). In Stage 2:\nInstruction-Tuning Data Generation, we further process 471K captions into\nmulti-turn instruction-tuning tasks to support complex reasoning. Finally, in\nStage 3: Modality Representation Transfer, these textual captions\nrepresentations are transformed into visual representations, resulting in\ndiverse synthetic image representations. This three-stage process enables us to\nconstruct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for\ninstruction-tuning, without relying on real images. By eliminating the\ndependency on real images while maintaining data quality and diversity, our\nframework offers a cost-effective and scalable solution for VLMs training. Code\nis available at https://github.com/Yu-xm/Unicorn.git.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
