{"id": "2503.22732", "pdf": "https://arxiv.org/pdf/2503.22732", "abs": "https://arxiv.org/abs/2503.22732", "authors": ["Mohamed Amine Ferrag", "Norbert Tihanyi", "Merouane Debbah"], "title": "Reasoning Beyond Limits: Advances and Open Problems for LLMs", "categories": ["cs.LG", "cs.CL"], "comment": "41 pages", "summary": "Recent generative reasoning breakthroughs have transformed how large language\nmodels (LLMs) tackle complex problems by dynamically retrieving and refining\ninformation while generating coherent, multi-step thought processes. Techniques\nsuch as inference-time scaling, reinforcement learning, supervised fine-tuning,\nand distillation have been successfully applied to models like DeepSeek-R1,\nOpenAI's o1 & o3, GPT-4o, Qwen-32B, and various Llama variants, resulting in\nenhanced reasoning capabilities. In this paper, we provide a comprehensive\nanalysis of the top 27 LLM models released between 2023 and 2025 (including\nmodels such as Mistral AI Small 3 24B, DeepSeek-R1, Search-o1, QwQ-32B, and\nphi-4). Then, we present an extensive overview of training methodologies that\nspans general training approaches, mixture-of-experts (MoE) and architectural\ninnovations, retrieval-augmented generation (RAG), chain-of-thought and\nself-improvement techniques, as well as test-time compute scaling,\ndistillation, and reinforcement learning (RL) methods. Finally, we discuss the\nkey challenges in advancing LLM capabilities, including improving multi-step\nreasoning without human supervision, overcoming limitations in chained tasks,\nbalancing structured prompts with flexibility, and enhancing long-context\nretrieval and external tool integration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time", "scaling", "compute scaling", "test-time compute", "o1"], "score": 6}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24320", "pdf": "https://arxiv.org/pdf/2503.24320", "abs": "https://arxiv.org/abs/2503.24320", "authors": ["Wenyan Cong", "Hanqing Zhu", "Peihao Wang", "Bangya Liu", "Dejia Xu", "Kevin Wang", "David Z. Pan", "Yan Wang", "Zhiwen Fan", "Zhangyang Wang"], "title": "Can Test-Time Scaling Improve World Foundation Model?", "categories": ["cs.CV"], "comment": null, "summary": "World foundation models, which simulate the physical world by predicting\nfuture states from current observations and inputs, have become central to many\napplications in physical intelligence, including autonomous driving and\nrobotics. However, these models require substantial computational resources for\npretraining and are further constrained by available data during post-training.\nAs such, scaling computation at test time emerges as both a critical and\npractical alternative to traditional model enlargement or re-training. In this\nwork, we introduce SWIFT, a test-time scaling framework tailored for WFMs.\nSWIFT integrates our extensible WFM evaluation toolkit with process-level\ninference strategies, including fast tokenization, probability-based Top-K\npruning, and efficient beam search. Empirical results on the COSMOS model\ndemonstrate that test-time scaling exists even in a compute-optimal way. Our\nfindings reveal that test-time scaling laws hold for WFMs and that SWIFT\nprovides a scalable and effective pathway for improving WFM inference without\nretraining or increasing model size. The code is available at\nhttps://github.com/Mia-Cong/SWIFT.git.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time", "scaling", "beam search"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24235", "pdf": "https://arxiv.org/pdf/2503.24235", "abs": "https://arxiv.org/abs/2503.24235", "authors": ["Qiyuan Zhang", "Fuyuan Lyu", "Zexu Sun", "Lei Wang", "Weixu Zhang", "Zhihan Guo", "Yufei Wang", "Irwin King", "Xue Liu", "Chen Ma"], "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23371", "pdf": "https://arxiv.org/pdf/2503.23371", "abs": "https://arxiv.org/abs/2503.23371", "authors": ["Jeonghyun Ko", "Gyeongyun Park", "Donghoon Lee", "Kyunam Lee"], "title": "FeRG-LLM : Feature Engineering by Reason Generation Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL 2025 Findings", "summary": "One of the key tasks in machine learning for tabular data is feature\nengineering. Although it is vital for improving the performance of models, it\ndemands considerable human expertise and deep domain knowledge, making it\nlabor-intensive endeavor. To address this issue, we propose a novel framework,\n\\textbf{FeRG-LLM} (\\textbf{Fe}ature engineering by \\textbf{R}eason\n\\textbf{G}eneration \\textbf{L}arge \\textbf{L}anguage \\textbf{M}odels), a large\nlanguage model designed to automatically perform feature engineering at an\n8-billion-parameter scale. We have constructed two-stage conversational\ndialogues that enable language models to analyze machine learning tasks and\ndiscovering new features, exhibiting their Chain-of-Thought (CoT) capabilities.\nWe use these dialogues to fine-tune Llama 3.1 8B model and integrate Direct\nPreference Optimization (DPO) to receive feedback improving quality of new\nfeatures and the model's performance. Our experiments show that FeRG-LLM\nperforms comparably to or better than Llama 3.1 70B on most datasets, while\nusing fewer resources and achieving reduced inference time. It outperforms\nother studies in classification tasks and performs well in regression tasks.\nMoreover, since it does not rely on cloud-hosted LLMs like GPT-4 with extra API\ncosts when generating features, it can be deployed locally, addressing security\nconcerns.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23257", "pdf": "https://arxiv.org/pdf/2503.23257", "abs": "https://arxiv.org/abs/2503.23257", "authors": ["Mohammadmahdi Honarmand", "Onur Cezmi Mutlu", "Parnian Azizian", "Saimourya Surabhi", "Dennis P. Wall"], "title": "FIESTA: Fisher Information-based Efficient Selective Test-time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Robust facial expression recognition in unconstrained, \"in-the-wild\"\nenvironments remains challenging due to significant domain shifts between\ntraining and testing distributions. Test-time adaptation (TTA) offers a\npromising solution by adapting pre-trained models during inference without\nrequiring labeled test data. However, existing TTA approaches typically rely on\nmanually selecting which parameters to update, potentially leading to\nsuboptimal adaptation and high computational costs. This paper introduces a\nnovel Fisher-driven selective adaptation framework that dynamically identifies\nand updates only the most critical model parameters based on their importance\nas quantified by Fisher information. By integrating this principled parameter\nselection approach with temporal consistency constraints, our method enables\nefficient and effective adaptation specifically tailored for video-based facial\nexpression recognition. Experiments on the challenging AffWild2 benchmark\ndemonstrate that our approach significantly outperforms existing TTA methods,\nachieving a 7.7% improvement in F1 score over the base model while adapting\nonly 22,000 parameters-more than 20 times fewer than comparable methods. Our\nablation studies further reveal that parameter importance can be effectively\nestimated from minimal data, with sampling just 1-3 frames sufficient for\nsubstantial performance gains. The proposed approach not only enhances\nrecognition accuracy but also dramatically reduces computational overhead,\nmaking test-time adaptation more practical for real-world affective computing\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23282", "pdf": "https://arxiv.org/pdf/2503.23282", "abs": "https://arxiv.org/abs/2503.23282", "authors": ["Felix Wimbauer", "Weirong Chen", "Dominik Muhle", "Christian Rupprecht", "Daniel Cremers"], "title": "AnyCam: Learning to Recover Camera Poses and Intrinsics from Casual Videos", "categories": ["cs.CV"], "comment": "CVPR 2025 - For more details and code, please check out our project\n  page under https://fwmb.github.io/anycam", "summary": "Estimating camera motion and intrinsics from casual videos is a core\nchallenge in computer vision. Traditional bundle-adjustment based methods, such\nas SfM and SLAM, struggle to perform reliably on arbitrary data. Although\nspecialized SfM approaches have been developed for handling dynamic scenes,\nthey either require intrinsics or computationally expensive test-time\noptimization and often fall short in performance. Recently, methods like Dust3r\nhave reformulated the SfM problem in a more data-driven way. While such\ntechniques show promising results, they are still 1) not robust towards dynamic\nobjects and 2) require labeled data for supervised training. As an alternative,\nwe propose AnyCam, a fast transformer model that directly estimates camera\nposes and intrinsics from a dynamic video sequence in feed-forward fashion. Our\nintuition is that such a network can learn strong priors over realistic camera\nposes. To scale up our training, we rely on an uncertainty-based loss\nformulation and pre-trained depth and flow networks instead of motion or\ntrajectory supervision. This allows us to use diverse, unlabelled video\ndatasets obtained mostly from YouTube. Additionally, we ensure that the\npredicted trajectory does not accumulate drift over time through a lightweight\ntrajectory refinement step. We test AnyCam on established datasets, where it\ndelivers accurate camera poses and intrinsics both qualitatively and\nquantitatively. Furthermore, even with trajectory refinement, AnyCam is\nsignificantly faster than existing works for SfM in dynamic settings. Finally,\nby combining camera information, uncertainty, and depth, our model can produce\nhigh-quality 4D pointclouds.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scale"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23924", "pdf": "https://arxiv.org/pdf/2503.23924", "abs": "https://arxiv.org/abs/2503.23924", "authors": ["Ziyang Ma", "Zuchao Li", "Lefei Zhang", "Gui-Song Xia", "Bo Du", "Liangpei Zhang", "Dacheng Tao"], "title": "Model Hemorrhage and the Robustness Limits of Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "33 pages, 18 figures", "summary": "Large language models (LLMs) demonstrate strong performance across natural\nlanguage processing tasks, yet undergo significant performance degradation when\nmodified for deployment through quantization, pruning, or decoding strategy\nadjustments. We define this phenomenon as model hemorrhage - performance\ndecline caused by parameter alterations and architectural changes. Through\nsystematic analysis of various LLM frameworks, we identify key vulnerability\npatterns: layer expansion frequently disrupts attention mechanisms, compression\ntechniques induce information loss cascades, and decoding adjustments amplify\nprediction divergences. Our investigation reveals transformer architectures\nexhibit inherent robustness thresholds that determine hemorrhage severity\nacross modification types. We propose three mitigation strategies:\ngradient-aware pruning preserves critical weight pathways, dynamic quantization\nscaling maintains activation integrity, and decoding calibration aligns\ngeneration trajectories with original model distributions. This work\nestablishes foundational metrics for evaluating model stability during\nadaptation, providing practical guidelines for maintaining performance while\nenabling efficient LLM deployment. Our findings advance understanding of neural\nnetwork resilience under architectural transformations, particularly for\nlarge-scale language models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23367", "pdf": "https://arxiv.org/pdf/2503.23367", "abs": "https://arxiv.org/abs/2503.23367", "authors": ["Hang Guo", "Yawei Li", "Taolin Zhang", "Jiangshan Wang", "Tao Dai", "Shu-Tao Xia", "Luca Benini"], "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23388", "pdf": "https://arxiv.org/pdf/2503.23388", "abs": "https://arxiv.org/abs/2503.23388", "authors": ["Fanding Huang", "Jingyan Jiang", "Qinting Jiang", "Hebei Li", "Faisal Nadeem Khan", "Zhi Wang"], "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted to CVPR 2025", "summary": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23450", "pdf": "https://arxiv.org/pdf/2503.23450", "abs": "https://arxiv.org/abs/2503.23450", "authors": ["Bohao Xing", "Kaishen Yuan", "Zitong Yu", "Xin Liu", "Heikki Kälviäinen"], "title": "AU-TTT: Vision Test-Time Training model for Facial Action Unit Detection", "categories": ["cs.CV"], "comment": null, "summary": "Facial Action Units (AUs) detection is a cornerstone of objective facial\nexpression analysis and a critical focus in affective computing. Despite its\nimportance, AU detection faces significant challenges, such as the high cost of\nAU annotation and the limited availability of datasets. These constraints often\nlead to overfitting in existing methods, resulting in substantial performance\ndegradation when applied across diverse datasets. Addressing these issues is\nessential for improving the reliability and generalizability of AU detection\nmethods. Moreover, many current approaches leverage Transformers for their\neffectiveness in long-context modeling, but they are hindered by the quadratic\ncomplexity of self-attention. Recently, Test-Time Training (TTT) layers have\nemerged as a promising solution for long-sequence modeling. Additionally, TTT\napplies self-supervised learning for iterative updates during both training and\ninference, offering a potential pathway to mitigate the generalization\nchallenges inherent in AU detection tasks. In this paper, we propose a novel\nvision backbone tailored for AU detection, incorporating bidirectional TTT\nblocks, named AU-TTT. Our approach introduces TTT Linear to the AU detection\ntask and optimizes image scanning mechanisms for enhanced performance.\nAdditionally, we design an AU-specific Region of Interest (RoI) scanning\nmechanism to capture fine-grained facial features critical for AU detection.\nExperimental results demonstrate that our method achieves competitive\nperformance in both within-domain and cross-domain scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time training"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "reliability", "fine-grained"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22832", "pdf": "https://arxiv.org/pdf/2503.22832", "abs": "https://arxiv.org/abs/2503.22832", "authors": ["Simeng Sun", "Cheng-Ping Hsieh", "Faisal Ladhak", "Erik Arakelyan", "Santiago Akle Serano", "Boris Ginsburg"], "title": "L0-Reasoning Bench: Evaluating Procedural Correctness in Language Models via Simple Program Execution", "categories": ["cs.PL", "cs.CL"], "comment": null, "summary": "Complex reasoning tasks often rely on the ability to consistently and\naccurately apply simple rules across incremental steps, a foundational\ncapability which we term \"level-0\" reasoning. To systematically evaluate this\ncapability, we introduce L0-Bench, a language model benchmark for testing\nprocedural correctness -- the ability to generate correct reasoning processes,\ncomplementing existing benchmarks that primarily focus on outcome correctness.\nGiven synthetic Python functions with simple operations, L0-Bench grades models\non their ability to generate step-by-step, error-free execution traces. The\nsynthetic nature of L0-Bench enables systematic and scalable generation of test\nprograms along various axes (e.g., number of trace steps). We evaluate a\ndiverse array of recent closed-source and open-weight models on a baseline test\nset. All models exhibit degradation as the number of target trace steps\nincreases, while larger models and reasoning-enhanced models better maintain\ncorrectness over multiple steps. Additionally, we use L0-Bench to explore\ntest-time scaling along three dimensions: input context length, number of\nsolutions for majority voting, and inference steps. Our results suggest\nsubstantial room to improve \"level-0\" reasoning and potential directions to\nbuild more reliable reasoning systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23495", "pdf": "https://arxiv.org/pdf/2503.23495", "abs": "https://arxiv.org/abs/2503.23495", "authors": ["Ashim Dahal", "Saydul Akbar Murad", "Nick Rahimi"], "title": "Embedding Shift Dissection on CLIP: Effects of Augmentations on VLM's Representation Learning", "categories": ["cs.CV"], "comment": "accepted at MIV at CVPR 2025", "summary": "Understanding the representation shift on Vision Language Models like CLIP\nunder different augmentations provides valuable insights on Mechanistic\nInterpretability. In this study, we show the shift on CLIP's embeddings on 9\ncommon augmentation techniques: noise, blur, color jitter, scale and rotate,\nflip, elastic and perspective transforms, random brightness and contrast, and\ncoarse dropout of pixel blocks. We scrutinize the embedding shifts under\nsimilarity on attention map, patch, edge, detail preservation, cosine\nsimilarity, L2 distance, pairwise distance and dendrogram clusters and provide\nqualitative analysis on sample images. Our findings suggest certain\naugmentations like noise, perspective transform and shift scaling have higher\ndegree of drastic impact on embedding shift. This study provides a concrete\nfoundation for future work on VLM's robustness for mechanical interpretation\nand adversarial data defense.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23145", "pdf": "https://arxiv.org/pdf/2503.23145", "abs": "https://arxiv.org/abs/2503.23145", "authors": ["Anjiang Wei", "Tarun Suresh", "Jiannan Cao", "Naveen Kannan", "Yuheng Wu", "Kai Yan", "Thiago S. F. X. Teixeira", "Ke Wang", "Alex Aiken"], "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Inductive program synthesis, or programming by example, requires synthesizing\nfunctions from input-output examples that generalize to unseen inputs. While\nlarge language model agents have shown promise in programming tasks guided by\nnatural language, their ability to perform inductive program synthesis is\nunderexplored. Existing evaluation protocols rely on static sets of examples\nand held-out tests, offering no feedback when synthesized functions are\nincorrect and failing to reflect real-world scenarios such as reverse\nengineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge,\na new evaluation framework where agents interact with a hidden target function\nby querying it with new inputs, synthesizing candidate functions, and\niteratively refining their solutions using a differential testing oracle. This\ninteractive setting encourages agents to perform function calls and\nself-correction based on feedback. We construct the first large-scale benchmark\nfor general-purpose inductive program synthesis, featuring 1114 functions.\nAmong 18 models evaluated, o3-mini performs best with a success rate of 52.7%,\nhighlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on\ncurated synthesis traces yields up to a 31% relative performance gain. CodeARC\nprovides a more realistic and challenging testbed for evaluating LLM-based\nprogram synthesis and inductive reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "self-correction"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "testbed"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24290", "pdf": "https://arxiv.org/pdf/2503.24290", "abs": "https://arxiv.org/abs/2503.24290", "authors": ["Jingcheng Hu", "Yinmin Zhang", "Qi Han", "Daxin Jiang", "Xiangyu Zhang", "Heung-Yeung Shum"], "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of\nlarge-scale reasoning-oriented RL training focusing on scalability, simplicity\nand accessibility. Through extensive experiments, we demonstrate that a\nminimalist approach, vanilla PPO with GAE ($\\lambda=1$, $\\gamma=1$) and\nstraightforward rule-based rewards, without any KL regularization, is\nsufficient to scale up both response length and benchmark performance, similar\nto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as\nDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on\nAIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating\nremarkable efficiency -- requiring only a tenth of the training steps, compared\nto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our\nsource code, parameter settings, training data, and model weights across\nvarious sizes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24376", "pdf": "https://arxiv.org/pdf/2503.24376", "abs": "https://arxiv.org/abs/2503.24376", "authors": ["Yi Chen", "Yuying Ge", "Rui Wang", "Yixiao Ge", "Lu Qiu", "Ying Shan", "Xihui Liu"], "title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Technical Report (In Progress); Code released at:\n  https://github.com/TencentARC/SEED-Bench-R1", "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "chain of thought"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24388", "pdf": "https://arxiv.org/pdf/2503.24388", "abs": "https://arxiv.org/abs/2503.24388", "authors": ["Zhonghan Zhao", "Wenwei Zhang", "Haian Huang", "Kuikun Liu", "Jianfei Gao", "Gaoang Wang", "Kai Chen"], "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than $17\\times$ sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23947", "pdf": "https://arxiv.org/pdf/2503.23947", "abs": "https://arxiv.org/abs/2503.23947", "authors": ["Guhnoo Yun", "Juhan Yoo", "Kijung Kim", "Jeongho Lee", "Paul Hongsuck Seo", "Dong Hwan Kim"], "title": "Spectral-Adaptive Modulation Networks for Visual Perception", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies have shown that 2D convolution and self-attention exhibit\ndistinct spectral behaviors, and optimizing their spectral properties can\nenhance vision model performance. However, theoretical analyses remain limited\nin explaining why 2D convolution is more effective in high-pass filtering than\nself-attention and why larger kernels favor shape bias, akin to self-attention.\nIn this paper, we employ graph spectral analysis to theoretically simulate and\ncompare the frequency responses of 2D convolution and self-attention within a\nunified framework. Our results corroborate previous empirical findings and\nreveal that node connectivity, modulated by window size, is a key factor in\nshaping spectral functions. Leveraging this insight, we introduce a\n\\textit{spectral-adaptive modulation} (SPAM) mixer, which processes visual\nfeatures in a spectral-adaptive manner using multi-scale convolutional kernels\nand a spectral re-scaling mechanism to refine spectral components. Based on\nSPAM, we develop SPANetV2 as a novel vision backbone. Extensive experiments\ndemonstrate that SPANetV2 outperforms state-of-the-art models across multiple\nvision tasks, including ImageNet-1K classification, COCO object detection, and\nADE20K semantic segmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24376", "pdf": "https://arxiv.org/pdf/2503.24376", "abs": "https://arxiv.org/abs/2503.24376", "authors": ["Yi Chen", "Yuying Ge", "Rui Wang", "Yixiao Ge", "Lu Qiu", "Ying Shan", "Xihui Liu"], "title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Technical Report (In Progress); Code released at:\n  https://github.com/TencentARC/SEED-Bench-R1", "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "chain of thought"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23219", "pdf": "https://arxiv.org/pdf/2503.23219", "abs": "https://arxiv.org/abs/2503.23219", "authors": ["Sanjoy Chowdhury", "Hanan Gani", "Nishit Anand", "Sayan Nag", "Ruohan Gao", "Mohamed Elhoseiny", "Salman Khan", "Dinesh Manocha"], "title": "Aurelia: Test-time Reasoning Distillation in Audio-Visual LLMs", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in reasoning optimization have greatly enhanced the\nperformance of large language models (LLMs). However, existing work fails to\naddress the complexities of audio-visual scenarios, underscoring the need for\nfurther research. In this paper, we introduce AURELIA, a novel actor-critic\nbased audio-visual (AV) reasoning framework that distills structured,\nstep-by-step reasoning into AVLLMs at test time, improving their ability to\nprocess complex multi-modal inputs without additional training or fine-tuning.\nTo further advance AVLLM reasoning skills, we present AVReasonBench, a\nchallenging benchmark comprising 4500 audio-visual questions, each paired with\ndetailed step-by-step reasoning. Our benchmark spans six distinct tasks,\nincluding AV-GeoIQ, which evaluates AV reasoning combined with geographical and\ncultural knowledge. Evaluating 18 AVLLMs on AVReasonBench reveals significant\nlimitations in their multi-modal reasoning capabilities. Using AURELIA, we\nachieve up to a 100% relative improvement, demonstrating its effectiveness.\nThis performance gain highlights the potential of reasoning-enhanced data\ngeneration for advancing AVLLMs in real-world applications. Our code and data\nwill be publicly released at: https: //github.com/schowdhury671/aurelia.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23893", "pdf": "https://arxiv.org/pdf/2503.23893", "abs": "https://arxiv.org/abs/2503.23893", "authors": ["Maximilian Springenberg", "Noelia Otero", "Yuxin Xue", "Jackie Ma"], "title": "DiffScale: Continuous Downscaling and Bias Correction of Subseasonal Wind Speed Forecasts using Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.IV"], "comment": "28 pages, 18 figures, preprint under review", "summary": "Renewable resources are strongly dependent on local and large-scale weather\nsituations. Skillful subseasonal to seasonal (S2S) forecasts -- beyond two\nweeks and up to two months -- can offer significant socioeconomic advantages to\nthe energy sector. This study aims to enhance wind speed predictions using a\ndiffusion model with classifier-free guidance to downscale S2S forecasts of\nsurface wind speed. We propose DiffScale, a diffusion model that super-resolves\nspatial information for continuous downscaling factors and lead times.\nLeveraging weather priors as guidance for the generative process of diffusion\nmodels, we adopt the perspective of conditional probabilities on sampling\nsuper-resolved S2S forecasts. We aim to directly estimate the density\nassociated with the target S2S forecasts at different spatial resolutions and\nlead times without auto-regression or sequence prediction, resulting in an\nefficient and flexible model. Synthetic experiments were designed to\nsuper-resolve wind speed S2S forecasts from the European Center for\nMedium-Range Weather Forecast (ECMWF) from a coarse resolution to a finer\nresolution of ERA5 reanalysis data, which serves as a high-resolution target.\nThe innovative aspect of DiffScale lies in its flexibility to downscale\narbitrary scaling factors, enabling it to generalize across various grid\nresolutions and lead times -without retraining the model- while correcting\nmodel errors, making it a versatile tool for improving S2S wind speed\nforecasts. We achieve a significant improvement in prediction quality,\noutperforming baselines up to week 3.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22727", "pdf": "https://arxiv.org/pdf/2503.22727", "abs": "https://arxiv.org/abs/2503.22727", "authors": ["Alejandro Lozano", "Min Woo Sun", "James Burgess", "Jeffrey J. Nirschl", "Christopher Polzak", "Yuhui Zhang", "Liangyu Chen", "Jeffrey Gu", "Ivan Lopez", "Josiah Aklilu", "Anita Rau", "Austin Wolfgang Katzer", "Collin Chiu", "Orr Zohar", "Xiaohan Wang", "Alfred Seunghoon Song", "Chiang Chia-Chun", "Robert Tibshirani", "Serena Yeung-Levy"], "title": "A Large-Scale Vision-Language Dataset Derived from Open Scientific Literature to Advance Biomedical Generalist AI", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite the excitement behind biomedical artificial intelligence (AI), access\nto high-quality, diverse, and large-scale data - the foundation for modern AI\nsystems - is still a bottleneck to unlocking its full potential. To address\nthis gap, we introduce Biomedica, an open-source dataset derived from the\nPubMed Central Open Access subset, containing over 6 million scientific\narticles and 24 million image-text pairs, along with 27 metadata fields\n(including expert human annotations). To overcome the challenges of accessing\nour large-scale dataset, we provide scalable streaming and search APIs through\na web server, facilitating seamless integration with AI systems. We demonstrate\nthe utility of the Biomedica dataset by building embedding models, chat-style\nmodels, and retrieval-augmented chat agents. Notably, all our AI models surpass\nprevious open systems in their respective categories, underscoring the critical\nrole of diverse, high-quality, and large-scale biomedical data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22862", "pdf": "https://arxiv.org/pdf/2503.22862", "abs": "https://arxiv.org/abs/2503.22862", "authors": ["Soumitri Chattopadhyay", "Basar Demir", "Marc Niethammer"], "title": "Zero-shot Domain Generalization of Foundational Models for 3D Medical Image Segmentation: An Experimental Study", "categories": ["cs.CV"], "comment": null, "summary": "Domain shift, caused by variations in imaging modalities and acquisition\nprotocols, limits model generalization in medical image segmentation. While\nfoundation models (FMs) trained on diverse large-scale data hold promise for\nzero-shot generalization, their application to volumetric medical data remains\nunderexplored. In this study, we examine their ability towards domain\ngeneralization (DG), by conducting a comprehensive experimental study\nencompassing 6 medical segmentation FMs and 12 public datasets spanning\nmultiple modalities and anatomies. Our findings reveal the potential of\npromptable FMs in bridging the domain gap via smart prompting techniques.\nAdditionally, by probing into multiple facets of zero-shot DG, we offer\nvaluable insights into the viability of FMs for DG and identify promising\navenues for future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22948", "pdf": "https://arxiv.org/pdf/2503.22948", "abs": "https://arxiv.org/abs/2503.22948", "authors": ["Tianyang Xu", "Xiaoze Liu", "Feijie Wu", "Xiaoqian Wang", "Jing Gao"], "title": "SUV: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have transformed natural language processing by\nlearning from massive datasets, yet this rapid progress has also drawn legal\nscrutiny, as the ability to unintentionally generate copyrighted content has\nalready prompted several prominent lawsuits. In this work, we introduce SUV\n(Selective Unlearning for Verbatim data), a selective unlearning framework\ndesigned to prevent LLM from memorizing copyrighted content while preserving\nits overall utility. In detail, the proposed method constructs a dataset that\ncaptures instances of copyrighted infringement cases by the targeted LLM. With\nthe dataset, we unlearn the content from the LLM by means of Direct Preference\nOptimization (DPO), which replaces the verbatim copyrighted content with\nplausible and coherent alternatives. Since DPO may hinder the LLM's performance\nin other unrelated tasks, we integrate gradient projection and Fisher\ninformation regularization to mitigate the degradation. We validate our\napproach using a large-scale dataset of 500 famous books (predominantly\ncopyrighted works) and demonstrate that SUV significantly reduces verbatim\nmemorization with negligible impact on the performance on unrelated tasks.\nExtensive experiments on both our dataset and public benchmarks confirm the\nscalability and efficacy of our approach, offering a promising solution for\nmitigating copyright risks in real-world LLM applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22890", "pdf": "https://arxiv.org/pdf/2503.22890", "abs": "https://arxiv.org/abs/2503.22890", "authors": ["Ke Zhang", "Vishal M. Patel"], "title": "MedCL: Learning Consistent Anatomy Distribution for Scribble-supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Curating large-scale fully annotated datasets is expensive, laborious, and\ncumbersome, especially for medical images. Several methods have been proposed\nin the literature that make use of weak annotations in the form of scribbles.\nHowever, these approaches require large amounts of scribble annotations, and\nare only applied to the segmentation of regular organs, which are often\nunavailable for the disease species that fall in the long-tailed distribution.\nMotivated by the fact that the medical labels have anatomy distribution priors,\nwe propose a scribble-supervised clustering-based framework, called MedCL, to\nlearn the inherent anatomy distribution of medical labels. Our approach\nconsists of two steps: i) Mix the features with intra- and inter-image mix\noperations, and ii) Perform feature clustering and regularize the anatomy\ndistribution at both local and global levels. Combined with a small amount of\nweak supervision, the proposed MedCL is able to segment both regular organs and\nchallenging irregular pathologies. We implement MedCL based on SAM and UNet\nbackbones, and evaluate the performance on three open datasets of regular\nstructure (MSCMRseg), multiple organs (BTCV) and irregular pathology (MyoPS).\nIt is shown that even with less scribble supervision, MedCL substantially\noutperforms the conventional segmentation methods. Our code is available at\nhttps://github.com/BWGZK/MedCL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23029", "pdf": "https://arxiv.org/pdf/2503.23029", "abs": "https://arxiv.org/abs/2503.23029", "authors": ["Yichun Feng", "Jiawei Wang", "Ruikun He", "Lu Zhou", "Yixue Li"], "title": "A Retrieval-Augmented Knowledge Mining Method with Deep Thinking LLMs for Biomedical Research and Clinical Support", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge graphs and large language models (LLMs) are key tools for\nbiomedical knowledge integration and reasoning, facilitating structured\norganization of scientific articles and discovery of complex semantic\nrelationships. However, current methods face challenges: knowledge graph\nconstruction is limited by complex terminology, data heterogeneity, and rapid\nknowledge evolution, while LLMs show limitations in retrieval and reasoning,\nmaking it difficult to uncover cross-document associations and reasoning\npathways. To address these issues, we propose a pipeline that uses LLMs to\nconstruct a biomedical knowledge graph (BioStrataKG) from large-scale articles\nand builds a cross-document question-answering dataset (BioCDQA) to evaluate\nlatent knowledge retrieval and multi-hop reasoning. We then introduce\nIntegrated and Progressive Retrieval-Augmented Reasoning (IP-RAR) to enhance\nretrieval accuracy and knowledge reasoning. IP-RAR maximizes information recall\nthrough Integrated Reasoning-based Retrieval and refines knowledge via\nProgressive Reasoning-based Generation, using self-reflection to achieve deep\nthinking and precise contextual understanding. Experiments show that IP-RAR\nimproves document retrieval F1 score by 20\\% and answer generation accuracy by\n25\\% over existing methods. This framework helps doctors efficiently integrate\ntreatment evidence for personalized medication plans and enables researchers to\nanalyze advancements and research gaps, accelerating scientific discovery and\ndecision-making.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23077", "pdf": "https://arxiv.org/pdf/2503.23077", "abs": "https://arxiv.org/abs/2503.23077", "authors": ["Yue Liu", "Jiaying Wu", "Yufei He", "Hongcheng Gao", "Hongyu Chen", "Baolong Bi", "Jiaheng Zhang", "Zhiqi Huang", "Bryan Hooi"], "title": "Efficient Inference for Large Reasoning Models: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfield\\footnote{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22976", "pdf": "https://arxiv.org/pdf/2503.22976", "abs": "https://arxiv.org/abs/2503.22976", "authors": ["Jiahui Zhang", "Yurui Chen", "Yanpeng Zhou", "Yueming Xu", "Ze Huang", "Jilin Mei", "Junhui Chen", "Yu-Jie Yuan", "Xinyue Cai", "Guowei Huang", "Xingyue Quan", "Hang Xu", "Li Zhang"], "title": "From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D", "categories": ["cs.CV"], "comment": "Project page: https://fudan-zvg.github.io/spar", "summary": "Recent advances in LVLMs have improved vision-language understanding, but\nthey still struggle with spatial perception, limiting their ability to reason\nabout complex 3D scenes. Unlike previous approaches that incorporate 3D\nrepresentations into models to improve spatial understanding, we aim to unlock\nthe potential of VLMs by leveraging spatially relevant image data. To this end,\nwe introduce a novel 2D spatial data generation and annotation pipeline built\nupon scene data with 3D ground-truth. This pipeline enables the creation of a\ndiverse set of spatial tasks, ranging from basic perception tasks to more\ncomplex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a\nlarge-scale dataset generated from thousands of scenes across multiple public\ndatasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a\nmore comprehensive evaluation of spatial capabilities compared to existing\nspatial benchmarks, supporting both single-view and multi-view inputs. Training\non both SPAR-7M and large-scale 2D datasets enables our models to achieve\nstate-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on\n3D task-specific datasets yields competitive results, underscoring the\neffectiveness of our dataset in enhancing spatial reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "annotation"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22984", "pdf": "https://arxiv.org/pdf/2503.22984", "abs": "https://arxiv.org/abs/2503.22984", "authors": ["Zhuowei Li", "Tianchen Zhao", "Xiang Xu", "Zheng Zhang", "Zhihua Li", "Xuanbai Chen", "Qin Zhang", "Alessandro Bergamo", "Anil K. Jain", "Yifan Xing"], "title": "Optimal Transport-Guided Source-Free Adaptation for Face Anti-Spoofing", "categories": ["cs.CV", "I.5.4; I.2.10; I.4.8; I.2.6; C.3"], "comment": "15 pages, 7 figures", "summary": "Developing a face anti-spoofing model that meets the security requirements of\nclients worldwide is challenging due to the domain gap between training\ndatasets and diverse end-user test data. Moreover, for security and privacy\nreasons, it is undesirable for clients to share a large amount of their face\ndata with service providers. In this work, we introduce a novel method in which\nthe face anti-spoofing model can be adapted by the client itself to a target\ndomain at test time using only a small sample of data while keeping model\nparameters and training data inaccessible to the client. Specifically, we\ndevelop a prototype-based base model and an optimal transport-guided adaptor\nthat enables adaptation in either a lightweight training or training-free\nfashion, without updating base model's parameters. Furthermore, we propose\ngeodesic mixup, an optimal transport-based synthesis method that generates\naugmented training data along the geodesic path between source prototypes and\ntarget data distribution. This allows training a lightweight classifier to\neffectively adapt to target-specific characteristics while retaining essential\nknowledge learned from the source domain. In cross-domain and cross-attack\nsettings, compared with recent methods, our method achieves average relative\nimprovements of 19.17% in HTER and 8.58% in AUC, respectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22986", "pdf": "https://arxiv.org/pdf/2503.22986", "abs": "https://arxiv.org/abs/2503.22986", "authors": ["Yunsong Wang", "Tianxin Huang", "Hanlin Chen", "Gim Hee Lee"], "title": "FreeSplat++: Generalizable 3D Gaussian Splatting for Efficient Indoor Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Recently, the integration of the efficient feed-forward scheme into 3D\nGaussian Splatting (3DGS) has been actively explored. However, most existing\nmethods focus on sparse view reconstruction of small regions and cannot produce\neligible whole-scene reconstruction results in terms of either quality or\nefficiency. In this paper, we propose FreeSplat++, which focuses on extending\nthe generalizable 3DGS to become an alternative approach to large-scale indoor\nwhole-scene reconstruction, which has the potential of significantly\naccelerating the reconstruction speed and improving the geometric accuracy. To\nfacilitate whole-scene reconstruction, we initially propose the Low-cost\nCross-View Aggregation framework to efficiently process extremely long input\nsequences. Subsequently, we introduce a carefully designed pixel-wise triplet\nfusion method to incrementally aggregate the overlapping 3D Gaussian primitives\nfrom multiple views, adaptively reducing their redundancy. Furthermore, we\npropose a weighted floater removal strategy that can effectively reduce\nfloaters, which serves as an explicit depth fusion approach that is crucial in\nwhole-scene reconstruction. After the feed-forward reconstruction of 3DGS\nprimitives, we investigate a depth-regularized per-scene fine-tuning process.\nLeveraging the dense, multi-view consistent depth maps obtained during the\nfeed-forward prediction phase for an extra constraint, we refine the entire\nscene's 3DGS primitive to enhance rendering quality while preserving geometric\naccuracy. Extensive experiments confirm that our FreeSplat++ significantly\noutperforms existing generalizable 3DGS methods, especially in whole-scene\nreconstructions. Compared to conventional per-scene optimized 3DGS approaches,\nour method with depth-regularized per-scene fine-tuning demonstrates\nsubstantial improvements in reconstruction accuracy and a notable reduction in\ntraining time.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23044", "pdf": "https://arxiv.org/pdf/2503.23044", "abs": "https://arxiv.org/abs/2503.23044", "authors": ["Yuanyuan Gao", "Hao Li", "Jiaqi Chen", "Zhengyu Zou", "Zhihang Zhong", "Dingwen Zhang", "Xiao Sun", "Junwei Han"], "title": "CityGS-X: A Scalable Architecture for Efficient and Geometrically Accurate Large-Scale Scene Reconstruction", "categories": ["cs.CV"], "comment": "Project page: https://lifuguan.github.io/CityGS-X/", "summary": "Despite its significant achievements in large-scale scene reconstruction, 3D\nGaussian Splatting still faces substantial challenges, including slow\nprocessing, high computational costs, and limited geometric accuracy. These\ncore issues arise from its inherently unstructured design and the absence of\nefficient parallelization. To overcome these challenges simultaneously, we\nintroduce CityGS-X, a scalable architecture built on a novel parallelized\nhybrid hierarchical 3D representation (PH^2-3D). As an early attempt, CityGS-X\nabandons the cumbersome merge-and-partition process and instead adopts a\nnewly-designed batch-level multi-task rendering process. This architecture\nenables efficient multi-GPU rendering through dynamic Level-of-Detail voxel\nallocations, significantly improving scalability and performance. Through\nextensive experiments, CityGS-X consistently outperforms existing methods in\nterms of faster training times, larger rendering capacities, and more accurate\ngeometric details in large-scale scenes. Notably, CityGS-X can train and render\na scene with 5,000+ images in just 5 hours using only 4 * 4090 GPUs, a task\nthat would make other alternative methods encounter Out-Of-Memory (OOM) issues\nand fail completely. This implies that CityGS-X is far beyond the capacity of\nother existing methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23361", "pdf": "https://arxiv.org/pdf/2503.23361", "abs": "https://arxiv.org/abs/2503.23361", "authors": ["Linxin Song", "Xuwei Ding", "Jieyu Zhang", "Taiwei Shi", "Ryotaro Shimizu", "Rahul Gupta", "Yang Liu", "Jian Kang", "Jieyu Zhao"], "title": "Discovering Knowledge Deficiencies of Language Models on Massive Knowledge Base", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) possess impressive linguistic capabilities but\noften fail to faithfully retain factual knowledge, leading to hallucinations\nand unreliable outputs. Understanding LLMs' knowledge deficiencies by\nexhaustively evaluating against full-scale knowledge bases is computationally\nprohibitive, especially for closed-weight models. We propose stochastic error\nascent (SEA), a scalable and efficient framework for discovering knowledge\ndeficiencies (errors) in closed-weight LLMs under a strict query budget. Rather\nthan naively probing all knowledge candidates, SEA formulates error discovery\nas a stochastic optimization process: it iteratively retrieves new high-error\ncandidates by leveraging the semantic similarity to previously observed\nfailures. To further enhance search efficiency and coverage, SEA employs\nhierarchical retrieval across document and paragraph levels, and constructs a\nrelation directed acyclic graph to model error propagation and identify\nsystematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors\nthan Automated Capability Discovery and 26.7% more than AutoBencher, while\nreducing the cost-per-error by 599x and 9x, respectively. Human evaluation\nconfirms the high quality of generated questions, while ablation and\nconvergence analyses validate the contribution of each component in SEA.\nFurther analysis on the discovered errors reveals correlated failure patterns\nacross LLM families and recurring deficits, highlighting the need for better\ndata coverage and targeted fine-tuning in future LLM development.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23383", "pdf": "https://arxiv.org/pdf/2503.23383", "abs": "https://arxiv.org/abs/2503.23383", "authors": ["Xuefeng Li", "Haoyang Zou", "Pengfei Liu"], "title": "ToRL: Scaling Tool-Integrated RL", "categories": ["cs.CL"], "comment": null, "summary": "We introduce ToRL (Tool-Integrated Reinforcement Learning), a framework for\ntraining large language models (LLMs) to autonomously use computational tools\nvia reinforcement learning. Unlike supervised fine-tuning, ToRL allows models\nto explore and discover optimal strategies for tool use. Experiments with\nQwen2.5-Math models show significant improvements: ToRL-7B reaches 43.3\\%\naccuracy on AIME~24, surpassing reinforcement learning without tool integration\nby 14\\% and the best existing Tool-Integrated Reasoning (TIR) model by 17\\%.\nFurther analysis reveals emergent behaviors such as strategic tool invocation,\nself-regulation of ineffective code, and dynamic adaptation between\ncomputational and analytical reasoning, all arising purely through\nreward-driven learning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23106", "pdf": "https://arxiv.org/pdf/2503.23106", "abs": "https://arxiv.org/abs/2503.23106", "authors": ["Chao Tao", "Dandan Zhong", "Weiliang Mu", "Zhuofei Du", "Haiyang Wu"], "title": "A large-scale image-text dataset benchmark for farmland segmentation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The traditional deep learning paradigm that solely relies on labeled data has\nlimitations in representing the spatial relationships between farmland elements\nand the surrounding environment.It struggles to effectively model the dynamic\ntemporal evolution and spatial heterogeneity of farmland. Language,as a\nstructured knowledge carrier,can explicitly express the spatiotemporal\ncharacteristics of farmland, such as its shape, distribution,and surrounding\nenvironmental information.Therefore,a language-driven learning paradigm can\neffectively alleviate the challenges posed by the spatiotemporal heterogeneity\nof farmland.However,in the field of remote sensing imagery of farmland,there is\ncurrently no comprehensive benchmark dataset to support this research\ndirection.To fill this gap,we introduced language based descriptions of\nfarmland and developed FarmSeg-VL dataset,the first fine-grained image-text\ndataset designed for spatiotemporal farmland segmentation.Firstly, this article\nproposed a semi-automatic annotation method that can accurately assign caption\nto each image, ensuring high data quality and semantic richness while improving\nthe efficiency of dataset construction.Secondly,the FarmSeg-VL exhibits\nsignificant spatiotemporal characteristics.In terms of the temporal\ndimension,it covers all four seasons.In terms of the spatial dimension,it\ncovers eight typical agricultural regions across China.In addition, in terms of\ncaptions,FarmSeg-VL covers rich spatiotemporal characteristics of\nfarmland,including its inherent properties,phenological characteristics,\nspatial distribution,topographic and geomorphic features,and the distribution\nof surrounding environments.Finally,we present a performance analysis of VLMs\nand the deep learning models that rely solely on labels trained on the\nFarmSeg-VL,demonstrating its potential as a standard benchmark for farmland\nsegmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "fine-grained", "dimension"], "score": 5}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23427", "pdf": "https://arxiv.org/pdf/2503.23427", "abs": "https://arxiv.org/abs/2503.23427", "authors": ["Wenhan Liu", "Xinyu Ma", "Yutao Zhu", "Lixin Su", "Shuaiqiang Wang", "Dawei Yin", "Zhicheng Dou"], "title": "CoRanking: Collaborative Ranking with Small and Large Ranking Agents", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated superior listwise ranking\nperformance. However, their superior performance often relies on large-scale\nparameters (\\eg, GPT-4) and a repetitive sliding window process, which\nintroduces significant efficiency challenges. In this paper, we propose\n\\textbf{CoRanking}, a novel collaborative ranking framework that combines small\nand large ranking models for efficient and effective ranking. CoRanking first\nemploys a small-size reranker to pre-rank all the candidate passages, bringing\nrelevant ones to the top part of the list (\\eg, top-20). Then, the LLM listwise\nreranker is applied to only rerank these top-ranked passages instead of the\nwhole list, substantially enhancing overall ranking efficiency. Although more\nefficient, previous studies have revealed that the LLM listwise reranker have\nsignificant positional biases on the order of input passages. Directly feed the\ntop-ranked passages from small reranker may result in the sub-optimal\nperformance of LLM listwise reranker. To alleviate this problem, we introduce a\npassage order adjuster trained via reinforcement learning, which reorders the\ntop passages from the small reranker to align with the LLM's preferences of\npassage order. Extensive experiments on three IR benchmarks demonstrate that\nCoRanking significantly improves efficiency (reducing ranking latency by about\n70\\%) while achieving even better effectiveness compared to using only the LLM\nlistwise reranker.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "ranking"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23121", "pdf": "https://arxiv.org/pdf/2503.23121", "abs": "https://arxiv.org/abs/2503.23121", "authors": ["Guohong Huang", "Ling-An Zeng", "Zexin Zheng", "Shengbo Gu", "Wei-Shi Zheng"], "title": "Efficient Explicit Joint-level Interaction Modeling with Mamba for Text-guided HOI Generation", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025", "summary": "We propose a novel approach for generating text-guided human-object\ninteractions (HOIs) that achieves explicit joint-level interaction modeling in\na computationally efficient manner. Previous methods represent the entire human\nbody as a single token, making it difficult to capture fine-grained joint-level\ninteractions and resulting in unrealistic HOIs. However, treating each\nindividual joint as a token would yield over twenty times more tokens,\nincreasing computational overhead. To address these challenges, we introduce an\nEfficient Explicit Joint-level Interaction Model (EJIM). EJIM features a\nDual-branch HOI Mamba that separately and efficiently models spatiotemporal HOI\ninformation, as well as a Dual-branch Condition Injector for integrating text\nsemantics and object geometry into human and object motions. Furthermore, we\ndesign a Dynamic Interaction Block and a progressive masking mechanism to\niteratively filter out irrelevant joints, ensuring accurate and nuanced\ninteraction modeling. Extensive quantitative and qualitative evaluations on\npublic datasets demonstrate that EJIM surpasses previous works by a large\nmargin while using only 5\\% of the inference time. Code is available\n\\href{https://github.com/Huanggh531/EJIM}{here}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23503", "pdf": "https://arxiv.org/pdf/2503.23503", "abs": "https://arxiv.org/abs/2503.23503", "authors": ["Sid Bharthulwar", "John Rho", "Katrina Brown"], "title": "Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-Language Models", "categories": ["cs.CL"], "comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs", "summary": "We present a framework for optimizing prompts in vision-language models to\nelicit multimodal reasoning without model retraining. Using an evolutionary\nalgorithm to guide prompt updates downstream of visual tasks, our approach\nimproves upon baseline prompt-updating algorithms, which lack evolution-style\n\"survival of the fittest\" iteration. Crucially, we find this approach enables\nthe language model to independently discover progressive problem-solving\ntechniques across several evolution generations. For example, the model reasons\nthat to \"break down\" visually complex spatial tasks, making a tool call to a\nPython interpreter to perform tasks (such as cropping, image segmentation, or\nsaturation changes) would improve performance significantly. Our\nexperimentation shows that explicitly evoking this \"tool calling\" call, via\nsystem-level XML $...\\texttt{<tool>} ... \\texttt{</tool>}...$ tags, can\neffectively flag Python interpreter access for the same language model to\ngenerate relevant programs, generating advanced multimodal functionality. This\nfunctionality can be crystallized into a system-level prompt that induces\nimproved performance at inference time, and our experimentation suggests up to\n$\\approx 50\\%$ relative improvement across select visual tasks. Downstream\nperformance is trained and evaluated across subtasks from MathVista, M3CoT, and\nGeoBench-VLM datasets. Importantly, our approach shows that evolutionary prompt\noptimization guides language models towards self-reasoning discoveries, which\nresult in improved zero-shot generalization across tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23512", "pdf": "https://arxiv.org/pdf/2503.23512", "abs": "https://arxiv.org/abs/2503.23512", "authors": ["Qiang Yi", "Yangfan He", "Jianhui Wang", "Xinyuan Song", "Shiyao Qian", "Miao Zhang", "Li Sun", "Tianyu Shi"], "title": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel at generating creative narratives but\nstruggle with long-term coherence and emotional consistency in complex stories.\nTo address this, we propose SCORE (Story Coherence and Retrieval Enhancement),\na framework integrating three components: 1) Dynamic State Tracking (monitoring\nobjects/characters via symbolic logic), 2) Context-Aware Summarization\n(hierarchical episode summaries for temporal progression), and 3) Hybrid\nRetrieval (combining TF-IDF keyword relevance with cosine similarity-based\nsemantic embeddings). The system employs a temporally-aligned\nRetrieval-Augmented Generation (RAG) pipeline to validate contextual\nconsistency. Evaluations show SCORE achieves 23.6% higher coherence (NCI-2.0\nbenchmark), 89.7% emotional consistency (EASM metric), and 41.8% fewer\nhallucinations versus baseline GPT models. Its modular design supports\nincremental knowledge graph construction for persistent story memory and\nmulti-LLM backend compatibility, offering an explainable solution for\nindustrial-scale narrative systems requiring long-term consistency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "summarization"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23162", "pdf": "https://arxiv.org/pdf/2503.23162", "abs": "https://arxiv.org/abs/2503.23162", "authors": ["Zhenyu Tang", "Chaoran Feng", "Xinhua Cheng", "Wangbo Yu", "Junwu Zhang", "Yuan Liu", "Xiaoxiao Long", "Wenping Wang", "Li Yuan"], "title": "NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations", "categories": ["cs.CV"], "comment": "Project page: https://pku-yuangroup.github.io/NeuralGS/", "summary": "3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering\nspeed, but with millions of 3D Gaussians and significant storage and\ntransmission costs. Recent 3DGS compression methods mainly concentrate on\ncompressing Scaffold-GS, achieving impressive performance but with an\nadditional voxel structure and a complex encoding and quantization strategy. In\nthis paper, we aim to develop a simple yet effective method called NeuralGS\nthat explores in another way to compress the original 3DGS into a compact\nrepresentation without the voxel structure and complex quantization strategies.\nOur observation is that neural fields like NeRF can represent complex 3D scenes\nwith Multi-Layer Perceptron (MLP) neural networks using only a few megabytes.\nThus, NeuralGS effectively adopts the neural field representation to encode the\nattributes of 3D Gaussians with MLPs, only requiring a small storage size even\nfor a large-scale scene. To achieve this, we adopt a clustering strategy and\nfit the Gaussians with different tiny MLPs for each cluster, based on\nimportance scores of Gaussians as fitting weights. We experiment on multiple\ndatasets, achieving a 45-times average model size reduction without harming the\nvisual quality. The compression performance of our method on original 3DGS is\ncomparable to the dedicated Scaffold-GS-based compression methods, which\ndemonstrate the huge potential of directly compressing original 3DGS with\nneural fields.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23688", "pdf": "https://arxiv.org/pdf/2503.23688", "abs": "https://arxiv.org/abs/2503.23688", "authors": ["William Guey", "Pierrick Bougault", "Vitor D. de Moura", "Wei Zhang", "Jose O. Gomes"], "title": "Mapping Geopolitical Bias in 11 Large Language Models: A Bilingual, Dual-Framing Analysis of U.S.-China Tensions", "categories": ["cs.CL", "cs.HC"], "comment": "Preliminary version,20 pages, 10 figures, 1 table", "summary": "This study systematically analyzes geopolitical bias across 11 prominent\nLarge Language Models (LLMs) by examining their responses to seven critical\ntopics in U.S.-China relations. Utilizing a bilingual (English and Chinese) and\ndual-framing (affirmative and reverse) methodology, we generated 19,712 prompts\ndesigned to detect ideological leanings in model outputs. Responses were\nquantitatively assessed on a normalized scale from -2 (strongly Pro-China) to\n+2 (strongly Pro-U.S.) and categorized according to stance, neutrality, and\nrefusal rates. The findings demonstrate significant and consistent ideological\nalignments correlated with the LLMs' geographic origins; U.S.-based models\npredominantly favored Pro-U.S. stances, while Chinese-origin models exhibited\npronounced Pro-China biases. Notably, language and prompt framing substantially\ninfluenced model responses, with several LLMs exhibiting stance reversals based\non prompt polarity or linguistic context. Additionally, we introduced\ncomprehensive metrics to evaluate response consistency across languages and\nframing conditions, identifying variability and vulnerabilities in model\nbehaviors. These results offer practical insights that can guide organizations\nand individuals in selecting LLMs best aligned with their operational\npriorities and geopolitical considerations, underscoring the importance of\ncareful model evaluation in politically sensitive applications. Furthermore,\nthe research highlights specific prompt structures and linguistic variations\nthat can strategically trigger distinct responses from models, revealing\nmethods for effectively navigating and influencing LLM outputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23266", "pdf": "https://arxiv.org/pdf/2503.23266", "abs": "https://arxiv.org/abs/2503.23266", "authors": ["Shihao Cheng", "Jinlu Zhang", "Yue Liu", "Zhigang Tu"], "title": "OwlSight: A Robust Illumination Adaptation Framework for Dark Video Human Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Human action recognition in low-light environments is crucial for various\nreal-world applications. However, the existing approaches overlook the full\nutilization of brightness information throughout the training phase, leading to\nsuboptimal performance. To address this limitation, we propose OwlSight, a\nbiomimetic-inspired framework with whole-stage illumination enhancement to\ninteract with action classification for accurate dark video human action\nrecognition. Specifically, OwlSight incorporates a Time-Consistency Module\n(TCM) to capture shallow spatiotemporal features meanwhile maintaining temporal\ncoherence, which are then processed by a Luminance Adaptation Module (LAM) to\ndynamically adjust the brightness based on the input luminance distribution.\nFurthermore, a Reflect Augmentation Module (RAM) is presented to maximize\nillumination utilization and simultaneously enhance action recognition via two\ninteractive paths. Additionally, we build Dark-101, a large-scale dataset\ncomprising 18,310 dark videos across 101 action categories, significantly\nsurpassing existing datasets (e.g., ARID1.5 and Dark-48) in scale and\ndiversity. Extensive experiments demonstrate that the proposed OwlSight\nachieves state-of-the-art performance across four low-light action recognition\nbenchmarks. Notably, it outperforms previous best approaches by 5.36% on\nARID1.5 and 1.72% on Dark-101, highlighting its effectiveness in challenging\ndark environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23740", "pdf": "https://arxiv.org/pdf/2503.23740", "abs": "https://arxiv.org/abs/2503.23740", "authors": ["Lu Fan", "Jiashu Pu", "Rongsheng Zhang", "Xiao-Ming Wu"], "title": "LANID: LLM-assisted New Intent Discovery", "categories": ["cs.CL", "cs.AI"], "comment": "Published in LREC-COLING 2024", "summary": "Task-oriented Dialogue Systems (TODS) often face the challenge of\nencountering new intents. New Intent Discovery (NID) is a crucial task that\naims to identify these novel intents while maintaining the capability to\nrecognize existing ones. Previous efforts to adapt TODS to new intents have\nstruggled with inadequate semantic representation or have depended on external\nknowledge, which is often not scalable or flexible. Recently, Large Language\nModels (LLMs) have demonstrated strong zero-shot capabilities; however, their\nscale can be impractical for real-world applications that involve extensive\nqueries. To address the limitations of existing NID methods by leveraging LLMs,\nwe propose LANID, a framework that enhances the semantic representation of\nlightweight NID encoders with the guidance of LLMs. Specifically, LANID employs\nthe $K$-nearest neighbors and Density-Based Spatial Clustering of Applications\nwith Noise (DBSCAN) algorithms to sample selective utterance pairs from the\ntraining set. It then queries an LLM to ascertain the relationships between\nthese pairs. The data produced from this process is utilized to design a\ncontrastive fine-tuning task, which is then used to train a small encoder with\na contrastive triplet loss. Our experimental results demonstrate the efficacy\nof the proposed method across three distinct NID datasets, surpassing strong\nbaselines in both unsupervised and semi-supervised settings. Our code is\navailable at https://github.com/floatSDSDS/LANID.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23297", "pdf": "https://arxiv.org/pdf/2503.23297", "abs": "https://arxiv.org/abs/2503.23297", "authors": ["Zhenyang Liu", "Yikai Wang", "Sixiao Zheng", "Tongying Pan", "Longfei Liang", "Yanwei Fu", "Xiangyang Xue"], "title": "ReasonGrounder: LVLM-Guided Hierarchical Feature Splatting for Open-Vocabulary 3D Visual Grounding and Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary 3D visual grounding and reasoning aim to localize objects in\na scene based on implicit language descriptions, even when they are occluded.\nThis ability is crucial for tasks such as vision-language navigation and\nautonomous robotics. However, current methods struggle because they rely\nheavily on fine-tuning with 3D annotations and mask proposals, which limits\ntheir ability to handle diverse semantics and common knowledge required for\neffective reasoning. In this work, we propose ReasonGrounder, an LVLM-guided\nframework that uses hierarchical 3D feature Gaussian fields for adaptive\ngrouping based on physical scale, enabling open-vocabulary 3D grounding and\nreasoning. ReasonGrounder interprets implicit instructions using large\nvision-language models (LVLM) and localizes occluded objects through 3D\nGaussian splatting. By incorporating 2D segmentation masks from the SAM and\nmulti-view CLIP embeddings, ReasonGrounder selects Gaussian groups based on\nobject scale, enabling accurate localization through both explicit and implicit\nlanguage understanding, even in novel, occluded views. We also contribute\nReasoningGD, a new dataset containing over 10K scenes and 2 million annotations\nfor evaluating open-vocabulary 3D grounding and amodal perception under\nocclusion. Experiments show that ReasonGrounder significantly improves 3D\ngrounding accuracy in real-world scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23300", "pdf": "https://arxiv.org/pdf/2503.23300", "abs": "https://arxiv.org/abs/2503.23300", "authors": ["Wenqi Jia", "Bolin Lai", "Miao Liu", "Danfei Xu", "James M. Rehg"], "title": "Learning Predictive Visuomotor Coordination", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Understanding and predicting human visuomotor coordination is crucial for\napplications in robotics, human-computer interaction, and assistive\ntechnologies. This work introduces a forecasting-based task for visuomotor\nmodeling, where the goal is to predict head pose, gaze, and upper-body motion\nfrom egocentric visual and kinematic observations. We propose a\n\\textit{Visuomotor Coordination Representation} (VCR) that learns structured\ntemporal dependencies across these multimodal signals. We extend a\ndiffusion-based motion modeling framework that integrates egocentric vision and\nkinematic sequences, enabling temporally coherent and accurate visuomotor\npredictions. Our approach is evaluated on the large-scale EgoExo4D dataset,\ndemonstrating strong generalization across diverse real-world activities. Our\nresults highlight the importance of multimodal integration in understanding\nvisuomotor coordination, contributing to research in visuomotor learning and\nhuman behavior modeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23307", "pdf": "https://arxiv.org/pdf/2503.23307", "abs": "https://arxiv.org/abs/2503.23307", "authors": ["Cong Wei", "Bo Sun", "Haoyu Ma", "Ji Hou", "Felix Juefei-Xu", "Zecheng He", "Xiaoliang Dai", "Luxin Zhang", "Kunpeng Li", "Tingbo Hou", "Animesh Sinha", "Peter Vajda", "Wenhu Chen"], "title": "MoCha: Towards Movie-Grade Talking Character Synthesis", "categories": ["cs.CV"], "comment": "https://congwei1230.github.io/MoCha/", "summary": "Recent advancements in video generation have achieved impressive motion\nrealism, yet they often overlook character-driven storytelling, a crucial task\nfor automated film, animation generation. We introduce Talking Characters, a\nmore realistic task to generate talking character animations directly from\nspeech and text. Unlike talking head, Talking Characters aims at generating the\nfull portrait of one or more characters beyond the facial region. In this\npaper, we propose MoCha, the first of its kind to generate talking characters.\nTo ensure precise synchronization between video and speech, we propose a\nspeech-video window attention mechanism that effectively aligns speech and\nvideo tokens. To address the scarcity of large-scale speech-labeled video\ndatasets, we introduce a joint training strategy that leverages both\nspeech-labeled and text-labeled video data, significantly improving\ngeneralization across diverse character actions. We also design structured\nprompt templates with character tags, enabling, for the first time,\nmulti-character conversation with turn-based dialogue-allowing AI-generated\ncharacters to engage in context-aware conversations with cinematic coherence.\nExtensive qualitative and quantitative evaluations, including human preference\nstudies and benchmark comparisons, demonstrate that MoCha sets a new standard\nfor AI-generated cinematic storytelling, achieving superior realism,\nexpressiveness, controllability and generalization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "human preference", "dialogue"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23829", "pdf": "https://arxiv.org/pdf/2503.23829", "abs": "https://arxiv.org/abs/2503.23829", "authors": ["Yi Su", "Dian Yu", "Linfeng Song", "Juntao Li", "Haitao Mi", "Zhaopeng Tu", "Min Zhang", "Dong Yu"], "title": "Expanding RL with Verifiable Rewards Across Diverse Domains", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement in binary judgments across different large\nlanguage models (LLMs) when objective reference answers exist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporate model-based soft scoring into RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nBy fine-tuning a base 7B model using various RL algorithms against our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "agreement", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23330", "pdf": "https://arxiv.org/pdf/2503.23330", "abs": "https://arxiv.org/abs/2503.23330", "authors": ["Hongxiang Jiang", "Jihao Yin", "Qixiong Wang", "Jiaqi Feng", "Guo Chen"], "title": "EagleVision: Object-level Attribute Multimodal LLM for Remote Sensing", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nimpressive results in various visual tasks. However, in remote sensing (RS),\nhigh resolution and small proportion of objects pose challenges to existing\nMLLMs, which struggle with object-centric tasks, particularly in precise\nlocalization and fine-grained attribute description for each object. These RS\nMLLMs have not yet surpassed classical visual perception models, as they only\nprovide coarse image understanding, leading to limited gains in real-world\nscenarios. To address this gap, we establish EagleVision, an MLLM tailored for\nremote sensing that excels in object detection and attribute comprehension.\nEquipped with the Attribute Disentangle module, EagleVision learns\ndisentanglement vision tokens to express distinct attributes. To support\nobject-level visual-language alignment, we construct EVAttrs-95K, the first\nlarge-scale object attribute understanding dataset in RS for instruction\ntuning, along with a novel evaluation benchmark, EVBench. EagleVision achieves\nstate-of-the-art performance on both fine-grained object detection and object\nattribute understanding tasks, highlighting the mutual promotion between\ndetection and understanding capabilities in MLLMs. The code, model, data, and\ndemo will be available at https://github.com/XiangTodayEatsWhat/EagleVision.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23895", "pdf": "https://arxiv.org/pdf/2503.23895", "abs": "https://arxiv.org/abs/2503.23895", "authors": ["Yuqiao Tan", "Shizhu He", "Huanxuan Liao", "Jun Zhao", "Kang Liu"], "title": "Better wit than wealth: Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "preprint", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23331", "pdf": "https://arxiv.org/pdf/2503.23331", "abs": "https://arxiv.org/abs/2503.23331", "authors": ["Hongwei Zheng", "Han Li", "Wenrui Dai", "Ziyang Zheng", "Chenglin Li", "Junni Zou", "Hongkai Xiong"], "title": "HiPART: Hierarchical Pose AutoRegressive Transformer for Occluded 3D Human Pose Estimation", "categories": ["cs.CV", "cs.LG"], "comment": "CVPR2025", "summary": "Existing 2D-to-3D human pose estimation (HPE) methods struggle with the\nocclusion issue by enriching information like temporal and visual cues in the\nlifting stage. In this paper, we argue that these methods ignore the limitation\nof the sparse skeleton 2D input representation, which fundamentally restricts\nthe 2D-to-3D lifting and worsens the occlusion issue. To address these, we\npropose a novel two-stage generative densification method, named Hierarchical\nPose AutoRegressive Transformer (HiPART), to generate hierarchical 2D dense\nposes from the original sparse 2D pose. Specifically, we first develop a\nmulti-scale skeleton tokenization module to quantize the highly dense 2D pose\ninto hierarchical tokens and propose a Skeleton-aware Alignment to strengthen\ntoken connections. We then develop a Hierarchical AutoRegressive Modeling\nscheme for hierarchical 2D pose generation. With generated hierarchical poses\nas inputs for 2D-to-3D lifting, the proposed method shows strong robustness in\noccluded scenarios and achieves state-of-the-art performance on the\nsingle-frame-based 3D HPE. Moreover, it outperforms numerous multi-frame\nmethods while reducing parameter and computational complexity and can also\ncomplement them to further enhance performance and robustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23359", "pdf": "https://arxiv.org/pdf/2503.23359", "abs": "https://arxiv.org/abs/2503.23359", "authors": ["Linfeng Tang", "Yeda Wang", "Meiqi Gong", "Zizhuo Li", "Yuxin Deng", "Xunpeng Yi", "Chunyu Li", "Han Xu", "Hao Zhang", "Jiayi Ma"], "title": "VideoFusion: A Spatio-Temporal Collaborative Network for Mutli-modal Video Fusion and Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Compared to images, videos better align with real-world acquisition scenarios\nand possess valuable temporal cues. However, existing multi-sensor fusion\nresearch predominantly integrates complementary context from multiple images\nrather than videos. This primarily stems from two factors: 1) the scarcity of\nlarge-scale multi-sensor video datasets, limiting research in video fusion, and\n2) the inherent difficulty of jointly modeling spatial and temporal\ndependencies in a unified framework. This paper proactively compensates for the\ndilemmas. First, we construct M3SVD, a benchmark dataset with $220$ temporally\nsynchronized and spatially registered infrared-visible video pairs comprising\n153,797 frames, filling the data gap for the video fusion community. Secondly,\nwe propose VideoFusion, a multi-modal video fusion model that fully exploits\ncross-modal complementarity and temporal dynamics to generate spatio-temporally\ncoherent videos from (potentially degraded) multi-modal inputs. Specifically,\n1) a differential reinforcement module is developed for cross-modal information\ninteraction and enhancement, 2) a complete modality-guided fusion strategy is\nemployed to adaptively integrate multi-modal features, and 3) a bi-temporal\nco-attention mechanism is devised to dynamically aggregate forward-backward\ntemporal contexts to reinforce cross-frame feature representations. Extensive\nexperiments reveal that VideoFusion outperforms existing image-oriented fusion\nparadigms in sequential scenarios, effectively mitigating temporal\ninconsistency and interference.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24198", "pdf": "https://arxiv.org/pdf/2503.24198", "abs": "https://arxiv.org/abs/2503.24198", "authors": ["Jingxian Xu", "Mengyu Zhou", "Weichang Liu", "Hanbing Liu", "Shi Han", "Dongmei Zhang"], "title": "TwT: Thinking without Tokens by Habitual Reasoning Distillation with Multi-Teachers' Guidance", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made significant strides in problem-solving\nby incorporating reasoning processes. However, this enhanced reasoning\ncapability results in an increased number of output tokens during inference,\nleading to higher computational costs. To address this challenge, we propose\nTwT (Thinking without Tokens), a method that reduces inference-time costs\nthrough habitual reasoning distillation with multi-teachers' guidance, while\nmaintaining high performance. Our approach introduces a Habitual Reasoning\nDistillation method, which internalizes explicit reasoning into the model's\nhabitual behavior through a Teacher-Guided compression strategy inspired by\nhuman cognition. Additionally, we propose Dual-Criteria Rejection Sampling\n(DCRS), a technique that generates a high-quality and diverse distillation\ndataset using multiple teacher models, making our method suitable for\nunsupervised scenarios. Experimental results demonstrate that TwT effectively\nreduces inference costs while preserving superior performance, achieving up to\na 13.6% improvement in accuracy with fewer output tokens compared to other\ndistillation methods, offering a highly practical solution for efficient LLM\ndeployment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "criteria"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23379", "pdf": "https://arxiv.org/pdf/2503.23379", "abs": "https://arxiv.org/abs/2503.23379", "authors": ["Haiduo Huang", "Yadong Zhang", "Pengju Ren"], "title": "KernelDNA: Dynamic Kernel Sharing via Decoupled Naive Adapters", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Dynamic convolution enhances model capacity by adaptively combining multiple\nkernels, yet faces critical trade-offs: prior works either (1) incur\nsignificant parameter overhead by scaling kernel numbers linearly, (2)\ncompromise inference speed through complex kernel interactions, or (3) struggle\nto jointly optimize dynamic attention and static kernels. We also observe that\npre-trained Convolutional Neural Networks (CNNs) exhibit inter-layer redundancy\nakin to that in Large Language Models (LLMs). Specifically, dense convolutional\nlayers can be efficiently replaced by derived ``child\" layers generated from a\nshared ``parent\" convolutional kernel through an adapter.\n  To address these limitations and implement the weight-sharing mechanism, we\npropose a lightweight convolution kernel plug-in, named KernelDNA. It decouples\nkernel adaptation into input-dependent dynamic routing and pre-trained static\nmodulation, ensuring both parameter efficiency and hardware-friendly inference.\nUnlike existing dynamic convolutions that expand parameters via multi-kernel\nensembles, our method leverages cross-layer weight sharing and adapter-based\nmodulation, enabling dynamic kernel specialization without altering the\nstandard convolution structure. This design preserves the native computational\nefficiency of standard convolutions while enhancing representation power\nthrough input-adaptive kernel adjustments. Experiments on image classification\nand dense prediction tasks demonstrate that KernelDNA achieves state-of-the-art\naccuracy-efficiency balance among dynamic convolution variants. Our codes are\navailable at https://github.com/haiduo/KernelDNA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23398", "pdf": "https://arxiv.org/pdf/2503.23398", "abs": "https://arxiv.org/abs/2503.23398", "authors": ["Leander Girrbach", "Stephan Alaniz", "Genevieve Smith", "Zeynep Akata"], "title": "A Large Scale Analysis of Gender Biases in Text-to-Image Generative Models", "categories": ["cs.CV", "cs.CY"], "comment": null, "summary": "With the increasing use of image generation technology, understanding its\nsocial biases, including gender bias, is essential. This paper presents the\nfirst large-scale study on gender bias in text-to-image (T2I) models, focusing\non everyday situations. While previous research has examined biases in\noccupations, we extend this analysis to gender associations in daily\nactivities, objects, and contexts. We create a dataset of 3,217 gender-neutral\nprompts and generate 200 images per prompt from five leading T2I models. We\nautomatically detect the perceived gender of people in the generated images and\nfilter out images with no person or multiple people of different genders,\nleaving 2,293,295 images. To enable a broad analysis of gender bias in T2I\nmodels, we group prompts into semantically similar concepts and calculate the\nproportion of male- and female-gendered images for each prompt. Our analysis\nshows that T2I models reinforce traditional gender roles, reflect common gender\nstereotypes in household roles, and underrepresent women in financial related\nactivities. Women are predominantly portrayed in care- and human-centered\nscenarios, and men in technical or physical labor scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23402", "pdf": "https://arxiv.org/pdf/2503.23402", "abs": "https://arxiv.org/abs/2503.23402", "authors": ["Junsu Kim", "Yunhoe Ku", "Dongyoon Han", "Seungryul Baek"], "title": "Diffusion Meets Few-shot Class Incremental Learning", "categories": ["cs.CV", "cs.AI"], "comment": "pre-print", "summary": "Few-shot class-incremental learning (FSCIL) is challenging due to extremely\nlimited training data; while aiming to reduce catastrophic forgetting and learn\nnew information. We propose Diffusion-FSCIL, a novel approach that employs a\ntext-to-image diffusion model as a frozen backbone. Our conjecture is that\nFSCIL can be tackled using a large generative model's capabilities benefiting\nfrom 1) generation ability via large-scale pre-training; 2) multi-scale\nrepresentation; 3) representational flexibility through the text encoder. To\nmaximize the representation capability, we propose to extract multiple\ncomplementary diffusion features to play roles as latent replay with slight\nsupport from feature distillation for preventing generative biases. Our\nframework realizes efficiency through 1) using a frozen backbone; 2) minimal\ntrainable components; 3) batch processing of multiple feature extractions.\nExtensive experiments on CUB-200, miniImageNet, and CIFAR-100 show that\nDiffusion-FSCIL surpasses state-of-the-art methods, preserving performance on\npreviously learned classes and adapting effectively to new ones.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23407", "pdf": "https://arxiv.org/pdf/2503.23407", "abs": "https://arxiv.org/abs/2503.23407", "authors": ["Wei Zeng", "Xuebin Chang", "Jianghao Su", "Xiang Gu", "Jian Sun", "Zongben Xu"], "title": "GMapLatent: Geometric Mapping in Latent Space", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cross-domain generative models based on encoder-decoder AI architectures have\nattracted much attention in generating realistic images, where domain alignment\nis crucial for generation accuracy. Domain alignment methods usually deal\ndirectly with the initial distribution; however, mismatched or mixed clusters\ncan lead to mode collapse and mixture problems in the decoder, compromising\nmodel generalization capabilities. In this work, we innovate a cross-domain\nalignment and generation model that introduces a canonical latent space\nrepresentation based on geometric mapping to align the cross-domain latent\nspaces in a rigorous and precise manner, thus avoiding mode collapse and\nmixture in the encoder-decoder generation architectures. We name this model\nGMapLatent. The core of the method is to seamlessly align latent spaces with\nstrict cluster correspondence constraints using the canonical parameterizations\nof cluster-decorated latent spaces. We first (1) transform the latent space to\na canonical parameter domain by composing barycenter translation, optimal\ntransport merging and constrained harmonic mapping, and then (2) compute\ngeometric registration with cluster constraints over the canonical parameter\ndomains. This process realizes a bijective (one-to-one and onto) mapping\nbetween newly transformed latent spaces and generates a precise alignment of\ncluster pairs. Cross-domain generation is then achieved through the aligned\nlatent spaces embedded in the encoder-decoder pipeline. Experiments on\ngray-scale and color images validate the efficiency, efficacy and applicability\nof GMapLatent, and demonstrate that the proposed model has superior performance\nover existing models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23422", "pdf": "https://arxiv.org/pdf/2503.23422", "abs": "https://arxiv.org/abs/2503.23422", "authors": ["Xin Zuo", "Jiaran Jiang", "Jifeng Shen", "Wankou Yang"], "title": "Improving underwater semantic segmentation with underwater image quality attention and muti-scale aggregation attention", "categories": ["cs.CV"], "comment": "Accepted by Pattern Analysis and Applications", "summary": "Underwater image understanding is crucial for both submarine navigation and\nseabed exploration. However, the low illumination in underwater environments\ndegrades the imaging quality, which in turn seriously deteriorates the\nperformance of underwater semantic segmentation, particularly for outlining the\nobject region boundaries. To tackle this issue, we present UnderWater SegFormer\n(UWSegFormer), a transformer-based framework for semantic segmentation of\nlow-quality underwater images. Firstly, we propose the Underwater Image Quality\nAttention (UIQA) module. This module enhances the representation of highquality\nsemantic information in underwater image feature channels through a channel\nself-attention mechanism. In order to address the issue of loss of imaging\ndetails due to the underwater environment, the Multi-scale Aggregation\nAttention(MAA) module is proposed. This module aggregates sets of semantic\nfeatures at different scales by extracting discriminative information from\nhigh-level features,thus compensating for the semantic loss of detail in\nunderwater objects. Finally, during training, we introduce Edge Learning Loss\n(ELL) in order to enhance the model's learning of underwater object edges and\nimprove the model's prediction accuracy. Experiments conducted on the SUIM and\nDUT-USEG (DUT) datasets have demonstrated that the proposed method has\nadvantages in terms of segmentation completeness, boundary clarity, and\nsubjective perceptual details when compared to SOTA methods. In addition, the\nproposed method achieves the highest mIoU of 82.12 and 71.41 on the SUIM and\nDUT datasets, respectively. Code will be available at\nhttps://github.com/SAWRJJ/UWSegFormer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24364", "pdf": "https://arxiv.org/pdf/2503.24364", "abs": "https://arxiv.org/abs/2503.24364", "authors": ["Łukasz Borchmann", "Marek Wydmuch"], "title": "Query and Conquer: Execution-Guided SQL Generation", "categories": ["cs.CL"], "comment": null, "summary": "We propose a novel approach for generating complex outputs that significantly\nimproves accuracy in text-to-SQL tasks. Our method leverages execution results\nto select the most semantically consistent query from multiple candidates,\nenabling smaller, cost-effective models to surpass computationally intensive\nreasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference\ncost by as much as 30 times. It integrates effortlessly with existing models,\noffering a practical and scalable pathway to state-of-the-art SQL generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24377", "pdf": "https://arxiv.org/pdf/2503.24377", "abs": "https://arxiv.org/abs/2503.24377", "authors": ["Rui Wang", "Hongru Wang", "Boyang Xue", "Jianhui Pang", "Shudong Liu", "Yi Chen", "Jiahao Qiu", "Derek Fai Wong", "Heng Ji", "Kam-Fai Wong"], "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "In Progress; Paper list Repo:\n  https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23459", "pdf": "https://arxiv.org/pdf/2503.23459", "abs": "https://arxiv.org/abs/2503.23459", "authors": ["Chenglong Lu", "Shen Liang", "Xuewei Wang", "Wei Wang"], "title": "Reinforcement Learning-based Token Pruning in Vision Transformers: A Markov Game Approach", "categories": ["cs.CV"], "comment": "Accepted by IEEE International Conference on Multimedia & Expo (ICME)\n  2025", "summary": "Vision Transformers (ViTs) have computational costs scaling quadratically\nwith the number of tokens, calling for effective token pruning policies. Most\nexisting policies are handcrafted, lacking adaptivity to varying inputs.\nMoreover, they fail to consider the sequential nature of token pruning across\nmultiple layers. In this work, for the first time (as far as we know), we\nexploit Reinforcement Learning (RL) to data-adaptively learn a pruning policy.\nFormulating token pruning as a sequential decision-making problem, we model it\nas a Markov Game and utilize Multi-Agent Proximal Policy Optimization (MAPPO)\nwhere each agent makes an individualized pruning decision for a single token.\nWe also develop reward functions that enable simultaneous collaboration and\ncompetition of these agents to balance efficiency and accuracy. On the\nwell-known ImageNet-1k dataset, our method improves the inference speed by up\nto 44% while incurring only a negligible accuracy drop of 0.4%. The source code\nis available at https://github.com/daashuai/rl4evit.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["proximal policy optimization", "reinforcement learning", "policy optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22742", "pdf": "https://arxiv.org/pdf/2503.22742", "abs": "https://arxiv.org/abs/2503.22742", "authors": ["William Claster", "Suhas KM", "Dhairya Gundechia"], "title": "Adaptive Integrated Layered Attention (AILA)", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.NE"], "comment": null, "summary": "We propose Adaptive Integrated Layered Attention (AILA), a neural network\narchitecture that combines dense skip connections with different mechanisms for\nadaptive feature reuse across network layers. We evaluate AILA on three\nchallenging tasks: price forecasting for various commodities and indices (S&P\n500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the\nCIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In\nall cases, AILA matches strong deep learning baselines (LSTMs, Transformers,\nand ResNets), achieving it at a fraction of the training and inference time.\nNotably, we implement and test two versions of the model - AILA-Architecture 1,\nwhich uses simple linear layers as the connection mechanism between layers, and\nAILA-Architecture 2, which implements an attention mechanism to selectively\nfocus on outputs from previous layers. Both architectures are applied in a\nsingle-task learning setting, with each model trained separately for individual\ntasks. Results confirm that AILA's adaptive inter-layer connections yield\nrobust gains by flexibly reusing pertinent features at multiple network depths.\nThe AILA approach thus presents an extension to existing architectures,\nimproving long-range sequence modeling, image recognition with optimised\ncomputational speed, and SOTA classification performance in practice.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23468", "pdf": "https://arxiv.org/pdf/2503.23468", "abs": "https://arxiv.org/abs/2503.23468", "authors": ["Eytan Kats", "Kai Geißler", "Jochen G. Hirsch", "Stefan Heldman", "Mattias P. Heinrich"], "title": "Internal Organ Localization Using Depth Images", "categories": ["cs.CV"], "comment": "Accepted for German Conference on Medical Image Computing 2025 (BVM\n  2025)", "summary": "Automated patient positioning is a crucial step in streamlining MRI workflows\nand enhancing patient throughput. RGB-D camera-based systems offer a promising\napproach to automate this process by leveraging depth information to estimate\ninternal organ positions. This paper investigates the feasibility of a\nlearning-based framework to infer approximate internal organ positions from the\nbody surface. Our approach utilizes a large-scale dataset of MRI scans to train\na deep learning model capable of accurately predicting organ positions and\nshapes from depth images alone. We demonstrate the effectiveness of our method\nin localization of multiple internal organs, including bones and soft tissues.\nOur findings suggest that RGB-D camera-based systems integrated into MRI\nworkflows have the potential to streamline scanning procedures and improve\npatient experience by enabling accurate and automated patient positioning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22879", "pdf": "https://arxiv.org/pdf/2503.22879", "abs": "https://arxiv.org/abs/2503.22879", "authors": ["Hung-Yueh Chiang", "Chi-Chih Chang", "Natalia Frumkin", "Kai-Chiang Wu", "Mohamed S. Abdelfattah", "Diana Marculescu"], "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PF"], "comment": null, "summary": "State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input $x$, combined\nwith a per-state-group quantization for input-dependent parameters $B$ and $C$.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms several state-of-the-art SSM quantization methods and delivers\n1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps://github.com/enyac-group/Quamba.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23502", "pdf": "https://arxiv.org/pdf/2503.23502", "abs": "https://arxiv.org/abs/2503.23502", "authors": ["Jannik Endres", "Oliver Hahn", "Charles Corbière", "Simone Schaub-Meyer", "Stefan Roth", "Alexandre Alahi"], "title": "Boosting Omnidirectional Stereo Matching with a Pre-trained Depth Foundation Model", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project page: https://vita-epfl.github.io/DFI-OmniStereo-website/", "summary": "Omnidirectional depth perception is essential for mobile robotics\napplications that require scene understanding across a full 360{\\deg} field of\nview. Camera-based setups offer a cost-effective option by using stereo depth\nestimation to generate dense, high-resolution depth maps without relying on\nexpensive active sensing. However, existing omnidirectional stereo matching\napproaches achieve only limited depth accuracy across diverse environments,\ndepth ranges, and lighting conditions, due to the scarcity of real-world data.\nWe present DFI-OmniStereo, a novel omnidirectional stereo matching method that\nleverages a large-scale pre-trained foundation model for relative monocular\ndepth estimation within an iterative optimization-based stereo matching\narchitecture. We introduce a dedicated two-stage training strategy to utilize\nthe relative monocular depth features for our omnidirectional stereo matching\nbefore scale-invariant fine-tuning. DFI-OmniStereo achieves state-of-the-art\nresults on the real-world Helvipad dataset, reducing disparity MAE by\napproximately 16% compared to the previous best omnidirectional stereo method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22989", "pdf": "https://arxiv.org/pdf/2503.22989", "abs": "https://arxiv.org/abs/2503.22989", "authors": ["Gabriel Recchia", "Chatrik Singh Mangat", "Issac Li", "Gayatri Krishnakumar"], "title": "FindTheFlaws: Annotated Errors for Detecting Flawed Reasoning and Scalable Oversight Research", "categories": ["cs.AI", "cs.CL", "I.2"], "comment": "43 pages, 3 figures. for associated repository, see\n  https://github.com/modulo-research/findtheflaws", "summary": "As AI models tackle increasingly complex problems, ensuring reliable human\noversight becomes more challenging due to the difficulty of verifying\nsolutions. Approaches to scaling AI supervision include debate, in which two\nagents engage in structured dialogue to help a judge evaluate claims; critique,\nin which models identify potential flaws in proposed solutions; and\nprover-verifier games, in which a capable 'prover' model generates solutions\nthat must be verifiable by a less capable 'verifier'. Evaluations of the\nscalability of these and similar approaches to difficult problems benefit from\ndatasets that include (1) long-form expert-verified correct solutions and (2)\nlong-form flawed solutions with annotations highlighting specific errors, but\nfew are available.\n  To address this gap, we present FindTheFlaws, a group of five diverse\ndatasets spanning medicine, mathematics, science, coding, and the Lojban\nlanguage. Each dataset contains questions and long-form solutions with expert\nannotations validating their correctness or identifying specific error(s) in\nthe reasoning. We evaluate frontier models' critiquing capabilities and observe\na range of performance that can be leveraged for scalable oversight\nexperiments: models performing more poorly on particular datasets can serve as\njudges/verifiers for more capable models. Additionally, for some task/dataset\ncombinations, expert baselines exceed even top model performance, making them\nmore beneficial for scalable oversight experiments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23037", "pdf": "https://arxiv.org/pdf/2503.23037", "abs": "https://arxiv.org/abs/2503.23037", "authors": ["Aske Plaat", "Max van Duijn", "Niki van Stein", "Mike Preuss", "Peter van der Putten", "Kees Joost Batenburg"], "title": "Agentic Large Language Models, a survey", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "There is great interest in agentic LLMs, large language models that act as\nagents. We review the growing body of work in this area and provide a research\nagenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We\norganize the literature according to these three categories. The research in\nthe first category focuses on reasoning, reflection, and retrieval, aiming to\nimprove decision making; the second category focuses on action models, robots,\nand tools, aiming for agents that act as useful assistants; the third category\nfocuses on multi-agent systems, aiming for collaborative task solving and\nsimulating interaction to study emergent social behavior. We find that works\nmutually benefit from results in other categories: retrieval enables tool use,\nreflection improves multi-agent collaboration, and reasoning benefits all\ncategories. We discuss applications of agentic LLMs and provide an agenda for\nfurther research. Important applications are in medical diagnosis, logistics\nand financial market analysis. Meanwhile, self-reflective agents playing roles\nand interacting with one another augment the process of scientific research\nitself. Further, agentic LLMs may provide a solution for the problem of LLMs\nrunning out of training data: inference-time behavior generates new training\nstates, such that LLMs can keep learning without needing ever larger datasets.\nWe note that there is risk associated with LLM assistants taking action in the\nreal world, while agentic LLMs are also likely to benefit society.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23508", "pdf": "https://arxiv.org/pdf/2503.23508", "abs": "https://arxiv.org/abs/2503.23508", "authors": ["Yuming Chen", "Jiangyan Feng", "Haodong Zhang", "Lijun Gong", "Feng Zhu", "Rui Zhao", "Qibin Hou", "Ming-Ming Cheng", "Yibing Song"], "title": "Re-Aligning Language to Visual Objects with an Agentic Workflow", "categories": ["cs.CV"], "comment": "33 pages, 20 figures, 17 tables, ICLR 2025", "summary": "Language-based object detection (LOD) aims to align visual objects with\nlanguage expressions. A large amount of paired data is utilized to improve LOD\nmodel generalizations. During the training process, recent studies leverage\nvision-language models (VLMs) to automatically generate human-like expressions\nfor visual objects, facilitating training data scaling up. In this process, we\nobserve that VLM hallucinations bring inaccurate object descriptions (e.g.,\nobject name, color, and shape) to deteriorate VL alignment quality. To reduce\nVLM hallucinations, we propose an agentic workflow controlled by an LLM to\nre-align language to visual objects via adaptively adjusting image and text\nprompts. We name this workflow Real-LOD, which includes planning, tool use, and\nreflection steps. Given an image with detected objects and VLM raw language\nexpressions, Real-LOD reasons its state automatically and arranges action based\non our neural symbolic designs (i.e., planning). The action will adaptively\nadjust the image and text prompts and send them to VLMs for object\nre-description (i.e., tool use). Then, we use another LLM to analyze these\nrefined expressions for feedback (i.e., reflection). These steps are conducted\nin a cyclic form to gradually improve language descriptions for re-aligning to\nvisual objects. We construct a dataset that contains a tiny amount of 0.18M\nimages with re-aligned language expression and train a prevalent LOD model to\nsurpass existing LOD methods by around 50% on the standard benchmarks. Our\nReal-LOD workflow, with automatic VL refinement, reveals a potential to\npreserve data quality along with scaling up data quantity, which further\nimproves LOD performance from a data-alignment perspective.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23100", "pdf": "https://arxiv.org/pdf/2503.23100", "abs": "https://arxiv.org/abs/2503.23100", "authors": ["Zehua Liu", "Han Wu", "Ruifeng She", "Xiaojin Fu", "Xiongwei Han", "Tao Zhong", "Mingxuan Yuan"], "title": "Beyond Standard MoE: Mixture of Latent Experts for Resource-Efficient Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Mixture of Experts (MoE) has emerged as a pivotal architectural paradigm for\nefficient scaling of Large Language Models (LLMs), operating through selective\nactivation of parameter subsets for each input token. Nevertheless,\nconventional MoE architectures encounter substantial challenges, including\nexcessive memory utilization and communication overhead during training and\ninference, primarily attributable to the proliferation of expert modules. In\nthis paper, we introduce Mixture of Latent Experts (MoLE), a novel\nparameterization methodology that facilitates the mapping of specific experts\ninto a shared latent space. Specifically, all expert operations are\nsystematically decomposed into two principal components: a shared projection\ninto a lower-dimensional latent space, followed by expert-specific\ntransformations with significantly reduced parametric complexity. This\nfactorized approach substantially diminishes parameter count and computational\nrequirements. Beyond the pretraining implementation of the MoLE architecture,\nwe also establish a rigorous mathematical framework for transforming\npre-trained MoE models into the MoLE architecture, characterizing the\nsufficient conditions for optimal factorization and developing a systematic\ntwo-phase algorithm for this conversion process. Our comprehensive theoretical\nanalysis demonstrates that MoLE significantly enhances computational efficiency\nacross multiple dimensions while preserving model representational capacity.\nEmpirical evaluations corroborate our theoretical findings, confirming that\nMoLE achieves performance comparable to standard MoE implementations while\nsubstantially reducing resource requirements.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23519", "pdf": "https://arxiv.org/pdf/2503.23519", "abs": "https://arxiv.org/abs/2503.23519", "authors": ["Haruya Ishikawa", "Yoshimitsu Aoki"], "title": "BoundMatch: Boundary detection applied to semi-supervised segmentation for urban-driving scenes", "categories": ["cs.CV"], "comment": "15 pages, 7 figures", "summary": "Semi-supervised semantic segmentation (SS-SS) aims to mitigate the heavy\nannotation burden of dense pixel labeling by leveraging abundant unlabeled\nimages alongside a small labeled set. While current teacher-student consistency\nregularization methods achieve strong results, they often overlook a critical\nchallenge: the precise delineation of object boundaries. In this paper, we\npropose BoundMatch, a novel multi-task SS-SS framework that explicitly\nintegrates semantic boundary detection into the consistency regularization\npipeline. Our core mechanism, Boundary Consistency Regularized Multi-Task\nLearning (BCRM), enforces prediction agreement between teacher and student\nmodels on both segmentation masks and detailed semantic boundaries. To further\nenhance performance and sharpen contours, BoundMatch incorporates two\nlightweight fusion modules: Boundary-Semantic Fusion (BSF) injects learned\nboundary cues into the segmentation decoder, while Spatial Gradient Fusion\n(SGF) refines boundary predictions using mask gradients, leading to\nhigher-quality boundary pseudo-labels. This framework is built upon SAMTH, a\nstrong teacher-student baseline featuring a Harmonious Batch Normalization\n(HBN) update strategy for improved stability. Extensive experiments on diverse\ndatasets including Cityscapes, BDD100K, SYNTHIA, ADE20K, and Pascal VOC show\nthat BoundMatch achieves competitive performance against state-of-the-art\nmethods while significantly improving boundary-specific evaluation metrics. We\nalso demonstrate its effectiveness in realistic large-scale unlabeled data\nscenarios and on lightweight architectures designed for mobile deployment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "agreement", "consistency"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23106", "pdf": "https://arxiv.org/pdf/2503.23106", "abs": "https://arxiv.org/abs/2503.23106", "authors": ["Chao Tao", "Dandan Zhong", "Weiliang Mu", "Zhuofei Du", "Haiyang Wu"], "title": "A large-scale image-text dataset benchmark for farmland segmentation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The traditional deep learning paradigm that solely relies on labeled data has\nlimitations in representing the spatial relationships between farmland elements\nand the surrounding environment.It struggles to effectively model the dynamic\ntemporal evolution and spatial heterogeneity of farmland. Language,as a\nstructured knowledge carrier,can explicitly express the spatiotemporal\ncharacteristics of farmland, such as its shape, distribution,and surrounding\nenvironmental information.Therefore,a language-driven learning paradigm can\neffectively alleviate the challenges posed by the spatiotemporal heterogeneity\nof farmland.However,in the field of remote sensing imagery of farmland,there is\ncurrently no comprehensive benchmark dataset to support this research\ndirection.To fill this gap,we introduced language based descriptions of\nfarmland and developed FarmSeg-VL dataset,the first fine-grained image-text\ndataset designed for spatiotemporal farmland segmentation.Firstly, this article\nproposed a semi-automatic annotation method that can accurately assign caption\nto each image, ensuring high data quality and semantic richness while improving\nthe efficiency of dataset construction.Secondly,the FarmSeg-VL exhibits\nsignificant spatiotemporal characteristics.In terms of the temporal\ndimension,it covers all four seasons.In terms of the spatial dimension,it\ncovers eight typical agricultural regions across China.In addition, in terms of\ncaptions,FarmSeg-VL covers rich spatiotemporal characteristics of\nfarmland,including its inherent properties,phenological characteristics,\nspatial distribution,topographic and geomorphic features,and the distribution\nof surrounding environments.Finally,we present a performance analysis of VLMs\nand the deep learning models that rely solely on labels trained on the\nFarmSeg-VL,demonstrating its potential as a standard benchmark for farmland\nsegmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "fine-grained", "dimension"], "score": 5}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23573", "pdf": "https://arxiv.org/pdf/2503.23573", "abs": "https://arxiv.org/abs/2503.23573", "authors": ["Maximilian Augustin", "Yannic Neuhaus", "Matthias Hein"], "title": "DASH: Detection and Assessment of Systematic Hallucinations of VLMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Vision-language models (VLMs) are prone to object hallucinations, where they\nerroneously indicate the presenceof certain objects in an image. Existing\nbenchmarks quantify hallucinations using relatively small, labeled datasets.\nHowever, this approach is i) insufficient to assess hallucinations that arise\nin open-world settings, where VLMs are widely used, and ii) inadequate for\ndetecting systematic errors in VLMs. We propose DASH (Detection and Assessment\nof Systematic Hallucinations), an automatic, large-scale pipeline designed to\nidentify systematic hallucinations of VLMs on real-world images in an\nopen-world setting. A key component is DASH-OPT for image-based retrieval,\nwhere we optimize over the ''natural image manifold'' to generate images that\nmislead the VLM. The output of DASH consists of clusters of real and\nsemantically similar images for which the VLM hallucinates an object. We apply\nDASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in\ntotal, find more than 19k clusters with 950k images. We study the transfer of\nthe identified systematic hallucinations to other VLMs and show that\nfine-tuning PaliGemma with the model-specific images obtained with DASH\nmitigates object hallucinations. Code and data are available at\nhttps://YanNeu.github.io/DASH.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23580", "pdf": "https://arxiv.org/pdf/2503.23580", "abs": "https://arxiv.org/abs/2503.23580", "authors": ["Zheng-Peng Duan", "Jiawei Zhang", "Xin Jin", "Ziheng Zhang", "Zheng Xiong", "Dongqing Zou", "Jimmy Ren", "Chun-Le Guo", "Chongyi Li"], "title": "DiT4SR: Taming Diffusion Transformer for Real-World Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale pre-trained diffusion models are becoming increasingly popular in\nsolving the Real-World Image Super-Resolution (Real-ISR) problem because of\ntheir rich generative priors. The recent development of diffusion transformer\n(DiT) has witnessed overwhelming performance over the traditional UNet-based\narchitecture in image generation, which also raises the question: Can we adopt\nthe advanced DiT-based diffusion model for Real-ISR? To this end, we propose\nour DiT4SR, one of the pioneering works to tame the large-scale DiT model for\nReal-ISR. Instead of directly injecting embeddings extracted from\nlow-resolution (LR) images like ControlNet, we integrate the LR embeddings into\nthe original attention mechanism of DiT, allowing for the bidirectional flow of\ninformation between the LR latent and the generated latent. The sufficient\ninteraction of these two streams allows the LR stream to evolve with the\ndiffusion process, producing progressively refined guidance that better aligns\nwith the generated latent at each diffusion step. Additionally, the LR guidance\nis injected into the generated latent via a cross-stream convolution layer,\ncompensating for DiT's limited ability to capture local information. These\nsimple but effective designs endow the DiT model with superior performance in\nReal-ISR, which is demonstrated by extensive experiments. Project Page:\nhttps://adam-duan.github.io/projects/dit4sr/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23746", "pdf": "https://arxiv.org/pdf/2503.23746", "abs": "https://arxiv.org/abs/2503.23746", "authors": ["Dizhan Xue", "Jing Cui", "Shengsheng Qian", "Chuanrui Hu", "Changsheng Xu"], "title": "Short-video Propagation Influence Rating: A New Real-world Dataset and A New Large Graph Model", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM", "cs.SI"], "comment": null, "summary": "Short-video platforms have gained immense popularity, captivating the\ninterest of millions, if not billions, of users globally. Recently, researchers\nhave highlighted the significance of analyzing the propagation of short-videos,\nwhich typically involves discovering commercial values, public opinions, user\nbehaviors, etc. This paper proposes a new Short-video Propagation Influence\nRating (SPIR) task and aims to promote SPIR from both the dataset and method\nperspectives. First, we propose a new Cross-platform Short-Video (XS-Video)\ndataset, which aims to provide a large-scale and real-world short-video\npropagation network across various platforms to facilitate the research on\nshort-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926\nsamples, and 535 topics across 5 biggest Chinese platforms, annotated with the\npropagation influence from level 0 to 9. To the best of our knowledge, this is\nthe first large-scale short-video dataset that contains cross-platform data or\nprovides all of the views, likes, shares, collects, fans, comments, and comment\ncontent. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a\nnovel three-stage training mechanism, to bridge heterogeneous graph-structured\ndata with the powerful reasoning ability and knowledge of Large Language Models\n(LLMs). Our NetGPT can comprehend and analyze the short-video propagation\ngraph, enabling it to predict the long-term propagation influence of\nshort-videos. Comprehensive experimental results evaluated by both\nclassification and regression metrics on our XS-Video dataset indicate the\nsuperiority of our method for SPIR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23715", "pdf": "https://arxiv.org/pdf/2503.23715", "abs": "https://arxiv.org/abs/2503.23715", "authors": ["Kun Liu", "Qi Liu", "Xinchen Liu", "Jie Li", "Yongdong Zhang", "Jiebo Luo", "Xiaodong He", "Wu Liu"], "title": "HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Text-to-video (T2V) generation has made tremendous progress in generating\ncomplicated scenes based on texts. However, human-object interaction (HOI)\noften cannot be precisely generated by current T2V models due to the lack of\nlarge-scale videos with accurate captions for HOI. To address this issue, we\nintroduce HOIGen-1M, the first largescale dataset for HOI Generation,\nconsisting of over one million high-quality videos collected from diverse\nsources. In particular, to guarantee the high quality of videos, we first\ndesign an efficient framework to automatically curate HOI videos using the\npowerful multimodal large language models (MLLMs), and then the videos are\nfurther cleaned by human annotators. Moreover, to obtain accurate textual\ncaptions for HOI videos, we design a novel video description method based on a\nMixture-of-Multimodal-Experts (MoME) strategy that not only generates\nexpressive captions but also eliminates the hallucination by individual MLLM.\nFurthermore, due to the lack of an evaluation framework for generated HOI\nvideos, we propose two new metrics to assess the quality of generated videos in\na coarse-to-fine manner. Extensive experiments reveal that current T2V models\nstruggle to generate high-quality HOI videos and confirm that our HOIGen-1M\ndataset is instrumental for improving HOI video generation. Project webpage is\navailable at https://liuqi-creat.github.io/HOIGen.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23722", "pdf": "https://arxiv.org/pdf/2503.23722", "abs": "https://arxiv.org/abs/2503.23722", "authors": ["Xiang Hu", "Yuhao Wang", "Pingping Zhang", "Huchuan Lu"], "title": "LATex: Leveraging Attribute-based Text Knowledge for Aerial-Ground Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "Aerial-Ground person Re-IDentification (AG-ReID) aims to retrieve specific\npersons across heterogeneous cameras in different views. Previous methods\nusually adopt large-scale models, focusing on view-invariant features. However,\nthey overlook the semantic information in person attributes. Additionally,\nexisting training strategies often rely on full fine-tuning large-scale models,\nwhich significantly increases training costs. To address these issues, we\npropose a novel framework named LATex for AG-ReID, which adopts prompt-tuning\nstrategies to leverage attribute-based text knowledge. More specifically, we\nfirst introduce the Contrastive Language-Image Pre-training (CLIP) model as the\nbackbone, and propose an Attribute-aware Image Encoder (AIE) to extract global\nsemantic features and attribute-aware features. Then, with these features, we\npropose a Prompted Attribute Classifier Group (PACG) to generate person\nattribute predictions and obtain the encoded representations of predicted\nattributes. Finally, we design a Coupled Prompt Template (CPT) to transform\nattribute tokens and view information into structured sentences. These\nsentences are processed by the text encoder of CLIP to generate more\ndiscriminative features. As a result, our framework can fully leverage\nattribute-based text knowledge to improve the AG-ReID. Extensive experiments on\nthree AG-ReID benchmarks demonstrate the effectiveness of our proposed LATex.\nThe source code will be available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24354", "pdf": "https://arxiv.org/pdf/2503.24354", "abs": "https://arxiv.org/abs/2503.24354", "authors": ["Rana Muhammad Shahroz Khan", "Dongwen Tang", "Pingzhi Li", "Kai Wang", "Tianlong Chen"], "title": "ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Parameter generation has emerged as a novel paradigm for neural network\ndevelopment, offering an alternative to traditional neural network training by\nsynthesizing high-quality model weights directly. In the context of Low-Rank\nAdaptation (LoRA) for evolving ($\\textit{i.e.}$, constantly updated) large\nlanguage models (LLMs), this approach promises efficient adaptation without\ncostly retraining. However, existing methods face critical limitations in\nsimultaneously achieving scalability and controllability. In this paper, we\nintroduce $\\texttt{ORAL}$, a novel $\\textbf{conditional recurrent diffusion}$\nframework that addresses these challenges. $\\texttt{ORAL}$ incorporates a novel\nconditioning mechanism that integrates model architecture and textual task\nspecifications, enabling the generation of task-specific LoRA parameters that\ncan seamlessly transfer across evolving foundation models. Our approach\nsuccessfully scales to billions-of-parameter LLMs and maintains\ncontrollability. Through extensive experiments across seven language tasks,\nfour vision tasks, and three multimodal tasks using five pre-trained LLMs, we\ndemonstrate that $\\texttt{ORAL}$ generates high-quality LoRA parameters that\nachieve comparable or superior performance to vanilla trained counterparts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23746", "pdf": "https://arxiv.org/pdf/2503.23746", "abs": "https://arxiv.org/abs/2503.23746", "authors": ["Dizhan Xue", "Jing Cui", "Shengsheng Qian", "Chuanrui Hu", "Changsheng Xu"], "title": "Short-video Propagation Influence Rating: A New Real-world Dataset and A New Large Graph Model", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM", "cs.SI"], "comment": null, "summary": "Short-video platforms have gained immense popularity, captivating the\ninterest of millions, if not billions, of users globally. Recently, researchers\nhave highlighted the significance of analyzing the propagation of short-videos,\nwhich typically involves discovering commercial values, public opinions, user\nbehaviors, etc. This paper proposes a new Short-video Propagation Influence\nRating (SPIR) task and aims to promote SPIR from both the dataset and method\nperspectives. First, we propose a new Cross-platform Short-Video (XS-Video)\ndataset, which aims to provide a large-scale and real-world short-video\npropagation network across various platforms to facilitate the research on\nshort-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926\nsamples, and 535 topics across 5 biggest Chinese platforms, annotated with the\npropagation influence from level 0 to 9. To the best of our knowledge, this is\nthe first large-scale short-video dataset that contains cross-platform data or\nprovides all of the views, likes, shares, collects, fans, comments, and comment\ncontent. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a\nnovel three-stage training mechanism, to bridge heterogeneous graph-structured\ndata with the powerful reasoning ability and knowledge of Large Language Models\n(LLMs). Our NetGPT can comprehend and analyze the short-video propagation\ngraph, enabling it to predict the long-term propagation influence of\nshort-videos. Comprehensive experimental results evaluated by both\nclassification and regression metrics on our XS-Video dataset indicate the\nsuperiority of our method for SPIR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23775", "pdf": "https://arxiv.org/pdf/2503.23775", "abs": "https://arxiv.org/abs/2503.23775", "authors": ["Lucas Heublein", "Nisha L. Raichur", "Tobias Feigl", "Tobias Brieger", "Fin Heuer", "Lennart Asbach", "Alexander Rügamer", "Felix Ott"], "title": "Evaluation of (Un-)Supervised Machine Learning Methods for GNSS Interference Classification with Real-World Data Discrepancies", "categories": ["cs.CV", "cs.LG", "68-00, 68T01, 68T30", "E.0; E.2; H.4; I.5"], "comment": "34 pages, 25 figures", "summary": "The accuracy and reliability of vehicle localization on roads are crucial for\napplications such as self-driving cars, toll systems, and digital tachographs.\nTo achieve accurate positioning, vehicles typically use global navigation\nsatellite system (GNSS) receivers to validate their absolute positions.\nHowever, GNSS-based positioning can be compromised by interference signals,\nnecessitating the identification, classification, determination of purpose, and\nlocalization of such interference to mitigate or eliminate it. Recent\napproaches based on machine learning (ML) have shown superior performance in\nmonitoring interference. However, their feasibility in real-world applications\nand environments has yet to be assessed. Effective implementation of ML\ntechniques requires training datasets that incorporate realistic interference\nsignals, including real-world noise and potential multipath effects that may\noccur between transmitter, receiver, and satellite in the operational area.\nAdditionally, these datasets require reference labels. Creating such datasets\nis often challenging due to legal restrictions, as causing interference to GNSS\nsources is strictly prohibited. Consequently, the performance of ML-based\nmethods in practical applications remains unclear. To address this gap, we\ndescribe a series of large-scale measurement campaigns conducted in real-world\nsettings at two highway locations in Germany and the Seetal Alps in Austria,\nand in large-scale controlled indoor environments. We evaluate the latest\nsupervised ML-based methods to report on their performance in real-world\nsettings and present the applicability of pseudo-labeling for unsupervised\nlearning. We demonstrate the challenges of combining datasets due to data\ndiscrepancies and evaluate outlier detection, domain adaptation, and data\naugmentation techniques to present the models' capabilities to adapt to changes\nin the datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23844", "pdf": "https://arxiv.org/pdf/2503.23844", "abs": "https://arxiv.org/abs/2503.23844", "authors": ["Xuyang Li", "Chenyu Li", "Pedram Ghamisi", "Danfeng Hong"], "title": "FlexiMo: A Flexible Remote Sensing Foundation Model", "categories": ["cs.CV"], "comment": null, "summary": "The rapid expansion of multi-source satellite imagery drives innovation in\nEarth observation, opening unprecedented opportunities for Remote Sensing\nFoundation Models to harness diverse data. However, many existing models remain\nconstrained by fixed spatial resolutions and patch sizes, limiting their\nability to fully exploit the heterogeneous spatial characteristics inherent in\nsatellite imagery. To address these challenges, we propose FlexiMo, a flexible\nremote sensing foundation model that endows the pre-trained model with the\nflexibility to adapt to arbitrary spatial resolutions. Central to FlexiMo is a\nspatial resolution-aware module that employs a parameter-free alignment\nembedding mechanism to dynamically recalibrate patch embeddings based on the\ninput image's resolution and dimensions. This design not only preserves\ncritical token characteristics and ensures multi-scale feature fidelity but\nalso enables efficient feature extraction without requiring modifications to\nthe underlying network architecture. In addition, FlexiMo incorporates a\nlightweight channel adaptation module that leverages prior spectral information\nfrom sensors. This mechanism allows the model to process images with varying\nnumbers of channels while maintaining the data's intrinsic physical properties.\nExtensive experiments on diverse multimodal, multi-resolution, and multi-scale\ndatasets demonstrate that FlexiMo significantly enhances model generalization\nand robustness. In particular, our method achieves outstanding performance\nacross a range of downstream tasks, including scene classification, land cover\nclassification, urban building segmentation, and cloud detection. By enabling\nparameter-efficient and physically consistent adaptation, FlexiMo paves the way\nfor more adaptable and effective foundation models in real-world remote sensing\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23905", "pdf": "https://arxiv.org/pdf/2503.23905", "abs": "https://arxiv.org/abs/2503.23905", "authors": ["Qihan Huang", "Long Chan", "Jinlong Liu", "Wanggui He", "Hao Jiang", "Mingli Song", "Jingyuan Chen", "Chang Yao", "Jie Song"], "title": "Boosting MLLM Reasoning with Text-Debiased Hint-GRPO", "categories": ["cs.CV"], "comment": null, "summary": "MLLM reasoning has drawn widespread research for its excellent\nproblem-solving capability. Current reasoning methods fall into two types: PRM,\nwhich supervises the intermediate reasoning steps, and ORM, which supervises\nthe final results. Recently, DeepSeek-R1 has challenged the traditional view\nthat PRM outperforms ORM, which demonstrates strong generalization performance\nusing an ORM method (i.e., GRPO). However, current MLLM's GRPO algorithms still\nstruggle to handle challenging and complex multimodal reasoning tasks (e.g.,\nmathematical reasoning). In this work, we reveal two problems that impede the\nperformance of GRPO on the MLLM: Low data utilization and Text-bias. Low data\nutilization refers to that GRPO cannot acquire positive rewards to update the\nMLLM on difficult samples, and text-bias is a phenomenon that the MLLM bypasses\nimage condition and solely relies on text condition for generation after GRPO\ntraining. To tackle these problems, this work proposes Hint-GRPO that improves\ndata utilization by adaptively providing hints for samples of varying\ndifficulty, and text-bias calibration that mitigates text-bias by calibrating\nthe token prediction logits with image condition in test-time. Experiment\nresults on three base MLLMs across eleven datasets demonstrate that our\nproposed methods advance the reasoning capability of original MLLM by a large\nmargin, exhibiting superior performance to existing MLLM reasoning methods. Our\ncode is available at https://github.com/hqhQAQ/Hint-GRPO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23993", "pdf": "https://arxiv.org/pdf/2503.23993", "abs": "https://arxiv.org/abs/2503.23993", "authors": ["Ming Yuan", "Sichao Wang", "Chuang Zhang", "Lei He", "Qing Xu", "Jianqiang Wang"], "title": "DenseFormer: Learning Dense Depth Map from Sparse Depth and Image via Conditional Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The depth completion task is a critical problem in autonomous driving,\ninvolving the generation of dense depth maps from sparse depth maps and RGB\nimages. Most existing methods employ a spatial propagation network to\niteratively refine the depth map after obtaining an initial dense depth. In\nthis paper, we propose DenseFormer, a novel method that integrates the\ndiffusion model into the depth completion task. By incorporating the denoising\nmechanism of the diffusion model, DenseFormer generates the dense depth map by\nprogressively refining an initial random depth distribution through multiple\niterations. We propose a feature extraction module that leverages a feature\npyramid structure, along with multi-layer deformable attention, to effectively\nextract and integrate features from sparse depth maps and RGB images, which\nserve as the guiding condition for the diffusion process. Additionally, this\npaper presents a depth refinement module that applies multi-step iterative\nrefinement across various ranges to the dense depth results generated by the\ndiffusion process. The module utilizes image features enriched with multi-scale\ninformation and sparse depth input to further enhance the accuracy of the\npredicted depth map. Extensive experiments on the KITTI outdoor scene dataset\ndemonstrate that DenseFormer outperforms classical depth completion methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24014", "pdf": "https://arxiv.org/pdf/2503.24014", "abs": "https://arxiv.org/abs/2503.24014", "authors": ["Minh David Thao Chan", "Ruoyu Zhao", "Yukuan Jia", "Ruiqing Mao", "Sheng Zhou"], "title": "Optimization of Layer Skipping and Frequency Scaling for Convolutional Neural Networks under Latency Constraint", "categories": ["cs.CV"], "comment": "12 pages, 6 figures, Accepted in Proc. Eur. Conf. Comput. Vis. (ECCV)\n  Workshops. Milan, Italy: Springer, September 2024", "summary": "The energy consumption of Convolutional Neural Networks (CNNs) is a critical\nfactor in deploying deep learning models on resource-limited equipment such as\nmobile devices and autonomous vehicles. We propose an approach involving\nProportional Layer Skipping (PLS) and Frequency Scaling (FS). Layer skipping\nreduces computational complexity by selectively bypassing network layers,\nwhereas frequency scaling adjusts the frequency of the processor to optimize\nenergy use under latency constraints. Experiments of PLS and FS on ResNet-152\nwith the CIFAR-10 dataset demonstrated significant reductions in computational\ndemands and energy consumption with minimal accuracy loss. This study offers\npractical solutions for improving real-time processing in resource-limited\nsettings and provides insights into balancing computational efficiency and\nmodel performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24121", "pdf": "https://arxiv.org/pdf/2503.24121", "abs": "https://arxiv.org/abs/2503.24121", "authors": ["Valentin Boussot", "Cédric Hémon", "Jean-Claude Nunes", "Jason Downling", "Simon Rouzé", "Caroline Lafond", "Anaïs Barateau", "Jean-Louis Dillenseger"], "title": "IMPACT: A Generic Semantic Loss for Multimodal Medical Image Registration", "categories": ["cs.CV", "cs.LG"], "comment": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI). This is a preprint version and has not been\n  peer-reviewed", "summary": "Image registration is fundamental in medical imaging, enabling precise\nalignment of anatomical structures for diagnosis, treatment planning,\nimage-guided treatment or longitudinal monitoring. This work introduces IMPACT\n(Image Metric with Pretrained model-Agnostic Comparison for Transmodality\nregistration), a generic semantic similarity metric designed for seamless\nintegration into diverse image registration frameworks (such as Elastix and\nVoxelmorph). It compares deep learning-based features extracted from medical\nimages without requiring task-specific training, ensuring broad applicability\nacross various modalities. By leveraging the features of the large-scale\npretrained TotalSegmentator models and the ability to integrate Segment\nAnything Model (SAM) and other large-scale segmentation networks, this approach\noffers significant advantages. It provides robust, scalable, and efficient\nsolutions for multimodal image registration. The IMPACT loss was evaluated on\nfive challenging registration tasks involving thoracic CT/CBCT, and pelvic\nMR/CT datasets. Quantitative metrics, such as Target Registration Error and\nDice Similarity Coefficient, demonstrated significant improvements in\nanatomical alignment compared to baseline methods. Qualitative analyses further\nconfirmed the increased robustness of the proposed metric in the face of noise,\nartifacts, and modality variations. IMPACT's versatility and efficiency make it\na valuable tool for advancing registration performance in clinical and research\napplications, addressing critical challenges in multimodal medical imaging.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24180", "pdf": "https://arxiv.org/pdf/2503.24180", "abs": "https://arxiv.org/abs/2503.24180", "authors": ["Ziming Cheng", "Zhiyuan Huang", "Junting Pan", "Zhaohui Hou", "Mingjie Zhan"], "title": "Navi-plus: Managing Ambiguous GUI Navigation Tasks with Follow-up", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Graphical user interfaces (GUI) automation agents are emerging as powerful\ntools, enabling humans to accomplish increasingly complex tasks on smart\ndevices. However, users often inadvertently omit key information when conveying\ntasks, which hinders agent performance in the current agent paradigm that does\nnot support immediate user intervention. To address this issue, we introduce a\n$\\textbf{Self-Correction GUI Navigation}$ task that incorporates interactive\ninformation completion capabilities within GUI agents. We developed the\n$\\textbf{Navi-plus}$ dataset with GUI follow-up question-answer pairs,\nalongside a $\\textbf{Dual-Stream Trajectory Evaluation}$ method to benchmark\nthis new capability. Our results show that agents equipped with the ability to\nask GUI follow-up questions can fully recover their performance when faced with\nambiguous user tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24229", "pdf": "https://arxiv.org/pdf/2503.24229", "abs": "https://arxiv.org/abs/2503.24229", "authors": ["Daichi Otsuka", "Shinichi Mae", "Ryosuke Yamada", "Hirokatsu Kataoka"], "title": "Pre-training with 3D Synthetic Data: Learning 3D Point Cloud Instance Segmentation from 3D Synthetic Scenes", "categories": ["cs.CV"], "comment": null, "summary": "In the recent years, the research community has witnessed growing use of 3D\npoint cloud data for the high applicability in various real-world applications.\nBy means of 3D point cloud, this modality enables to consider the actual size\nand spatial understanding. The applied fields include mechanical control of\nrobots, vehicles, or other real-world systems. Along this line, we would like\nto improve 3D point cloud instance segmentation which has emerged as a\nparticularly promising approach for these applications. However, the creation\nof 3D point cloud datasets entails enormous costs compared to 2D image\ndatasets. To train a model of 3D point cloud instance segmentation, it is\nnecessary not only to assign categories but also to provide detailed\nannotations for each point in the large-scale 3D space. Meanwhile, the increase\nof recent proposals for generative models in 3D domain has spurred proposals\nfor using a generative model to create 3D point cloud data. In this work, we\npropose a pre-training with 3D synthetic data to train a 3D point cloud\ninstance segmentation model based on generative model for 3D scenes represented\nby point cloud data. We directly generate 3D point cloud data with Point-E for\ninserting a generated data into a 3D scene. More recently in 2025, although\nthere are other accurate 3D generation models, even using the Point-E as an\nearly 3D generative model can effectively support the pre-training with 3D\nsynthetic data. In the experimental section, we compare our pre-training method\nwith baseline methods indicated improved performance, demonstrating the\nefficacy of 3D generative models for 3D point cloud instance segmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24345", "pdf": "https://arxiv.org/pdf/2503.24345", "abs": "https://arxiv.org/abs/2503.24345", "authors": ["Fang Yan", "Jianfeng Wu", "Jiawen Li", "Wei Wang", "Jiaxuan Lu", "Wen Chen", "Zizhao Gao", "Jianan Li", "Hong Yan", "Jiabo Ma", "Minda Chen", "Yang Lu", "Qing Chen", "Yizhi Wang", "Xitong Ling", "Xuenian Wang", "Zihan Wang", "Qiang Huang", "Shengyi Hua", "Mianxin Liu", "Lei Ma", "Tian Shen", "Xiaofan Zhang", "Yonghong He", "Hao Chen", "Shaoting Zhang", "Zhe Wang"], "title": "PathOrchestra: A Comprehensive Foundation Model for Computational Pathology with Over 100 Diverse Clinical-Grade Tasks", "categories": ["cs.CV"], "comment": null, "summary": "The complexity and variability inherent in high-resolution pathological\nimages present significant challenges in computational pathology. While\npathology foundation models leveraging AI have catalyzed transformative\nadvancements, their development demands large-scale datasets, considerable\nstorage capacity, and substantial computational resources. Furthermore,\nensuring their clinical applicability and generalizability requires rigorous\nvalidation across a broad spectrum of clinical tasks. Here, we present\nPathOrchestra, a versatile pathology foundation model trained via\nself-supervised learning on a dataset comprising 300K pathological slides from\n20 tissue and organ types across multiple centers. The model was rigorously\nevaluated on 112 clinical tasks using a combination of 61 private and 51 public\ndatasets. These tasks encompass digital slide preprocessing, pan-cancer\nclassification, lesion identification, multi-cancer subtype classification,\nbiomarker assessment, gene expression prediction, and the generation of\nstructured reports. PathOrchestra demonstrated exceptional performance across\n27,755 WSIs and 9,415,729 ROIs, achieving over 0.950 accuracy in 47 tasks,\nincluding pan-cancer classification across various organs, lymphoma subtype\ndiagnosis, and bladder cancer screening. Notably, it is the first model to\ngenerate structured reports for high-incidence colorectal cancer and\ndiagnostically complex lymphoma-areas that are infrequently addressed by\nfoundational models but hold immense clinical potential. Overall, PathOrchestra\nexemplifies the feasibility and efficacy of a large-scale, self-supervised\npathology foundation model, validated across a broad range of clinical-grade\ntasks. Its high accuracy and reduced reliance on extensive data annotation\nunderline its potential for clinical integration, offering a pathway toward\nmore efficient and high-quality medical services.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24368", "pdf": "https://arxiv.org/pdf/2503.24368", "abs": "https://arxiv.org/abs/2503.24368", "authors": ["Xiaoran Zhang", "Eric Z. Chen", "Lin Zhao", "Xiao Chen", "Yikang Liu", "Boris Maihe", "James S. Duncan", "Terrence Chen", "Shanhui Sun"], "title": "Adapting Vision Foundation Models for Real-time Ultrasound Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel approach that adapts hierarchical vision foundation models\nfor real-time ultrasound image segmentation. Existing ultrasound segmentation\nmethods often struggle with adaptability to new tasks, relying on costly manual\nannotations, while real-time approaches generally fail to match\nstate-of-the-art performance. To overcome these limitations, we introduce an\nadaptive framework that leverages the vision foundation model Hiera to extract\nmulti-scale features, interleaved with DINOv2 representations to enhance visual\nexpressiveness. These enriched features are then decoded to produce precise and\nrobust segmentation. We conduct extensive evaluations on six public datasets\nand one in-house dataset, covering both cardiac and thyroid ultrasound\nsegmentation. Experiments show that our approach outperforms state-of-the-art\nmethods across multiple datasets and excels with limited supervision,\nsurpassing nnUNet by over 20\\% on average in the 1\\% and 10\\% data settings.\nOur method achieves $\\sim$77 FPS inference speed with TensorRT on a single GPU,\nenabling real-time clinical applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24379", "pdf": "https://arxiv.org/pdf/2503.24379", "abs": "https://arxiv.org/abs/2503.24379", "authors": ["Shengqiong Wu", "Weicai Ye", "Jiahao Wang", "Quande Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Shuicheng Yan", "Hao Fei", "Tat-Seng Chua"], "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://sqwu.top/Any2Cap/", "summary": "To address the bottleneck of accurate user intent interpretation within the\ncurrent video generation community, we present Any2Caption, a novel framework\nfor controllable video generation under any condition. The key idea is to\ndecouple various condition interpretation steps from the video synthesis step.\nBy leveraging modern multimodal large language models (MLLMs), Any2Caption\ninterprets diverse inputs--text, images, videos, and specialized cues such as\nregion, motion, and camera poses--into dense, structured captions that offer\nbackbone video generators with better guidance. We also introduce Any2CapIns, a\nlarge-scale dataset with 337K instances and 407K conditions for\nany-condition-to-caption instruction tuning. Comprehensive evaluations\ndemonstrate significant improvements of our system in controllability and video\nquality across various aspects of existing video generation models. Project\nPage: https://sqwu.top/Any2Cap/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24381", "pdf": "https://arxiv.org/pdf/2503.24381", "abs": "https://arxiv.org/abs/2503.24381", "authors": ["Yuping Wang", "Xiangyu Huang", "Xiaokang Sun", "Mingxuan Yan", "Shuo Xing", "Zhengzhong Tu", "Jiachen Li"], "title": "UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MA", "cs.RO"], "comment": "14 pages; Dataset: https://huggingface.co/datasets/tasl-lab/uniocc;\n  Code: https://github.com/tasl-lab/UniOcc", "summary": "We introduce UniOcc, a comprehensive, unified benchmark for occupancy\nforecasting (i.e., predicting future occupancies based on historical\ninformation) and current-frame occupancy prediction from camera images. UniOcc\nunifies data from multiple real-world datasets (i.e., nuScenes, Waymo) and\nhigh-fidelity driving simulators (i.e., CARLA, OpenCOOD), which provides 2D/3D\noccupancy labels with per-voxel flow annotations and support for cooperative\nautonomous driving. In terms of evaluation, unlike existing studies that rely\non suboptimal pseudo labels for evaluation, UniOcc incorporates novel metrics\nthat do not depend on ground-truth occupancy, enabling robust assessment of\nadditional aspects of occupancy quality. Through extensive experiments on\nstate-of-the-art models, we demonstrate that large-scale, diverse training data\nand explicit flow information significantly enhance occupancy prediction and\nforecasting performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24391", "pdf": "https://arxiv.org/pdf/2503.24391", "abs": "https://arxiv.org/abs/2503.24391", "authors": ["Xingyu Chen", "Yue Chen", "Yuliang Xiu", "Andreas Geiger", "Anpei Chen"], "title": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training", "categories": ["cs.CV"], "comment": "Page: https://easi3r.github.io/ Code:\n  https://github.com/Inception3D/Easi3R", "summary": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22713", "pdf": "https://arxiv.org/pdf/2503.22713", "abs": "https://arxiv.org/abs/2503.22713", "authors": ["Nooshin Bahador", "Milad Lankarany"], "title": "Chirp Localization via Fine-Tuned Transformer Model: A Proof-of-Concept Study", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "comment": "19 pages, 8 figures", "summary": "Spectrograms are pivotal in time-frequency signal analysis, widely used in\naudio processing and computational neuroscience. Chirp-like patterns in\nelectroencephalogram (EEG) spectrograms (marked by linear or exponential\nfrequency sweep) are key biomarkers for seizure dynamics, but automated tools\nfor their detection, localization, and feature extraction are lacking. This\nstudy bridges this gap by fine-tuning a Vision Transformer (ViT) model on\nsynthetic spectrograms, augmented with Low-Rank Adaptation (LoRA) to boost\nadaptability. We generated 100000 synthetic spectrograms with chirp parameters,\ncreating the first large-scale benchmark for chirp localization. These\nspectrograms mimic neural chirps using linear or exponential frequency sweep,\nGaussian noise, and smoothing. A ViT model, adapted for regression, predicted\nchirp parameters. LoRA fine-tuned the attention layers, enabling efficient\nupdates to the pre-trained backbone. Training used MSE loss and the AdamW\noptimizer, with a learning rate scheduler and early stopping to curb\noverfitting. Only three features were targeted: Chirp Start Time (Onset Time),\nChirp Start Frequency (Onset Frequency), and Chirp End Frequency (Offset\nFrequency). Performance was evaluated via Pearson correlation between predicted\nand actual labels. Results showed strong alignment: 0.9841 correlation for\nchirp start time, with stable inference times (137 to 140s) and minimal bias in\nerror distributions. This approach offers a tool for chirp analysis in EEG\ntime-frequency representation, filling a critical methodological void.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22742", "pdf": "https://arxiv.org/pdf/2503.22742", "abs": "https://arxiv.org/abs/2503.22742", "authors": ["William Claster", "Suhas KM", "Dhairya Gundechia"], "title": "Adaptive Integrated Layered Attention (AILA)", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.NE"], "comment": null, "summary": "We propose Adaptive Integrated Layered Attention (AILA), a neural network\narchitecture that combines dense skip connections with different mechanisms for\nadaptive feature reuse across network layers. We evaluate AILA on three\nchallenging tasks: price forecasting for various commodities and indices (S&P\n500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the\nCIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In\nall cases, AILA matches strong deep learning baselines (LSTMs, Transformers,\nand ResNets), achieving it at a fraction of the training and inference time.\nNotably, we implement and test two versions of the model - AILA-Architecture 1,\nwhich uses simple linear layers as the connection mechanism between layers, and\nAILA-Architecture 2, which implements an attention mechanism to selectively\nfocus on outputs from previous layers. Both architectures are applied in a\nsingle-task learning setting, with each model trained separately for individual\ntasks. Results confirm that AILA's adaptive inter-layer connections yield\nrobust gains by flexibly reusing pertinent features at multiple network depths.\nThe AILA approach thus presents an extension to existing architectures,\nimproving long-range sequence modeling, image recognition with optimised\ncomputational speed, and SOTA classification performance in practice.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23265", "pdf": "https://arxiv.org/pdf/2503.23265", "abs": "https://arxiv.org/abs/2503.23265", "authors": ["Björn Möller", "Lucas Görnhardt", "Tim Fingscheidt"], "title": "A Lightweight Image Super-Resolution Transformer Trained on Low-Resolution Images Only", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Transformer architectures prominently lead single-image super-resolution\n(SISR) benchmarks, reconstructing high-resolution (HR) images from their\nlow-resolution (LR) counterparts. Their strong representative power, however,\ncomes with a higher demand for training data compared to convolutional neural\nnetworks (CNNs). For many real-world SR applications, the availability of\nhigh-quality HR training images is not given, sparking interest in LR-only\ntraining methods. The LR-only SISR benchmark mimics this condition by allowing\nonly low-resolution (LR) images for model training. For a 4x super-resolution,\nthis effectively reduces the amount of available training data to 6.25% of the\nHR image pixels, which puts the employment of a data-hungry transformer model\ninto question. In this work, we are the first to utilize a lightweight vision\ntransformer model with LR-only training methods addressing the unsupervised\nSISR LR-only benchmark. We adopt and configure a recent LR-only training method\nfrom microscopy image super-resolution to macroscopic real-world data,\nresulting in our multi-scale training method for bicubic degradation (MSTbic).\nFurthermore, we compare it with reference methods and prove its effectiveness\nboth for a transformer and a CNN model. We evaluate on the classic SR benchmark\ndatasets Set5, Set14, BSD100, Urban100, and Manga109, and show superior\nperformance over state-of-the-art (so far: CNN-based) LR-only SISR methods. The\ncode is available on GitHub:\nhttps://github.com/ifnspaml/SuperResolutionMultiscaleTraining.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23752", "pdf": "https://arxiv.org/pdf/2503.23752", "abs": "https://arxiv.org/abs/2503.23752", "authors": ["Jin Zhou", "Yi Zhou", "Pengfei Xu", "Hui Huang"], "title": "StrokeFusion: Vector Sketch Generation via Joint Stroke-UDF Encoding and Latent Sequence Diffusion", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "In the field of sketch generation, raster-format trained models often produce\nnon-stroke artifacts, while vector-format trained models typically lack a\nholistic understanding of sketches, leading to compromised recognizability.\nMoreover, existing methods struggle to extract common features from similar\nelements (e.g., eyes of animals) appearing at varying positions across\nsketches. To address these challenges, we propose StrokeFusion, a two-stage\nframework for vector sketch generation. It contains a dual-modal sketch feature\nlearning network that maps strokes into a high-quality latent space. This\nnetwork decomposes sketches into normalized strokes and jointly encodes stroke\nsequences with Unsigned Distance Function (UDF) maps, representing sketches as\nsets of stroke feature vectors. Building upon this representation, our\nframework exploits a stroke-level latent diffusion model that simultaneously\nadjusts stroke position, scale, and trajectory during generation. This enables\nhigh-fidelity sketch generation while supporting stroke interpolation editing.\nExtensive experiments on the QuickDraw dataset demonstrate that our framework\noutperforms state-of-the-art techniques, validating its effectiveness in\npreserving structural integrity and semantic features. Code and models will be\nmade publicly available upon publication.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23877", "pdf": "https://arxiv.org/pdf/2503.23877", "abs": "https://arxiv.org/abs/2503.23877", "authors": ["Junyao Shi", "Zhuolun Zhao", "Tianyou Wang", "Ian Pedroza", "Amy Luo", "Jie Wang", "Jason Ma", "Dinesh Jayaraman"], "title": "ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "ICRA 2025. Project website: https://zeromimic.github.io/", "summary": "Many recent advances in robotic manipulation have come through imitation\nlearning, yet these rely largely on mimicking a particularly hard-to-acquire\nform of demonstrations: those collected on the same robot in the same room with\nthe same objects as the trained policy must handle at test time. In contrast,\nlarge pre-recorded human video datasets demonstrating manipulation skills\nin-the-wild already exist, which contain valuable information for robots. Is it\npossible to distill a repository of useful robotic skill policies out of such\ndata without any additional requirements on robot-specific demonstrations or\nexploration? We present the first such system ZeroMimic, that generates\nimmediately deployable image goal-conditioned skill policies for several common\ncategories of manipulation tasks (opening, closing, pouring, pick&place,\ncutting, and stirring) each capable of acting upon diverse objects and across\ndiverse unseen task setups. ZeroMimic is carefully designed to exploit recent\nadvances in semantic and geometric visual understanding of human videos,\ntogether with modern grasp affordance detectors and imitation policy classes.\nAfter training ZeroMimic on the popular EpicKitchens dataset of ego-centric\nhuman videos, we evaluate its out-of-the-box performance in varied real-world\nand simulated kitchen settings with two different robot embodiments,\ndemonstrating its impressive abilities to handle these varied tasks. To enable\nplug-and-play reuse of ZeroMimic policies on other task setups and robots, we\nrelease software and policy checkpoints of our skill policies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24354", "pdf": "https://arxiv.org/pdf/2503.24354", "abs": "https://arxiv.org/abs/2503.24354", "authors": ["Rana Muhammad Shahroz Khan", "Dongwen Tang", "Pingzhi Li", "Kai Wang", "Tianlong Chen"], "title": "ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Parameter generation has emerged as a novel paradigm for neural network\ndevelopment, offering an alternative to traditional neural network training by\nsynthesizing high-quality model weights directly. In the context of Low-Rank\nAdaptation (LoRA) for evolving ($\\textit{i.e.}$, constantly updated) large\nlanguage models (LLMs), this approach promises efficient adaptation without\ncostly retraining. However, existing methods face critical limitations in\nsimultaneously achieving scalability and controllability. In this paper, we\nintroduce $\\texttt{ORAL}$, a novel $\\textbf{conditional recurrent diffusion}$\nframework that addresses these challenges. $\\texttt{ORAL}$ incorporates a novel\nconditioning mechanism that integrates model architecture and textual task\nspecifications, enabling the generation of task-specific LoRA parameters that\ncan seamlessly transfer across evolving foundation models. Our approach\nsuccessfully scales to billions-of-parameter LLMs and maintains\ncontrollability. Through extensive experiments across seven language tasks,\nfour vision tasks, and three multimodal tasks using five pre-trained LLMs, we\ndemonstrate that $\\texttt{ORAL}$ generates high-quality LoRA parameters that\nachieve comparable or superior performance to vanilla trained counterparts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
