{"id": "2504.01005", "pdf": "https://arxiv.org/pdf/2504.01005", "abs": "https://arxiv.org/abs/2504.01005", "authors": ["Nishad Singhi", "Hritik Bansal", "Arian Hosseini", "Aditya Grover", "Kai-Wei Chang", "Marcus Rohrbach", "Anna Rohrbach"], "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "29 pages", "summary": "Scaling test-time compute has emerged as a key strategy for enhancing the\nreasoning capabilities of large language models (LLMs), particularly in tasks\nlike mathematical problem-solving. A traditional approach, Self-Consistency\n(SC), generates multiple solutions to a problem and selects the most common\nanswer via majority voting. Another common method involves scoring each\nsolution with a reward model (verifier) and choosing the best one. Recent\nadvancements in Generative Reward Models (GenRM) reframe verification as a\nnext-token prediction task, enabling inference-time scaling along a new axis.\nSpecifically, GenRM generates multiple verification chains-of-thought to score\neach solution. Under a limited inference budget, this introduces a fundamental\ntrade-off: should you spend the budget on scaling solutions via SC or generate\nfewer solutions and allocate compute to verification via GenRM? To address\nthis, we evaluate GenRM against SC under a fixed inference budget.\nInterestingly, we find that SC is more compute-efficient than GenRM for most\npractical inference budgets across diverse models and datasets. For instance,\nGenRM first matches SC after consuming up to 8x the inference compute and\nrequires significantly more compute to outperform it. Furthermore, we derive\ninference scaling laws for the GenRM paradigm, revealing that compute-optimal\ninference favors scaling solution generation more aggressively than scaling the\nnumber of verifications. Our work provides practical guidance on optimizing\ntest-time scaling by balancing solution generation and verification. The code\nis available at https://github.com/nishadsinghi/sc-genrm-scaling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time", "scaling", "test-time compute", "inference compute"], "score": 5}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00891", "pdf": "https://arxiv.org/pdf/2504.00891", "abs": "https://arxiv.org/abs/2504.00891", "authors": ["Jian Zhao", "Runze Liu", "Kaiyan Zhang", "Zhimu Zhou", "Junqi Gao", "Dong Li", "Jiafei Lyu", "Zhouyi Qian", "Biqing Qi", "Xiu Li", "Bowen Zhou"], "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have shown that it is\npromising to utilize Process Reward Models (PRMs) as verifiers to enhance the\nperformance of LLMs. However, current PRMs face three key challenges: (1)\nlimited process supervision and generalization capabilities, (2) dependence on\nscalar value prediction without leveraging the generative abilities of LLMs,\nand (3) inability to scale the test-time compute of PRMs. In this work, we\nintroduce GenPRM, a generative process reward model that performs explicit\nChain-of-Thought (CoT) reasoning with code verification before providing\njudgment for each reasoning step. To obtain high-quality process supervision\nlabels and rationale data, we propose Relative Progress Estimation (RPE) and a\nrationale synthesis framework that incorporates code verification. Experimental\nresults on ProcessBench and several mathematical reasoning tasks show that\nGenPRM significantly outperforms prior PRMs with only 23K training data from\nMATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and\na 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally,\nGenPRM demonstrates strong abilities to serve as a critic model for policy\nmodel refinement. This work establishes a new paradigm for process supervision\nthat bridges the gap between PRMs and critic models in LLMs. Our code, model,\nand data will be available in https://ryanliu112.github.io/GenPRM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale", "test-time compute"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00869", "pdf": "https://arxiv.org/pdf/2504.00869", "abs": "https://arxiv.org/abs/2504.00869", "authors": ["Xiaoke Huang", "Juncheng Wu", "Hui Liu", "Xianfeng Tang", "Yuyin Zhou"], "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages; 7 figures; Data, code, and models:\n  https://github.com/UCSC-VLAA/m1", "summary": "Test-time scaling has emerged as a powerful technique for enhancing the\nreasoning capabilities of large language models. However, its effectiveness in\nmedical reasoning remains uncertain, as the medical domain fundamentally\ndiffers from mathematical tasks in terms of knowledge representation and\ndecision-making processes. In this paper, we provide the first comprehensive\ninvestigation of test-time scaling for medical reasoning and present m1, a\nsimple yet effective approach that increases a model's medical reasoning\ncapability at inference. Our evaluation across diverse medical tasks\ndemonstrates that test-time scaling consistently enhances medical reasoning,\nenabling lightweight fine-tuned models under 10B parameters to establish new\nstate-of-the-art performance, while our 32B model rivals previous 70B-scale\nmedical LLMs. However, we identify an optimal reasoning token budget of\napproximately 4K, beyond which performance may degrade due to overthinking.\nBudget forcing, which extends test-time computation through iterative prompts,\nhelps models double-check answers but does not necessarily improve the overall\nmedical QA performance and, in some cases, even introduces errors into\npreviously correct responses. Our case-by-case analysis identifies insufficient\nmedical knowledge as a key bottleneck that prevents further performance gains\nthrough test-time scaling. We find that increasing data scale, improving data\nquality, and expanding model capacity consistently enhance medical knowledge\ngrounding, enabling continued performance improvements, particularly on\nchallenging medical benchmarks where smaller models reach saturation. These\nfindings underscore fundamental differences between medical and mathematical\nreasoning in LLMs, highlighting that enriched medical knowledge, other than\nincreased reasoning depth alone, is essential for realizing the benefits of\ntest-time scaling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00993", "pdf": "https://arxiv.org/pdf/2504.00993", "abs": "https://arxiv.org/abs/2504.00993", "authors": ["Juncheng Wu", "Wenlong Deng", "Xingxuan Li", "Sheng Liu", "Taomian Mi", "Yifan Peng", "Ziyang Xu", "Yi Liu", "Hyunjin Cho", "Chang-In Choi", "Yihan Cao", "Hui Ren", "Xiang Li", "Xiaoxiao Li", "Yuyin Zhou"], "title": "MedReason: Eliciting Factual Medical Reasoning Steps in LLMs via Knowledge Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical tasks such as diagnosis and treatment planning require precise and\ncomplex reasoning, particularly in life-critical domains. Unlike mathematical\nreasoning, medical reasoning demands meticulous, verifiable thought processes\nto ensure reliability and accuracy. However, there is a notable lack of\ndatasets that provide transparent, step-by-step reasoning to validate and\nenhance the medical reasoning ability of AI models. To bridge this gap, we\nintroduce MedReason, a large-scale high-quality medical reasoning dataset\ndesigned to enable faithful and explainable medical problem-solving in large\nlanguage models (LLMs). We utilize a structured medical knowledge graph (KG) to\nconvert clinical QA pairs into logical chains of reasoning, or ``thinking\npaths'', which trace connections from question elements to answers via relevant\nKG entities. Each path is validated for consistency with clinical logic and\nevidence-based medicine. Our pipeline generates detailed reasoning for various\nmedical questions from 7 medical datasets, resulting in a dataset of 32,682\nquestion-answer pairs, each with detailed, step-by-step explanations.\nExperiments demonstrate that fine-tuning with our dataset consistently boosts\nmedical problem-solving capabilities, achieving significant gains of up to 7.7%\nfor DeepSeek-Ditill-8B. Our top-performing model, MedReason-8B, outperforms the\nHuatuo-o1-8B, a state-of-the-art medical reasoning model, by up to 4.2% on the\nclinical benchmark MedBullets. We also engage medical professionals from\ndiverse specialties to assess our dataset's quality, ensuring MedReason offers\naccurate and coherent medical reasoning. Our data, models, and code will be\npublicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "o1", "reasoning model"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00294", "pdf": "https://arxiv.org/pdf/2504.00294", "abs": "https://arxiv.org/abs/2504.00294", "authors": ["Vidhisha Balachandran", "Jingya Chen", "Lingjiao Chen", "Shivam Garg", "Neel Joshi", "Yash Lara", "John Langford", "Besmira Nushi", "Vibhav Vineet", "Yue Wu", "Safoora Yousefi"], "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2"], "comment": null, "summary": "Inference-time scaling can enhance the reasoning capabilities of large\nlanguage models (LLMs) on complex problems that benefit from step-by-step\nproblem solving. Although lengthening generated scratchpads has proven\neffective for mathematical tasks, the broader impact of this approach on other\ntasks remains less clear. In this work, we investigate the benefits and\nlimitations of scaling methods across nine state-of-the-art models and eight\nchallenging tasks, including math and STEM reasoning, calendar planning,\nNP-hard problems, navigation, and spatial reasoning. We compare conventional\nmodels (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g.,\no1) through evaluation protocols that involve repeated model calls, either\nindependently or sequentially with feedback. These evaluations approximate\nlower and upper performance bounds and potential for future performance\nimprovements for each model, whether through enhanced training or multi-model\ninference systems. Our extensive empirical analysis reveals that the advantages\nof inference-time scaling vary across tasks and diminish as problem complexity\nincreases. In addition, simply using more tokens does not necessarily translate\nto higher accuracy in these challenging regimes. Results from multiple\nindependent runs with conventional models using perfect verifiers show that,\nfor some tasks, these models can achieve performance close to the average\nperformance of today's most advanced reasoning models. However, for other\ntasks, a significant performance gap remains, even in very high scaling\nregimes. Encouragingly, all models demonstrate significant gains when inference\nis further scaled with perfect verifiers or strong feedback, suggesting ample\npotential for future improvements.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "o1"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00270", "pdf": "https://arxiv.org/pdf/2504.00270", "abs": "https://arxiv.org/abs/2504.00270", "authors": ["Tianqi", "Ding", "Dawei Xiang", "Yijiashun Qi", "Ze Yang", "Zunduo Zhao", "Tianyao Sun", "Pengbin Feng", "Haoyu Wang"], "title": "NeRF-Based defect detection", "categories": ["cs.CV"], "comment": "6 pages, 11 figures, 2025 2nd International Conference on Remote\n  Sensing, Mapping and Image Processing (RSMIP 2025)", "summary": "The rapid growth of industrial automation has highlighted the need for\nprecise and efficient defect detection in large-scale machinery. Traditional\ninspection techniques, involving manual procedures such as scaling tall\nstructures for visual evaluation, are labor-intensive, subjective, and often\nhazardous. To overcome these challenges, this paper introduces an automated\ndefect detection framework built on Neural Radiance Fields (NeRF) and the\nconcept of digital twins. The system utilizes UAVs to capture images and\nreconstruct 3D models of machinery, producing both a standard reference model\nand a current-state model for comparison. Alignment of the models is achieved\nthrough the Iterative Closest Point (ICP) algorithm, enabling precise point\ncloud analysis to detect deviations that signify potential defects. By\neliminating manual inspection, this method improves accuracy, enhances\noperational safety, and offers a scalable solution for defect detection. The\nproposed approach demonstrates great promise for reliable and efficient\nindustrial applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "accuracy"], "score": 3}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00748", "pdf": "https://arxiv.org/pdf/2504.00748", "abs": "https://arxiv.org/abs/2504.00748", "authors": ["Yunsoo Kim", "Michal W. S. Ong", "Daniel W. Rogalsky", "Manuel Rodriguez-Justo", "Honghan Wu", "Adam P. Levine"], "title": "IHC-LLMiner: Automated extraction of tumour immunohistochemical profiles from PubMed abstracts using large language models", "categories": ["cs.CL"], "comment": "currently under review", "summary": "Immunohistochemistry (IHC) is essential in diagnostic pathology and\nbiomedical research, offering critical insights into protein expression and\ntumour biology. This study presents an automated pipeline, IHC-LLMiner, for\nextracting IHC-tumour profiles from PubMed abstracts, leveraging advanced\nbiomedical text mining. There are two subtasks: abstract classification\n(include/exclude as relevant) and IHC-tumour profile extraction on relevant\nincluded abstracts. The best-performing model, \"Gemma-2 finetuned\", achieved\n91.5% accuracy and an F1 score of 91.4, outperforming GPT4-O by 9.5% accuracy\nwith 5.9 times faster inference time. From an initial dataset of 107,759\nabstracts identified for 50 immunohistochemical markers, the classification\ntask identified 30,481 relevant abstracts (Include) using the Gemma-2 finetuned\nmodel. For IHC-tumour profile extraction, the Gemma-2 finetuned model achieved\nthe best performance with 63.3% Correct outputs. Extracted IHC-tumour profiles\n(tumour types and markers) were normalised to Unified Medical Language System\n(UMLS) concepts to ensure consistency and facilitate IHC-tumour profile\nlandscape analysis. The extracted IHC-tumour profiles demonstrated excellent\nconcordance with available online summary data and provided considerable added\nvalue in terms of both missing IHC-tumour profiles and quantitative\nassessments. Our proposed LLM based pipeline provides a practical solution for\nlarge-scale IHC-tumour profile data mining, enhancing the accessibility and\nutility of such data for research and clinical applications as well as enabling\nthe generation of quantitative and structured data to support cancer-specific\nknowledge base development. Models and training datasets are available at\nhttps://github.com/knowlab/IHC-LLMiner.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00810", "pdf": "https://arxiv.org/pdf/2504.00810", "abs": "https://arxiv.org/abs/2504.00810", "authors": ["Zhaojian Yu", "Yinghao Wu", "Yilun Zhao", "Arman Cohan", "Xiao-Ping Zhang"], "title": "Z1: Efficient Test-time Scaling with Code", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) can achieve enhanced complex problem-solving\nthrough test-time computing scaling, yet this often entails longer contexts and\nnumerous reasoning token costs. In this paper, we propose an efficient\ntest-time scaling method that trains LLMs on code-related reasoning\ntrajectories, facilitating their reduction of excess thinking tokens while\nmaintaining performance. First, we create Z1-Code-Reasoning-107K, a curated\ndataset of simple and complex coding problems paired with their short and long\nsolution trajectories. Second, we present a novel Shifted Thinking Window to\nmitigate overthinking overhead by removing context-delimiting tags (e.g.,\n<think>. . . </think>) and capping reasoning tokens. Trained with long and\nshort trajectory data and equipped with Shifted Thinking Window, our model,\nZ1-7B, demonstrates the ability to adjust its reasoning level as the complexity\nof problems and exhibits efficient test-time scaling across different reasoning\ntasks that matches R1-Distill-Qwen-7B performance with about 30% of its average\nthinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B\ndemonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond).\nOur analysis of efficient reasoning elicitation also provides valuable insights\nfor future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00879", "pdf": "https://arxiv.org/pdf/2504.00879", "abs": "https://arxiv.org/abs/2504.00879", "authors": ["Fenglei Hao", "Yuliang Yang", "Ruiyuan Su", "Zhengran Zhao", "Yukun Qiao", "Mengyu Zhu"], "title": "WISE-TTT:Worldwide Information Segmentation Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Video multi-target segmentation remains a major challenge in long sequences,\nmainly due to the inherent limitations of existing architectures in capturing\nglobal temporal dependencies. We introduce WISE-TTT, a synergistic architecture\nintegrating Test-Time Training (TTT) mechanisms with the Transformer\narchitecture through co-design. The TTT layer systematically compresses\nhistorical temporal data to generate hidden states containing worldwide\ninformation(Lossless memory to maintain long contextual integrity), while\nachieving multi-stage contextual aggregation through splicing. Crucially, our\nframework provides the first empirical validation that implementing worldwide\ninformation across multiple network layers is essential for optimal dependency\nutilization.Ablation studies show TTT modules at high-level features boost\nglobal modeling. This translates to 3.1% accuracy improvement(J&F metric) on\nDavis2017 long-term benchmarks -- the first proof of hierarchical context\nsuperiority in video segmentation. We provide the first systematic evidence\nthat worldwide information critically impacts segmentation performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time training"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.01017", "pdf": "https://arxiv.org/pdf/2504.01017", "abs": "https://arxiv.org/abs/2504.01017", "authors": ["David Fan", "Shengbang Tong", "Jiachen Zhu", "Koustuv Sinha", "Zhuang Liu", "Xinlei Chen", "Michael Rabbat", "Nicolas Ballas", "Yann LeCun", "Amir Bar", "Saining Xie"], "title": "Scaling Language-Free Visual Representation Learning", "categories": ["cs.CV"], "comment": "Project page at https://davidfan.io/webssl/", "summary": "Visual Self-Supervised Learning (SSL) currently underperforms Contrastive\nLanguage-Image Pretraining (CLIP) in multimodal settings such as Visual\nQuestion Answering (VQA). This multimodal gap is often attributed to the\nsemantics introduced by language supervision, even though visual SSL and CLIP\nmodels are often trained on different data. In this work, we ask the question:\n\"Do visual self-supervised approaches lag behind CLIP due to the lack of\nlanguage supervision, or differences in the training data?\" We study this\nquestion by training both visual SSL and CLIP models on the same MetaCLIP data,\nand leveraging VQA as a diverse testbed for vision encoders. In this controlled\nsetup, visual SSL models scale better than CLIP models in terms of data and\nmodel capacity, and visual SSL performance does not saturate even after scaling\nup to 7B parameters. Consequently, we observe visual SSL methods achieve\nCLIP-level performance on a wide range of VQA and classic vision benchmarks.\nThese findings demonstrate that pure visual SSL can match language-supervised\nvisual pretraining at scale, opening new opportunities for vision-centric\nrepresentation learning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["testbed", "question answering"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2503.24388", "pdf": "https://arxiv.org/pdf/2503.24388", "abs": "https://arxiv.org/abs/2503.24388", "authors": ["Zhonghan Zhao", "Wenwei Zhang", "Haian Huang", "Kuikun Liu", "Jianfei Gao", "Gaoang Wang", "Kai Chen"], "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than $17\\times$ sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00025", "pdf": "https://arxiv.org/pdf/2504.00025", "abs": "https://arxiv.org/abs/2504.00025", "authors": ["Uwe Peters", "Benjamin Chin-Yee"], "title": "Generalization Bias in Large Language Model Summarization of Scientific Research", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Artificial intelligence chatbots driven by large language models (LLMs) have\nthe potential to increase public science literacy and support scientific\nresearch, as they can quickly summarize complex scientific information in\naccessible terms. However, when summarizing scientific texts, LLMs may omit\ndetails that limit the scope of research conclusions, leading to\ngeneralizations of results broader than warranted by the original study. We\ntested 10 prominent LLMs, including ChatGPT-4o, ChatGPT-4.5, DeepSeek, LLaMA\n3.3 70B, and Claude 3.7 Sonnet, comparing 4900 LLM-generated summaries to their\noriginal scientific texts. Even when explicitly prompted for accuracy, most\nLLMs produced broader generalizations of scientific results than those in the\noriginal texts, with DeepSeek, ChatGPT-4o, and LLaMA 3.3 70B overgeneralizing\nin 26 to 73% of cases. In a direct comparison of LLM-generated and\nhuman-authored science summaries, LLM summaries were nearly five times more\nlikely to contain broad generalizations (OR = 4.85, 95% CI [3.06, 7.70]).\nNotably, newer models tended to perform worse in generalization accuracy than\nearlier ones. Our results indicate a strong bias in many widely used LLMs\ntowards overgeneralizing scientific conclusions, posing a significant risk of\nlarge-scale misinterpretations of research findings. We highlight potential\nmitigation strategies, including lowering LLM temperature settings and\nbenchmarking LLMs for generalization accuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "summarization"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00037", "pdf": "https://arxiv.org/pdf/2504.00037", "abs": "https://arxiv.org/abs/2504.00037", "authors": ["Guoyizhe Wei", "Rama Chellappa"], "title": "ViT-Linearizer: Distilling Quadratic Knowledge into Linear-Time Vision Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Transformers (ViTs) have delivered remarkable progress through global\nself-attention, yet their quadratic complexity can become prohibitive for\nhigh-resolution inputs. In this work, we present ViT-Linearizer, a\ncross-architecture distillation framework that transfers rich ViT\nrepresentations into a linear-time, recurrent-style model. Our approach\nleverages 1) activation matching, an intermediate constraint that encourages\nstudent to align its token-wise dependencies with those produced by the\nteacher, and 2) masked prediction, a contextual reconstruction objective that\nrequires the student to predict the teacher's representations for unseen\n(masked) tokens, to effectively distill the quadratic self-attention knowledge\ninto the student while maintaining efficient complexity. Empirically, our\nmethod provides notable speedups particularly for high-resolution tasks,\nsignificantly addressing the hardware challenges in inference. Additionally, it\nalso elevates Mamba-based architectures' performance on standard vision\nbenchmarks, achieving a competitive 84.3% top-1 accuracy on ImageNet with a\nbase-sized model. Our results underscore the good potential of RNN-based\nsolutions for large-scale visual tasks, bridging the gap between theoretical\nefficiency and real-world practice.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00139", "pdf": "https://arxiv.org/pdf/2504.00139", "abs": "https://arxiv.org/abs/2504.00139", "authors": ["Yannick Burkhardt", "Simon Schaefer", "Stefan Leutenegger"], "title": "SuperEvent: Cross-Modal Learning of Event-based Keypoint Detection", "categories": ["cs.CV"], "comment": "In Review for ICCV25", "summary": "Event-based keypoint detection and matching holds significant potential,\nenabling the integration of event sensors into highly optimized Visual SLAM\nsystems developed for frame cameras over decades of research. Unfortunately,\nexisting approaches struggle with the motion-dependent appearance of keypoints\nand the complex noise prevalent in event streams, resulting in severely limited\nfeature matching capabilities and poor performance on downstream tasks. To\nmitigate this problem, we propose SuperEvent, a data-driven approach to predict\nstable keypoints with expressive descriptors. Due to the absence of event\ndatasets with ground truth keypoint labels, we leverage existing frame-based\nkeypoint detectors on readily available event-aligned and synchronized\ngray-scale frames for self-supervision: we generate temporally sparse keypoint\npseudo-labels considering that events are a product of both scene appearance\nand camera motion. Combined with our novel, information-rich event\nrepresentation, we enable SuperEvent to effectively learn robust keypoint\ndetection and description in event streams. Finally, we demonstrate the\nusefulness of SuperEvent by its integration into a modern sparse keypoint and\ndescriptor-based SLAM framework originally developed for traditional cameras,\nsurpassing the state-of-the-art in event-based SLAM by a wide margin. Source\ncode and multimedia material are available at\nsmartroboticslab.github.io/SuperEvent.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00191", "pdf": "https://arxiv.org/pdf/2504.00191", "abs": "https://arxiv.org/abs/2504.00191", "authors": ["Lin Zhao", "Xin Yu", "Yikang Liu", "Xiao Chen", "Eric Z. Chen", "Terrence Chen", "Shanhui Sun"], "title": "Leveraging Diffusion Model and Image Foundation Model for Improved Correspondence Matching in Coronary Angiography", "categories": ["cs.CV"], "comment": null, "summary": "Accurate correspondence matching in coronary angiography images is crucial\nfor reconstructing 3D coronary artery structures, which is essential for\nprecise diagnosis and treatment planning of coronary artery disease (CAD).\nTraditional matching methods for natural images often fail to generalize to\nX-ray images due to inherent differences such as lack of texture, lower\ncontrast, and overlapping structures, compounded by insufficient training data.\nTo address these challenges, we propose a novel pipeline that generates\nrealistic paired coronary angiography images using a diffusion model\nconditioned on 2D projections of 3D reconstructed meshes from Coronary Computed\nTomography Angiography (CCTA), providing high-quality synthetic data for\ntraining. Additionally, we employ large-scale image foundation models to guide\nfeature aggregation, enhancing correspondence matching accuracy by focusing on\nsemantically relevant regions and keypoints. Our approach demonstrates superior\nmatching performance on synthetic datasets and effectively generalizes to\nreal-world datasets, offering a practical solution for this task. Furthermore,\nour work investigates the efficacy of different foundation models in\ncorrespondence matching, providing novel insights into leveraging advanced\nimage foundation models for medical imaging applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00247", "pdf": "https://arxiv.org/pdf/2504.00247", "abs": "https://arxiv.org/abs/2504.00247", "authors": ["S. Mazdak Abulnaga", "Andrew Hoopes", "Neel Dey", "Malte Hoffmann", "Marianne Rakic", "Bruce Fischl", "John Guttag", "Adrian Dalca"], "title": "MultiMorph: On-demand Atlas Construction", "categories": ["cs.CV", "cs.AI"], "comment": "accepted to CVPR 2025", "summary": "We present MultiMorph, a fast and efficient method for constructing\nanatomical atlases on the fly. Atlases capture the canonical structure of a\ncollection of images and are essential for quantifying anatomical variability\nacross populations. However, current atlas construction methods often require\ndays to weeks of computation, thereby discouraging rapid experimentation. As a\nresult, many scientific studies rely on suboptimal, precomputed atlases from\nmismatched populations, negatively impacting downstream analyses. MultiMorph\naddresses these challenges with a feedforward model that rapidly produces\nhigh-quality, population-specific atlases in a single forward pass for any 3D\nbrain dataset, without any fine-tuning or optimization. MultiMorph is based on\na linear group-interaction layer that aggregates and shares features within the\ngroup of input images. Further, by leveraging auxiliary synthetic data,\nMultiMorph generalizes to new imaging modalities and population groups at\ntest-time. Experimentally, MultiMorph outperforms state-of-the-art\noptimization-based and learning-based atlas construction methods in both small\nand large population settings, with a 100-fold reduction in time. This makes\nMultiMorph an accessible framework for biomedical researchers without machine\nlearning expertise, enabling rapid, high-quality atlas generation for diverse\nstudies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00274", "pdf": "https://arxiv.org/pdf/2504.00274", "abs": "https://arxiv.org/abs/2504.00274", "authors": ["Joshua Rodriguez", "Om Sanan", "Guillermo Vizarreta-Luna", "Steven A. Conrad"], "title": "Text Chunking for Document Classification for Urban System Management using Large Language Models", "categories": ["cs.CL", "cs.HC", "I.7.5; J.1"], "comment": "16 pages, 6 figures, 4 tables, 2 algorithms; Replication data and\n  code can be found https://github.com/josh-rodriguez-csu/ChunkingforLLMs", "summary": "Urban systems are managed using complex textual documentation that need\ncoding and analysis to set requirements and evaluate built environment\nperformance. This paper contributes to the study of applying large-language\nmodels (LLM) to qualitative coding activities to reduce resource requirements\nwhile maintaining comparable reliability to humans. Qualitative coding and\nassessment face challenges like resource limitations and bias, accuracy, and\nconsistency between human evaluators. Here we report the application of LLMs to\ndeductively code 10 case documents on the presence of 17 digital twin\ncharacteristics for the management of urban systems. We utilize two prompting\nmethods to compare the semantic processing of LLMs with human coding efforts:\nwhole text analysis and text chunk analysis using OpenAI's GPT-4o, GPT-4o-mini,\nand o1-mini models. We found similar trends of internal variability between\nmethods and results indicate that LLMs may perform on par with human coders\nwhen initialized with specific deductive coding contexts. GPT-4o, o1-mini and\nGPT-4o-mini showed significant agreement with human raters when employed using\na chunking method. The application of both GPT-4o and GPT-4o-mini as an\nadditional rater with three manual raters showed statistically significant\nagreement across all raters, indicating that the analysis of textual documents\nis benefited by LLMs. Our findings reveal nuanced sub-themes of LLM application\nsuggesting LLMs follow human memory coding processes where whole-text analysis\nmay introduce multiple meanings. The novel contributions of this paper lie in\nassessing the performance of OpenAI GPT models and introduces the chunk-based\nprompting approach, which addresses context aggregation biases by preserving\nlocalized context.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["agreement", "consistency", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00394", "pdf": "https://arxiv.org/pdf/2504.00394", "abs": "https://arxiv.org/abs/2504.00394", "authors": ["Lei Wang", "Yujie Zhong", "Xiaopeng Sun", "Jingchun Cheng", "Chengjian Feng", "Qiong Cao", "Lin Ma", "Zhaoxin Fan"], "title": "AP-CAP: Advancing High-Quality Data Synthesis for Animal Pose Estimation via a Controllable Image Generation Pipeline", "categories": ["cs.CV"], "comment": null, "summary": "The task of 2D animal pose estimation plays a crucial role in advancing deep\nlearning applications in animal behavior analysis and ecological research.\nDespite notable progress in some existing approaches, our study reveals that\nthe scarcity of high-quality datasets remains a significant bottleneck,\nlimiting the full potential of current methods. To address this challenge, we\npropose a novel Controllable Image Generation Pipeline for synthesizing animal\npose estimation data, termed AP-CAP. Within this pipeline, we introduce a\nMulti-Modal Animal Image Generation Model capable of producing images with\nexpected poses. To enhance the quality and diversity of the generated data, we\nfurther propose three innovative strategies: (1) Modality-Fusion-Based Animal\nImage Synthesis Strategy to integrate multi-source appearance representations,\n(2) Pose-Adjustment-Based Animal Image Synthesis Strategy to dynamically\ncapture diverse pose variations, and (3) Caption-Enhancement-Based Animal Image\nSynthesis Strategy to enrich visual semantic understanding. Leveraging the\nproposed model and strategies, we create the MPCH Dataset\n(Modality-Pose-Caption Hybrid), the first hybrid dataset that innovatively\ncombines synthetic and real data, establishing the largest-scale multi-source\nheterogeneous benchmark repository for animal pose estimation to date.\nExtensive experiments demonstrate the superiority of our method in improving\nboth the performance and generalization capability of animal pose estimators.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00406", "pdf": "https://arxiv.org/pdf/2504.00406", "abs": "https://arxiv.org/abs/2504.00406", "authors": ["Jiuzhou Han", "Wray Buntine", "Ehsan Shareghi"], "title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models demonstrate remarkable reasoning capabilities but often\nproduce unreliable or incorrect responses. Existing verification methods are\ntypically model-specific or domain-restricted, requiring significant\ncomputational resources and lacking scalability across diverse reasoning tasks.\nTo address these limitations, we propose VerifiAgent, a unified verification\nagent that integrates two levels of verification: meta-verification, which\nassesses completeness and consistency in model responses, and tool-based\nadaptive verification, where VerifiAgent autonomously selects appropriate\nverification tools based on the reasoning type, including mathematical,\nlogical, or commonsense reasoning. This adaptive approach ensures both\nefficiency and robustness across different verification scenarios. Experimental\nresults show that VerifiAgent outperforms baseline verification methods (e.g.,\ndeductive verifier, backward verifier) among all reasoning tasks. Additionally,\nit can further enhance reasoning accuracy by leveraging feedback from\nverification results. VerifiAgent can also be effectively applied to inference\nscaling, achieving better results with fewer generated samples and costs\ncompared to existing process reward models in the mathematical reasoning\ndomain. Code is available at https://github.com/Jiuzhouh/VerifiAgent", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00429", "pdf": "https://arxiv.org/pdf/2504.00429", "abs": "https://arxiv.org/abs/2504.00429", "authors": ["Yinghe Zhang", "Chi Liu", "Shuai Zhou", "Sheng Shen", "Peng Gui"], "title": "Unleashing the Power of Pre-trained Encoders for Universal Adversarial Attack Detection", "categories": ["cs.CV"], "comment": null, "summary": "Adversarial attacks pose a critical security threat to real-world AI systems\nby injecting human-imperceptible perturbations into benign samples to induce\nmisclassification in deep learning models. While existing detection methods,\nsuch as Bayesian uncertainty estimation and activation pattern analysis, have\nachieved progress through feature engineering, their reliance on handcrafted\nfeature design and prior knowledge of attack patterns limits generalization\ncapabilities and incurs high engineering costs. To address these limitations,\nthis paper proposes a lightweight adversarial detection framework based on the\nlarge-scale pre-trained vision-language model CLIP. Departing from conventional\nadversarial feature characterization paradigms, we innovatively adopt an\nanomaly detection perspective. By jointly fine-tuning CLIP's dual visual-text\nencoders with trainable adapter networks and learnable prompts, we construct a\ncompact representation space tailored for natural images. Notably, our\ndetection architecture achieves substantial improvements in generalization\ncapability across both known and unknown attack patterns compared to\ntraditional methods, while significantly reducing training overhead. This study\nprovides a novel technical pathway for establishing a parameter-efficient and\nattack-agnostic defense paradigm, markedly enhancing the robustness of vision\nsystems against evolving adversarial threats.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00437", "pdf": "https://arxiv.org/pdf/2504.00437", "abs": "https://arxiv.org/abs/2504.00437", "authors": ["Qi Song", "Chenghong Li", "Haotong Lin", "Sida Peng", "Rui Huang"], "title": "ADGaussian: Generalizable Gaussian Splatting for Autonomous Driving with Multi-modal Inputs", "categories": ["cs.CV"], "comment": "The project page can be found at\n  https://maggiesong7.github.io/research/ADGaussian/", "summary": "We present a novel approach, termed ADGaussian, for generalizable street\nscene reconstruction. The proposed method enables high-quality rendering from\nsingle-view input. Unlike prior Gaussian Splatting methods that primarily focus\non geometry refinement, we emphasize the importance of joint optimization of\nimage and depth features for accurate Gaussian prediction. To this end, we\nfirst incorporate sparse LiDAR depth as an additional input modality,\nformulating the Gaussian prediction process as a joint learning framework of\nvisual information and geometric clue. Furthermore, we propose a multi-modal\nfeature matching strategy coupled with a multi-scale Gaussian decoding model to\nenhance the joint refinement of multi-modal features, thereby enabling\nefficient multi-modal Gaussian learning. Extensive experiments on two\nlarge-scale autonomous driving datasets, Waymo and KITTI, demonstrate that our\nADGaussian achieves state-of-the-art performance and exhibits superior\nzero-shot generalization capabilities in novel-view shifting.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00623", "pdf": "https://arxiv.org/pdf/2504.00623", "abs": "https://arxiv.org/abs/2504.00623", "authors": ["Kazuki Yano", "Sho Takase", "Sosuke Kobayashi", "Shun Kiyono", "Jun Suzuki"], "title": "Efficient Construction of Model Family through Progressive Training Using Model Expansion", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) gain widespread practical application,\nproviding the model family of different parameter sizes has become standard\npractice to address diverse computational requirements. Conventionally, each\nmodel in a family is trained independently, resulting in computational costs\nthat scale additively with the number of models. We propose an efficient method\nfor constructing the model family through progressive training, where smaller\nmodels are incrementally expanded to larger sizes to create a complete model\nfamily. Through extensive experiments with a model family ranging from 1B to 8B\nparameters, we demonstrate that our method reduces computational costs by\napproximately 25% while maintaining comparable performance to independently\ntrained models. Furthermore, by strategically adjusting maximum learning rates\nbased on model size, our method outperforms the independent training across\nvarious metrics. Beyond performance gains, our approach offers an additional\nadvantage: models in our family tend to yield more consistent behavior across\ndifferent model sizes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00664", "pdf": "https://arxiv.org/pdf/2504.00664", "abs": "https://arxiv.org/abs/2504.00664", "authors": ["Motasem S Obeidat", "Md Sultan Al Nahian", "Ramakanth Kavuluru"], "title": "Do LLMs Surpass Encoders for Biomedical NER?", "categories": ["cs.CL"], "comment": "Accepted to appear in IEEE ICHI 2025", "summary": "Recognizing spans of biomedical concepts and their types (e.g., drug or gene)\nin free text, often called biomedical named entity recognition (NER), is a\nbasic component of information extraction (IE) pipelines. Without a strong NER\ncomponent, other applications, such as knowledge discovery and information\nretrieval, are not practical. State-of-the-art in NER shifted from traditional\nML models to deep neural networks with transformer-based encoder models (e.g.,\nBERT) emerging as the current standard. However, decoder models (also called\nlarge language models or LLMs) are gaining traction in IE. But LLM-driven NER\noften ignores positional information due to the generative nature of decoder\nmodels. Furthermore, they are computationally very expensive (both in inference\ntime and hardware needs). Hence, it is worth exploring if they actually excel\nat biomedical NER and assess any associated trade-offs (performance vs\nefficiency). This is exactly what we do in this effort employing the same BIO\nentity tagging scheme (that retains positional information) using five\ndifferent datasets with varying proportions of longer entities. Our results\nshow that the LLMs chosen (Mistral and Llama: 8B range) often outperform best\nencoder models (BERT-(un)cased, BiomedBERT, and DeBERTav3: 300M range) by 2-8%\nin F-scores except for one dataset, where they equal encoder performance. This\ngain is more prominent among longer entities of length >= 3 tokens. However,\nLLMs are one to two orders of magnitude more expensive at inference time and\nmay need cost prohibitive hardware. Thus, when performance differences are\nsmall or real time user feedback is needed, encoder models might still be more\nsuitable than LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00476", "pdf": "https://arxiv.org/pdf/2504.00476", "abs": "https://arxiv.org/abs/2504.00476", "authors": ["Haobo Yuan", "Tao Zhang", "Xiangtai Li", "Lu Qi", "Zilong Huang", "Shilin Xu", "Jiashi Feng", "Ming-Hsuan Yang"], "title": "4th PVUW MeViS 3rd Place Report: Sa2VA", "categories": ["cs.CV"], "comment": "Technical Report, 4 pages, Code:\n  https://github.com/magic-research/Sa2VA", "summary": "Referring video object segmentation (RVOS) is a challenging task that\nrequires the model to segment the object in a video given the language\ndescription. MeViS is a recently proposed dataset that contains motion\nexpressions of the target objects, leading to a challenging benchmark, compared\nwith existing RVOS benchmarks. On the other hand, for referring expression\ntasks, a new trend is to adopt multi-modal large language model (MLLM) to\nachieve better image and text alignment. In this report, we show that with a\nsimple modification to the test time inference method on stronger MLLMs, we can\nlead to stronger results on MeVIS. In particular, we adopt the recent method\nSa2VA, a unified model for dense grounded understanding of both images and\nvideos. By enlarging the scope of key frames, without any further training, we\ncan achieve the 3rd place in the 4th PVUW workshop.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00481", "pdf": "https://arxiv.org/pdf/2504.00481", "abs": "https://arxiv.org/abs/2504.00481", "authors": ["Yueru Chen", "Wei Zhang", "Dingquan Li", "Jing Wang", "Ge Li"], "title": "Hierarchical Attention Networks for Lossless Point Cloud Attribute Compression", "categories": ["cs.CV", "eess.SP"], "comment": "Accepted by DCC 2025", "summary": "In this paper, we propose a deep hierarchical attention context model for\nlossless attribute compression of point clouds, leveraging a multi-resolution\nspatial structure and residual learning. A simple and effective Level of Detail\n(LoD) structure is introduced to yield a coarse-to-fine representation. To\nenhance efficiency, points within the same refinement level are encoded in\nparallel, sharing a common context point group. By hierarchically aggregating\ninformation from neighboring points, our attention model learns contextual\ndependencies across varying scales and densities, enabling comprehensive\nfeature extraction. We also adopt normalization for position coordinates and\nattributes to achieve scale-invariant compression. Additionally, we segment the\npoint cloud into multiple slices to facilitate parallel processing, further\noptimizing time complexity. Experimental results demonstrate that the proposed\nmethod offers better coding performance than the latest G-PCC for color and\nreflectance attributes while maintaining more efficient encoding and decoding\nruntimes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00725", "pdf": "https://arxiv.org/pdf/2504.00725", "abs": "https://arxiv.org/abs/2504.00725", "authors": ["Matheus Belarmino", "Rackel Coelho", "Roberto Lotudo", "Jayr Pereira"], "title": "Aplicação de Large Language Models na Análise e Síntese de Documentos Jurídicos: Uma Revisão de Literatura", "categories": ["cs.CL"], "comment": "in Portuguese language", "summary": "Large Language Models (LLMs) have been increasingly used to optimize the\nanalysis and synthesis of legal documents, enabling the automation of tasks\nsuch as summarization, classification, and retrieval of legal information. This\nstudy aims to conduct a systematic literature review to identify the state of\nthe art in prompt engineering applied to LLMs in the legal context. The results\nindicate that models such as GPT-4, BERT, Llama 2, and Legal-Pegasus are widely\nemployed in the legal field, and techniques such as Few-shot Learning,\nZero-shot Learning, and Chain-of-Thought prompting have proven effective in\nimproving the interpretation of legal texts. However, challenges such as biases\nin models and hallucinations still hinder their large-scale implementation. It\nis concluded that, despite the great potential of LLMs for the legal field,\nthere is a need to improve prompt engineering strategies to ensure greater\naccuracy and reliability in the generated results.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy", "summarization"], "score": 3}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00756", "pdf": "https://arxiv.org/pdf/2504.00756", "abs": "https://arxiv.org/abs/2504.00756", "authors": ["Lin Zhang", "Zhouhong Gu", "Xiaoran Shi", "Hongwei Feng", "Yanghua Xiao"], "title": "RECKON: Large-scale Reference-based Efficient Knowledge Evaluation for Large Language Model", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) advance, efficient knowledge evaluation\nbecomes crucial to verifying their capabilities. Traditional methods, relying\non benchmarks, face limitations such as high resource costs and information\nloss. We propose the Large-scale Reference-based Efficient Knowledge Evaluation\nfor Large Language Model (RECKON), which directly uses reference data to\nevaluate models. RECKON organizes unstructured data into manageable units and\ngenerates targeted questions for each cluster, improving evaluation accuracy\nand efficiency. Experimental results show that RECKON reduces resource\nconsumption by 56.5% compared to traditional methods while achieving over 97%\naccuracy across various domains, including world knowledge, code, legal, and\nbiomedical datasets. Code is available at https://github.com/MikeGu721/reckon", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00647", "pdf": "https://arxiv.org/pdf/2504.00647", "abs": "https://arxiv.org/abs/2504.00647", "authors": ["Xinnan Zhu", "Yicheng Zhu", "Tixin Chen", "Wentao Wu", "Yuanjie Dang"], "title": "FDDet: Frequency-Decoupling for Boundary Refinement in Temporal Action Detection", "categories": ["cs.CV"], "comment": null, "summary": "Temporal action detection aims to locate and classify actions in untrimmed\nvideos. While recent works focus on designing powerful feature processors for\npre-trained representations, they often overlook the inherent noise and\nredundancy within these features. Large-scale pre-trained video encoders tend\nto introduce background clutter and irrelevant semantics, leading to context\nconfusion and imprecise boundaries. To address this, we propose a\nfrequency-aware decoupling network that improves action discriminability by\nfiltering out noisy semantics captured by pre-trained models. Specifically, we\nintroduce an adaptive temporal decoupling scheme that suppresses irrelevant\ninformation while preserving fine-grained atomic action details, yielding more\ntask-specific representations. In addition, we enhance inter-frame modeling by\ncapturing temporal variations to better distinguish actions from background\nredundancy. Furthermore, we present a long-short-term category-aware relation\nnetwork that jointly models local transitions and long-range dependencies,\nimproving localization precision. The refined atomic features and\nfrequency-guided dynamics are fed into a standard detection head to produce\naccurate action predictions. Extensive experiments on THUMOS14, HACS, and\nActivityNet-1.3 show that our method, powered by InternVideo2-6B features,\nachieves state-of-the-art performance on temporal action detection benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00759", "pdf": "https://arxiv.org/pdf/2504.00759", "abs": "https://arxiv.org/abs/2504.00759", "authors": ["Dehua Huo", "Weida Zhan", "Jinxin Guo", "Depeng Zhu", "Yu Chen", "YiChun Jiang", "Yueyi Han", "Deng Han", "Jin Li"], "title": "MSSFC-Net:Enhancing Building Interpretation with Multi-Scale Spatial-Spectral Feature Collaboration", "categories": ["cs.CV"], "comment": null, "summary": "Building interpretation from remote sensing imagery primarily involves two\nfundamental tasks: building extraction and change detection. However, most\nexisting methods address these tasks independently, overlooking their inherent\ncorrelation and failing to exploit shared feature representations for mutual\nenhancement. Furthermore, the diverse spectral,spatial, and scale\ncharacteristics of buildings pose additional challenges in jointly modeling\nspatial-spectral multi-scale features and effectively balancing precision and\nrecall. The limited synergy between spatial and spectral representations often\nresults in reduced detection accuracy and incomplete change localization.To\naddress these challenges, we propose a Multi-Scale Spatial-Spectral Feature\nCooperative Dual-Task Network (MSSFC-Net) for joint building extraction and\nchange detection in remote sensing images. The framework integrates both tasks\nwithin a unified architecture, leveraging their complementary nature to\nsimultaneously extract building and change features. Specifically,a Dual-branch\nMulti-scale Feature Extraction module (DMFE) with Spatial-Spectral Feature\nCollaboration (SSFC) is designed to enhance multi-scale representation\nlearning, effectively capturing shallow texture details and deep semantic\ninformation, thus improving building extraction performance. For temporal\nfeature aggregation, we introduce a Multi-scale Differential Fusion Module\n(MDFM) that explicitly models the interaction between differential and\ndual-temporal features. This module refines the network's capability to detect\nlarge-area changes and subtle structural variations in buildings. Extensive\nexperiments conducted on three benchmark datasets demonstrate that MSSFC-Net\nachieves superior performance in both building extraction and change detection\ntasks, effectively improving detection accuracy while maintaining completeness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00812", "pdf": "https://arxiv.org/pdf/2504.00812", "abs": "https://arxiv.org/abs/2504.00812", "authors": ["Yiqun Duan", "Sameera Ramasinghe", "Stephen Gould", "Ajanthan Thalaiyasingam"], "title": "Scaling Prompt Instructed Zero Shot Composed Image Retrieval with Image-Only Data", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Composed Image Retrieval (CIR) is the task of retrieving images matching a\nreference image augmented with a text, where the text describes changes to the\nreference image in natural language. Traditionally, models designed for CIR\nhave relied on triplet data containing a reference image, reformulation text,\nand a target image. However, curating such triplet data often necessitates\nhuman intervention, leading to prohibitive costs. This challenge has hindered\nthe scalability of CIR model training even with the availability of abundant\nunlabeled data. With the recent advances in foundational models, we advocate a\nshift in the CIR training paradigm where human annotations can be efficiently\nreplaced by large language models (LLMs). Specifically, we demonstrate the\ncapability of large captioning and language models in efficiently generating\ndata for CIR only relying on unannotated image collections. Additionally, we\nintroduce an embedding reformulation architecture that effectively combines\nimage and text modalities. Our model, named InstructCIR, outperforms\nstate-of-the-art methods in zero-shot composed image retrieval on CIRR and\nFashionIQ datasets. Furthermore, we demonstrate that by increasing the amount\nof generated data, our zero-shot model gets closer to the performance of\nsupervised baselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00848", "pdf": "https://arxiv.org/pdf/2504.00848", "abs": "https://arxiv.org/abs/2504.00848", "authors": ["Yushan Zhang", "Aljoša Ošep", "Laura Leal-Taixé", "Tim Meinhardt"], "title": "Zero-Shot 4D Lidar Panoptic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot 4D segmentation and recognition of arbitrary objects in Lidar is\ncrucial for embodied navigation, with applications ranging from streaming\nperception to semantic mapping and localization. However, the primary challenge\nin advancing research and developing generalized, versatile methods for\nspatio-temporal scene understanding in Lidar lies in the scarcity of datasets\nthat provide the necessary diversity and scale of annotations.To overcome these\nchallenges, we propose SAL-4D (Segment Anything in Lidar--4D), a method that\nutilizes multi-modal robotic sensor setups as a bridge to distill recent\ndevelopments in Video Object Segmentation (VOS) in conjunction with\noff-the-shelf Vision-Language foundation models to Lidar. We utilize VOS models\nto pseudo-label tracklets in short video sequences, annotate these tracklets\nwith sequence-level CLIP tokens, and lift them to the 4D Lidar space using\ncalibrated multi-modal sensory setups to distill them to our SAL-4D model. Due\nto temporal consistent predictions, we outperform prior art in 3D Zero-Shot\nLidar Panoptic Segmentation (LPS) over $5$ PQ, and unlock Zero-Shot 4D-LPS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00254", "pdf": "https://arxiv.org/pdf/2504.00254", "abs": "https://arxiv.org/abs/2504.00254", "authors": ["Huandong Chang", "Zicheng Ma", "Mingyuan Ma", "Zhenting Qi", "Andrew Sabot", "Hong Jiang", "H. T. Kung"], "title": "ElaLoRA: Elastic & Learnable Low-Rank Adaptation for Efficient Model Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted technique for\nfine-tuning large-scale pre-trained models with minimal parameter updates.\nHowever, existing methods rely on fixed ranks or focus solely on either rank\npruning or expansion, failing to adapt ranks dynamically to match the\nimportance of different layers during training. In this work, we propose\nElaLoRA, an adaptive low-rank adaptation framework that dynamically prunes and\nexpands ranks based on gradient-derived importance scores. To the best of our\nknowledge, ElaLoRA is the first method that enables both rank pruning and\nexpansion during fine-tuning. Experiments across multiple benchmarks\ndemonstrate that ElaLoRA consistently outperforms existing PEFT methods across\ndifferent parameter budgets. Furthermore, our studies validate that layers\nreceiving higher rank allocations contribute more significantly to model\nperformance, providing theoretical justification for our adaptive strategy. By\nintroducing a principled and adaptive rank allocation mechanism, ElaLoRA offers\na scalable and efficient fine-tuning solution, particularly suited for\nresource-constrained environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00883", "pdf": "https://arxiv.org/pdf/2504.00883", "abs": "https://arxiv.org/abs/2504.00883", "authors": ["Zhenyi Liao", "Qingsong Xie", "Yanhao Zhang", "Zijian Kong", "Haonan Lu", "Zhenyu Yang", "Zhijie Deng"], "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Increasing attention has been placed on improving the reasoning capacities of\nmulti-modal large language models (MLLMs). As the cornerstone for AI agents\nthat function in the physical realm, video-based visual-spatial intelligence\n(VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This\nwork conducts a first, in-depth study on improving the visual-spatial reasoning\nof MLLMs via R1-Zero-like training. Technically, we first identify that the\nvisual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models\ncannot be activated via Chain of Thought (CoT) prompts. We then incorporate\nGRPO training for improved visual-spatial reasoning, using the carefully\ncurated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation,\nwe identify the necessity to keep the KL penalty (even with a small value) in\nGRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from\nQwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o.\nMoreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves\nperformance comparable to that of the best open-source model\nLLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning\nand direct preference optimization baselines and observe strong performance\nsuperiority. The code and dataset will be available soon.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00509", "pdf": "https://arxiv.org/pdf/2504.00509", "abs": "https://arxiv.org/abs/2504.00509", "authors": ["Kai Yan", "Yufei Xu", "Zhengyin Du", "Xuesong Yao", "Zheyu Wang", "Xiaowen Guo", "Jiecao Chen"], "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "21 pages, 3 figures, 10 tables", "summary": "The rapid escalation from elementary school-level to frontier problems of the\ndifficulty for LLM benchmarks in recent years have weaved a miracle for\nresearchers that we are only inches away from surpassing human intelligence.\nHowever, is the LLMs' remarkable reasoning ability indeed comes from true\nintelligence by human standards, or are they simply reciting solutions\nwitnessed during training at an Internet level? To study this problem, we\npropose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's\nrecitation behavior when asked simple reasoning problems but with conditions\nsubtly shifted, and conduct empirical analysis on our benchmark. Surprisingly,\nwe found existing cutting-edge LLMs unanimously exhibits extremely severe\nrecitation behavior; by changing one phrase in the condition, top models such\nas OpenAI-o1 and DeepSeek-R1 can suffer $60\\%$ performance loss on elementary\nschool-level arithmetic and reasoning problems. Such findings are a wake-up\ncall to the LLM community that compels us to re-evaluate the true intelligence\nlevel of cutting-edge LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00532", "pdf": "https://arxiv.org/pdf/2504.00532", "abs": "https://arxiv.org/abs/2504.00532", "authors": ["Hongru Ma", "Yanjie Liang", "Jiasheng Si", "Weiyu Zhang", "Hongjiao Guan", "Chaoqun Zheng", "Bing Xu", "Wenpeng Lu"], "title": "SRLCG: Self-Rectified Large-Scale Code Generation with Multidimensional Chain-of-Thought and Dynamic Backtracking", "categories": ["cs.SE", "cs.CL"], "comment": "23 pages", "summary": "Large language models (LLMs) have revolutionized code generation,\nsignificantly enhancing developer productivity. However, for a vast number of\nusers with minimal coding knowledge, LLMs provide little support, as they\nprimarily generate isolated code snippets rather than complete, large-scale\nproject code. Without coding expertise, these users struggle to interpret,\nmodify, and iteratively refine the outputs of LLMs, making it impossible to\nassemble a complete project. To address this issue, we propose Self-Rectified\nLarge-Scale Code Generator (SRLCG), a framework that generates complete\nmulti-file project code from a single prompt. SRLCG employs a novel\nmultidimensional chain-of-thought (CoT) and self-rectification to guide LLMs in\ngenerating correct and robust code files, then integrates them into a complete\nand coherent project using our proposed dynamic backtracking algorithm.\nExperimental results show that SRLCG generates code 15x longer than\nDeepSeek-V3, 16x longer than GPT-4, and at least 10x longer than other leading\nCoT-based baselines. Furthermore, they confirm its improved correctness,\nrobustness, and performance compared to baselines in large-scale code\ngeneration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["code generation"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00908", "pdf": "https://arxiv.org/pdf/2504.00908", "abs": "https://arxiv.org/abs/2504.00908", "authors": ["Haoxuan Li", "Wei Song", "Aofan Liu", "Peiwu Qin"], "title": "DBF-UNet: A Two-Stage Framework for Carotid Artery Segmentation with Pseudo-Label Generation", "categories": ["cs.CV"], "comment": null, "summary": "Medical image analysis faces significant challenges due to limited annotation\ndata, particularly in three-dimensional carotid artery segmentation tasks,\nwhere existing datasets exhibit spatially discontinuous slice annotations with\nonly a small portion of expert-labeled slices in complete 3D volumetric data.\nTo address this challenge, we propose a two-stage segmentation framework.\nFirst, we construct continuous vessel centerlines by interpolating between\nannotated slice centroids and propagate labels along these centerlines to\ngenerate interpolated annotations for unlabeled slices. The slices with expert\nannotations are used for fine-tuning SAM-Med2D, while the interpolated labels\non unlabeled slices serve as prompts to guide segmentation during inference. In\nthe second stage, we propose a novel Dense Bidirectional Feature Fusion UNet\n(DBF-UNet). This lightweight architecture achieves precise segmentation of\ncomplete 3D vascular structures. The network incorporates bidirectional feature\nfusion in the encoder and integrates multi-scale feature aggregation with dense\nconnectivity for effective feature reuse. Experimental validation on public\ndatasets demonstrates that our proposed method effectively addresses the sparse\nannotation challenge in carotid artery segmentation while achieving superior\nperformance compared to existing approaches. The source code is available at\nhttps://github.com/Haoxuanli-Thu/DBF-UNet.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00939", "pdf": "https://arxiv.org/pdf/2504.00939", "abs": "https://arxiv.org/abs/2504.00939", "authors": ["Alexander Martin", "Reno Kriz", "William Gantt Walden", "Kate Sanders", "Hannah Recknor", "Eugene Yang", "Francis Ferraro", "Benjamin Van Durme"], "title": "WikiVideo: Article Generation from Multiple Videos", "categories": ["cs.CV", "cs.CL"], "comment": "Repo can be found here: https://github.com/alexmartin1722/wikivideo", "summary": "We present the challenging task of automatically creating a high-level\nWikipedia-style article that aggregates information from multiple diverse\nvideos about real-world events, such as natural disasters or political\nelections. Videos are intuitive sources for retrieval-augmented generation\n(RAG), but most contemporary RAG workflows focus heavily on text and existing\nmethods for video-based summarization focus on low-level scene understanding\nrather than high-level event semantics. To close this gap, we introduce\nWikiVideo, a benchmark consisting of expert-written articles and densely\nannotated videos that provide evidence for articles' claims, facilitating the\nintegration of video into RAG pipelines and enabling the creation of in-depth\ncontent that is grounded in multimodal sources. We further propose\nCollaborative Article Generation (CAG), a novel interactive method for article\ncreation from multiple videos. CAG leverages an iterative interaction between\nan r1-style reasoning model and a VideoLLM to draw higher level inferences\nabout the target event than is possible with VideoLLMs alone, which fixate on\nlow-level visual features. We benchmark state-of-the-art VideoLLMs and CAG in\nboth oracle retrieval and RAG settings and find that CAG consistently\noutperforms alternative methods, while suggesting intriguing avenues for future\nwork.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "summarization"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00939", "pdf": "https://arxiv.org/pdf/2504.00939", "abs": "https://arxiv.org/abs/2504.00939", "authors": ["Alexander Martin", "Reno Kriz", "William Gantt Walden", "Kate Sanders", "Hannah Recknor", "Eugene Yang", "Francis Ferraro", "Benjamin Van Durme"], "title": "WikiVideo: Article Generation from Multiple Videos", "categories": ["cs.CV", "cs.CL"], "comment": "Repo can be found here: https://github.com/alexmartin1722/wikivideo", "summary": "We present the challenging task of automatically creating a high-level\nWikipedia-style article that aggregates information from multiple diverse\nvideos about real-world events, such as natural disasters or political\nelections. Videos are intuitive sources for retrieval-augmented generation\n(RAG), but most contemporary RAG workflows focus heavily on text and existing\nmethods for video-based summarization focus on low-level scene understanding\nrather than high-level event semantics. To close this gap, we introduce\nWikiVideo, a benchmark consisting of expert-written articles and densely\nannotated videos that provide evidence for articles' claims, facilitating the\nintegration of video into RAG pipelines and enabling the creation of in-depth\ncontent that is grounded in multimodal sources. We further propose\nCollaborative Article Generation (CAG), a novel interactive method for article\ncreation from multiple videos. CAG leverages an iterative interaction between\nan r1-style reasoning model and a VideoLLM to draw higher level inferences\nabout the target event than is possible with VideoLLMs alone, which fixate on\nlow-level visual features. We benchmark state-of-the-art VideoLLMs and CAG in\nboth oracle retrieval and RAG settings and find that CAG consistently\noutperforms alternative methods, while suggesting intriguing avenues for future\nwork.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "summarization"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00992", "pdf": "https://arxiv.org/pdf/2504.00992", "abs": "https://arxiv.org/abs/2504.00992", "authors": ["Elisabetta Fedele", "Boyang Sun", "Leonidas Guibas", "Marc Pollefeys", "Francis Engelmann"], "title": "SuperDec: 3D Scene Decomposition with Superquadric Primitives", "categories": ["cs.CV"], "comment": null, "summary": "We present SuperDec, an approach for creating compact 3D scene\nrepresentations via decomposition into superquadric primitives. While most\nrecent works leverage geometric primitives to obtain photorealistic 3D scene\nrepresentations, we propose to leverage them to obtain a compact yet expressive\nrepresentation. We propose to solve the problem locally on individual objects\nand leverage the capabilities of instance segmentation methods to scale our\nsolution to full 3D scenes. In doing that, we design a new architecture which\nefficiently decompose point clouds of arbitrary objects in a compact set of\nsuperquadrics. We train our architecture on ShapeNet and we prove its\ngeneralization capabilities on object instances extracted from the ScanNet++\ndataset as well as on full Replica scenes. Finally, we show how a compact\nrepresentation based on superquadrics can be useful for a diverse range of\ndownstream applications, including robotic tasks and controllable visual\ncontent generation and editing.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.01008", "pdf": "https://arxiv.org/pdf/2504.01008", "abs": "https://arxiv.org/abs/2504.01008", "authors": ["Peter Kocsis", "Lukas Höllein", "Matthias Nießner"], "title": "IntrinsiX: High-Quality PBR Generation using Image Priors", "categories": ["cs.CV", "cs.AI", "I.4.8; I.4.9; I.2.10"], "comment": "Project page: https://peter-kocsis.github.io/IntrinsiX/ Video:\n  https://youtu.be/b0wVA44R93Y", "summary": "We introduce IntrinsiX, a novel method that generates high-quality intrinsic\nimages from text description. In contrast to existing text-to-image models\nwhose outputs contain baked-in scene lighting, our approach predicts\nphysically-based rendering (PBR) maps. This enables the generated outputs to be\nused for content creation scenarios in core graphics applications that\nfacilitate re-lighting, editing, and texture generation tasks. In order to\ntrain our generator, we exploit strong image priors, and pre-train separate\nmodels for each PBR material component (albedo, roughness, metallic, normals).\nWe then align these models with a new cross-intrinsic attention formulation\nthat concatenates key and value features in a consistent fashion. This allows\nus to exchange information between each output modality and to obtain\nsemantically coherent PBR predictions. To ground each intrinsic component, we\npropose a rendering loss which provides image-space signals to constrain the\nmodel, thus facilitating sharp details also in the output BRDF properties. Our\nresults demonstrate detailed intrinsic generation with strong generalization\ncapabilities that outperforms existing intrinsic image decomposition methods\nused with generated images by a significant margin. Finally, we show a series\nof applications, including re-lighting, editing, and text-conditioned\nroom-scale PBR texture generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.01010", "pdf": "https://arxiv.org/pdf/2504.01010", "abs": "https://arxiv.org/abs/2504.01010", "authors": ["Dylan Lester", "James Gao", "Samuel Sutphin", "Pingping Zhu", "Husnu Narman", "Ammar Alzarrad"], "title": "A YOLO-Based Semi-Automated Labeling Approach to Improve Fault Detection Efficiency in Railroad Videos", "categories": ["cs.CV", "eess.IV"], "comment": "Published on American Society of Engineering Education (ASEE) North\n  Central Section Conference, 2025", "summary": "Manual labeling for large-scale image and video datasets is often\ntime-intensive, error-prone, and costly, posing a significant barrier to\nefficient machine learning workflows in fault detection from railroad videos.\nThis study introduces a semi-automated labeling method that utilizes a\npre-trained You Only Look Once (YOLO) model to streamline the labeling process\nand enhance fault detection accuracy in railroad videos. By initiating the\nprocess with a small set of manually labeled data, our approach iteratively\ntrains the YOLO model, using each cycle's output to improve model accuracy and\nprogressively reduce the need for human intervention.\n  To facilitate easy correction of model predictions, we developed a system to\nexport YOLO's detection data as an editable text file, enabling rapid\nadjustments when detections require refinement. This approach decreases\nlabeling time from an average of 2 to 4 minutes per image to 30 seconds to 2\nminutes, effectively minimizing labor costs and labeling errors. Unlike costly\nAI based labeling solutions on paid platforms, our method provides a\ncost-effective alternative for researchers and practitioners handling large\ndatasets in fault detection and other detection based machine learning\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00254", "pdf": "https://arxiv.org/pdf/2504.00254", "abs": "https://arxiv.org/abs/2504.00254", "authors": ["Huandong Chang", "Zicheng Ma", "Mingyuan Ma", "Zhenting Qi", "Andrew Sabot", "Hong Jiang", "H. T. Kung"], "title": "ElaLoRA: Elastic & Learnable Low-Rank Adaptation for Efficient Model Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted technique for\nfine-tuning large-scale pre-trained models with minimal parameter updates.\nHowever, existing methods rely on fixed ranks or focus solely on either rank\npruning or expansion, failing to adapt ranks dynamically to match the\nimportance of different layers during training. In this work, we propose\nElaLoRA, an adaptive low-rank adaptation framework that dynamically prunes and\nexpands ranks based on gradient-derived importance scores. To the best of our\nknowledge, ElaLoRA is the first method that enables both rank pruning and\nexpansion during fine-tuning. Experiments across multiple benchmarks\ndemonstrate that ElaLoRA consistently outperforms existing PEFT methods across\ndifferent parameter budgets. Furthermore, our studies validate that layers\nreceiving higher rank allocations contribute more significantly to model\nperformance, providing theoretical justification for our adaptive strategy. By\nintroducing a principled and adaptive rank allocation mechanism, ElaLoRA offers\na scalable and efficient fine-tuning solution, particularly suited for\nresource-constrained environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00420", "pdf": "https://arxiv.org/pdf/2504.00420", "abs": "https://arxiv.org/abs/2504.00420", "authors": ["Yuanqi Yao", "Siao Liu", "Haoming Song", "Delin Qu", "Qizhi Chen", "Yan Ding", "Bin Zhao", "Zhigang Wang", "Xuelong Li", "Dong Wang"], "title": "Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Building a lifelong robot that can effectively leverage prior knowledge for\ncontinuous skill acquisition remains significantly challenging. Despite the\nsuccess of experience replay and parameter-efficient methods in alleviating\ncatastrophic forgetting problem, naively applying these methods causes a\nfailure to leverage the shared primitives between skills. To tackle these\nissues, we propose Primitive Prompt Learning (PPL), to achieve lifelong robot\nmanipulation via reusable and extensible primitives. Within our two stage\nlearning scheme, we first learn a set of primitive prompts to represent shared\nprimitives through multi-skills pre-training stage, where motion-aware prompts\nare learned to capture semantic and motion shared primitives across different\nskills. Secondly, when acquiring new skills in lifelong span, new prompts are\nappended and optimized with frozen pretrained prompts, boosting the learning\nvia knowledge transfer from old skills to new ones. For evaluation, we\nconstruct a large-scale skill dataset and conduct extensive experiments in both\nsimulation and real-world tasks, demonstrating PPL's superior performance over\nstate-of-the-art methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00719", "pdf": "https://arxiv.org/pdf/2504.00719", "abs": "https://arxiv.org/abs/2504.00719", "authors": ["Thomas E. Huber", "Jules Lecomte", "Borislav Polovnikov", "Axel von Arnim"], "title": "Scaling Up Resonate-and-Fire Networks for Fast Deep Learning", "categories": ["cs.NE", "cs.CV"], "comment": "19 pages, 3 figures", "summary": "Spiking neural networks (SNNs) present a promising computing paradigm for\nneuromorphic processing of event-based sensor data. The resonate-and-fire (RF)\nneuron, in particular, appeals through its biological plausibility, complex\ndynamics, yet computational simplicity. Despite theoretically predicted\nbenefits, challenges in parameter initialization and efficient learning\ninhibited the implementation of RF networks, constraining their use to a single\nlayer. In this paper, we address these shortcomings by deriving the RF neuron\nas a structured state space model (SSM) from the HiPPO framework. We introduce\nS5-RF, a new SSM layer comprised of RF neurons based on the S5 model, that\nfeatures a generic initialization scheme and fast training within a deep\narchitecture. S5-RF scales for the first time a RF network to a deep SNN with\nup to four layers and achieves with 78.8% a new state-of-the-art result for\nrecurrent SNNs on the Spiking Speech Commands dataset in under three hours of\ntraining time. Moreover, compared to the reference SNNs that solve our\nbenchmarking tasks, it achieves similar performance with much fewer spiking\noperations. Our code is publicly available at\nhttps://github.com/ThomasEHuber/s5-rf.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
