{"id": "2504.01317", "pdf": "https://arxiv.org/pdf/2504.01317", "abs": "https://arxiv.org/abs/2504.01317", "authors": ["Zhendong Tan", "Xingjun Zhang", "Chaoyi Hu", "Yancheng Pan", "Shaoxun Wang"], "title": "Adaptive Rectification Sampling for Test-Time Compute Scaling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The newly released OpenAI-o1 and DeepSeek-R1 have demonstrated that test-time\nscaling can significantly improve model performance, especially in complex\ntasks such as logical reasoning. Common test-time scaling methods involve\ngenerating more chain of thoughts (CoTs) or longer CoTs with self-correction.\nHowever, while self-correction can improve performance, it may lead to\nsignificant token waste and reduce readability of the CoT if the reasoning\nsteps are already correct. To demonstrate that large language models (LLMs) can\nrectify errors at a more fine-grained level, we propose Adaptive Rectification\nSampling (AR-Sampling), which can guide the LLMs to self-correction at the\nappropriate step. AR-Sampling leverages a process-supervised reward model (PRM)\nas a verifier and constructed trigger sentences to guide the model in adaptive\nstep-level rethinking. Through the experiments on GSM8K and MATH500, it\nindicate that our approach enables the models to rethink in more fine-grained\nlevel, improving the accuracy of solutions, while generating a reasonable\nnumber of additional tokens.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "compute scaling", "test-time compute", "o1", "self-correction"], "score": 6}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01931", "pdf": "https://arxiv.org/pdf/2504.01931", "abs": "https://arxiv.org/abs/2504.01931", "authors": ["Souradip Chakraborty", "Mohammadreza Pourreza", "Ruoxi Sun", "Yiwen Song", "Nino Scherrer", "Jindong Gu", "Furong Huang", "Amrit Singh Bedi", "Ahmad Beirami", "Hamid Palangi", "Tomas Pfister"], "title": "Review, Refine, Repeat: Understanding Iterative Decoding of AI Agents with Dynamic Evaluation and Selection", "categories": ["cs.CL"], "comment": null, "summary": "While AI agents have shown remarkable performance at various tasks, they\nstill struggle with complex multi-modal applications, structured generation and\nstrategic planning. Improvements via standard fine-tuning is often impractical,\nas solving agentic tasks usually relies on black box API access without control\nover model parameters. Inference-time methods such as Best-of-N (BON) sampling\noffer a simple yet effective alternative to improve performance. However, BON\nlacks iterative feedback integration mechanism. Hence, we propose Iterative\nAgent Decoding (IAD) which combines iterative refinement with dynamic candidate\nevaluation and selection guided by a verifier. IAD differs in how feedback is\ndesigned and integrated, specifically optimized to extract maximal signal from\nreward scores. We conduct a detailed comparison of baselines across key metrics\non Sketch2Code, Text2SQL, and Webshop where IAD consistently outperforms\nbaselines, achieving 3--6% absolute gains on Sketch2Code and Text2SQL (with and\nwithout LLM judges) and 8--10% gains on Webshop across multiple metrics. To\nbetter understand the source of IAD's gains, we perform controlled experiments\nto disentangle the effect of adaptive feedback from stochastic sampling, and\nfind that IAD's improvements are primarily driven by verifier-guided\nrefinement, not merely sampling diversity. We also show that both IAD and BON\nexhibit inference-time scaling with increased compute when guided by an optimal\nverifier. Our analysis highlights the critical role of verifier quality in\neffective inference-time optimization and examines the impact of noisy and\nsparse rewards on scaling behavior. Together, these findings offer key insights\ninto the trade-offs and principles of effective inference-time optimization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "iterative refinement"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01400", "pdf": "https://arxiv.org/pdf/2504.01400", "abs": "https://arxiv.org/abs/2504.01400", "authors": ["Xingshan Zeng", "Weiwen Liu", "Xu Huang", "Zezhong Wang", "Lingzhi Wang", "Liangyou Li", "Yasheng Wang", "Lifeng Shang", "Xin Jiang", "Ruiming Tang", "Qun Liu"], "title": "ToolACE-R: Tool Learning with Adaptive Self-Refinement", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Tool learning, which allows Large Language Models (LLMs) to leverage external\ntools for solving complex user tasks, has emerged as a promising avenue for\nextending model capabilities. However, current approaches primarily focus on\ndata synthesis for fine-tuning LLMs to invoke tools effectively, largely\nignoring how to fully stimulate the potential of the model. In this paper, we\npropose ToolACE-R, a novel method that introduces adaptive self-refinement for\ntool invocations. Our approach features a model-aware iterative training\nprocedure that progressively incorporates more training samples based on the\nmodel's evolving capabilities. Additionally, it allows LLMs to iteratively\nrefine their tool calls, optimizing performance without requiring external\nfeedback. To further enhance computational efficiency, we integrate an adaptive\nmechanism when scaling the inference time, enabling the model to autonomously\ndetermine when to stop the refinement process. We conduct extensive experiments\nacross several benchmark datasets, showing that ToolACE-R achieves competitive\nperformance compared to advanced API-based models, even without any refinement.\nFurthermore, its performance can be further improved efficiently through\nadaptive self-refinement. Our results demonstrate the effectiveness of the\nproposed method, which is compatible with base models of various sizes,\noffering a promising direction for more efficient tool learning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01738", "pdf": "https://arxiv.org/pdf/2504.01738", "abs": "https://arxiv.org/abs/2504.01738", "authors": ["Philip Lippmann", "Jie Yang"], "title": "Style over Substance: Distilled Language Models Reason Via Stylistic Replication", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Specialized reasoning language models (RLMs) have demonstrated that scaling\ntest-time computation through detailed reasoning traces significantly enhances\nperformance. Although these traces effectively facilitate knowledge\ndistillation into smaller, instruction-tuned models, the precise nature of\ntransferred reasoning remains unclear. In this study, we investigate to what\nextent distilled models internalize replicated stylistic patterns during\nreasoning. To this end, we systematically analyze reasoning traces, identifying\nstructural and lexical patterns that characterize successful reasoning. We then\nintroduce two new datasets -- a dataset of emergent reasoning traces and a\nsynthetic dataset explicitly constructed to replicate these stylistic patterns\n-- to precisely examine their influence on distilled models' reasoning\ncapabilities. We find that models trained on the synthetic traces achieve\ncomparable performance, indicating that distilled reasoning abilities rely\nsignificantly on surface-level patterns. Surprisingly, we observe an increase\nin performance even when the synthetic traces are altered to lead to the wrong\nanswer. Our findings highlight how stylistic patterns can be leveraged to\nefficiently enhance LM reasoning across diverse model families.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01801", "pdf": "https://arxiv.org/pdf/2504.01801", "abs": "https://arxiv.org/abs/2504.01801", "authors": ["Zhijun Wang", "Jiahuan Li", "Hao Zhou", "Rongxiang Weng", "Jingang Wang", "Xin Huang", "Xue Han", "Junlan Feng", "Chao Deng", "Shujian Huang"], "title": "Investigating and Scaling up Code-Switching for Multilingual Language Model Pre-Training", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite the extreme language imbalance in the pre-training data. In this paper,\nwe closely examine the reasons behind this phenomenon, focusing on the\npre-training corpus. We find that the existence of code-switching, alternating\nbetween different languages within a context, is key to multilingual\ncapabilities. We conduct an analysis to investigate code-switching in the\npre-training corpus, examining its presence and categorizing it into four types\nwithin two quadrants. We then assess its impact on multilingual performance.\nThese types of code-switching data are unbalanced in proportions and\ndemonstrate different effects on facilitating language transfer. To better\nexplore the power of code-switching for language alignment during pre-training,\nwe investigate the strategy of synthetic code-switching. We continuously scale\nup the synthetic code-switching data and observe remarkable improvements in\nboth benchmarks and representation space. Extensive experiments indicate that\nincorporating synthetic code-switching data enables better language alignment\nand generalizes well to high, medium, and low-resource languages with\npre-training corpora of varying qualities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01281", "pdf": "https://arxiv.org/pdf/2504.01281", "abs": "https://arxiv.org/abs/2504.01281", "authors": ["Sakhinana Sagar Srinivas", "Venkataramana Runkana"], "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01127", "pdf": "https://arxiv.org/pdf/2504.01127", "abs": "https://arxiv.org/abs/2504.01127", "authors": ["Ziyi Liu", "Priyanka Dey", "Zhenyu Zhao", "Jen-tse Huang", "Rahul Gupta", "Yang Liu", "Jieyu Zhao"], "title": "Can LLMs Grasp Implicit Cultural Values? Benchmarking LLMs' Metacognitive Cultural Intelligence with CQ-Bench", "categories": ["cs.CL"], "comment": null, "summary": "Cultural Intelligence (CQ) refers to the ability to understand unfamiliar\ncultural contexts-a crucial skill for large language models (LLMs) to\neffectively engage with globally diverse users. While existing research often\nfocuses on explicitly stated cultural norms, such approaches fail to capture\nthe subtle, implicit values that underlie real-world conversations. To address\nthis gap, we introduce CQ-Bench, a benchmark specifically designed to assess\nLLMs' capability to infer implicit cultural values from natural conversational\ncontexts. We generate a multi-character conversation-based stories dataset\nusing values from the World Value Survey and GlobalOpinions datasets, with\ntopics including ethical, religious, social, and political. Our dataset\nconstruction pipeline includes rigorous validation procedures-incorporation,\nconsistency, and implicitness checks-using GPT-4o, with 98.2% human-model\nagreement in the final validation. Our benchmark consists of three tasks of\nincreasing complexity: attitude detection, value selection, and value\nextraction. We find that while o1 and Deepseek-R1 models reach human-level\nperformance in value selection (0.809 and 0.814), they still fall short in\nnuanced attitude detection, with F1 scores of 0.622 and 0.635, respectively. In\nthe value extraction task, GPT-4o-mini and o3-mini score 0.602 and 0.598,\nhighlighting the difficulty of open-ended cultural reasoning. Notably,\nfine-tuning smaller models (e.g., LLaMA-3.2-3B) on only 500 culturally rich\nexamples improves performance by over 10%, even outperforming stronger\nbaselines (o3-mini) in some cases. Using CQ-Bench, we provide insights into the\ncurrent challenges in LLMs' CQ research and suggest practical pathways for\nenhancing LLMs' cross-cultural reasoning abilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "agreement", "consistency"], "score": 4}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01040", "pdf": "https://arxiv.org/pdf/2504.01040", "abs": "https://arxiv.org/abs/2504.01040", "authors": ["Ilir Tahiraj", "Jeremialie Swadiryus", "Felix Fent", "Markus Lienkamp"], "title": "Cal or No Cal? -- Real-Time Miscalibration Detection of LiDAR and Camera Sensors", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The goal of extrinsic calibration is the alignment of sensor data to ensure\nan accurate representation of the surroundings and enable sensor fusion\napplications. From a safety perspective, sensor calibration is a key enabler of\nautonomous driving. In the current state of the art, a trend from target-based\noffline calibration towards targetless online calibration can be observed.\nHowever, online calibration is subject to strict real-time and resource\nconstraints which are not met by state-of-the-art methods. This is mainly due\nto the high number of parameters to estimate, the reliance on geometric\nfeatures, or the dependence on specific vehicle maneuvers. To meet these\nrequirements and ensure the vehicle's safety at any time, we propose a\nmiscalibration detection framework that shifts the focus from the direct\nregression of calibration parameters to a binary classification of the\ncalibration state, i.e., calibrated or miscalibrated. Therefore, we propose a\ncontrastive learning approach that compares embedded features in a latent space\nto classify the calibration state of two different sensor modalities. Moreover,\nwe provide a comprehensive analysis of the feature embeddings and challenging\ncalibration errors that highlight the performance of our approach. As a result,\nour method outperforms the current state-of-the-art in terms of detection\nperformance, inference time, and resource demand. The code is open source and\navailable on https://github.com/TUMFTM/MiscalibrationDetection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01128", "pdf": "https://arxiv.org/pdf/2504.01128", "abs": "https://arxiv.org/abs/2504.01128", "authors": ["Andrei Dumitriu", "Florin Tatui", "Florin Miron", "Aakash Ralhan", "Radu Tudor Ionescu", "Radu Timofte"], "title": "RipVIS: Rip Currents Video Instance Segmentation Benchmark for Beach Monitoring and Safety", "categories": ["cs.CV", "cs.AI", "I.4.0, I.4.9"], "comment": null, "summary": "Rip currents are strong, localized and narrow currents of water that flow\noutwards into the sea, causing numerous beach-related injuries and fatalities\nworldwide. Accurate identification of rip currents remains challenging due to\ntheir amorphous nature and the lack of annotated data, which often requires\nexpert knowledge. To address these issues, we present RipVIS, a large-scale\nvideo instance segmentation benchmark explicitly designed for rip current\nsegmentation. RipVIS is an order of magnitude larger than previous datasets,\nfeaturing $184$ videos ($212,328$ frames), of which $150$ videos ($163,528$\nframes) are with rip currents, collected from various sources, including\ndrones, mobile phones, and fixed beach cameras. Our dataset encompasses diverse\nvisual contexts, such as wave-breaking patterns, sediment flows, and water\ncolor variations, across multiple global locations, including USA, Mexico,\nCosta Rica, Portugal, Italy, Greece, Romania, Sri Lanka, Australia and New\nZealand. Most videos are annotated at $5$ FPS to ensure accuracy in dynamic\nscenarios, supplemented by an additional $34$ videos ($48,800$ frames) without\nrip currents. We conduct comprehensive experiments with Mask R-CNN, Cascade\nMask R-CNN, SparseInst and YOLO11, fine-tuning these models for the task of rip\ncurrent segmentation. Results are reported in terms of multiple metrics, with a\nparticular focus on the $F_2$ score to prioritize recall and reduce false\nnegatives. To enhance segmentation performance, we introduce a novel\npost-processing step based on Temporal Confidence Aggregation (TCA). RipVIS\naims to set a new standard for rip current segmentation, contributing towards\nsafer beach environments. We offer a benchmark website to share data, models,\nand results with the research community, encouraging ongoing collaboration and\nfuture contributions, at https://ripvis.ai.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety", "accuracy"], "score": 4}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01243", "pdf": "https://arxiv.org/pdf/2504.01243", "abs": "https://arxiv.org/abs/2504.01243", "authors": ["Jaskaran Singh Walia", "Shravan Venkatraman", "Pavithra LK"], "title": "FUSION: Frequency-guided Underwater Spatial Image recOnstructioN", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO", "eess.IV"], "comment": null, "summary": "Underwater images suffer from severe degradations, including color\ndistortions, reduced visibility, and loss of structural details due to\nwavelength-dependent attenuation and scattering. Existing enhancement methods\nprimarily focus on spatial-domain processing, neglecting the frequency domain's\npotential to capture global color distributions and long-range dependencies. To\naddress these limitations, we propose FUSION, a dual-domain deep learning\nframework that jointly leverages spatial and frequency domain information.\nFUSION independently processes each RGB channel through multi-scale\nconvolutional kernels and adaptive attention mechanisms in the spatial domain,\nwhile simultaneously extracting global structural information via FFT-based\nfrequency attention. A Frequency Guided Fusion module integrates complementary\nfeatures from both domains, followed by inter-channel fusion and adaptive\nchannel recalibration to ensure balanced color distributions. Extensive\nexperiments on benchmark datasets (UIEB, EUVP, SUIM-E) demonstrate that FUSION\nachieves state-of-the-art performance, consistently outperforming existing\nmethods in reconstruction fidelity (highest PSNR of 23.717 dB and SSIM of 0.883\non UIEB), perceptual quality (lowest LPIPS of 0.112 on UIEB), and visual\nenhancement metrics (best UIQM of 3.414 on UIEB), while requiring significantly\nfewer parameters (0.28M) and lower computational complexity, demonstrating its\nsuitability for real-time underwater imaging applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01369", "pdf": "https://arxiv.org/pdf/2504.01369", "abs": "https://arxiv.org/abs/2504.01369", "authors": ["Lin Zhang", "Zhouhong Gu", "Suhang Zheng", "Tao Wang", "Tianyu Li", "Hongwei Feng", "Yanghua Xiao"], "title": "LITE: LLM-Impelled efficient Taxonomy Evaluation", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "This paper presents LITE, an LLM-based evaluation method designed for\nefficient and flexible assessment of taxonomy quality. To address challenges in\nlarge-scale taxonomy evaluation, such as efficiency, fairness, and consistency,\nLITE adopts a top-down hierarchical evaluation strategy, breaking down the\ntaxonomy into manageable substructures and ensuring result reliability through\ncross-validation and standardized input formats. LITE also introduces a penalty\nmechanism to handle extreme cases and provides both quantitative performance\nanalysis and qualitative insights by integrating evaluation metrics closely\naligned with task objectives. Experimental results show that LITE demonstrates\nhigh reliability in complex evaluation tasks, effectively identifying semantic\nerrors, logical contradictions, and structural flaws in taxonomies, while\noffering directions for improvement. Code is available at\nhttps://github.com/Zhang-l-i-n/TAXONOMY_DETECT .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "reliability"], "score": 3}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01298", "pdf": "https://arxiv.org/pdf/2504.01298", "abs": "https://arxiv.org/abs/2504.01298", "authors": ["Shiyong Liu", "Zhihao Li", "Xiao Tang", "Jianzhuang Liu"], "title": "Direction-Aware Hybrid Representation Learning for 3D Hand Pose and Shape Estimation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 workshop", "summary": "Most model-based 3D hand pose and shape estimation methods directly regress\nthe parametric model parameters from an image to obtain 3D joints under weak\nsupervision. However, these methods involve solving a complex optimization\nproblem with many local minima, making training difficult. To address this\nchallenge, we propose learning direction-aware hybrid features (DaHyF) that\nfuse implicit image features and explicit 2D joint coordinate features. This\nfusion is enhanced by the pixel direction information in the camera coordinate\nsystem to estimate pose, shape, and camera viewpoint. Our method directly\npredicts 3D hand poses with DaHyF representation and reduces jittering during\nmotion capture using prediction confidence based on contrastive learning. We\nevaluate our method on the FreiHAND dataset and show that it outperforms\nexisting state-of-the-art methods by more than 33% in accuracy. DaHyF also\nachieves the top ranking on both the HO3Dv2 and HO3Dv3 leaderboards for the\nmetric of Mean Joint Error (after scale and translation alignment). Compared to\nthe second-best results, the largest improvement observed is 10%. We also\ndemonstrate its effectiveness in real-time motion capture scenarios with hand\nposition variability, occlusion, and motion blur.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01326", "pdf": "https://arxiv.org/pdf/2504.01326", "abs": "https://arxiv.org/abs/2504.01326", "authors": ["Jin Lian", "Zhongyu Wan", "Ming Gao", "JunFeng Chen"], "title": "CFMD: Dynamic Cross-layer Feature Fusion for Salient Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cross-layer feature pyramid networks (CFPNs) have achieved notable progress\nin multi-scale feature fusion and boundary detail preservation for salient\nobject detection. However, traditional CFPNs still suffer from two core\nlimitations: (1) a computational bottleneck caused by complex feature weighting\noperations, and (2) degraded boundary accuracy due to feature blurring in the\nupsampling process. To address these challenges, we propose CFMD, a novel\ncross-layer feature pyramid network that introduces two key innovations. First,\nwe design a context-aware feature aggregation module (CFLMA), which\nincorporates the state-of-the-art Mamba architecture to construct a dynamic\nweight distribution mechanism. This module adaptively adjusts feature\nimportance based on image context, significantly improving both representation\nefficiency and generalization. Second, we introduce an adaptive dynamic\nupsampling unit (CFLMD) that preserves spatial details during resolution\nrecovery. By adjusting the upsampling range dynamically and initializing with a\nbilinear strategy, the module effectively reduces feature overlap and maintains\nfine-grained boundary structures. Extensive experiments on three standard\nbenchmarks using three mainstream backbone networks demonstrate that CFMD\nachieves substantial improvements in pixel-level accuracy and boundary\nsegmentation quality, especially in complex scenes. The results validate the\neffectiveness of CFMD in jointly enhancing computational efficiency and\nsegmentation performance, highlighting its strong potential in salient object\ndetection tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01698", "pdf": "https://arxiv.org/pdf/2504.01698", "abs": "https://arxiv.org/abs/2504.01698", "authors": ["Yi-Long Lu", "Chunhui Zhang", "Jiajun Song", "Lifeng Fan", "Wei Wang"], "title": "ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in rule-based reinforcement learning (RL), applied during\nthe post-training phase of large language models (LLMs), have significantly\nenhanced their capabilities in structured reasoning tasks such as mathematics\nand logical inference. However, the effectiveness of RL in social reasoning,\nparticularly in Theory of Mind (ToM), the ability to infer others' mental\nstates, remains largely unexplored. In this study, we demonstrate that RL\nmethods effectively unlock ToM reasoning capabilities even in small-scale LLMs\n(0.5B to 7B parameters). Using a modest dataset comprising 3200 questions\nacross diverse scenarios, our RL-trained 7B model achieves 84.50\\% accuracy on\nthe Hi-ToM benchmark, surpassing models like GPT-4o and DeepSeek-v3 despite\nsignificantly fewer parameters. While smaller models ($\\leq$3B parameters)\nsuffer from reasoning collapse, larger models (7B parameters) maintain stable\nperformance through consistent belief tracking. Additionally, our RL-based\nmodels demonstrate robust generalization to higher-order, out-of-distribution\nToM problems, novel textual presentations, and previously unseen datasets.\nThese findings highlight RL's potential to enhance social cognitive reasoning,\nbridging the gap between structured problem-solving and nuanced social\ninference in LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01428", "pdf": "https://arxiv.org/pdf/2504.01428", "abs": "https://arxiv.org/abs/2504.01428", "authors": ["Zhuangzhuang Chen", "Hualiang Wang", "Chubin Ou", "Xiaomeng Li"], "title": "MuTri: Multi-view Tri-alignment for OCT to OCTA 3D Image Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Optical coherence tomography angiography (OCTA) shows its great importance in\nimaging microvascular networks by providing accurate 3D imaging of blood\nvessels, but it relies upon specialized sensors and expensive devices. For this\nreason, previous works show the potential to translate the readily available 3D\nOptical Coherence Tomography (OCT) images into 3D OCTA images. However,\nexisting OCTA translation methods directly learn the mapping from the OCT\ndomain to the OCTA domain in continuous and infinite space with guidance from\nonly a single view, i.e., the OCTA project map, resulting in suboptimal\nresults. To this end, we propose the multi-view Tri-alignment framework for OCT\nto OCTA 3D image translation in discrete and finite space, named MuTri. In the\nfirst stage, we pre-train two vector-quantized variational auto-encoder (VQ-\nVAE) by reconstructing 3D OCT and 3D OCTA data, providing semantic prior for\nsubsequent multi-view guidances. In the second stage, our multi-view\ntri-alignment facilitates another VQVAE model to learn the mapping from the OCT\ndomain to the OCTA domain in discrete and finite space. Specifically, a\ncontrastive-inspired semantic alignment is proposed to maximize the mutual\ninformation with the pre-trained models from OCT and OCTA views, to facilitate\ncodebook learning. Meanwhile, a vessel structure alignment is proposed to\nminimize the structure discrepancy with the pre-trained models from the OCTA\nproject map view, benefiting from learning the detailed vessel structure\ninformation. We also collect the first large-scale dataset, namely, OCTA2024,\nwhich contains a pair of OCT and OCTA volumes from 846 subjects.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01789", "pdf": "https://arxiv.org/pdf/2504.01789", "abs": "https://arxiv.org/abs/2504.01789", "authors": ["Sumeth Yuenyong", "Thodsaporn Chay-intr", "Kobkrit Viriyayudhakorn"], "title": "OpenThaiGPT 1.6 and R1: Thai-Centric Open Source and Reasoning Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We present OpenThaiGPT 1.6 and R1 (OTG-1.6 and OTG-R1), Thai-centric Large\nLanguage Models (LLMs) developed through distinct methodologies to enhance\ngeneralization and reasoning capabilities. OTG-1.6 employs Task Arithmetic\nmodel merging for broad generalization, while OTG-R1 integrates multi-stage\ntraining with the Less-Is-More Reasoning Hypothesis (LIMO) for advanced\nreasoning. Benchmark evaluations demonstrate superior performance across Thai\nlanguage tasks, achieving competitive results against larger-scale open-source\nThai LLMs. This paper details the proposed models, training processes,\nbenchmarks, and results, highlighting improvements over previous models and\nestablishing new performance standards for Thai-centric LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01449", "pdf": "https://arxiv.org/pdf/2504.01449", "abs": "https://arxiv.org/abs/2504.01449", "authors": ["Zaipeng Duan", "Xuzhong Hu", "Pei An", "Jie Ma"], "title": "Multimodal Point Cloud Semantic Segmentation With Virtual Point Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR-based 3D point cloud recognition has been proven beneficial in various\napplications. However, the sparsity and varying density pose a significant\nchallenge in capturing intricate details of objects, particularly for\nmedium-range and small targets. Therefore, we propose a multi-modal point cloud\nsemantic segmentation method based on Virtual Point Enhancement (VPE), which\nintegrates virtual points generated from images to address these issues. These\nvirtual points are dense but noisy, and directly incorporating them can\nincrease computational burden and degrade performance. Therefore, we introduce\na spatial difference-driven adaptive filtering module that selectively extracts\nvaluable pseudo points from these virtual points based on density and distance,\nenhancing the density of medium-range targets. Subsequently, we propose a\nnoise-robust sparse feature encoder that incorporates noise-robust feature\nextraction and fine-grained feature enhancement. Noise-robust feature\nextraction exploits the 2D image space to reduce the impact of noisy points,\nwhile fine-grained feature enhancement boosts sparse geometric features through\ninner-voxel neighborhood point aggregation and downsampled voxel aggregation.\nThe results on the SemanticKITTI and nuScenes, two large-scale benchmark data\nsets, have validated effectiveness, significantly improving 2.89\\% mIoU with\nthe introduction of 7.7\\% virtual points on nuScenes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01452", "pdf": "https://arxiv.org/pdf/2504.01452", "abs": "https://arxiv.org/abs/2504.01452", "authors": ["Encheng Su", "Hu Cao", "Alois Knoll"], "title": "BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "2024 IEEE International Conference on Bioinformatics and Biomedicine\n  (BIBM)", "summary": "Accurate segmentation of polyps and skin lesions is essential for diagnosing\ncolorectal and skin cancers. While various segmentation methods for polyps and\nskin lesions using fully supervised deep learning techniques have been\ndeveloped, the pixel-level annotation of medical images by doctors is both\ntime-consuming and costly. Foundational vision models like the Segment Anything\nModel (SAM) have demonstrated superior performance; however, directly applying\nSAM to medical segmentation may not yield satisfactory results due to the lack\nof domain-specific medical knowledge. In this paper, we propose BiSeg-SAM, a\nSAM-guided weakly supervised prompting and boundary refinement network for the\nsegmentation of polyps and skin lesions. Specifically, we fine-tune SAM\ncombined with a CNN module to learn local features. We introduce a WeakBox with\ntwo functions: automatically generating box prompts for the SAM model and using\nour proposed Multi-choice Mask-to-Box (MM2B) transformation for rough\nmask-to-box conversion, addressing the mismatch between coarse labels and\nprecise predictions. Additionally, we apply scale consistency (SC) loss for\nprediction scale alignment. Our DetailRefine module enhances boundary precision\nand segmentation accuracy by refining coarse predictions using a limited amount\nof ground truth labels. This comprehensive approach enables BiSeg-SAM to\nachieve excellent multi-task segmentation performance. Our method demonstrates\nsignificant superiority over state-of-the-art (SOTA) methods when tested on\nfive polyp datasets and one skin cancer dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01903", "pdf": "https://arxiv.org/pdf/2504.01903", "abs": "https://arxiv.org/abs/2504.01903", "authors": ["Zijun Wang", "Haoqin Tu", "Yuhan Wang", "Juncheng Wu", "Jieru Mei", "Brian R. Bartoldson", "Bhavya Kailkhura", "Cihang Xie"], "title": "STAR-1: Safer Alignment of Reasoning LLMs with 1K Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset\nspecifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built\non three core principles -- diversity, deliberative reasoning, and rigorous\nfiltering -- STAR-1 aims to address the critical needs for safety alignment in\nLRMs. Specifically, we begin by integrating existing open-source safety\ndatasets from diverse sources. Then, we curate safety policies to generate\npolicy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based\nsafety scoring system to select training examples aligned with best practices.\nExperimental results show that fine-tuning LRMs with STAR-1 leads to an average\n40% improvement in safety performance across four benchmarks, while only\nincurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability\nmeasured across five reasoning tasks. Extensive ablation studies further\nvalidate the importance of our design principles in constructing STAR-1 and\nanalyze its efficacy across both LRMs and traditional LLMs. Our project page is\nhttps://ucsc-vlaa.github.io/STAR-1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01919", "pdf": "https://arxiv.org/pdf/2504.01919", "abs": "https://arxiv.org/abs/2504.01919", "authors": ["Baban Gain", "Dibyanayan Bandyopadhyay", "Asif Ekbal"], "title": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the\nlandscape of machine translation (MT), particularly for low-resource languages\nand domains that lack sufficient parallel corpora, linguistic tools, and\ncomputational infrastructure. This survey presents a comprehensive overview of\nrecent progress in leveraging LLMs for MT. We analyze techniques such as\nfew-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning\nthat enable effective adaptation to under-resourced settings. The paper also\nexplores synthetic data generation strategies using LLMs, including\nback-translation and lexical augmentation. Additionally, we compare LLM-based\ntranslation with traditional encoder-decoder models across diverse language\npairs, highlighting the strengths and limitations of each. We discuss\npersistent challenges such as hallucinations, evaluation inconsistencies, and\ninherited biases while also evaluating emerging LLM-driven metrics for\ntranslation quality. This survey offers practical insights and outlines future\ndirections for building robust, inclusive, and scalable MT systems in the era\nof large-scale generative models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01928", "pdf": "https://arxiv.org/pdf/2504.01928", "abs": "https://arxiv.org/abs/2504.01928", "authors": ["Boshi Wang", "Huan Sun"], "title": "Is the Reversal Curse a Binding Problem? Uncovering Limitations of Transformers from a Basic Generalization Failure", "categories": ["cs.CL", "cs.LG"], "comment": "Code and data:\n  https://github.com/OSU-NLP-Group/reversal-curse-binding", "summary": "Despite their impressive capabilities, LLMs exhibit a basic generalization\nfailure known as the Reversal Curse, where they struggle to learn reversible\nfactual associations. Understanding why this occurs could help identify\nweaknesses in current models and advance their generalization and robustness.\nIn this paper, we conjecture that the Reversal Curse in LLMs is a manifestation\nof the long-standing binding problem in cognitive science, neuroscience and AI.\nSpecifically, we identify two primary causes of the Reversal Curse stemming\nfrom transformers' limitations in conceptual binding: the inconsistency and\nentanglements of concept representations. We perform a series of experiments\nthat support these conjectures. Our exploration leads to a model design based\non JEPA (Joint-Embedding Predictive Architecture) that for the first time\nbreaks the Reversal Curse without side-stepping it with specialized data\naugmentation or non-causal masking, and moreover, generalization could be\nfurther improved by incorporating special memory layers that support\ndisentangled concept representations. We demonstrate that the skill of reversal\nunlocks a new kind of memory integration that enables models to solve\nlarge-scale arithmetic reasoning problems via parametric forward-chaining,\noutperforming frontier LLMs based on non-parametric memory and prolonged\nexplicit reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01681", "pdf": "https://arxiv.org/pdf/2504.01681", "abs": "https://arxiv.org/abs/2504.01681", "authors": ["Maelyson R. F. Santos", "Marcelo A. F. Gomes"], "title": "Study of scaling laws in language families", "categories": ["physics.soc-ph", "cs.CL"], "comment": "10 pages, 4 figures", "summary": "This article investigates scaling laws within language families using data\nfrom over six thousand languages and analyzing emergent patterns observed in\nZipf-like classification graphs. Both macroscopic (based on number of languages\nby family) and microscopic (based on numbers of speakers by language on a\nfamily) aspects of these classifications are examined. Particularly noteworthy\nis the discovery of a distinct division among the fourteen largest contemporary\nlanguage families, excluding Afro-Asiatic and Nilo-Saharan languages. These\nfamilies are found to be distributed across three language family quadruplets,\neach characterized by significantly different exponents in the Zipf graphs.\nThis finding sheds light on the underlying structure and organization of major\nlanguage families, revealing intriguing insights into the nature of linguistic\ndiversity and distribution.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01666", "pdf": "https://arxiv.org/pdf/2504.01666", "abs": "https://arxiv.org/abs/2504.01666", "authors": ["Sarah Alyami", "Hamzah Luqman"], "title": "CLIP-SLA: Parameter-Efficient CLIP Adaptation for Continuous Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Continuous sign language recognition (CSLR) focuses on interpreting and\ntranscribing sequences of sign language gestures in videos. In this work, we\npropose CLIP sign language adaptation (CLIP-SLA), a novel CSLR framework that\nleverages the powerful pre-trained visual encoder from the CLIP model to sign\nlanguage tasks through parameter-efficient fine-tuning (PEFT). We introduce two\nvariants, SLA-Adapter and SLA-LoRA, which integrate PEFT modules into the CLIP\nvisual encoder, enabling fine-tuning with minimal trainable parameters. The\neffectiveness of the proposed frameworks is validated on four datasets:\nPhoenix2014, Phoenix2014-T, CSL-Daily, and Isharah-500, where both CLIP-SLA\nvariants outperformed several SOTA models with fewer trainable parameters.\nExtensive ablation studies emphasize the effectiveness and flexibility of the\nproposed methods with different vision-language models for CSLR. These findings\nshowcase the potential of adapting large-scale pre-trained models for scalable\nand efficient CSLR, which pave the way for future advancements in sign language\nunderstanding.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01901", "pdf": "https://arxiv.org/pdf/2504.01901", "abs": "https://arxiv.org/abs/2504.01901", "authors": ["Haochen Wang", "Yucheng Zhao", "Tiancai Wang", "Haoqiang Fan", "Xiangyu Zhang", "Zhaoxiang Zhang"], "title": "Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "The rapid development of Large Multimodal Models (LMMs) for 2D images and\nvideos has spurred efforts to adapt these models for interpreting 3D scenes.\nHowever, the absence of large-scale 3D vision-language datasets has posed a\nsignificant obstacle. To address this issue, typical approaches focus on\ninjecting 3D awareness into 2D LMMs by designing 3D input-level scene\nrepresentations. This work provides a new perspective. We introduce\nreconstructive visual instruction tuning with 3D-awareness (Ross3D), which\nintegrates 3D-aware visual supervision into the training procedure.\nSpecifically, it incorporates cross-view and global-view reconstruction. The\nformer requires reconstructing masked views by aggregating overlapping\ninformation from other views. The latter aims to aggregate information from all\navailable views to recover Bird's-Eye-View images, contributing to a\ncomprehensive overview of the entire scene. Empirically, Ross3D achieves\nstate-of-the-art performance across various 3D scene understanding benchmarks.\nMore importantly, our semi-supervised experiments demonstrate significant\npotential in leveraging large amounts of unlabeled 3D vision-only data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01722", "pdf": "https://arxiv.org/pdf/2504.01722", "abs": "https://arxiv.org/abs/2504.01722", "authors": ["Kaan Karaman", "Yuchang Jiang", "Damien Robert", "Vivien Sainte Fare Garnot", "Maria João Santos", "Jan Dirk Wegner"], "title": "{GSR4B}: Biomass Map Super-Resolution with Sentinel-1/2 Guidance", "categories": ["cs.CV"], "comment": "Accepted for an oral presentation at the ISPRS Geospatial Week 2025", "summary": "Accurate Above-Ground Biomass (AGB) mapping at both large scale and high\nspatio-temporal resolution is essential for applications ranging from climate\nmodeling to biodiversity assessment, and sustainable supply chain monitoring.\nAt present, fine-grained AGB mapping relies on costly airborne laser scanning\nacquisition campaigns usually limited to regional scales. Initiatives such as\nthe ESA CCI map attempt to generate global biomass products from diverse\nspaceborne sensors but at a coarser resolution. To enable global,\nhigh-resolution (HR) mapping, several works propose to regress AGB from HR\nsatellite observations such as ESA Sentinel-1/2 images. We propose a novel way\nto address HR AGB estimation, by leveraging both HR satellite observations and\nexisting low-resolution (LR) biomass products. We cast this problem as Guided\nSuper-Resolution (GSR), aiming at upsampling LR biomass maps (sources) from\n$100$ to $10$ m resolution, using auxiliary HR co-registered satellite images\n(guides). We compare super-resolving AGB maps with and without guidance,\nagainst direct regression from satellite images, on the public BioMassters\ndataset. We observe that Multi-Scale Guidance (MSG) outperforms direct\nregression both for regression ($-780$ t/ha RMSE) and perception ($+2.0$ dB\nPSNR) metrics, and better captures high-biomass values, without significant\ncomputational overhead. Interestingly, unlike the RGB+Depth setting they were\noriginally designed for, our best-performing AGB GSR approaches are those that\nmost preserve the guide image texture. Our results make a strong case for\nadopting the GSR framework for accurate HR biomass mapping at scale. Our code\nand model weights are made publicly available\n(https://github.com/kaankaramanofficial/GSR4B).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01724", "pdf": "https://arxiv.org/pdf/2504.01724", "abs": "https://arxiv.org/abs/2504.01724", "authors": ["Yuxuan Luo", "Zhengkun Rong", "Lizhen Wang", "Longhao Zhang", "Tianshu Hu", "Yongming Zhu"], "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While recent image-based human animation methods achieve realistic body and\nfacial motion synthesis, critical gaps remain in fine-grained holistic\ncontrollability, multi-scale adaptability, and long-term temporal coherence,\nwhich leads to their lower expressiveness and robustness. We propose a\ndiffusion transformer (DiT) based framework, DreamActor-M1, with hybrid\nguidance to overcome these limitations. For motion guidance, our hybrid control\nsignals that integrate implicit facial representations, 3D head spheres, and 3D\nbody skeletons achieve robust control of facial expressions and body movements,\nwhile producing expressive and identity-preserving animations. For scale\nadaptation, to handle various body poses and image scales ranging from\nportraits to full-body views, we employ a progressive training strategy using\ndata with varying resolutions and scales. For appearance guidance, we integrate\nmotion patterns from sequential frames with complementary visual references,\nensuring long-term temporal coherence for unseen regions during complex\nmovements. Experiments demonstrate that our method outperforms the\nstate-of-the-art works, delivering expressive results for portraits,\nupper-body, and full-body generation with robust long-term consistency. Project\nPage: https://grisoon.github.io/DreamActor-M1/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01732", "pdf": "https://arxiv.org/pdf/2504.01732", "abs": "https://arxiv.org/abs/2504.01732", "authors": ["Ulas Gunes", "Matias Turkulainen", "Xuqian Ren", "Arno Solin", "Juho Kannala", "Esa Rahtu"], "title": "FIORD: A Fisheye Indoor-Outdoor Dataset with LIDAR Ground Truth for 3D Scene Reconstruction and Benchmarking", "categories": ["cs.CV"], "comment": "SCIA 2025", "summary": "The development of large-scale 3D scene reconstruction and novel view\nsynthesis methods mostly rely on datasets comprising perspective images with\nnarrow fields of view (FoV). While effective for small-scale scenes, these\ndatasets require large image sets and extensive structure-from-motion (SfM)\nprocessing, limiting scalability. To address this, we introduce a fisheye image\ndataset tailored for scene reconstruction tasks. Using dual 200-degree fisheye\nlenses, our dataset provides full 360-degree coverage of 5 indoor and 5 outdoor\nscenes. Each scene has sparse SfM point clouds and precise LIDAR-derived dense\npoint clouds that can be used as geometric ground-truth, enabling robust\nbenchmarking under challenging conditions such as occlusions and reflections.\nWhile the baseline experiments focus on vanilla Gaussian Splatting and NeRF\nbased Nerfacto methods, the dataset supports diverse approaches for scene\nreconstruction, novel view synthesis, and image-based rendering.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01872", "pdf": "https://arxiv.org/pdf/2504.01872", "abs": "https://arxiv.org/abs/2504.01872", "authors": ["Jintao Zhang", "Zimin Xia", "Mingyue Dong", "Shuhan Shen", "Linwei Yue", "Xianwei Zheng"], "title": "CoMatcher: Multi-View Collaborative Feature Matching", "categories": ["cs.CV", "I.4.8; I.2.10; I.5.4"], "comment": "15 pages, 7 figures, to be published in CVPR 2025", "summary": "This paper proposes a multi-view collaborative matching strategy for reliable\ntrack construction in complex scenarios. We observe that the pairwise matching\nparadigms applied to image set matching often result in ambiguous estimation\nwhen the selected independent pairs exhibit significant occlusions or extreme\nviewpoint changes. This challenge primarily stems from the inherent uncertainty\nin interpreting intricate 3D structures based on limited two-view observations,\nas the 3D-to-2D projection leads to significant information loss. To address\nthis, we introduce CoMatcher, a deep multi-view matcher to (i) leverage\ncomplementary context cues from different views to form a holistic 3D scene\nunderstanding and (ii) utilize cross-view projection consistency to infer a\nreliable global solution. Building on CoMatcher, we develop a groupwise\nframework that fully exploits cross-view relationships for large-scale matching\ntasks. Extensive experiments on various complex scenarios demonstrate the\nsuperiority of our method over the mainstream two-view matching paradigm.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01886", "pdf": "https://arxiv.org/pdf/2504.01886", "abs": "https://arxiv.org/abs/2504.01886", "authors": ["Yanzhou Su", "Tianbin Li", "Jiyao Liu", "Chenglong Ma", "Junzhi Ning", "Cheng Tang", "Sibo Ju", "Jin Ye", "Pengcheng Chen", "Ming Hu", "Shixiang Tang", "Lihao Liu", "Bin Fu", "Wenqi Shao", "Xiaowei Hu", "Xiangwen Liao", "Yuanfeng Ji", "Junjun He"], "title": "GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in general medical AI have made significant strides, but\nexisting models often lack the reasoning capabilities needed for complex\nmedical decision-making. This paper presents GMAI-VL-R1, a multimodal medical\nreasoning model enhanced by reinforcement learning (RL) to improve its\nreasoning abilities. Through iterative training, GMAI-VL-R1 optimizes\ndecision-making, significantly boosting diagnostic accuracy and clinical\nsupport. We also develop a reasoning data synthesis method, generating\nstep-by-step reasoning data via rejection sampling, which further enhances the\nmodel's generalization. Experimental results show that after RL training,\nGMAI-VL-R1 excels in tasks such as medical image diagnosis and visual question\nanswering. While the model demonstrates basic memorization with supervised\nfine-tuning, RL is crucial for true generalization. Our work establishes new\nevaluation benchmarks and paves the way for future advancements in medical\nreasoning models. Code, data, and model will be released at\n\\href{https://github.com/uni-medical/GMAI-VL-R1}{this link}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01901", "pdf": "https://arxiv.org/pdf/2504.01901", "abs": "https://arxiv.org/abs/2504.01901", "authors": ["Haochen Wang", "Yucheng Zhao", "Tiancai Wang", "Haoqiang Fan", "Xiangyu Zhang", "Zhaoxiang Zhang"], "title": "Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "The rapid development of Large Multimodal Models (LMMs) for 2D images and\nvideos has spurred efforts to adapt these models for interpreting 3D scenes.\nHowever, the absence of large-scale 3D vision-language datasets has posed a\nsignificant obstacle. To address this issue, typical approaches focus on\ninjecting 3D awareness into 2D LMMs by designing 3D input-level scene\nrepresentations. This work provides a new perspective. We introduce\nreconstructive visual instruction tuning with 3D-awareness (Ross3D), which\nintegrates 3D-aware visual supervision into the training procedure.\nSpecifically, it incorporates cross-view and global-view reconstruction. The\nformer requires reconstructing masked views by aggregating overlapping\ninformation from other views. The latter aims to aggregate information from all\navailable views to recover Bird's-Eye-View images, contributing to a\ncomprehensive overview of the entire scene. Empirically, Ross3D achieves\nstate-of-the-art performance across various 3D scene understanding benchmarks.\nMore importantly, our semi-supervised experiments demonstrate significant\npotential in leveraging large amounts of unlabeled 3D vision-only data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01952", "pdf": "https://arxiv.org/pdf/2504.01952", "abs": "https://arxiv.org/abs/2504.01952", "authors": ["Wenxuan Wang", "Zijia Zhao", "Yisi Zhang", "Yepeng Tang", "Erdong Hu", "Xinlong Wang", "Jing Liu"], "title": "Image Difference Grounding with Natural Language", "categories": ["cs.CV"], "comment": null, "summary": "Visual grounding (VG) typically focuses on locating regions of interest\nwithin an image using natural language, and most existing VG methods are\nlimited to single-image interpretations. This limits their applicability in\nreal-world scenarios like automatic surveillance, where detecting subtle but\nmeaningful visual differences across multiple images is crucial. Besides,\nprevious work on image difference understanding (IDU) has either focused on\ndetecting all change regions without cross-modal text guidance, or on providing\ncoarse-grained descriptions of differences. Therefore, to push towards\nfiner-grained vision-language perception, we propose Image Difference Grounding\n(IDG), a task designed to precisely localize visual differences based on user\ninstructions. We introduce DiffGround, a large-scale and high-quality dataset\nfor IDG, containing image pairs with diverse visual variations along with\ninstructions querying fine-grained differences. Besides, we present a baseline\nmodel for IDG, DiffTracker, which effectively integrates feature differential\nenhancement and common suppression to precisely locate differences. Experiments\non the DiffGround dataset highlight the importance of our IDG dataset in\nenabling finer-grained IDU. To foster future research, both DiffGround data and\nDiffTracker model will be publicly released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01956", "pdf": "https://arxiv.org/pdf/2504.01956", "abs": "https://arxiv.org/abs/2504.01956", "authors": ["Hanyang Wang", "Fangfu Liu", "Jiawei Chi", "Yueqi Duan"], "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step", "categories": ["cs.CV"], "comment": "Project Page: https://hanyang-21.github.io/VideoScene", "summary": "Recovering 3D scenes from sparse views is a challenging task due to its\ninherent ill-posed problem. Conventional methods have developed specialized\nsolutions (e.g., geometry regularization or feed-forward deterministic model)\nto mitigate the issue. However, they still suffer from performance degradation\nby minimal overlap across input views with insufficient visual information.\nFortunately, recent video generative models show promise in addressing this\nchallenge as they are capable of generating video clips with plausible 3D\nstructures. Powered by large pretrained video diffusion models, some pioneering\nresearch start to explore the potential of video generative prior and create 3D\nscenes from sparse views. Despite impressive improvements, they are limited by\nslow inference time and the lack of 3D constraint, leading to inefficiencies\nand reconstruction artifacts that do not align with real-world geometry\nstructure. In this paper, we propose VideoScene to distill the video diffusion\nmodel to generate 3D scenes in one step, aiming to build an efficient and\neffective tool to bridge the gap from video to 3D. Specifically, we design a\n3D-aware leap flow distillation strategy to leap over time-consuming redundant\ninformation and train a dynamic denoising policy network to adaptively\ndetermine the optimal leap timestep during inference. Extensive experiments\ndemonstrate that our VideoScene achieves faster and superior 3D scene\ngeneration results than previous video diffusion models, highlighting its\npotential as an efficient tool for future video to 3D applications. Project\nPage: https://hanyang-21.github.io/VideoScene", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01960", "pdf": "https://arxiv.org/pdf/2504.01960", "abs": "https://arxiv.org/abs/2504.01960", "authors": ["Niluthpol Chowdhury Mithun", "Tuan Pham", "Qiao Wang", "Ben Southall", "Kshitij Minhas", "Bogdan Matei", "Stephan Mandt", "Supun Samarasekera", "Rakesh Kumar"], "title": "Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis", "categories": ["cs.CV", "cs.LG"], "comment": "WACV ULTRRA Workshop 2025", "summary": "Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance\nFields (NeRF) have achieved impressive results in real-time 3D reconstruction\nand novel view synthesis. However, these methods struggle in large-scale,\nunconstrained environments where sparse and uneven input coverage, transient\nocclusions, appearance variability, and inconsistent camera settings lead to\ndegraded quality. We propose GS-Diff, a novel 3DGS framework guided by a\nmulti-view diffusion model to address these limitations. By generating\npseudo-observations conditioned on multi-view inputs, our method transforms\nunder-constrained 3D reconstruction problems into well-posed ones, enabling\nrobust optimization even with sparse data. GS-Diff further integrates several\nenhancements, including appearance embedding, monocular depth priors, dynamic\nobject modeling, anisotropy regularization, and advanced rasterization\ntechniques, to tackle geometric and photometric challenges in real-world\nsettings. Experiments on four benchmarks demonstrate that GS-Diff consistently\noutperforms state-of-the-art baselines by significant margins.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01218", "pdf": "https://arxiv.org/pdf/2504.01218", "abs": "https://arxiv.org/abs/2504.01218", "authors": ["Piyush Nagasubramaniam", "Neeraj Karamchandani", "Chen Wu", "Sencun Zhu"], "title": "Prompting Forgetting: Unlearning in GANs via Textual Guidance", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "State-of-the-art generative models exhibit powerful image-generation\ncapabilities, introducing various ethical and legal challenges to service\nproviders hosting these models. Consequently, Content Removal Techniques (CRTs)\nhave emerged as a growing area of research to control outputs without\nfull-scale retraining. Recent work has explored the use of Machine Unlearning\nin generative models to address content removal. However, the focus of such\nresearch has been on diffusion models, and unlearning in Generative Adversarial\nNetworks (GANs) has remained largely unexplored. We address this gap by\nproposing Text-to-Unlearn, a novel framework that selectively unlearns concepts\nfrom pre-trained GANs using only text prompts, enabling feature unlearning,\nidentity unlearning, and fine-grained tasks like expression and multi-attribute\nremoval in models trained on human faces. Leveraging natural language\ndescriptions, our approach guides the unlearning process without requiring\nadditional datasets or supervised fine-tuning, offering a scalable and\nefficient solution. To evaluate its effectiveness, we introduce an automatic\nunlearning assessment method adapted from state-of-the-art image-text alignment\nmetrics, providing a comprehensive analysis of the unlearning methodology. To\nour knowledge, Text-to-Unlearn is the first cross-modal unlearning framework\nfor GANs, representing a flexible and efficient advancement in managing\ngenerative model behavior.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01483", "pdf": "https://arxiv.org/pdf/2504.01483", "abs": "https://arxiv.org/abs/2504.01483", "authors": ["Siran Li", "Ruiyang Liu", "Chen Liu", "Zhendong Wang", "Gaofeng He", "Yong-Lu Li", "Xiaogang Jin", "Huamin Wang"], "title": "GarmageNet: A Dataset and Scalable Representation for Generic Garment Modeling", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "High-fidelity garment modeling remains challenging due to the lack of\nlarge-scale, high-quality datasets and efficient representations capable of\nhandling non-watertight, multi-layer geometries. In this work, we introduce\nGarmage, a neural-network-and-CG-friendly garment representation that\nseamlessly encodes the accurate geometry and sewing pattern of complex\nmulti-layered garments as a structured set of per-panel geometry images. As a\ndual-2D-3D representation, Garmage achieves an unprecedented integration of 2D\nimage-based algorithms with 3D modeling workflows, enabling high fidelity,\nnon-watertight, multi-layered garment geometries with direct compatibility for\nindustrial-grade simulations.Built upon this representation, we present\nGarmageNet, a novel generation framework capable of producing detailed\nmulti-layered garments with body-conforming initial geometries and intricate\nsewing patterns, based on user prompts or existing in-the-wild sewing patterns.\nFurthermore, we introduce a robust stitching algorithm that recovers per-vertex\nstitches, ensuring seamless integration into flexible simulation pipelines for\ndownstream editing of sewing patterns, material properties, and dynamic\nsimulations. Finally, we release an industrial-standard, large-scale,\nhigh-fidelity garment dataset featuring detailed annotations, vertex-wise\ncorrespondences, and a robust pipeline for converting unstructured production\nsewing patterns into GarmageNet standard structural assets, paving the way for\nlarge-scale, industrial-grade garment generation systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01521", "pdf": "https://arxiv.org/pdf/2504.01521", "abs": "https://arxiv.org/abs/2504.01521", "authors": ["Jincheng Zhong", "Xiangcheng Zhang", "Jianmin Wang", "Mingsheng Long"], "title": "Domain Guidance: A Simple Transfer Approach for a Pre-trained Diffusion Model", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent advancements in diffusion models have revolutionized generative\nmodeling. However, the impressive and vivid outputs they produce often come at\nthe cost of significant model scaling and increased computational demands.\nConsequently, building personalized diffusion models based on off-the-shelf\nmodels has emerged as an appealing alternative. In this paper, we introduce a\nnovel perspective on conditional generation for transferring a pre-trained\nmodel. From this viewpoint, we propose *Domain Guidance*, a straightforward\ntransfer approach that leverages pre-trained knowledge to guide the sampling\nprocess toward the target domain. Domain Guidance shares a formulation similar\nto advanced classifier-free guidance, facilitating better domain alignment and\nhigher-quality generations. We provide both empirical and theoretical analyses\nof the mechanisms behind Domain Guidance. Our experimental results demonstrate\nits substantial effectiveness across various transfer benchmarks, achieving\nover a 19.6% improvement in FID and a 23.4% improvement in FD$_\\text{DINOv2}$\ncompared to standard fine-tuning. Notably, existing fine-tuned models can\nseamlessly integrate Domain Guidance to leverage these benefits, without\nadditional training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01561", "pdf": "https://arxiv.org/pdf/2504.01561", "abs": "https://arxiv.org/abs/2504.01561", "authors": ["Dandan Shan", "Zihan Li", "Yunxiang Li", "Qingde Li", "Jie Tian", "Qingqi Hong"], "title": "STPNet: Scale-aware Text Prompt Network for Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation of lesions plays a critical role in medical image\nanalysis and diagnosis. Traditional segmentation approaches that rely solely on\nvisual features often struggle with the inherent uncertainty in lesion\ndistribution and size. To address these issues, we propose STPNet, a\nScale-aware Text Prompt Network that leverages vision-language modeling to\nenhance medical image segmentation. Our approach utilizes multi-scale textual\ndescriptions to guide lesion localization and employs retrieval-segmentation\njoint learning to bridge the semantic gap between visual and linguistic\nmodalities. Crucially, STPNet retrieves relevant textual information from a\nspecialized medical text repository during training, eliminating the need for\ntext input during inference while retaining the benefits of cross-modal\nlearning. We evaluate STPNet on three datasets: COVID-Xray, COVID-CT, and\nKvasir-SEG. Experimental results show that our vision-language approach\noutperforms state-of-the-art segmentation methods, demonstrating the\neffectiveness of incorporating textual semantic knowledge into medical image\nanalysis. The code has been made publicly on\nhttps://github.com/HUANGLIZI/STPNet.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01571", "pdf": "https://arxiv.org/pdf/2504.01571", "abs": "https://arxiv.org/abs/2504.01571", "authors": ["Aleksander Plocharski", "Jan Swidzinski", "Przemyslaw Musialski"], "title": "Pro-DG: Procedural Diffusion Guidance for Architectural Facade Generation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "I.3.7; I.4.9; I.2.10"], "comment": "12 pages, 13 figures", "summary": "We present Pro-DG, a framework for procedurally controllable photo-realistic\nfacade generation that combines a procedural shape grammar with diffusion-based\nimage synthesis. Starting from a single input image, we reconstruct its facade\nlayout using grammar rules, then edit that structure through user-defined\ntransformations. As facades are inherently multi-hierarchical structures, we\nintroduce hierarchical matching procedure that aligns facade structures at\ndifferent levels which is used to introduce control maps to guide a generative\ndiffusion pipeline. This approach retains local appearance fidelity while\naccommodating large-scale edits such as floor duplication or window\nrearrangement. We provide a thorough evaluation, comparing Pro-DG against\ninpainting-based baselines and synthetic ground truths. Our user study and\nquantitative measurements indicate improved preservation of architectural\nidentity and higher edit accuracy. Our novel method is the first to integrate\nneuro-symbolically derived shape-grammars for modeling with modern generative\nmodel and highlights the broader potential of such approaches for precise and\ncontrollable image manipulation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
