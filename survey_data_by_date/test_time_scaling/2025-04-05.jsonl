{"id": "2504.02495", "pdf": "https://arxiv.org/pdf/2504.02495", "abs": "https://arxiv.org/abs/2504.02495", "authors": ["Zijun Liu", "Peiyi Wang", "Runxin Xu", "Shirong Ma", "Chong Ruan", "Peng Li", "Yang Liu", "Yu Wu"], "title": "Inference-Time Scaling for Generalist Reward Modeling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint, under review. 42 pages", "summary": "Reinforcement learning (RL) has been widely adopted in post-training for\nlarge language models (LLMs) at scale. Recently, the incentivization of\nreasoning capabilities in LLMs from RL indicates that $\\textit{proper learning\nmethods could enable effective inference-time scalability}$. A key challenge of\nRL is to obtain accurate reward signals for LLMs in various domains beyond\nverifiable questions or artificial rules. In this work, we investigate how to\nimprove reward modeling (RM) with more inference compute for general queries,\ni.e. the $\\textbf{inference-time scalability of generalist RM}$, and further,\nhow to improve the effectiveness of performance-compute scaling with proper\nlearning methods. For the RM approach, we adopt pointwise generative reward\nmodeling (GRM) to enable flexibility for different input types and potential\nfor inference-time scaling. For the learning method, we propose Self-Principled\nCritique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs\nthrough online RL, to generate principles adaptively and critiques accurately,\nresulting in $\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective\ninference-time scaling, we use parallel sampling to expand compute usage, and\nintroduce a meta RM to guide voting process for better scaling performance.\nEmpirically, we show that SPCT significantly improves the quality and\nscalability of GRMs, outperforming existing methods and models in various RM\nbenchmarks without severe biases, and could achieve better performance compared\nto training-time scaling. DeepSeek-GRM still meets challenges in some tasks,\nwhich we believe can be addressed by future efforts in generalist reward\nsystems. The models will be released and open-sourced.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "scale", "compute scaling", "inference compute"], "score": 5}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning"], "score": 2}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02404", "pdf": "https://arxiv.org/pdf/2504.02404", "abs": "https://arxiv.org/abs/2504.02404", "authors": ["Xiang Feng", "Wentao Jiang", "Zengmao Wang", "Yong Luo", "Pingbo Xu", "Baosheng Yu", "Hua Jin", "Bo Du", "Jing Zhang"], "title": "AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in Anesthesiology", "categories": ["cs.CL"], "comment": "23 pages, 9 figures", "summary": "The application of large language models (LLMs) in the medical field has\ngained significant attention, yet their reasoning capabilities in more\nspecialized domains like anesthesiology remain underexplored. In this paper, we\nsystematically evaluate the reasoning capabilities of LLMs in anesthesiology\nand analyze key factors influencing their performance. To this end, we\nintroduce AnesBench, a cross-lingual benchmark designed to assess\nanesthesiology-related reasoning across three levels: factual retrieval (System\n1), hybrid reasoning (System 1.x), and complex decision-making (System 2).\nThrough extensive experiments, we first explore how model characteristics,\nincluding model scale, Chain of Thought (CoT) length, and language\ntransferability, affect reasoning performance. Then, we further evaluate the\neffectiveness of different training strategies, leveraging our curated\nanesthesiology-related dataset, including continuous pre-training (CPT) and\nsupervised fine-tuning (SFT). Additionally, we also investigate how the\ntest-time reasoning techniques, such as Best-of-N sampling and beam search,\ninfluence reasoning performance, and assess the impact of reasoning-enhanced\nmodel distillation, specifically DeepSeek-R1. We will publicly release\nAnesBench, along with our CPT and SFT training datasets and evaluation code at\nhttps://github.com/MiliLab/AnesBench.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scale", "chain of thought", "beam search"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "multi-dimensional"], "score": 4}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02398", "pdf": "https://arxiv.org/pdf/2504.02398", "abs": "https://arxiv.org/abs/2504.02398", "authors": ["Gallil Maimon", "Michael Hassid", "Amit Roth", "Yossi Adi"], "title": "Scaling Analysis of Interleaved Speech-Text Language Models", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture.\nThey predict that SLMs require much more compute and data compared to text,\nleading some to question the feasibility of training high-quality SLMs.\nHowever, modern SLMs are often initialised from pre-trained TextLMs using\nspeech-text interleaving to allow knowledge transfer. This raises the question\n- Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper\nwe answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by\ntraining several dozen and analysing the scaling trends. We see that under this\nsetup SLMs scale more efficiently with compute. Additionally, our results\nindicate that the scaling-dynamics are significantly different than\ntextless-SLMs, suggesting one should allocate notably more of the compute\nbudget for increasing model size over training tokens. We also study the role\nof synthetic data and TextLM model families in unlocking this potential.\nResults suggest, that our scaled up model achieves comparable performance with\nleading models on speech semantic metrics while using less compute and data\nthan other approaches. We open source models, samples, and data -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/sims.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02304", "pdf": "https://arxiv.org/pdf/2504.02304", "abs": "https://arxiv.org/abs/2504.02304", "authors": ["Minheng Ni", "Ennan Wu", "Zidong Gong", "Zhengyuan Yang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Lijuan Wang", "Wangmeng Zuo"], "title": "Measurement of LLM's Philosophies of Human Nature", "categories": ["cs.CL"], "comment": null, "summary": "The widespread application of artificial intelligence (AI) in various tasks,\nalong with frequent reports of conflicts or violations involving AI, has\nsparked societal concerns about interactions with AI systems. Based on\nWrightsman's Philosophies of Human Nature Scale (PHNS), a scale empirically\nvalidated over decades to effectively assess individuals' attitudes toward\nhuman nature, we design the standardized psychological scale specifically\ntargeting large language models (LLM), named the Machine-based Philosophies of\nHuman Nature Scale (M-PHNS). By evaluating LLMs' attitudes toward human nature\nacross six dimensions, we reveal that current LLMs exhibit a systemic lack of\ntrust in humans, and there is a significant negative correlation between the\nmodel's intelligence level and its trust in humans. Furthermore, we propose a\nmental loop learning framework, which enables LLM to continuously optimize its\nvalue system during virtual interactions by constructing moral scenarios,\nthereby improving its attitude toward human nature. Experiments demonstrate\nthat mental loop learning significantly enhances their trust in humans compared\nto persona or instruction prompts. This finding highlights the potential of\nhuman-based psychological assessments for LLM, which can not only diagnose\ncognitive biases but also provide a potential solution for ethical learning in\nartificial intelligence. We release the M-PHNS evaluation code and data at\nhttps://github.com/kodenii/M-PHNS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation"], "score": 2}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02061", "pdf": "https://arxiv.org/pdf/2504.02061", "abs": "https://arxiv.org/abs/2504.02061", "authors": ["Yuxin Guo", "Shuailei Ma", "Shijie Ma", "Xiaoyi Bao", "Chen-Wei Xie", "Kecheng Zheng", "Tingyu Weng", "Siyang Sun", "Yun Zheng", "Wei Zou"], "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": "Accepted to ICLR 2025", "summary": "Audio is essential for multimodal video understanding. On the one hand, video\ninherently contains audio, which supplies complementary information to vision.\nBesides, video large language models (Video-LLMs) can encounter many\naudio-centric settings. However, existing Video-LLMs and Audio-Visual Large\nLanguage Models (AV-LLMs) exhibit deficiencies in exploiting audio information,\nleading to weak understanding and hallucinations. To solve the issues, we delve\ninto the model architecture and dataset. (1) From the architectural\nperspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent\nalignment of audio and visual modalities in both temporal and spatial\ndimensions ensures a comprehensive and accurate understanding of videos.\nSpecifically, we devise an audio-visual multi-scale adapter for multi-scale\ninformation aggregation, which achieves spatial alignment. For temporal\nalignment, we propose audio-visual interleaved merging. (2) From the dataset\nperspective, we curate an audio-visual caption and instruction-tuning dataset,\ncalled AVU. It comprises 5.2 million diverse, open-ended data tuples (video,\naudio, question, answer) and introduces a novel data partitioning strategy.\nExtensive experiments show our model not only achieves remarkable performance\nin audio-visual understanding, but also mitigates potential hallucinations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02154", "pdf": "https://arxiv.org/pdf/2504.02154", "abs": "https://arxiv.org/abs/2504.02154", "authors": ["Chao Huang", "Susan Liang", "Yunlong Tang", "Li Ma", "Yapeng Tian", "Chenliang Xu"], "title": "FreSca: Unveiling the Scaling Space in Diffusion Models", "categories": ["cs.CV"], "comment": "Project page: https://wikichao.github.io/FreSca/", "summary": "Diffusion models offer impressive controllability for image tasks, primarily\nthrough noise predictions that encode task-specific information and\nclassifier-free guidance enabling adjustable scaling. This scaling mechanism\nimplicitly defines a ``scaling space'' whose potential for fine-grained\nsemantic manipulation remains underexplored. We investigate this space,\nstarting with inversion-based editing where the difference between\nconditional/unconditional noise predictions carries key semantic information.\nOur core contribution stems from a Fourier analysis of noise predictions,\nrevealing that its low- and high-frequency components evolve differently\nthroughout diffusion. Based on this insight, we introduce FreSca, a\nstraightforward method that applies guidance scaling independently to different\nfrequency bands in the Fourier domain. FreSca demonstrably enhances existing\nimage editing methods without retraining. Excitingly, its effectiveness extends\nto image understanding tasks such as depth estimation, yielding quantitative\ngains across multiple datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02438", "pdf": "https://arxiv.org/pdf/2504.02438", "abs": "https://arxiv.org/abs/2504.02438", "authors": ["Chuanqi Cheng", "Jian Guan", "Wei Wu", "Rui Yan"], "title": "Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-form video processing fundamentally challenges vision-language models\n(VLMs) due to the high computational costs of handling extended temporal\nsequences. Existing token pruning and feature merging methods often sacrifice\ncritical temporal dependencies or dilute semantic information. We introduce\ndifferential distillation, a principled approach that systematically preserves\ntask-relevant information while suppressing redundancy. Based on this\nprinciple, we develop ViLaMP, a hierarchical video-language model that\nprocesses hour-long videos at ``mixed precision'' through two key mechanisms:\n(1) differential keyframe selection that maximizes query relevance while\nmaintaining temporal distinctiveness at the frame level and (2) differential\nfeature merging that preserves query-salient features in non-keyframes at the\npatch level. Hence, ViLaMP retains full information in keyframes while reducing\nnon-keyframes to their most salient features, resembling mixed-precision\ntraining. Extensive experiments demonstrate ViLaMP's superior performance\nacross four video understanding benchmarks, particularly on long-form content.\nNotably, ViLaMP can process ultra-long videos (up to 10K frames) on a single\nNVIDIA A100 GPU, achieving substantial computational efficiency while\nmaintaining state-of-the-art performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02160", "pdf": "https://arxiv.org/pdf/2504.02160", "abs": "https://arxiv.org/abs/2504.02160", "authors": ["Shaojin Wu", "Mengqi Huang", "Wenxu Wu", "Yufeng Cheng", "Fei Ding", "Qian He"], "title": "Less-to-More Generalization: Unlocking More Controllability by In-Context Generation", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://bytedance.github.io/UNO Code and model:\n  https://github.com/bytedance/UNO", "summary": "Although subject-driven generation has been extensively explored in image\ngeneration due to its wide applications, it still has challenges in data\nscalability and subject expansibility. For the first challenge, moving from\ncurating single-subject datasets to multiple-subject ones and scaling them is\nparticularly difficult. For the second, most recent methods center on\nsingle-subject generation, making it hard to apply when dealing with\nmulti-subject scenarios. In this study, we propose a highly-consistent data\nsynthesis pipeline to tackle this challenge. This pipeline harnesses the\nintrinsic in-context generation capabilities of diffusion transformers and\ngenerates high-consistency multi-subject paired data. Additionally, we\nintroduce UNO, which consists of progressive cross-modal alignment and\nuniversal rotary position embedding. It is a multi-image conditioned\nsubject-to-image model iteratively trained from a text-to-image model.\nExtensive experiments show that our method can achieve high consistency while\nensuring controllability in both single-subject and multi-subject driven\ngeneration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02572", "pdf": "https://arxiv.org/pdf/2504.02572", "abs": "https://arxiv.org/abs/2504.02572", "authors": ["Fabio Celli", "Georgios Spathulas"], "title": "Language Models reach higher Agreement than Humans in Historical Interpretation", "categories": ["cs.CL"], "comment": null, "summary": "This paper compares historical annotations by humans and Large Language\nModels. The findings reveal that both exhibit some cultural bias, but Large\nLanguage Models achieve a higher consensus on the interpretation of historical\nfacts from short texts. While humans tend to disagree on the basis of their\npersonal biases, Large Models disagree when they skip information or produce\nhallucinations. These findings have significant implications for digital\nhumanities, enabling large-scale annotation and quantitative analysis of\nhistorical data. This offers new educational and research opportunities to\nexplore historical interpretations from different Language Models, fostering\ncritical thinking about bias.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "agreement"], "score": 2}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02244", "pdf": "https://arxiv.org/pdf/2504.02244", "abs": "https://arxiv.org/abs/2504.02244", "authors": ["Xu Cao", "Pranav Virupaksha", "Wenqi Jia", "Bolin Lai", "Fiona Ryan", "Sangmin Lee", "James M. Rehg"], "title": "SocialGesture: Delving into Multi-person Gesture Understanding", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Previous research in human gesture recognition has largely overlooked\nmulti-person interactions, which are crucial for understanding the social\ncontext of naturally occurring gestures. This limitation in existing datasets\npresents a significant challenge in aligning human gestures with other\nmodalities like language and speech. To address this issue, we introduce\nSocialGesture, the first large-scale dataset specifically designed for\nmulti-person gesture analysis. SocialGesture features a diverse range of\nnatural scenarios and supports multiple gesture analysis tasks, including\nvideo-based recognition and temporal localization, providing a valuable\nresource for advancing the study of gesture during complex social interactions.\nFurthermore, we propose a novel visual question answering (VQA) task to\nbenchmark vision language models'(VLMs) performance on social gesture\nunderstanding. Our findings highlight several limitations of current gesture\nrecognition models, offering insights into future directions for improvement in\nthis field. SocialGesture is available at\nhuggingface.co/datasets/IrohXu/SocialGesture.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02674", "pdf": "https://arxiv.org/pdf/2504.02674", "abs": "https://arxiv.org/abs/2504.02674", "authors": ["Jacqueline Rowe", "Edward Gow-Smith", "Mark Hepple"], "title": "Limitations of Religious Data and the Importance of the Target Domain: Towards Machine Translation for Guinea-Bissau Creole", "categories": ["cs.CL"], "comment": "9 pages, 5 figures, 7 tables. To be published in Proceedings of the\n  8th Workshop on Technologies for Machine Translation of Low-Resource\n  Languages (NAACL 2025)", "summary": "We introduce a new dataset for machine translation of Guinea-Bissau Creole\n(Kiriol), comprising around 40 thousand parallel sentences to English and\nPortuguese. This dataset is made up of predominantly religious data (from the\nBible and texts from the Jehovah's Witnesses), but also a small amount of\ngeneral domain data (from a dictionary). This mirrors the typical resource\navailability of many low resource languages. We train a number of\ntransformer-based models to investigate how to improve domain transfer from\nreligious data to a more general domain. We find that adding even 300 sentences\nfrom the target domain when training substantially improves the translation\nperformance, highlighting the importance and need for data collection for\nlow-resource languages, even on a small-scale. We additionally find that\nPortuguese-to-Kiriol translation models perform better on average than other\nsource and target language pairs, and investigate how this relates to the\nmorphological complexity of the languages involved and the degree of lexical\noverlap between creoles and lexifiers. Overall, we hope our work will stimulate\nresearch into Kiriol and into how machine translation might better support\ncreole languages in general.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02768", "pdf": "https://arxiv.org/pdf/2504.02768", "abs": "https://arxiv.org/abs/2504.02768", "authors": ["Jaap Jumelet", "Leonie Weissweiler", "Arianna Bisazza"], "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs", "categories": ["cs.CL"], "comment": null, "summary": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages, 6 linguistic phenomena and containing\nmore than 125,000 minimal pairs. Our minimal pairs are created using a fully\nautomated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02807", "pdf": "https://arxiv.org/pdf/2504.02807", "abs": "https://arxiv.org/abs/2504.02807", "authors": ["Fan Zhou", "Zengzhi Wang", "Nikhil Ranjan", "Zhoujun Cheng", "Liping Tang", "Guowei He", "Zhengzhong Liu", "Eric P. Xing"], "title": "MegaMath: Pushing the Limits of Open Math Corpora", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "26 pages, 15 figures, 22 tables", "summary": "Mathematical reasoning is a cornerstone of human intelligence and a key\nbenchmark for advanced capabilities in large language models (LLMs). However,\nthe research community still lacks an open, large-scale, high-quality corpus\ntailored to the demands of math-centric LLM pre-training. We present MegaMath,\nan open dataset curated from diverse, math-focused sources through following\npractices: (1) Revisiting web data: We re-extracted mathematical documents from\nCommon Crawl with math-oriented HTML optimizations, fasttext-based filtering\nand deduplication, all for acquiring higher-quality data on the Internet. (2)\nRecalling Math-related code data: We identified high quality math-related code\nfrom large code training corpus, Stack-V2, further enhancing data diversity.\n(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,\nand interleaved text-code blocks from web data or code data. By integrating\nthese strategies and validating their effectiveness through extensive\nablations, MegaMath delivers 371B tokens with the largest quantity and top\nquality among existing open math pre-training datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02107", "pdf": "https://arxiv.org/pdf/2504.02107", "abs": "https://arxiv.org/abs/2504.02107", "authors": ["Jeffrey Li", "Mohammadreza Armandpour", "Iman Mirzadeh", "Sachin Mehta", "Vaishaal Shankar", "Raviteja Vemulapalli", "Samy Bengio", "Oncel Tuzel", "Mehrdad Farajtabar", "Hadi Pouransari", "Fartash Faghri"], "title": "TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining", "categories": ["cs.LG", "cs.CL"], "comment": "Code available at: https://github.com/apple/ml-tic-lm", "summary": "Large Language Models (LLMs) trained on historical web data inevitably become\noutdated. We investigate evaluation strategies and update methods for LLMs as\nnew data becomes available. We introduce a web-scale dataset for time-continual\npretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of\nmagnitude larger than previous continual language modeling benchmarks. We also\ndesign time-stratified evaluations across both general CC data and specific\ndomains (Wikipedia, StackExchange, and code documentation) to assess how well\nvarious continual learning methods adapt to new data while retaining past\nknowledge. Our findings demonstrate that, on general CC data, autoregressive\nmeta-schedules combined with a fixed-ratio replay of older data can achieve\ncomparable held-out loss to re-training from scratch, while requiring\nsignificantly less computation (2.6x). However, the optimal balance between\nincorporating new data and replaying old data differs as replay is crucial to\navoid forgetting on generic web data but less so on specific domains.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02345", "pdf": "https://arxiv.org/pdf/2504.02345", "abs": "https://arxiv.org/abs/2504.02345", "authors": ["Masakazu Yoshimura", "Junji Otsuka", "Radu Berdan", "Takeshi Ohashi"], "title": "SemiISP/SemiIE: Semi-Supervised Image Signal Processor and Image Enhancement Leveraging One-to-Many Mapping sRGB-to-RAW", "categories": ["cs.CV"], "comment": null, "summary": "DNN-based methods have been successful in Image Signal Processor (ISP) and\nimage enhancement (IE) tasks. However, the cost of creating training data for\nthese tasks is considerably higher than for other tasks, making it difficult to\nprepare large-scale datasets. Also, creating personalized ISP and IE with\nminimal training data can lead to new value streams since preferred image\nquality varies depending on the person and use case. While semi-supervised\nlearning could be a potential solution in such cases, it has rarely been\nutilized for these tasks. In this paper, we realize semi-supervised learning\nfor ISP and IE leveraging a RAW image reconstruction (sRGB-to-RAW) method.\nAlthough existing sRGB-to-RAW methods can generate pseudo-RAW image datasets\nthat improve the accuracy of RAW-based high-level computer vision tasks such as\nobject detection, their quality is not sufficient for ISP and IE tasks that\nrequire precise image quality definition. Therefore, we also propose a\nsRGB-to-RAW method that can improve the image quality of these tasks. The\nproposed semi-supervised learning with the proposed sRGB-to-RAW method\nsuccessfully improves the image quality of various models on various datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02507", "pdf": "https://arxiv.org/pdf/2504.02507", "abs": "https://arxiv.org/abs/2504.02507", "authors": ["Abhay Kumar", "Louis Owen", "Nilabhra Roy Chowdhury", "Fabian Güra"], "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) presents numerous challenges, including\ngradient instability and loss spikes. These phenomena can lead to catastrophic\ndivergence, requiring costly checkpoint restoration and data batch skipping.\nTraditional gradient clipping techniques, such as constant or norm-based\nmethods, fail to address these issues effectively due to their reliance on\nfixed thresholds or heuristics, leading to inefficient learning and requiring\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\ngradient clipping algorithm that dynamically adjusts the clipping threshold\nbased on statistical properties of gradient norms over time. Unlike prior\nreactive strategies, ZClip proactively adapts to training dynamics without\nmaking any prior assumptions on the scale and the temporal evolution of\ngradient norms. At its core, it leverages z-score-based anomaly detection to\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\nwhile not interfering with convergence otherwise. Our code is available at:\nhttps://github.com/bluorion-com/ZClip.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02416", "pdf": "https://arxiv.org/pdf/2504.02416", "abs": "https://arxiv.org/abs/2504.02416", "authors": ["Peifu Liu", "Huiyan Bai", "Tingfa Xu", "Jihui Wang", "Huan Chen", "Jianan Li"], "title": "Hyperspectral Remote Sensing Images Salient Object Detection: The First Benchmark Dataset and Baseline", "categories": ["cs.CV"], "comment": "Accepted by TGRS 2025", "summary": "The objective of hyperspectral remote sensing image salient object detection\n(HRSI-SOD) is to identify objects or regions that exhibit distinct spectrum\ncontrasts with the background. This area holds significant promise for\npractical applications; however, progress has been limited by a notable\nscarcity of dedicated datasets and methodologies. To bridge this gap and\nstimulate further research, we introduce the first HRSI-SOD dataset, termed\nHRSSD, which includes 704 hyperspectral images and 5327 pixel-level annotated\nsalient objects. The HRSSD dataset poses substantial challenges for salient\nobject detection algorithms due to large scale variation, diverse\nforeground-background relations, and multi-salient objects. Additionally, we\npropose an innovative and efficient baseline model for HRSI-SOD, termed the\nDeep Spectral Saliency Network (DSSN). The core of DSSN is the Cross-level\nSaliency Assessment Block, which performs pixel-wise attention and evaluates\nthe contributions of multi-scale similarity maps at each spatial location,\neffectively reducing erroneous responses in cluttered regions and emphasizes\nsalient regions across scales. Additionally, the High-resolution Fusion Module\ncombines bottom-up fusion strategy and learned spatial upsampling to leverage\nthe strengths of multi-scale saliency maps, ensuring accurate localization of\nsmall objects. Experiments on the HRSSD dataset robustly validate the\nsuperiority of DSSN, underscoring the critical need for specialized datasets\nand methodologies in this domain. Further evaluations on the HSOD-BIT and\nHS-SOD datasets demonstrate the generalizability of the proposed method. The\ndataset and source code are publicly available at\nhttps://github.com/laprf/HRSSD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02587", "pdf": "https://arxiv.org/pdf/2504.02587", "abs": "https://arxiv.org/abs/2504.02587", "authors": ["Yan Ma", "Steffi Chern", "Xuyang Shen", "Yiran Zhong", "Pengfei Liu"], "title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Code is public and available at: https://github.com/GAIR-NLP/MAYE", "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02417", "pdf": "https://arxiv.org/pdf/2504.02417", "abs": "https://arxiv.org/abs/2504.02417", "authors": ["Lili Liang", "Guanglu Sun"], "title": "Leveraging Static Relationships for Intra-Type and Inter-Type Message Passing in Video Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Question Answering (VideoQA) is an important research direction in the\nfield of artificial intelligence, enabling machines to understand video content\nand perform reasoning and answering based on natural language questions.\nAlthough methods based on static relationship reasoning have made certain\nprogress, there are still deficiencies in the accuracy of static relationship\nrecognition and representation, and they have not fully utilized the static\nrelationship information in videos for in-depth reasoning and analysis.\nTherefore, this paper proposes a reasoning method for intra-type and inter-type\nmessage passing based on static relationships. This method constructs a dual\ngraph for intra-type message passing reasoning and builds a heterogeneous graph\nbased on static relationships for inter-type message passing reasoning. The\nintra-type message passing reasoning model captures the neighborhood\ninformation of targets and relationships related to the question in the dual\ngraph, updating the dual graph to obtain intra-type clues for answering the\nquestion. The inter-type message passing reasoning model captures the\nneighborhood information of targets and relationships from different categories\nrelated to the question in the heterogeneous graph, updating the heterogeneous\ngraph to obtain inter-type clues for answering the question. Finally, the\nanswers are inferred by combining the intra-type and inter-type clues based on\nstatic relationships. Experimental results on the ANetQA and Next-QA datasets\ndemonstrate the effectiveness of this method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02605", "pdf": "https://arxiv.org/pdf/2504.02605", "abs": "https://arxiv.org/abs/2504.02605", "authors": ["Daoguang Zan", "Zhirong Huang", "Wei Liu", "Hanwu Chen", "Linhao Zhang", "Shulin Xin", "Lu Chen", "Qi Liu", "Xiaojian Zhong", "Aoyan Li", "Siyao Liu", "Yongsheng Xiao", "Liangqiang Chen", "Yuyu Zhang", "Jing Su", "Tianyu Liu", "Rui Long", "Kai Shen", "Liang Xiang"], "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02828", "pdf": "https://arxiv.org/pdf/2504.02828", "abs": "https://arxiv.org/abs/2504.02828", "authors": ["Jinqi Luo", "Tianjiao Ding", "Kwan Ho Ryan Chan", "Hancheng Min", "Chris Callison-Burch", "René Vidal"], "title": "Concept Lancet: Image Editing with Compositional Representation Transplant", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted in CVPR 2025. Project page at\n  https://peterljq.github.io/project/colan", "summary": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02515", "pdf": "https://arxiv.org/pdf/2504.02515", "abs": "https://arxiv.org/abs/2504.02515", "authors": ["Nedko Savov", "Naser Kazemi", "Mohammad Mahdi", "Danda Pani Paudel", "Xi Wang", "Luc Van Gool"], "title": "Exploration-Driven Generative Interactive Environments", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Modern world models require costly and time-consuming collection of large\nvideo datasets with action demonstrations by people or by environment-specific\nagents. To simplify training, we focus on using many virtual environments for\ninexpensive, automatically collected interaction data. Genie, a recent\nmulti-environment world model, demonstrates simulation abilities of many\nenvironments with shared behavior. Unfortunately, training their model requires\nexpensive demonstrations. Therefore, we propose a training framework merely\nusing a random agent in virtual environments. While the model trained in this\nmanner exhibits good controls, it is limited by the random exploration\npossibilities. To address this limitation, we propose AutoExplore Agent - an\nexploration agent that entirely relies on the uncertainty of the world model,\ndelivering diverse data from which it can learn the best. Our agent is fully\nindependent of environment-specific rewards and thus adapts easily to new\nenvironments. With this approach, the pretrained multi-environment model can\nquickly adapt to new environments achieving video fidelity and controllability\nimprovement. In order to obtain automatically large-scale interaction datasets\nfor pretraining, we group environments with similar behavior and controls. To\nthis end, we annotate the behavior and controls of 974 virtual environments - a\ndataset that we name RetroAct. For building our model, we first create an open\nimplementation of Genie - GenieRedux and apply enhancements and adaptations in\nour version GenieRedux-G. Our code and data are available at\nhttps://github.com/insait-institute/GenieRedux.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02522", "pdf": "https://arxiv.org/pdf/2504.02522", "abs": "https://arxiv.org/abs/2504.02522", "authors": ["Fatemeh Behrad", "Tinne Tuytelaars", "Johan Wagemans"], "title": "Charm: The Missing Piece in ViT fine-tuning for Image Aesthetic Assessment", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "The capacity of Vision transformers (ViTs) to handle variable-sized inputs is\noften constrained by computational complexity and batch processing limitations.\nConsequently, ViTs are typically trained on small, fixed-size images obtained\nthrough downscaling or cropping. While reducing computational burden, these\nmethods result in significant information loss, negatively affecting tasks like\nimage aesthetic assessment. We introduce Charm, a novel tokenization approach\nthat preserves Composition, High-resolution, Aspect Ratio, and Multi-scale\ninformation simultaneously. Charm prioritizes high-resolution details in\nspecific regions while downscaling others, enabling shorter fixed-size input\nsequences for ViTs while incorporating essential information. Charm is designed\nto be compatible with pre-trained ViTs and their learned positional embeddings.\nBy providing multiscale input and introducing variety to input tokens, Charm\nimproves ViT performance and generalizability for image aesthetic assessment.\nWe avoid cropping or changing the aspect ratio to further preserve information.\nExtensive experiments demonstrate significant performance improvements on\nvarious image aesthetic and quality assessment datasets (up to 8.1 %) using a\nlightweight ViT backbone. Code and pre-trained models are available at\nhttps://github.com/FBehrad/Charm.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02534", "pdf": "https://arxiv.org/pdf/2504.02534", "abs": "https://arxiv.org/abs/2504.02534", "authors": ["Mykola Lavreniuk", "Nataliia Kussul", "Andrii Shelestov", "Bohdan Yailymov", "Yevhenii Salii", "Volodymyr Kuzin", "Zoltan Szantoi"], "title": "Delineate Anything: Resolution-Agnostic Field Boundary Delineation on Satellite Imagery", "categories": ["cs.CV"], "comment": null, "summary": "The accurate delineation of agricultural field boundaries from satellite\nimagery is vital for land management and crop monitoring. However, current\nmethods face challenges due to limited dataset sizes, resolution discrepancies,\nand diverse environmental conditions. We address this by reformulating the task\nas instance segmentation and introducing the Field Boundary Instance\nSegmentation - 22M dataset (FBIS-22M), a large-scale, multi-resolution dataset\ncomprising 672,909 high-resolution satellite image patches (ranging from 0.25 m\nto 10 m) and 22,926,427 instance masks of individual fields, significantly\nnarrowing the gap between agricultural datasets and those in other computer\nvision domains. We further propose Delineate Anything, an instance segmentation\nmodel trained on our new FBIS-22M dataset. Our proposed model sets a new\nstate-of-the-art, achieving a substantial improvement of 88.5% in mAP@0.5 and\n103% in mAP@0.5:0.95 over existing methods, while also demonstrating\nsignificantly faster inference and strong zero-shot generalization across\ndiverse image resolutions and unseen geographic regions. Code, pre-trained\nmodels, and the FBIS-22M dataset are available at\nhttps://lavreniuk.github.io/Delineate-Anything.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02602", "pdf": "https://arxiv.org/pdf/2504.02602", "abs": "https://arxiv.org/abs/2504.02602", "authors": ["Abdul Rehman", "Talha Meraj", "Aiman Mahmood Minhas", "Ayisha Imran", "Mohsen Ali", "Waqas Sultani", "Mubarak Shah"], "title": "Leveraging Sparse Annotations for Leukemia Diagnosis on the Large Leukemia Dataset", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Leukemia is 10th most frequently diagnosed cancer and one of the leading\ncauses of cancer related deaths worldwide. Realistic analysis of Leukemia\nrequires White Blook Cells (WBC) localization, classification, and\nmorphological assessment. Despite deep learning advances in medical imaging,\nleukemia analysis lacks a large, diverse multi-task dataset, while existing\nsmall datasets lack domain diversity, limiting real world applicability. To\novercome dataset challenges, we present a large scale WBC dataset named Large\nLeukemia Dataset (LLD) and novel methods for detecting WBC with their\nattributes. Our contribution here is threefold. First, we present a large-scale\nLeukemia dataset collected through Peripheral Blood Films (PBF) from several\npatients, through multiple microscopes, multi cameras, and multi magnification.\nTo enhance diagnosis explainability and medical expert acceptance, each\nleukemia cell is annotated at 100x with 7 morphological attributes, ranging\nfrom Cell Size to Nuclear Shape. Secondly, we propose a multi task model that\nnot only detects WBCs but also predicts their attributes, providing an\ninterpretable and clinically meaningful solution. Third, we propose a method\nfor WBC detection with attribute analysis using sparse annotations. This\napproach reduces the annotation burden on hematologists, requiring them to mark\nonly a small area within the field of view. Our method enables the model to\nleverage the entire field of view rather than just the annotated regions,\nenhancing learning efficiency and diagnostic accuracy. From diagnosis\nexplainability to overcoming domain shift challenges, presented datasets could\nbe used for many challenging aspects of microscopic image analysis. The\ndatasets, code, and demo are available at:\nhttps://im.itu.edu.pk/sparse-leukemiaattri/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02612", "pdf": "https://arxiv.org/pdf/2504.02612", "abs": "https://arxiv.org/abs/2504.02612", "authors": ["Jiwoo Chung", "Sangeek Hyun", "Hyunjun Kim", "Eunseo Koh", "MinKyu Lee", "Jae-Pil Heo"], "title": "Fine-Tuning Visual Autoregressive Models for Subject-Driven Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in text-to-image generative models have enabled numerous\npractical applications, including subject-driven generation, which fine-tunes\npretrained models to capture subject semantics from only a few examples. While\ndiffusion-based models produce high-quality images, their extensive denoising\nsteps result in significant computational overhead, limiting real-world\napplicability. Visual autoregressive~(VAR) models, which predict next-scale\ntokens rather than spatially adjacent ones, offer significantly faster\ninference suitable for practical deployment. In this paper, we propose the\nfirst VAR-based approach for subject-driven generation. However, na\\\"{\\i}ve\nfine-tuning VAR leads to computational overhead, language drift, and reduced\ndiversity. To address these challenges, we introduce selective layer tuning to\nreduce complexity and prior distillation to mitigate language drift.\nAdditionally, we found that the early stages have a greater influence on the\ngeneration of subject than the latter stages, which merely synthesize local\ndetails. Based on this finding, we propose scale-wise weighted tuning, which\nprioritizes coarser resolutions for promoting the model to focus on the\nsubject-relevant information instead of local details. Extensive experiments\nvalidate that our method significantly outperforms diffusion-based baselines\nacross various metrics and demonstrates its practical usage.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02617", "pdf": "https://arxiv.org/pdf/2504.02617", "abs": "https://arxiv.org/abs/2504.02617", "authors": ["Lihua Liu", "Jiehong Lin", "Zhenxin Liu", "Kui Jia"], "title": "PicoPose: Progressive Pixel-to-Pixel Correspondence Learning for Novel Object Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Novel object pose estimation from RGB images presents a significant challenge\nfor zero-shot generalization, as it involves estimating the relative 6D\ntransformation between an RGB observation and a CAD model of an object that was\nnot seen during training. In this paper, we introduce PicoPose, a novel\nframework designed to tackle this task using a three-stage pixel-to-pixel\ncorrespondence learning process. Firstly, PicoPose matches features from the\nRGB observation with those from rendered object templates, identifying the\nbest-matched template and establishing coarse correspondences. Secondly,\nPicoPose smooths the correspondences by globally regressing a 2D affine\ntransformation, including in-plane rotation, scale, and 2D translation, from\nthe coarse correspondence map. Thirdly, PicoPose applies the affine\ntransformation to the feature map of the best-matched template and learns\ncorrespondence offsets within local regions to achieve fine-grained\ncorrespondences. By progressively refining the correspondences, PicoPose\nsignificantly improves the accuracy of object poses computed via PnP/RANSAC.\nPicoPose achieves state-of-the-art performance on the seven core datasets of\nthe BOP benchmark, demonstrating exceptional generalization to novel objects\nrepresented by CAD models or object reference images. Code and models are\navailable at https://github.com/foollh/PicoPose.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02697", "pdf": "https://arxiv.org/pdf/2504.02697", "abs": "https://arxiv.org/abs/2504.02697", "authors": ["Xingguang Zhang", "Nicholas Chimitt", "Xijun Wang", "Yu Yuan", "Stanley H. Chan"], "title": "Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation", "categories": ["cs.CV", "eess.IV"], "comment": "CVPR 2025, project page: https://xg416.github.io/MambaTM/", "summary": "Atmospheric turbulence is a major source of image degradation in long-range\nimaging systems. Although numerous deep learning-based turbulence mitigation\n(TM) methods have been proposed, many are slow, memory-hungry, and do not\ngeneralize well. In the spatial domain, methods based on convolutional\noperators have a limited receptive field, so they cannot handle a large spatial\ndependency required by turbulence. In the temporal domain, methods relying on\nself-attention can, in theory, leverage the lucky effects of turbulence, but\ntheir quadratic complexity makes it difficult to scale to many frames.\nTraditional recurrent aggregation methods face parallelization challenges.\n  In this paper, we present a new TM method based on two concepts: (1) A\nturbulence mitigation network based on the Selective State Space Model\n(MambaTM). MambaTM provides a global receptive field in each layer across\nspatial and temporal dimensions while maintaining linear computational\ncomplexity. (2) Learned Latent Phase Distortion (LPD). LPD guides the state\nspace model. Unlike classical Zernike-based representations of phase\ndistortion, the new LPD map uniquely captures the actual effects of turbulence,\nsignificantly improving the model's capability to estimate degradation by\nreducing the ill-posedness. Our proposed method exceeds current\nstate-of-the-art networks on various synthetic and real-world TM benchmarks\nwith significantly faster inference speed. The code is available at\nhttp://github.com/xg416/MambaTM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02730", "pdf": "https://arxiv.org/pdf/2504.02730", "abs": "https://arxiv.org/abs/2504.02730", "authors": ["Hui Zhang", "Qinglin Zhao", "Mengchu Zhou", "Li Feng"], "title": "HQViT: Hybrid Quantum Vision Transformer for Image Classification", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 8 figures", "summary": "Transformer-based architectures have revolutionized the landscape of deep\nlearning. In computer vision domain, Vision Transformer demonstrates remarkable\nperformance on par with or even surpassing that of convolutional neural\nnetworks. However, the quadratic computational complexity of its self-attention\nmechanism poses challenges for classical computing, making model training with\nhigh-dimensional input data, e.g., images, particularly expensive. To address\nsuch limitations, we propose a Hybrid Quantum Vision Transformer (HQViT), that\nleverages the principles of quantum computing to accelerate model training\nwhile enhancing model performance. HQViT introduces whole-image processing with\namplitude encoding to better preserve global image information without\nadditional positional encoding. By leveraging quantum computation on the most\ncritical steps and selectively handling other components in a classical way, we\nlower the cost of quantum resources for HQViT. The qubit requirement is\nminimized to $O(log_2N)$ and the number of parameterized quantum gates is only\n$O(log_2d)$, making it well-suited for Noisy Intermediate-Scale Quantum\ndevices. By offloading the computationally intensive attention coefficient\nmatrix calculation to the quantum framework, HQViT reduces the classical\ncomputational load by $O(T^2d)$. Extensive experiments across various computer\nvision datasets demonstrate that HQViT outperforms existing models, achieving a\nmaximum improvement of up to $10.9\\%$ (on the MNIST 10-classification task)\nover the state of the art. This work highlights the great potential to combine\nquantum and classical computing to cope with complex image classification\ntasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02817", "pdf": "https://arxiv.org/pdf/2504.02817", "abs": "https://arxiv.org/abs/2504.02817", "authors": ["Kangle Deng", "Hsueh-Ti Derek Liu", "Yiheng Zhu", "Xiaoxia Sun", "Chong Shang", "Kiran Bhat", "Deva Ramanan", "Jun-Yan Zhu", "Maneesh Agrawala", "Tinghui Zhou"], "title": "Efficient Autoregressive Shape Generation via Octree-Based Adaptive Tokenization", "categories": ["cs.CV"], "comment": "Project Page: https://oat-3d.github.io/", "summary": "Many 3D generative models rely on variational autoencoders (VAEs) to learn\ncompact shape representations. However, existing methods encode all shapes into\na fixed-size token, disregarding the inherent variations in scale and\ncomplexity across 3D data. This leads to inefficient latent representations\nthat can compromise downstream generation. We address this challenge by\nintroducing Octree-based Adaptive Tokenization, a novel framework that adjusts\nthe dimension of latent representations according to shape complexity. Our\napproach constructs an adaptive octree structure guided by a\nquadric-error-based subdivision criterion and allocates a shape latent vector\nto each octree cell using a query-based transformer. Building upon this\ntokenization, we develop an octree-based autoregressive generative model that\neffectively leverages these variable-sized representations in shape generation.\nExtensive experiments demonstrate that our approach reduces token counts by 50%\ncompared to fixed-size methods while maintaining comparable visual quality.\nWhen using a similar token length, our method produces significantly\nhigher-quality shapes. When incorporated with our downstream generative model,\nour method creates more detailed and diverse 3D content than existing\napproaches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02828", "pdf": "https://arxiv.org/pdf/2504.02828", "abs": "https://arxiv.org/abs/2504.02828", "authors": ["Jinqi Luo", "Tianjiao Ding", "Kwan Ho Ryan Chan", "Hancheng Min", "Chris Callison-Burch", "René Vidal"], "title": "Concept Lancet: Image Editing with Compositional Representation Transplant", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted in CVPR 2025. Project page at\n  https://peterljq.github.io/project/colan", "summary": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02151", "pdf": "https://arxiv.org/pdf/2504.02151", "abs": "https://arxiv.org/abs/2504.02151", "authors": ["Jiztom Kavalakkatt Francis", "Matthew J Darr"], "title": "Multivariate Temporal Regression at Scale: A Three-Pillar Framework Combining ML, XAI, and NLP", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "7 pages", "summary": "The rapid use of artificial intelligence (AI) in processes such as coding,\nimage processing, and data prediction means it is crucial to understand and\nvalidate the data we are working with fully. This paper dives into the hurdles\nof analyzing high-dimensional data, especially when it gets too complex.\nTraditional methods in data analysis often look at direct connections between\ninput variables, which can miss out on the more complicated relationships\nwithin the data.\n  To address these issues, we explore several tested techniques, such as\nremoving specific variables to see their impact and using statistical analysis\nto find connections between multiple variables. We also consider the role of\nsynthetic data and how information can sometimes be redundant across different\nsensors. These analyses are typically very computationally demanding and often\nrequire much human effort to make sense of the results.\n  A common approach is to treat the entire dataset as one unit and apply\nadvanced models to handle it. However, this can become problematic with larger,\nnoisier datasets and more complex models. So, we suggest methods to identify\noverall patterns that can help with tasks like classification or regression\nbased on the idea that more straightforward approaches might be more\nunderstandable.\n  Our research looks at two datasets: a real-world dataset and a synthetic one.\nThe goal is to create a methodology that highlights key features on a global\nscale that lead to predictions, making it easier to validate or quantify the\ndata set. By reducing the dimensionality with this method, we can simplify the\nmodels used and thus clarify the insights we gain. Furthermore, our method can\nreveal unexplored relationships between specific inputs and outcomes, providing\na way to validate these new connections further.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02587", "pdf": "https://arxiv.org/pdf/2504.02587", "abs": "https://arxiv.org/abs/2504.02587", "authors": ["Yan Ma", "Steffi Chern", "Xuyang Shen", "Yiran Zhong", "Pengfei Liu"], "title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Code is public and available at: https://github.com/GAIR-NLP/MAYE", "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
{"id": "2504.02797", "pdf": "https://arxiv.org/pdf/2504.02797", "abs": "https://arxiv.org/abs/2504.02797", "authors": ["Prashanth Chandran", "Agon Serifi", "Markus Gross", "Moritz Bächer"], "title": "Spline-based Transformers", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We introduce Spline-based Transformers, a novel class of Transformer models\nthat eliminate the need for positional encoding. Inspired by workflows using\nsplines in computer animation, our Spline-based Transformers embed an input\nsequence of elements as a smooth trajectory in latent space. Overcoming\ndrawbacks of positional encoding such as sequence length extrapolation,\nSpline-based Transformers also provide a novel way for users to interact with\ntransformer latent spaces by directly manipulating the latent control points to\ncreate new latent trajectories and sequences. We demonstrate the superior\nperformance of our approach in comparison to conventional positional encoding\non a variety of datasets, ranging from synthetic 2D to large-scale real-world\ndatasets of images, 3D shapes, and animations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-05.jsonl"}
