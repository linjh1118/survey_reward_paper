{"id": "2504.02890", "pdf": "https://arxiv.org/pdf/2504.02890", "abs": "https://arxiv.org/abs/2504.02890", "authors": ["Khanh-Tung Tran", "Barry O'Sullivan", "Hoang D. Nguyen"], "title": "Scaling Test-time Compute for Low-resource Languages: Multilingual Reasoning in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in test-time compute scaling have enabled Large Language\nModels (LLMs) to tackle deep reasoning tasks by generating a chain-of-thought\n(CoT) that includes trial and error, backtracking, and intermediate reasoning\nsteps before producing the final answer. However, these techniques have been\napplied predominantly to popular languages, such as English, leaving reasoning\nin low-resource languages underexplored and misaligned. In this work, we\ninvestigate the multilingual mechanism by which LLMs internally operate in a\nlatent space biased toward their inherently dominant language. To leverage this\nphenomenon for low-resource languages, we train models to generate the CoT in\nEnglish while outputting the final response in the target language, given input\nin the low-resource language. Our experiments demonstrate that this approach,\nnamed English-Pivoted CoT Training, outperforms other baselines, including\ntraining to generate both the CoT and the final response solely in the target\nlanguage, with up to 28.33% improvement. Further analysis provides novel\ninsights into the relationships between reasoning and multilinguality of LLMs,\nprompting for better approaches in developing multilingual large reasoning\nmodels", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "compute scaling", "test-time compute"], "score": 4}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03624", "pdf": "https://arxiv.org/pdf/2504.03624", "abs": "https://arxiv.org/abs/2504.03624", "authors": ["NVIDIA", ":", "Aaron Blakeman", "Aarti Basant", "Abhinav Khattar", "Adithya Renduchintala", "Akhiad Bercovich", "Aleksander Ficek", "Alexis Bjorlin", "Ali Taghibakhshi", "Amala Sanjay Deshmukh", "Ameya Sunil Mahabaleshwarkar", "Andrew Tao", "Anna Shors", "Ashwath Aithal", "Ashwin Poojary", "Ayush Dattagupta", "Balaram Buddharaju", "Bobby Chen", "Boris Ginsburg", "Boxin Wang", "Brandon Norick", "Brian Butterfield", "Bryan Catanzaro", "Carlo del Mundo", "Chengyu Dong", "Christine Harvey", "Christopher Parisien", "Dan Su", "Daniel Korzekwa", "Danny Yin", "Daria Gitman", "David Mosallanezhad", "Deepak Narayanan", "Denys Fridman", "Dima Rekesh", "Ding Ma", "Dmytro Pykhtar", "Dong Ahn", "Duncan Riach", "Dusan Stosic", "Eileen Long", "Elad Segal", "Ellie Evans", "Eric Chung", "Erick Galinkin", "Evelina Bakhturina", "Ewa Dobrowolska", "Fei Jia", "Fuxiao Liu", "Gargi Prasad", "Gerald Shen", "Guilin Liu", "Guo Chen", "Haifeng Qian", "Helen Ngo", "Hongbin Liu", "Hui Li", "Igor Gitman", "Ilia Karmanov", "Ivan Moshkov", "Izik Golan", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jarno Seppanen", "Jason Lu", "Jason Sewall", "Jiaqi Zeng", "Jiaxuan You", "Jimmy Zhang", "Jing Zhang", "Jining Huang", "Jinze Xue", "Jocelyn Huang", "Joey Conway", "John Kamalu", "Jon Barker", "Jonathan Cohen", "Joseph Jennings", "Jupinder Parmar", "Karan Sapra", "Kari Briski", "Kateryna Chumachenko", "Katherine Luna", "Keshav Santhanam", "Kezhi Kong", "Kirthi Sivamani", "Krzysztof Pawelec", "Kumar Anik", "Kunlun Li", "Lawrence McAfee", "Leon Derczynski", "Lindsey Pavao", "Luis Vega", "Lukas Voegtle", "Maciej Bala", "Maer Rodrigues de Melo", "Makesh Narsimhan Sreedhar", "Marcin Chochowski", "Markus Kliegl", "Marta Stepniewska-Dziubinska", "Matthieu Le", "Matvei Novikov", "Mehrzad Samadi", "Michael Andersch", "Michael Evans", "Miguel Martinez", "Mike Chrzanowski", "Mike Ranzinger", "Mikolaj Blaz", "Misha Smelyanskiy", "Mohamed Fawzy", "Mohammad Shoeybi", "Mostofa Patwary", "Nayeon Lee", "Nima Tajbakhsh", "Ning Xu", "Oleg Rybakov", "Oleksii Kuchaiev", "Olivier Delalleau", "Osvald Nitski", "Parth Chadha", "Pasha Shamis", "Paulius Micikevicius", "Pavlo Molchanov", "Peter Dykas", "Philipp Fischer", "Pierre-Yves Aquilanti", "Piotr Bialecki", "Prasoon Varshney", "Pritam Gundecha", "Przemek Tredak", "Rabeeh Karimi", "Rahul Kandu", "Ran El-Yaniv", "Raviraj Joshi", "Roger Waleffe", "Ruoxi Zhang", "Sabrina Kavanaugh", "Sahil Jain", "Samuel Kriman", "Sangkug Lym", "Sanjeev Satheesh", "Saurav Muralidharan", "Sean Narenthiran", "Selvaraj Anandaraj", "Seonmyeong Bak", "Sergey Kashirsky", "Seungju Han", "Shantanu Acharya", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Sharon Clay", "Shelby Thomas", "Shrimai Prabhumoye", "Shubham Pachori", "Shubham Toshniwal", "Shyamala Prayaga", "Siddhartha Jain", "Sirshak Das", "Slawek Kierat", "Somshubra Majumdar", "Song Han", "Soumye Singhal", "Sriharsha Niverty", "Stefania Alborghetti", "Suseella Panguluri", "Swetha Bhendigeri", "Syeda Nahida Akter", "Szymon Migacz", "Tal Shiri", "Terry Kong", "Timo Roman", "Tomer Ronen", "Trisha Saar", "Tugrul Konuk", "Tuomas Rintamaki", "Tyler Poon", "Ushnish De", "Vahid Noroozi", "Varun Singh", "Vijay Korthikanti", "Vitaly Kurin", "Wasi Uddin Ahmad", "Wei Du", "Wei Ping", "Wenliang Dai", "Wonmin Byeon", "Xiaowei Ren", "Yao Xu", "Yejin Choi", "Yian Zhang", "Ying Lin", "Yoshi Suhara", "Zhiding Yu", "Zhiqi Li", "Zhiyu Li", "Zhongbo Zhu", "Zhuolin Yang", "Zijia Chen"], "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. All Nemotron-H models will be released, with\nsupport in Hugging Face, NeMo, and Megatron-LM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "inference time", "scaling"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03635", "pdf": "https://arxiv.org/pdf/2504.03635", "abs": "https://arxiv.org/abs/2504.03635", "authors": ["Xinyi Wang", "Shawn Tan", "Mingyu Jin", "William Yang Wang", "Rameswar Panda", "Yikang Shen"], "title": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale", "scaling law"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03096", "pdf": "https://arxiv.org/pdf/2504.03096", "abs": "https://arxiv.org/abs/2504.03096", "authors": ["Zhen Hao Sia", "Yogesh Singh Rawat"], "title": "Scaling Open-Vocabulary Action Detection", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we focus on scaling open-vocabulary action detection. Existing\napproaches for action detection are predominantly limited to closed-set\nscenarios and rely on complex, parameter-heavy architectures. Extending these\nmodels to the open-vocabulary setting poses two key challenges: (1) the lack of\nlarge-scale datasets with many action classes for robust training, and (2)\nparameter-heavy adaptations to a pretrained vision-language contrastive model\nto convert it for detection, risking overfitting the additional non-pretrained\nparameters to base action classes. Firstly, we introduce an encoder-only\nmultimodal model for video action detection, reducing the reliance on\nparameter-heavy additions for video action detection. Secondly, we introduce a\nsimple weakly supervised training strategy to exploit an existing closed-set\naction detection dataset for pretraining. Finally, we depart from the ill-posed\nbase-to-novel benchmark used by prior works in open-vocabulary action detection\nand devise a new benchmark to evaluate on existing closed-set action detection\ndatasets without ever using them for training, showing novel results to serve\nas baselines for future work.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03177", "pdf": "https://arxiv.org/pdf/2504.03177", "abs": "https://arxiv.org/abs/2504.03177", "authors": ["Yuki Kawana", "Tatsuya Harada"], "title": "Detection Based Part-level Articulated Object Reconstruction from Single RGBD Image", "categories": ["cs.CV"], "comment": "Accepted to NeurIPS 2023", "summary": "We propose an end-to-end trainable, cross-category method for reconstructing\nmultiple man-made articulated objects from a single RGBD image, focusing on\npart-level shape reconstruction and pose and kinematics estimation. We depart\nfrom previous works that rely on learning instance-level latent space, focusing\non man-made articulated objects with predefined part counts. Instead, we\npropose a novel alternative approach that employs part-level representation,\nrepresenting instances as combinations of detected parts. While our\ndetect-then-group approach effectively handles instances with diverse part\nstructures and various part counts, it faces issues of false positives, varying\npart sizes and scales, and an increasing model size due to end-to-end training.\nTo address these challenges, we propose 1) test-time kinematics-aware part\nfusion to improve detection performance while suppressing false positives, 2)\nanisotropic scale normalization for part shape learning to accommodate various\npart sizes and scales, and 3) a balancing strategy for cross-refinement between\nfeature space and output space to improve part detection while maintaining\nmodel size. Evaluation on both synthetic and real data demonstrates that our\nmethod successfully reconstructs variously structured multiple instances that\nprevious works cannot handle, and outperforms prior works in shape\nreconstruction and kinematics estimation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03561", "pdf": "https://arxiv.org/pdf/2504.03561", "abs": "https://arxiv.org/abs/2504.03561", "authors": ["Runnan Fang", "Xiaobin Wang", "Yuan Liang", "Shuofei Qiao", "Jialong Wu", "Zekun Xi", "Ningyu Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen"], "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": "Work in progress", "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search", "MCTS"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03595", "pdf": "https://arxiv.org/pdf/2504.03595", "abs": "https://arxiv.org/abs/2504.03595", "authors": ["Fabio Lilliu", "Amir Laadhar", "Christian Thomsen", "Diego Reforgiato Recupero", "Torben Bach Pedersen"], "title": "Extending the SAREF4ENER Ontology with Flexibility Based on FlexOffers", "categories": ["cs.CL"], "comment": "13 pages, 5 figures, 4 tables. Submitted to SmartGridComm 2025", "summary": "A key element to support the increased amounts of renewable energy in the\nenergy system is flexibility, i.e., the possibility of changing energy loads in\ntime and amount. Many flexibility models have been designed; however, exact\nmodels fail to scale for long time horizons or many devices. Because of this,\nthe FlexOffer (FOs) model has been designed, to provide device-independent\napproximations of flexibility with good accuracy, and much better scaling for\nlong time horizons and many devices. An important aspect of the real-life\nimplementation of energy flexibility is enabling flexible data exchange with\nmany types of smart energy appliances and market systems, e.g., in smart\nbuildings. For this, ontologies standardizing data formats are required.\nHowever, the current industry standard ontology for integrating smart devices\nfor energy purposes, SAREF for Energy Flexibility (SAREF4ENER) only has limited\nsupport for flexibility and thus cannot support important use cases. In this\npaper we propose an extension of SAREF4ENER that integrates full support for\nthe complete FlexOffer model, including advanced use cases, while maintaining\nbackward compatibility. This novel ontology module can accurately describe\nflexibility for advanced devices such as electric vehicles, batteries, and heat\npumps. It can also capture the inherent uncertainty associated with many\nflexible load types.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03640", "pdf": "https://arxiv.org/pdf/2504.03640", "abs": "https://arxiv.org/abs/2504.03640", "authors": ["Kate Sanders", "Benjamin Van Durme"], "title": "Bonsai: Interpretable Tree-Adaptive Grounded Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50, 68T37", "I.2.7"], "comment": "9 pages, preprint", "summary": "To develop general-purpose collaborative agents, humans need reliable AI\nsystems that can (1) adapt to new domains and (2) transparently reason with\nuncertainty to allow for verification and correction. Black-box models\ndemonstrate powerful data processing abilities but do not satisfy these\ncriteria due to their opaqueness, domain specificity, and lack of uncertainty\nawareness. We introduce Bonsai, a compositional and probabilistic reasoning\nsystem that generates adaptable inference trees by retrieving relevant\ngrounding evidence and using it to compute likelihoods of sub-claims derived\nfrom broader natural language inferences. Bonsai's reasoning power is tunable\nat test-time via evidence scaling and it demonstrates reliable handling of\nvaried domains including transcripts, photographs, videos, audio, and\ndatabases. Question-answering and human alignment experiments demonstrate that\nBonsai matches the performance of domain-specific black-box methods while\ngenerating interpretable, grounded, and uncertainty-aware reasoning traces.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "human alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03561", "pdf": "https://arxiv.org/pdf/2504.03561", "abs": "https://arxiv.org/abs/2504.03561", "authors": ["Runnan Fang", "Xiaobin Wang", "Yuan Liang", "Shuofei Qiao", "Jialong Wu", "Zekun Xi", "Ningyu Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen"], "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": "Work in progress", "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search", "MCTS"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03640", "pdf": "https://arxiv.org/pdf/2504.03640", "abs": "https://arxiv.org/abs/2504.03640", "authors": ["Kate Sanders", "Benjamin Van Durme"], "title": "Bonsai: Interpretable Tree-Adaptive Grounded Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50, 68T37", "I.2.7"], "comment": "9 pages, preprint", "summary": "To develop general-purpose collaborative agents, humans need reliable AI\nsystems that can (1) adapt to new domains and (2) transparently reason with\nuncertainty to allow for verification and correction. Black-box models\ndemonstrate powerful data processing abilities but do not satisfy these\ncriteria due to their opaqueness, domain specificity, and lack of uncertainty\nawareness. We introduce Bonsai, a compositional and probabilistic reasoning\nsystem that generates adaptable inference trees by retrieving relevant\ngrounding evidence and using it to compute likelihoods of sub-claims derived\nfrom broader natural language inferences. Bonsai's reasoning power is tunable\nat test-time via evidence scaling and it demonstrates reliable handling of\nvaried domains including transcripts, photographs, videos, audio, and\ndatabases. Question-answering and human alignment experiments demonstrate that\nBonsai matches the performance of domain-specific black-box methods while\ngenerating interpretable, grounded, and uncertainty-aware reasoning traces.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "human alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02866", "pdf": "https://arxiv.org/pdf/2504.02866", "abs": "https://arxiv.org/abs/2504.02866", "authors": ["Xiucheng Liang", "Jinheng Xie", "Tianhong Zhao", "Rudi Stouffs", "Filip Biljecki"], "title": "OpenFACADES: An Open Framework for Architectural Caption and Attribute Data Enrichment via Street View Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Building properties, such as height, usage, and material composition, play a\ncrucial role in spatial data infrastructures, supporting applications such as\nenergy simulation, risk assessment, and environmental modeling. Despite their\nimportance, comprehensive and high-quality building attribute data remain\nscarce in many urban areas. Recent advances have enabled the extraction and\ntagging of objective building attributes using remote sensing and street-level\nimagery. However, establishing a method and pipeline that integrates diverse\nopen datasets, acquires holistic building imagery at scale, and infers\ncomprehensive building attributes remains a significant challenge. Among the\nfirst, this study bridges the gaps by introducing OpenFACADES, an open\nframework that leverages multimodal crowdsourced data to enrich building\nprofiles with both objective attributes and semantic descriptors through\nmultimodal large language models. Our methodology proceeds in three major\nsteps. First, we integrate street-level image metadata from Mapillary with\nOpenStreetMap geometries via isovist analysis, effectively identifying images\nthat provide suitable vantage points for observing target buildings. Second, we\nautomate the detection of building facades in panoramic imagery and tailor a\nreprojection approach to convert objects into holistic perspective views that\napproximate real-world observation. Third, we introduce an innovative approach\nthat harnesses and systematically investigates the capabilities of open-source\nlarge vision-language models (VLMs) for multi-attribute prediction and\nopen-vocabulary captioning in building-level analytics, leveraging a globally\nsourced dataset of 30,180 labeled images from seven cities. Evaluation shows\nthat fine-tuned VLM excel in multi-attribute inference, outperforming\nsingle-attribute computer vision models and zero-shot ChatGPT-4o.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02876", "pdf": "https://arxiv.org/pdf/2504.02876", "abs": "https://arxiv.org/abs/2504.02876", "authors": ["Yangxiao Lu", "Ruosen Li", "Liqiang Jing", "Jikai Wang", "Xinya Du", "Yunhui Guo", "Nicholas Ruozzi", "Yu Xiang"], "title": "Multimodal Reference Visual Grounding", "categories": ["cs.CV", "cs.LG"], "comment": "Project page with our code and dataset:\n  https://irvlutd.github.io/MultiGrounding", "summary": "Visual grounding focuses on detecting objects from images based on language\nexpressions. Recent Large Vision-Language Models (LVLMs) have significantly\nadvanced visual grounding performance by training large models with large-scale\ndatasets. However, the problem remains challenging, especially when similar\nobjects appear in the input image. For example, an LVLM may not be able to\ndifferentiate Diet Coke and regular Coke in an image. In this case, if\nadditional reference images of Diet Coke and regular Coke are available, it can\nhelp the visual grounding of similar objects.\n  In this work, we introduce a new task named Multimodal Reference Visual\nGrounding (MRVG). In this task, a model has access to a set of reference images\nof objects in a database. Based on these reference images and a language\nexpression, the model is required to detect a target object from a query image.\nWe first introduce a new dataset to study the MRVG problem. Then we introduce a\nnovel method, named MRVG-Net, to solve this visual grounding problem. We show\nthat by efficiently using reference images with few-shot object detection and\nusing Large Language Models (LLMs) for object matching, our method achieves\nsuperior visual grounding performance compared to the state-of-the-art LVLMs\nsuch as Qwen2.5-VL-7B. Our approach bridges the gap between few-shot detection\nand visual grounding, unlocking new capabilities for visual understanding.\nProject page with our code and dataset:\nhttps://irvlutd.github.io/MultiGrounding", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02895", "pdf": "https://arxiv.org/pdf/2504.02895", "abs": "https://arxiv.org/abs/2504.02895", "authors": ["Farida Al Haddad", "Yuxin Wang", "Malcolm Mielle"], "title": "UAC: Uncertainty-Aware Calibration of Neural Networks for Gesture Detection", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 2 figures", "summary": "Artificial intelligence has the potential to impact safety and efficiency in\nsafety-critical domains such as construction, manufacturing, and healthcare.\nFor example, using sensor data from wearable devices, such as inertial\nmeasurement units (IMUs), human gestures can be detected while maintaining\nprivacy, thereby ensuring that safety protocols are followed. However, strict\nsafety requirements in these domains have limited the adoption of AI, since\naccurate calibration of predicted probabilities and robustness against\nout-of-distribution (OOD) data is necessary.\n  This paper proposes UAC (Uncertainty-Aware Calibration), a novel two-step\nmethod to address these challenges in IMU-based gesture recognition. First, we\npresent an uncertainty-aware gesture network architecture that predicts both\ngesture probabilities and their associated uncertainties from IMU data. This\nuncertainty is then used to calibrate the probabilities of each potential\ngesture. Second, an entropy-weighted expectation of predictions over multiple\nIMU data windows is used to improve accuracy while maintaining correct\ncalibration.\n  Our method is evaluated using three publicly available IMU datasets for\ngesture detection and is compared to three state-of-the-art calibration methods\nfor neural networks: temperature scaling, entropy maximization, and Laplace\napproximation. UAC outperforms existing methods, achieving improved accuracy\nand calibration in both OOD and in-distribution scenarios. Moreover, we find\nthat, unlike our method, none of the state-of-the-art methods significantly\nimprove the calibration of IMU-based gesture recognition models. In conclusion,\nour work highlights the advantages of uncertainty-aware calibration of neural\nnetworks, demonstrating improvements in both calibration and accuracy for\ngesture detection using IMU data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02872", "pdf": "https://arxiv.org/pdf/2504.02872", "abs": "https://arxiv.org/abs/2504.02872", "authors": ["Ingmar Bakermans", "Daniel De Pascale", "Gonçalo Marcelino", "Giuseppe Cascavilla", "Zeno Geradts"], "title": "Scraping the Shadows: Deep Learning Breakthroughs in Dark Web Intelligence", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR"], "comment": "17 pages, 17 images", "summary": "Darknet markets (DNMs) facilitate the trade of illegal goods on a global\nscale. Gathering data on DNMs is critical to ensuring law enforcement agencies\ncan effectively combat crime. Manually extracting data from DNMs is an\nerror-prone and time-consuming task. Aiming to automate this process we develop\na framework for extracting data from DNMs and evaluate the application of three\nstate-of-the-art Named Entity Recognition (NER) models, ELMo-BiLSTM\n\\citep{ShahEtAl2022}, UniversalNER \\citep{ZhouEtAl2024}, and GLiNER\n\\citep{ZaratianaEtAl2023}, at the task of extracting complex entities from DNM\nproduct listing pages. We propose a new annotated dataset, which we use to\ntrain, fine-tune, and evaluate the models. Our findings show that\nstate-of-the-art NER models perform well in information extraction from DNMs,\nachieving 91% Precision, 96% Recall, and an F1 score of 94%. In addition,\nfine-tuning enhances model performance, with UniversalNER achieving the best\nperformance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02949", "pdf": "https://arxiv.org/pdf/2504.02949", "abs": "https://arxiv.org/abs/2504.02949", "authors": ["Xianwei Zhuang", "Yuxin Xie", "Yufan Deng", "Dongchao Yang", "Liming Liang", "Jinghan Ru", "Yuguo Yin", "Yuexian Zou"], "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Code is available at: https://github.com/VARGPT-family/VARGPT-v1.1.\n  arXiv admin note: text overlap with arXiv:2501.12327", "summary": "In this work, we present VARGPT-v1.1, an advanced unified visual\nautoregressive model that builds upon our previous framework VARGPT. The model\npreserves the dual paradigm of next-token prediction for visual understanding\nand next-scale generation for image synthesis. Specifically, VARGPT-v1.1\nintegrates: (1) a novel training strategy combining iterative visual\ninstruction tuning with reinforcement learning through Direct Preference\nOptimization (DPO), (2) an expanded training corpus containing 8.3M\nvisual-generative instruction pairs, (3) an upgraded language model backbone\nusing Qwen2, (4) enhanced image generation resolution, and (5) emergent image\nediting capabilities without architectural modifications. These advancements\nenable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal\nunderstanding and text-to-image instruction-following tasks, demonstrating\nsignificant improvements in both comprehension and generation metrics. Notably,\nthrough visual instruction tuning, the model acquires image editing\nfunctionality while maintaining architectural consistency with its predecessor,\nrevealing the potential for unified visual understanding, generation, and\nediting. Our findings suggest that well-designed unified visual autoregressive\nmodels can effectively adopt flexible training strategies from large language\nmodels (LLMs), exhibiting promising scalability. The codebase and model weights\nare publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02874", "pdf": "https://arxiv.org/pdf/2504.02874", "abs": "https://arxiv.org/abs/2504.02874", "authors": ["Luis Felipe", "Carlos Garcia", "Issam El Naqa", "Monique Shotande", "Aakash Tripathi", "Vivek Rudrapatna", "Ghulam Rasool", "Danielle Bitterman", "Gilmer Valdes"], "title": "TheBlueScrubs-v1, a comprehensive curated medical dataset derived from the internet", "categories": ["cs.CL"], "comment": "22 pages, 8 figures, 10 tables", "summary": "The need for robust and diverse data sets to train clinical large language\nmodels (cLLMs) is critical given that currently available public repositories\noften prove too limited in size or scope for comprehensive medical use. While\nresources like PubMed provide foundational medical literature, they capture\nonly a narrow range of formal publications and omit the broader medical\ndiscourse on the internet. To address these deficits, we introduce\nTheBlueScrubs-v1, a curated dataset of over 25 billion medical tokens - nearly\nthree times larger than PubMed - drawn from a broad-scale internet corpus. Our\ntwo-stage filtering pipeline employs a Logistic Regression model for document\nscreening (achieving an AUC of approximately 0.95 on external validation),\nfollowed by verification via a 70B-parameter Llama 3.1 instruct model. Each\ntext is assigned three LLM-based quality scores encompassing medical relevance,\nprecision and factual detail, and safety and ethical standards. Clinician\nreviews confirm high concordance with these automated evaluations, and a\nspecialized cancer classifier further labels approximately 11 billion oncology\ntokens. Two demonstration tasks highlight the dataset's practical value: first,\nwe distill the safety evaluations to a smaller BERT-style model that reaches an\nAUC near 0.96 on unseen data; second, we fine-tune a compact LLM on a filtered\nsubset, showing measurable improvements over standard baselines in medical\nbenchmarks as well as private ones. This Data Descriptor details the dataset's\ncreation and validation, underscoring its potential utility for medical AI\nresearch.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03006", "pdf": "https://arxiv.org/pdf/2504.03006", "abs": "https://arxiv.org/abs/2504.03006", "authors": ["Jing Gao", "Ce Zheng", "Laszlo A. Jeni", "Zackory Erickson"], "title": "DiSRT-In-Bed: Diffusion-Based Sim-to-Real Transfer Framework for In-Bed Human Mesh Recovery", "categories": ["cs.CV"], "comment": "16 pages, 19 figures. Accepted to CVPR 2025", "summary": "In-bed human mesh recovery can be crucial and enabling for several healthcare\napplications, including sleep pattern monitoring, rehabilitation support, and\npressure ulcer prevention. However, it is difficult to collect large real-world\nvisual datasets in this domain, in part due to privacy and expense constraints,\nwhich in turn presents significant challenges for training and deploying deep\nlearning models. Existing in-bed human mesh estimation methods often rely\nheavily on real-world data, limiting their ability to generalize across\ndifferent in-bed scenarios, such as varying coverings and environmental\nsettings. To address this, we propose a Sim-to-Real Transfer Framework for\nin-bed human mesh recovery from overhead depth images, which leverages\nlarge-scale synthetic data alongside limited or no real-world samples. We\nintroduce a diffusion model that bridges the gap between synthetic data and\nreal data to support generalization in real-world in-bed pose and body\ninference scenarios. Extensive experiments and ablation studies validate the\neffectiveness of our framework, demonstrating significant improvements in\nrobustness and adaptability across diverse healthcare scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02877", "pdf": "https://arxiv.org/pdf/2504.02877", "abs": "https://arxiv.org/abs/2504.02877", "authors": ["DongHyun Choi", "Lucas Spangher", "Chris Hidey", "Peter Grabowski", "Ramy Eskander"], "title": "Revisiting Funnel Transformers for Modern LLM Architectures with Comprehensive Ablations in Training and Inference Configurations", "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based Large Language Models, which suffer from high computational\ncosts, advance so quickly that techniques proposed to streamline earlier\niterations are not guaranteed to benefit more modern models. Building upon the\nFunnel Transformer proposed by Dai and Le (2020), which progressively\ncompresses intermediate representations, we investigate the impact of funneling\nin contemporary Gemma2 Transformer architectures. We systematically evaluate\nvarious funnel configurations and recovery methods, comparing: (1) standard\npretraining to funnel-aware pretraining strategies, (2) the impact of\nfunnel-aware fine-tuning, and (3) the type of sequence recovery operation. Our\nresults demonstrate that funneling creates information bottlenecks that\npropagate through deeper network layers, particularly in larger models (e.g.,\nGemma 7B), leading to at times unmanageable performance lost. However,\ncarefully selecting the funneling layer and employing effective recovery\nstrategies, can substantially mitigate performance losses, achieving up to a\n44\\% reduction in latency. Our findings highlight key trade-offs between\ncomputational efficiency and model accuracy, providing practical guidance for\ndeploying funnel-based approaches in large-scale natural language applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03011", "pdf": "https://arxiv.org/pdf/2504.03011", "abs": "https://arxiv.org/abs/2504.03011", "authors": ["Junying Wang", "Jingyuan Liu", "Xin Sun", "Krishna Kumar Singh", "Zhixin Shu", "He Zhang", "Jimei Yang", "Nanxuan Zhao", "Tuanfeng Y. Wang", "Simon S. Chen", "Ulrich Neumann", "Jae Shin Yoon"], "title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human Relighting and Harmonization", "categories": ["cs.CV"], "comment": "Project page:https://junyingw.github.io/paper/relighting. Accepted by\n  CVPR 2025", "summary": "This paper introduces Comprehensive Relighting, the first all-in-one approach\nthat can both control and harmonize the lighting from an image or video of\nhumans with arbitrary body parts from any scene. Building such a generalizable\nmodel is extremely challenging due to the lack of dataset, restricting existing\nimage-based relighting models to a specific scenario (e.g., face or static\nhuman). To address this challenge, we repurpose a pre-trained diffusion model\nas a general image prior and jointly model the human relighting and background\nharmonization in the coarse-to-fine framework. To further enhance the temporal\ncoherence of the relighting, we introduce an unsupervised temporal lighting\nmodel that learns the lighting cycle consistency from many real-world videos\nwithout any ground truth. In inference time, our temporal lighting module is\ncombined with the diffusion models through the spatio-temporal feature blending\nalgorithms without extra training; and we apply a new guided refinement as a\npost-processing to preserve the high-frequency details from the input image. In\nthe experiments, Comprehensive Relighting shows a strong generalizability and\nlighting temporal coherence, outperforming existing image-based human\nrelighting and harmonization methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02885", "pdf": "https://arxiv.org/pdf/2504.02885", "abs": "https://arxiv.org/abs/2504.02885", "authors": ["Hao Wang", "Shuchang Ye", "Jinghao Lin", "Usman Naseem", "Jinman Kim"], "title": "LVMed-R2: Perception and Reflection-driven Complex Reasoning for Medical Report Generation", "categories": ["cs.CL"], "comment": "10 pages, 3 figures, 1 table", "summary": "Large vision-language models (LVMs) hold a great promise for automating\nmedical report generation, potentially reducing the burden of manual reporting.\nState-of-the-art (SOTA) research fine-tunes general LVMs with medical data to\nalign radiology images to corresponding medical reports. However, there are two\nkey factors that limit these LVM's performance. Firstly, LVMs lack complex\nreasoning capability that leads to logical inconsistencies and potential\ndiagnostic errors in generated reports. Secondly, LVMs lack reflection\nmechanism that leads to an inability to discover errors in the thinking\nprocess. To address these gaps, we propose LVMed-R2, a new fine-tuning strategy\nthat introduces complex reasoning and reflection mechanisms for LVMs to enhance\nmedical report generation. To the best of our knowledge, this is the first work\nto introduce complex reasoning to the medical report generation (MRG) task. Our\nproposed complex reasoning contains medical knowledge injection and\nperception-enhancing modules which improve the accuracy of LVMs diagnosis,\ncoupled with a perception tree to provide guidance to limit the perception\nrange. Further, the reflection mechanism forces self-verification for outputs\nto correct for potential errors. We experimented by fine-tuning LVMs with our\nproposed LVMed-R2 strategy, using IU-Xray and MIMIC-CXR datasets. Our results,\nmeasured on natural language generation (NLG) metrics and clinical efficacy\n(CE) metrics, demonstrate that LVMs fine-tuned with the proposed reflection\nmechanism possess the ability to correct outputs and complex reasoning\neffectively and improve LVMs performance for MRG.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-verification"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02891", "pdf": "https://arxiv.org/pdf/2504.02891", "abs": "https://arxiv.org/abs/2504.02891", "authors": ["Kurmanbek Kaiyrbekov", "Nicholas J Dobbins", "Sean D Mooney"], "title": "Automated Survey Collection with LLM-based Conversational Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Objective: Traditional phone-based surveys are among the most accessible and\nwidely used methods to collect biomedical and healthcare data, however, they\nare often costly, labor intensive, and difficult to scale effectively. To\novercome these limitations, we propose an end-to-end survey collection\nframework driven by conversational Large Language Models (LLMs).\n  Materials and Methods: Our framework consists of a researcher responsible for\ndesigning the survey and recruiting participants, a conversational phone agent\npowered by an LLM that calls participants and administers the survey, a second\nLLM (GPT-4o) that analyzes the conversation transcripts generated during the\nsurveys, and a database for storing and organizing the results. To test our\nframework, we recruited 8 participants consisting of 5 native and 3 non-native\nenglish speakers and administered 40 surveys. We evaluated the correctness of\nLLM-generated conversation transcripts, accuracy of survey responses inferred\nby GPT-4o and overall participant experience.\n  Results: Survey responses were successfully extracted by GPT-4o from\nconversation transcripts with an average accuracy of 98% despite transcripts\nexhibiting an average per-line word error rate of 7.7%. While participants\nnoted occasional errors made by the conversational LLM agent, they reported\nthat the agent effectively conveyed the purpose of the survey, demonstrated\ngood comprehension, and maintained an engaging interaction.\n  Conclusions: Our study highlights the potential of LLM agents in conducting\nand analyzing phone surveys for healthcare applications. By reducing the\nworkload on human interviewers and offering a scalable solution, this approach\npaves the way for real-world, end-to-end AI-powered phone survey collection\nsystems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02965", "pdf": "https://arxiv.org/pdf/2504.02965", "abs": "https://arxiv.org/abs/2504.02965", "authors": ["Abhishek Sharma", "Dan Goldwasser"], "title": "CoLa -- Learning to Interactively Collaborate with Large LMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "LLMs' remarkable ability to tackle a wide range of language tasks opened new\nopportunities for collaborative human-AI problem solving. LLMs can amplify\nhuman capabilities by applying their intuitions and reasoning strategies at\nscale. We explore whether human guides can be simulated, by generalizing from\nhuman demonstrations of guiding an AI system to solve complex language\nproblems. We introduce CoLa, a novel self-guided learning paradigm for training\nautomated $\\textit{guides}$ and evaluate it on two QA datasets, a\npuzzle-solving task, and a constrained text generation task. Our empirical\nresults show that CoLa consistently outperforms competitive approaches across\nall domains. Moreover, a small-sized trained guide outperforms a strong model\nlike GPT-4 when acting as a guide. We compare the strategies employed by humans\nand automated guides by conducting a human study on a QA dataset. We show that\nautomated guides outperform humans by adapting their strategies to reasoners'\ncapabilities and conduct qualitative analyses highlighting distinct differences\nin guiding strategies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03164", "pdf": "https://arxiv.org/pdf/2504.03164", "abs": "https://arxiv.org/abs/2504.03164", "authors": ["Kexin Tian", "Jingrui Mao", "Yunlong Zhang", "Jiwan Jiang", "Yang Zhou", "Zhengzhong Tu"], "title": "NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Vision-Language Models (VLMs) have demonstrated strong\npotential for autonomous driving tasks. However, their spatial understanding\nand reasoning-key capabilities for autonomous driving-still exhibit significant\nlimitations. Notably, none of the existing benchmarks systematically evaluate\nVLMs' spatial reasoning capabilities in driving scenarios. To fill this gap, we\npropose NuScenes-SpatialQA, the first large-scale ground-truth-based\nQuestion-Answer (QA) benchmark specifically designed to evaluate the spatial\nunderstanding and reasoning capabilities of VLMs in autonomous driving. Built\nupon the NuScenes dataset, the benchmark is constructed through an automated 3D\nscene graph generation pipeline and a QA generation pipeline. The benchmark\nsystematically evaluates VLMs' performance in both spatial understanding and\nreasoning across multiple dimensions. Using this benchmark, we conduct\nextensive experiments on diverse VLMs, including both general and\nspatial-enhanced models, providing the first comprehensive evaluation of their\nspatial capabilities in autonomous driving. Surprisingly, the experimental\nresults show that the spatial-enhanced VLM outperforms in qualitative QA but\ndoes not demonstrate competitiveness in quantitative QA. In general, VLMs still\nface considerable challenges in spatial understanding and reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03151", "pdf": "https://arxiv.org/pdf/2504.03151", "abs": "https://arxiv.org/abs/2504.03151", "authors": ["Jing Bi", "Susan Liang", "Xiaofei Zhou", "Pinxin Liu", "Junjia Guo", "Yunlong Tang", "Luchuan Song", "Chao Huang", "Guangyu Sun", "Jinxi He", "Jiarui Wu", "Shu Yang", "Daoan Zhang", "Chen Chen", "Lianggong Bruce Wen", "Zhang Liu", "Jiebo Luo", "Chenliang Xu"], "title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reasoning is central to human intelligence, enabling structured\nproblem-solving across diverse tasks. Recent advances in large language models\n(LLMs) have greatly enhanced their reasoning abilities in arithmetic,\ncommonsense, and symbolic domains. However, effectively extending these\ncapabilities into multimodal contexts-where models must integrate both visual\nand textual inputs-continues to be a significant challenge. Multimodal\nreasoning introduces complexities, such as handling conflicting information\nacross modalities, which require models to adopt advanced interpretative\nstrategies. Addressing these challenges involves not only sophisticated\nalgorithms but also robust methodologies for evaluating reasoning accuracy and\ncoherence. This paper offers a concise yet insightful overview of reasoning\ntechniques in both textual and multimodal LLMs. Through a thorough and\nup-to-date comparison, we clearly formulate core reasoning challenges and\nopportunities, highlighting practical methods for post-training optimization\nand test-time inference. Our work provides valuable insights and guidance,\nbridging theoretical frameworks and practical implementations, and sets clear\ndirections for future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03198", "pdf": "https://arxiv.org/pdf/2504.03198", "abs": "https://arxiv.org/abs/2504.03198", "authors": ["Jiaxin Guo", "Wenzhen Dong", "Tianyu Huang", "Hao Ding", "Ziyi Wang", "Haomin Kuang", "Qi Dou", "Yun-Hui Liu"], "title": "Endo3R: Unified Online Reconstruction from Dynamic Monocular Endoscopic Video", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reconstructing 3D scenes from monocular surgical videos can enhance surgeon's\nperception and therefore plays a vital role in various computer-assisted\nsurgery tasks. However, achieving scale-consistent reconstruction remains an\nopen challenge due to inherent issues in endoscopic videos, such as dynamic\ndeformations and textureless surfaces. Despite recent advances, current methods\neither rely on calibration or instrument priors to estimate scale, or employ\nSfM-like multi-stage pipelines, leading to error accumulation and requiring\noffline optimization. In this paper, we present Endo3R, a unified 3D foundation\nmodel for online scale-consistent reconstruction from monocular surgical video,\nwithout any priors or extra optimization. Our model unifies the tasks by\npredicting globally aligned pointmaps, scale-consistent video depths, and\ncamera parameters without any offline optimization. The core contribution of\nour method is expanding the capability of the recent pairwise reconstruction\nmodel to long-term incremental dynamic reconstruction by an uncertainty-aware\ndual memory mechanism. The mechanism maintains history tokens of both\nshort-term dynamics and long-term spatial consistency. Notably, to tackle the\nhighly dynamic nature of surgical scenes, we measure the uncertainty of tokens\nvia Sampson distance and filter out tokens with high uncertainty. Regarding the\nscarcity of endoscopic datasets with ground-truth depth and camera poses, we\nfurther devise a self-supervised mechanism with a novel dynamics-aware flow\nloss. Abundant experiments on SCARED and Hamlyn datasets demonstrate our\nsuperior performance in zero-shot surgical video depth prediction and camera\npose estimation with online efficiency. Project page:\nhttps://wrld.github.io/Endo3R/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03235", "pdf": "https://arxiv.org/pdf/2504.03235", "abs": "https://arxiv.org/abs/2504.03235", "authors": ["Ibne Farabi Shihab", "Anuj Sharma"], "title": "Crash Time Matters: HybridMamba for Fine-Grained Temporal Localization in Traffic Surveillance Footage", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traffic crash detection in long-form surveillance videos is critical for\nemergency response and infrastructure planning but remains difficult due to the\nbrief and rare nature of crash events. We introduce HybridMamba, a novel\narchitecture that combines visual transformers with state-space temporal\nmodeling to achieve accurate crash time localization. Our method uses\nmulti-level token compression and hierarchical temporal processing to remain\ncomputationally efficient without sacrificing temporal resolution. Evaluated on\na large-scale dataset from the Iowa Department of Transportation, HybridMamba\nachieves a mean absolute error of 1.50 seconds, with 65.2 percent of\npredictions within one second of the ground truth. It outperforms recent\nvideo-language models such as TimeChat and VideoLLaMA2 by up to 2.8 seconds,\nwhile using significantly fewer parameters. Our results demonstrate strong\ngeneralization across videos ranging from 2 to 40 minutes in diverse\nconditions. HybridMamba offers a robust and efficient solution for fine-grained\ntemporal localization in traffic surveillance. The code will be released upon\npublication.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03234", "pdf": "https://arxiv.org/pdf/2504.03234", "abs": "https://arxiv.org/abs/2504.03234", "authors": ["Junjie Yang", "Ke Lin", "Xing Yu"], "title": "Think When You Need: Self-Adaptive Chain-of-Thought Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages", "summary": "Chain of Thought (CoT) reasoning enhances language models' performance but\noften leads to inefficient \"overthinking\" on simple problems. We identify that\nexisting approaches directly penalizing reasoning length fail to account for\nvarying problem complexity. Our approach constructs rewards through length and\nquality comparisons, guided by theoretical assumptions that jointly enhance\nsolution correctness with conciseness. Moreover, we further demonstrate our\nmethod to fuzzy tasks where ground truth is unavailable. Experiments across\nmultiple reasoning benchmarks demonstrate that our method maintains accuracy\nwhile generating significantly more concise explanations, effectively teaching\nmodels to \"think when needed.\"", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03254", "pdf": "https://arxiv.org/pdf/2504.03254", "abs": "https://arxiv.org/abs/2504.03254", "authors": ["Yimin Wei", "Aoran Xiao", "Yexian Ren", "Yuting Zhu", "Hongruixuan Chen", "Junshi Xia", "Naoto Yokoya"], "title": "SARLANG-1M: A Benchmark for Vision-Language Modeling in SAR Image Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic Aperture Radar (SAR) is a crucial remote sensing technology,\nenabling all-weather, day-and-night observation with strong surface penetration\nfor precise and continuous environmental monitoring and analysis. However, SAR\nimage interpretation remains challenging due to its complex physical imaging\nmechanisms and significant visual disparities from human perception. Recently,\nVision-Language Models (VLMs) have demonstrated remarkable success in RGB image\nunderstanding, offering powerful open-vocabulary interpretation and flexible\nlanguage interaction. However, their application to SAR images is severely\nconstrained by the absence of SAR-specific knowledge in their training\ndistributions, leading to suboptimal performance. To address this limitation,\nwe introduce SARLANG-1M, a large-scale benchmark tailored for multimodal SAR\nimage understanding, with a primary focus on integrating SAR with textual\nmodality. SARLANG-1M comprises more than 1 million high-quality SAR image-text\npairs collected from over 59 cities worldwide. It features hierarchical\nresolutions (ranging from 0.1 to 25 meters), fine-grained semantic descriptions\n(including both concise and detailed captions), diverse remote sensing\ncategories (1,696 object types and 16 land cover classes), and multi-task\nquestion-answering pairs spanning seven applications and 1,012 question types.\nExtensive experiments on mainstream VLMs demonstrate that fine-tuning with\nSARLANG-1M significantly enhances their performance in SAR image\ninterpretation, reaching performance comparable to human experts. The dataset\nand code will be made publicly available at\nhttps://github.com/Jimmyxichen/SARLANG-1M.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03546", "pdf": "https://arxiv.org/pdf/2504.03546", "abs": "https://arxiv.org/abs/2504.03546", "authors": ["Khai Le-Duc", "Tuyen Tran", "Bach Phan Tat", "Nguyen Kim Hai Bui", "Quan Dang", "Hung-Phong Tran", "Thanh-Thuy Nguyen", "Ly Nguyen", "Tuan-Minh Phan", "Thi Thu Phuong Tran", "Chris Ngo", "Nguyen X. Khanh", "Thanh Nguyen-Tang"], "title": "MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Preprint, 122 pages", "summary": "Multilingual speech translation (ST) in the medical domain enhances patient\ncare by enabling efficient communication across language barriers, alleviating\nspecialized workforce shortages, and facilitating improved diagnosis and\ntreatment, particularly during pandemics. In this work, we present the first\nsystematic study on medical ST, to our best knowledge, by releasing\nMultiMed-ST, a large-scale ST dataset for the medical domain, spanning all\ntranslation directions in five languages: Vietnamese, English, German, French,\nTraditional Chinese and Simplified Chinese, together with the models. With\n290,000 samples, our dataset is the largest medical machine translation (MT)\ndataset and the largest many-to-many multilingual ST among all domains.\nSecondly, we present the most extensive analysis study in ST research to date,\nincluding: empirical baselines, bilingual-multilingual comparative study,\nend-to-end vs. cascaded comparative study, task-specific vs. multi-task\nsequence-to-sequence (seq2seq) comparative study, code-switch analysis, and\nquantitative-qualitative error analysis. All code, data, and models are\navailable online: https://github.com/leduckhai/MultiMed-ST.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03442", "pdf": "https://arxiv.org/pdf/2504.03442", "abs": "https://arxiv.org/abs/2504.03442", "authors": ["Nasar Iqbal", "Niki Martinel"], "title": "Pyramid-based Mamba Multi-class Unsupervised Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in convolutional neural networks (CNNs) and transformer-based\nmethods have improved anomaly detection and localization, but challenges\npersist in precisely localizing small anomalies. While CNNs face limitations in\ncapturing long-range dependencies, transformer architectures often suffer from\nsubstantial computational overheads. We introduce a state space model\n(SSM)-based Pyramidal Scanning Strategy (PSS) for multi-class anomaly detection\nand localization--a novel approach designed to address the challenge of small\nanomaly localization. Our method captures fine-grained details at multiple\nscales by integrating the PSS with a pre-trained encoder for multi-scale\nfeature extraction and a feature-level synthetic anomaly generator. An\nimprovement of $+1\\%$ AP for multi-class anomaly localization and a +$1\\%$\nincrease in AU-PRO on MVTec benchmark demonstrate our method's superiority in\nprecise anomaly localization across diverse industrial scenarios. The code is\navailable at https://github.com/iqbalmlpuniud/Pyramid Mamba.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03468", "pdf": "https://arxiv.org/pdf/2504.03468", "abs": "https://arxiv.org/abs/2504.03468", "authors": ["Antoine Dumoulin", "Adnane Boukhayma", "Laurence Boissieux", "Bharath Bhushan Damodaran", "Pierre Hellier", "Stefanie Wuhrer"], "title": "D-Garment: Physics-Conditioned Latent Diffusion for Dynamic Garment Deformations", "categories": ["cs.CV"], "comment": "11 pages, 7 figures", "summary": "Adjusting and deforming 3D garments to body shapes, body motion, and cloth\nmaterial is an important problem in virtual and augmented reality. Applications\nare numerous, ranging from virtual change rooms to the entertainment and gaming\nindustry. This problem is challenging as garment dynamics influence geometric\ndetails such as wrinkling patterns, which depend on physical input including\nthe wearer's body shape and motion, as well as cloth material features.\nExisting work studies learning-based modeling techniques to generate garment\ndeformations from example data, and physics-inspired simulators to generate\nrealistic garment dynamics. We propose here a learning-based approach trained\non data generated with a physics-based simulator. Compared to prior work, our\n3D generative model learns garment deformations for loose cloth geometry,\nespecially for large deformations and dynamic wrinkles driven by body motion\nand cloth material. Furthermore, the model can be efficiently fitted to\nobservations captured using vision sensors. We propose to leverage the\ncapability of diffusion models to learn fine-scale detail: we model the 3D\ngarment in a 2D parameter space, and learn a latent diffusion model using this\nrepresentation independent from the mesh resolution. This allows to condition\nglobal and local geometric information with body and material information. We\nquantitatively and qualitatively evaluate our method on both simulated data and\ndata captured with a multi-view acquisition platform. Compared to strong\nbaselines, our method is more accurate in terms of Chamfer distance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03612", "pdf": "https://arxiv.org/pdf/2504.03612", "abs": "https://arxiv.org/abs/2504.03612", "authors": ["Bingxiang He", "Wenbin Zhang", "Jiaxi Song", "Cheng Qian", "Zixuan Fu", "Bowen Sun", "Ning Ding", "Haiwen Hong", "Longtao Huang", "Hui Xue", "Ganqu Cui", "Wanxiang Che", "Zhiyuan Liu", "Maosong Sun"], "title": "AIR: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset", "categories": ["cs.CL"], "comment": "29 pages, 11 figures", "summary": "Preference learning is critical for aligning large language models (LLMs)\nwith human values, yet its success hinges on high-quality datasets comprising\nthree core components: Preference \\textbf{A}nnotations, \\textbf{I}nstructions,\nand \\textbf{R}esponse Pairs. Current approaches conflate these components,\nobscuring their individual impacts and hindering systematic optimization. In\nthis work, we propose \\textbf{AIR}, a component-wise analysis framework that\nsystematically isolates and optimizes each component while evaluating their\nsynergistic effects. Through rigorous experimentation, AIR reveals actionable\nprinciples: annotation simplicity (point-wise generative scoring), instruction\ninference stability (variance-based filtering across LLMs), and response pair\nquality (moderate margins + high absolute scores). When combined, these\nprinciples yield +5.3 average gains over baseline method, even with only 14k\nhigh-quality pairs. Our work shifts preference dataset design from ad hoc\nscaling to component-aware optimization, offering a blueprint for efficient,\nreproducible alignment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "annotation"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03501", "pdf": "https://arxiv.org/pdf/2504.03501", "abs": "https://arxiv.org/abs/2504.03501", "authors": ["Ilan Naiman", "Emanuel Ben-Baruch", "Oron Anschel", "Alon Shoshan", "Igor Kviatkovsky", "Manoj Aggarwal", "Gerard Medioni"], "title": "LV-MAE: Learning Long Video Representations through Masked-Embedding Autoencoders", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we introduce long-video masked-embedding autoencoders (LV-MAE),\na self-supervised learning framework for long video representation. Our\napproach treats short- and long-span dependencies as two separate tasks. Such\ndecoupling allows for a more intuitive video processing where short-span\nspatiotemporal primitives are first encoded and are then used to capture\nlong-range dependencies across consecutive video segments. To achieve this, we\nleverage advanced off-the-shelf multimodal encoders to extract representations\nfrom short segments within the long video, followed by pre-training a\nmasked-embedding autoencoder capturing high-level interactions across segments.\nLV-MAE is highly efficient to train and enables the processing of much longer\nvideos by alleviating the constraint on the number of input frames.\nFurthermore, unlike existing methods that typically pre-train on short-video\ndatasets, our approach offers self-supervised pre-training using long video\nsamples (e.g., 20+ minutes video clips) at scale. Using LV-MAE representations,\nwe achieve state-of-the-art results on three long-video benchmarks -- LVU,\nCOIN, and Breakfast -- employing only a simple classification head for either\nattentive or linear probing. Finally, to assess LV-MAE pre-training and\nvisualize its reconstruction quality, we leverage the video-language aligned\nspace of short video representations to monitor LV-MAE through video-text\nretrieval.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03524", "pdf": "https://arxiv.org/pdf/2504.03524", "abs": "https://arxiv.org/abs/2504.03524", "authors": ["Gianluca Monaci", "Rafael S. Rezende", "Romain Deffayet", "Gabriela Csurka", "Guillaume Bono", "Hervé Déjean", "Stéphane Clinchant", "Christian Wolf"], "title": "RANa: Retrieval-Augmented Navigation", "categories": ["cs.CV", "cs.IR", "cs.RO"], "comment": null, "summary": "Methods for navigation based on large-scale learning typically treat each\nepisode as a new problem, where the agent is spawned with a clean memory in an\nunknown environment. While these generalization capabilities to an unknown\nenvironment are extremely important, we claim that, in a realistic setting, an\nagent should have the capacity of exploiting information collected during\nearlier robot operations. We address this by introducing a new\nretrieval-augmented agent, trained with RL, capable of querying a database\ncollected from previous episodes in the same environment and learning how to\nintegrate this additional context information. We introduce a unique agent\narchitecture for the general navigation task, evaluated on ObjectNav, ImageNav\nand Instance-ImageNav. Our retrieval and context encoding methods are\ndata-driven and heavily employ vision foundation models (FM) for both semantic\nand geometric understanding. We propose new benchmarks for these settings and\nwe show that retrieval allows zero-shot transfer across tasks and environments\nwhile significantly improving performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03563", "pdf": "https://arxiv.org/pdf/2504.03563", "abs": "https://arxiv.org/abs/2504.03563", "authors": ["Kaidong Li", "Tianxiao Zhang", "Kuan-Chuan Peng", "Guanghui Wang"], "title": "PF3Det: A Prompted Foundation Feature Assisted Visual LiDAR 3D Detector", "categories": ["cs.CV"], "comment": "This paper is accepted to the CVPR 2025 Workshop on Distillation of\n  Foundation Models for Autonomous Driving (WDFM-AD)", "summary": "3D object detection is crucial for autonomous driving, leveraging both LiDAR\npoint clouds for precise depth information and camera images for rich semantic\ninformation. Therefore, the multi-modal methods that combine both modalities\noffer more robust detection results. However, efficiently fusing LiDAR points\nand images remains challenging due to the domain gaps. In addition, the\nperformance of many models is limited by the amount of high quality labeled\ndata, which is expensive to create. The recent advances in foundation models,\nwhich use large-scale pre-training on different modalities, enable better\nmulti-modal fusion. Combining the prompt engineering techniques for efficient\ntraining, we propose the Prompted Foundational 3D Detector (PF3Det), which\nintegrates foundation model encoders and soft prompts to enhance LiDAR-camera\nfeature fusion. PF3Det achieves the state-of-the-art results under limited\ntraining data, improving NDS by 1.19% and mAP by 2.42% on the nuScenes dataset,\ndemonstrating its efficiency in 3D detection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02922", "pdf": "https://arxiv.org/pdf/2504.02922", "abs": "https://arxiv.org/abs/2504.02922", "authors": ["Julian Minder", "Clement Dumas", "Caden Juang", "Bilal Chugtai", "Neel Nanda"], "title": "Robustly identifying concepts introduced during chat fine-tuning using crosscoders", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "47 pages, 27 figures", "summary": "Model diffing is the study of how fine-tuning changes a model's\nrepresentations and internal algorithms. Many behaviours of interest are\nintroduced during fine-tuning, and model diffing offers a promising lens to\ninterpret such behaviors. Crosscoders are a recent model diffing method that\nlearns a shared dictionary of interpretable concepts represented as latent\ndirections in both the base and fine-tuned models, allowing us to track how\nconcepts shift or emerge during fine-tuning. Notably, prior work has observed\nconcepts with no direction in the base model, and it was hypothesized that\nthese model-specific latents were concepts introduced during fine-tuning.\nHowever, we identify two issues which stem from the crosscoders L1 training\nloss that can misattribute concepts as unique to the fine-tuned model, when\nthey really exist in both models. We develop Latent Scaling to flag these\nissues by more accurately measuring each latent's presence across models. In\nexperiments comparing Gemma 2 2B base and chat models, we observe that the\nstandard crosscoder suffers heavily from these issues. Building on these\ninsights, we train a crosscoder with BatchTopK loss and show that it\nsubstantially mitigates these issues, finding more genuinely chat-specific and\nhighly interpretable concepts. We recommend practitioners adopt similar\ntechniques. Using the BatchTopK crosscoder, we successfully identify a set of\ngenuinely chat-specific latents that are both interpretable and causally\neffective, representing concepts such as $\\textit{false information}$ and\n$\\textit{personal question}$, along with multiple refusal-related latents that\nshow nuanced preferences for different refusal triggers. Overall, our work\nadvances best practices for the crosscoder-based methodology for model diffing\nand demonstrates that it can provide concrete insights into how chat tuning\nmodifies language model behavior.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03137", "pdf": "https://arxiv.org/pdf/2504.03137", "abs": "https://arxiv.org/abs/2504.03137", "authors": ["Tu Ao", "Yanhua Yu", "Yuling Wang", "Yang Deng", "Zirui Guo", "Liang Pang", "Pinghui Wang", "Tat-Seng Chua", "Xiao Zhang", "Zhen Cai"], "title": "LightPROF: A Lightweight Reasoning Framework for Large Language Model on Knowledge Graph", "categories": ["cs.AI", "cs.CL"], "comment": "This paper has been accepted by AAAI 2025", "summary": "Large Language Models (LLMs) have impressive capabilities in text\nunderstanding and zero-shot reasoning. However, delays in knowledge updates may\ncause them to reason incorrectly or produce harmful results. Knowledge Graphs\n(KGs) provide rich and reliable contextual information for the reasoning\nprocess of LLMs by structurally organizing and connecting a wide range of\nentities and relations. Existing KG-based LLM reasoning methods only inject\nKGs' knowledge into prompts in a textual form, ignoring its structural\ninformation. Moreover, they mostly rely on close-source models or open-source\nmodels with large parameters, which poses challenges to high resource\nconsumption. To address this, we propose a novel Lightweight and efficient\nPrompt learning-ReasOning Framework for KGQA (LightPROF), which leverages the\nfull potential of LLMs to tackle complex reasoning tasks in a\nparameter-efficient manner. Specifically, LightPROF follows a\n\"Retrieve-Embed-Reason process\", first accurately, and stably retrieving the\ncorresponding reasoning graph from the KG through retrieval module. Next,\nthrough a Transformer-based Knowledge Adapter, it finely extracts and\nintegrates factual and structural information from the KG, then maps this\ninformation to the LLM's token embedding space, creating an LLM-friendly prompt\nto be used by the LLM for the final reasoning. Additionally, LightPROF only\nrequires training Knowledge Adapter and can be compatible with any open-source\nLLM. Extensive experiments on two public KGQA benchmarks demonstrate that\nLightPROF achieves superior performance with small-scale LLMs. Furthermore,\nLightPROF shows significant advantages in terms of input token count and\nreasoning time.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03160", "pdf": "https://arxiv.org/pdf/2504.03160", "abs": "https://arxiv.org/abs/2504.03160", "authors": ["Yuxiang Zheng", "Dayuan Fu", "Xiangkun Hu", "Xiaojie Cai", "Lyumanshan Ye", "Pengrui Lu", "Pengfei Liu"], "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["honesty"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02880", "pdf": "https://arxiv.org/pdf/2504.02880", "abs": "https://arxiv.org/abs/2504.02880", "authors": ["Junchi Zhou", "Haozhou Wang", "Yoichiro Kato", "Tejasri Nampally", "P. Rajalakshmi", "M. Balram", "Keisuke Katsura", "Hao Lu", "Yue Mu", "Wanneng Yang", "Yangmingrui Gao", "Feng Xiao", "Hongtao Chen", "Yuhao Chen", "Wenjuan Li", "Jingwen Wang", "Fenghua Yu", "Jian Zhou", "Wensheng Wang", "Xiaochun Hu", "Yuanzhu Yang", "Yanfeng Ding", "Wei Guo", "Shouyang Liu"], "title": "Global Rice Multi-Class Segmentation Dataset (RiceSEG): A Comprehensive and Diverse High-Resolution RGB-Annotated Images for the Development and Benchmarking of Rice Segmentation Algorithms", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Developing computer vision-based rice phenotyping techniques is crucial for\nprecision field management and accelerating breeding, thereby continuously\nadvancing rice production. Among phenotyping tasks, distinguishing image\ncomponents is a key prerequisite for characterizing plant growth and\ndevelopment at the organ scale, enabling deeper insights into eco-physiological\nprocesses. However, due to the fine structure of rice organs and complex\nillumination within the canopy, this task remains highly challenging,\nunderscoring the need for a high-quality training dataset. Such datasets are\nscarce, both due to a lack of large, representative collections of rice field\nimages and the time-intensive nature of annotation. To address this gap, we\nestablished the first comprehensive multi-class rice semantic segmentation\ndataset, RiceSEG. We gathered nearly 50,000 high-resolution, ground-based\nimages from five major rice-growing countries (China, Japan, India, the\nPhilippines, and Tanzania), encompassing over 6,000 genotypes across all growth\nstages. From these original images, 3,078 representative samples were selected\nand annotated with six classes (background, green vegetation, senescent\nvegetation, panicle, weeds, and duckweed) to form the RiceSEG dataset. Notably,\nthe sub-dataset from China spans all major genotypes and rice-growing\nenvironments from the northeast to the south. Both state-of-the-art\nconvolutional neural networks and transformer-based semantic segmentation\nmodels were used as baselines. While these models perform reasonably well in\nsegmenting background and green vegetation, they face difficulties during the\nreproductive stage, when canopy structures are more complex and multiple\nclasses are involved. These findings highlight the importance of our dataset\nfor developing specialized segmentation models for rice and other crops.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03238", "pdf": "https://arxiv.org/pdf/2504.03238", "abs": "https://arxiv.org/abs/2504.03238", "authors": ["Akis Nousias", "Efklidis Katsaros", "Evangelos Syrmos", "Panagiotis Radoglou-Grammatikis", "Thomas Lagkas", "Vasileios Argyriou", "Ioannis Moscholios", "Evangelos Markakis", "Sotirios Goudos", "Panagiotis Sarigiannidis"], "title": "Malware Detection in Docker Containers: An Image is Worth a Thousand Logs", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": "Accepted at ICC-W", "summary": "Malware detection is increasingly challenged by evolving techniques like\nobfuscation and polymorphism, limiting the effectiveness of traditional\nmethods. Meanwhile, the widespread adoption of software containers has\nintroduced new security challenges, including the growing threat of malicious\nsoftware injection, where a container, once compromised, can serve as entry\npoint for further cyberattacks. In this work, we address these security issues\nby introducing a method to identify compromised containers through machine\nlearning analysis of their file systems. We cast the entire software containers\ninto large RGB images via their tarball representations, and propose to use\nestablished Convolutional Neural Network architectures on a streaming,\npatch-based manner. To support our experiments, we release the COSOCO\ndataset--the first of its kind--containing 3364 large-scale RGB images of\nbenign and compromised software containers at\nhttps://huggingface.co/datasets/k3ylabs/cosoco-image-dataset. Our method\ndetects more malware and achieves higher F1 and Recall scores than all\nindividual and ensembles of VirusTotal engines, demonstrating its effectiveness\nand setting a new standard for identifying malware-compromised software\ncontainers.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03600", "pdf": "https://arxiv.org/pdf/2504.03600", "abs": "https://arxiv.org/abs/2504.03600", "authors": ["Jun Ma", "Zongxin Yang", "Sumin Kim", "Bihui Chen", "Mohammed Baharoon", "Adibvafa Fallahpour", "Reza Asakereh", "Hongwei Lyu", "Bo Wang"], "title": "MedSAM2: Segment Anything in 3D Medical Images and Videos", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "https://medsam2.github.io/", "summary": "Medical image and video segmentation is a critical task for precision\nmedicine, which has witnessed considerable progress in developing task or\nmodality-specific and generalist models for 2D images. However, there have been\nlimited studies on building general-purpose models for 3D images and videos\nwith comprehensive user studies. Here, we present MedSAM2, a promptable\nsegmentation foundation model for 3D image and video segmentation. The model is\ndeveloped by fine-tuning the Segment Anything Model 2 on a large medical\ndataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming\nprevious models across a wide range of organs, lesions, and imaging modalities.\nFurthermore, we implement a human-in-the-loop pipeline to facilitate the\ncreation of large-scale datasets resulting in, to the best of our knowledge,\nthe most extensive user study to date, involving the annotation of 5,000 CT\nlesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames,\ndemonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is\nalso integrated into widely used platforms with user-friendly interfaces for\nlocal and cloud deployment, making it a practical tool for supporting\nefficient, scalable, and high-quality segmentation in both research and\nhealthcare environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
