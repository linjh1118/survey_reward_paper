{"id": "2504.04718", "pdf": "https://arxiv.org/pdf/2504.04718", "abs": "https://arxiv.org/abs/2504.04718", "authors": ["Minki Kang", "Jongwon Jeong", "Jaewoong Cho"], "title": "T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent studies have demonstrated that test-time compute scaling effectively\nimproves the performance of small language models (sLMs). However, prior\nresearch has mainly examined test-time compute scaling with an additional\nlarger model as a verifier, leaving self-verification by sLMs underexplored. In\nthis work, we investigate whether sLMs can reliably self-verify their outputs\nunder test-time scaling. We find that even with knowledge distillation from\nlarger verifiers, sLMs struggle with verification tasks requiring memorization,\nsuch as numerical calculations and fact-checking. To address this limitation,\nwe propose Tool-integrated self-verification (T1), which delegates\nmemorization-heavy verification steps to external tools, such as a code\ninterpreter. Our theoretical analysis shows that tool integration reduces\nmemorization demands and improves test-time scaling performance. Experiments on\nthe MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under\ntest-time scaling outperforms the significantly larger Llama-3.1 8B model.\nMoreover, T1 generalizes effectively to both mathematical (MATH500) and\nmulti-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the\npotential of tool integration to substantially improve the self-verification\nabilities of sLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "compute scaling", "test-time compute", "self-verification"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03790", "pdf": "https://arxiv.org/pdf/2504.03790", "abs": "https://arxiv.org/abs/2504.03790", "authors": ["Gon√ßalo Faria", "Noah A. Smith"], "title": "Sample, Don't Search: Rethinking Test-Time Alignment for Language Models", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Increasing test-time computation has emerged as a promising direction for\nimproving language model performance, particularly in scenarios where model\nfinetuning is impractical or impossible due to computational constraints or\nprivate model weights. However, existing test-time search methods using a\nreward model (RM) often degrade in quality as compute scales, due to the\nover-optimization of what are inherently imperfect reward proxies. We introduce\nQAlign, a new test-time alignment approach. As we scale test-time compute,\nQAlign converges to sampling from the optimal aligned distribution for each\nindividual prompt. By adopting recent advances in Markov chain Monte Carlo for\ntext generation, our method enables better-aligned outputs without modifying\nthe underlying model or even requiring logit access. We demonstrate the\neffectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and\nGSM-Symbolic) using a task-specific RM, showing consistent improvements over\nexisting test-time compute methods like best-of-n and majority voting.\nFurthermore, when applied with more realistic RMs trained on the Tulu 3\npreference dataset, QAlign outperforms direct preference optimization (DPO),\nbest-of-n, majority voting, and weighted majority voting on a diverse range of\ndatasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical\nsolution to aligning language models at test time using additional computation\nwithout degradation, our approach expands the limits of the capability that can\nbe obtained from off-the-shelf language models without further training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time", "scale", "test-time compute"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "alignment", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03846", "pdf": "https://arxiv.org/pdf/2504.03846", "abs": "https://arxiv.org/abs/2504.03846", "authors": ["Wei-Lin Chen", "Zhepei Wei", "Xinyu Zhu", "Shi Feng", "Yu Meng"], "title": "Do LLM Evaluators Prefer Themselves for a Reason?", "categories": ["cs.CL"], "comment": "Preprint. 31 pages", "summary": "Large language models (LLMs) are increasingly used as automatic evaluators in\napplications such as benchmarking, reward modeling, and self-refinement. Prior\nwork highlights a potential self-preference bias where LLMs favor their own\ngenerated responses, a tendency often intensifying with model size and\ncapability. This raises a critical question: Is self-preference detrimental, or\ndoes it simply reflect objectively superior outputs from more capable models?\nDisentangling these has been challenging due to the usage of subjective tasks\nin previous studies. To address this, we investigate self-preference using\nverifiable benchmarks (mathematical reasoning, factual knowledge, code\ngeneration) that allow objective ground-truth assessment. This enables us to\ndistinguish harmful self-preference (favoring objectively worse responses) from\nlegitimate self-preference (favoring genuinely superior ones). We conduct\nlarge-scale experiments under controlled evaluation conditions across diverse\nmodel families (e.g., Llama, Qwen, Gemma, Mistral, Phi, GPT, DeepSeek). Our\nfindings reveal three key insights: (1) Better generators are better judges --\nLLM evaluators' accuracy strongly correlates with their task performance, and\nmuch of the self-preference in capable models is legitimate. (2) Harmful\nself-preference persists, particularly when evaluator models perform poorly as\ngenerators on specific task instances. Stronger models exhibit more pronounced\nharmful bias when they err, though such incorrect generations are less\nfrequent. (3) Inference-time scaling strategies, such as generating a long\nChain-of-Thought before evaluation, effectively reduce the harmful\nself-preference. These results provide a more nuanced understanding of\nLLM-based evaluation and practical insights for improving its reliability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "scale"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy", "mathematical reasoning"], "score": 4}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04083", "pdf": "https://arxiv.org/pdf/2504.04083", "abs": "https://arxiv.org/abs/2504.04083", "authors": ["Aviv Brokman", "Xuguang Ai", "Yuhang Jiang", "Shashank Gupta", "Ramakanth Kavuluru"], "title": "A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs: Experiments with OpenAI Models", "categories": ["cs.CL"], "comment": null, "summary": "Objective: Zero-shot methodology promises to cut down on costs of dataset\nannotation and domain expertise needed to make use of NLP. Generative large\nlanguage models trained to align with human goals have achieved high zero-shot\nperformance across a wide variety of tasks. As of yet, it is unclear how well\nthese models perform on biomedical relation extraction (RE). To address this\nknowledge gap, we explore patterns in the performance of OpenAI LLMs across a\ndiverse sampling of RE tasks.\n  Methods: We use OpenAI GPT-4-turbo and their reasoning model o1 to conduct\nend-to-end RE experiments on seven datasets. We use the JSON generation\ncapabilities of GPT models to generate structured output in two ways: (1) by\ndefining an explicit schema describing the structure of relations, and (2)\nusing a setting that infers the structure from the prompt language.\n  Results: Our work is the first to study and compare the performance of the\nGPT-4 and o1 for the end-to-end zero-shot biomedical RE task across a broad\narray of datasets. We found the zero-shot performances to be proximal to that\nof fine-tuned methods. The limitations of this approach are that it performs\npoorly on instances containing many relations and errs on the boundaries of\ntextual mentions.\n  Conclusion: Recent large language models exhibit promising zero-shot\ncapabilities in complex biomedical RE tasks, offering competitive performance\nwith reduced dataset curation and NLP modeling needs at the cost of increased\ncomputing, potentially increasing medical community accessibility. Addressing\nthe limitations we identify could further boost reliability. The code, data,\nand prompts for all our experiments are publicly available:\nhttps://github.com/bionlproc/ZeroShotRE", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1", "reasoning model"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "reliability"], "score": 4}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04342", "pdf": "https://arxiv.org/pdf/2504.04342", "abs": "https://arxiv.org/abs/2504.04342", "authors": ["Ayan Sengupta", "Siddhant Chaudhary", "Tanmoy Chakraborty"], "title": "Compression Laws for Large Language Models", "categories": ["cs.CL"], "comment": "16 pages, 11 figures, 6 tables", "summary": "We introduce compression laws for language language models (LLMs). While\nrecent scaling laws have sought to understand how LLMs scale with respect to\nmodel size, pre-training data, and computational resources, we focus on\nunderstanding how model compression affects the performance of a pre-trained\nLLM on downstream tasks. We empirically examine the effects of structured model\ncompression on LLMs through over $1000$ experiments across eight models with\nsizes ranging from $0.5B$ to $14B$ parameters. Our findings indicate that the\ntest cross-entropy loss increases quadratically with the compression ratio,\nwhereas performance on downstream tasks declines only linearly. Our study\nemphasizes the importance of recovery fine-tuning in enhancing generation loss,\nshowing that the test loss of compressed LLMs can improve by up to 55% with\nrecovery fine-tuning. At higher compression ratios (up to 90%), compressed LLMs\ndemonstrate a speed increase of 60% during inference compared to their\nuncompressed counterparts, compensating for the performance degradation at this\nlevel. However, for smaller models ($\\le 7B$), the computational gains are\nlimited, peaking at just 35%. We conclude that model compression can be highly\nbeneficial for larger models, especially when a smaller model within the same\ncomputational budget is not available. These insights provide the practical\nguidelines for utilizing model compression techniques for adopting LLMs in\nreal-life applications in resource-constrained settings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05097", "pdf": "https://arxiv.org/pdf/2504.05097", "abs": "https://arxiv.org/abs/2504.05097", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "State Tuning: State-based Test-Time Scaling on RWKV-7", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04981", "pdf": "https://arxiv.org/pdf/2504.04981", "abs": "https://arxiv.org/abs/2504.04981", "authors": ["Sohyun Lee", "Nayeong Kim", "Juwon Kang", "Seong Joon Oh", "Suha Kwak"], "title": "DiCoTTA: Domain-invariant Learning for Continual Test-time Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper studies continual test-time adaptation (CTTA), the task of\nadapting a model to constantly changing unseen domains in testing while\npreserving previously learned knowledge. Existing CTTA methods mostly focus on\nadaptation to the current test domain only, overlooking generalization to\narbitrary test domains a model may face in the future. To tackle this\nlimitation, we present a novel online domain-invariant learning framework for\nCTTA, dubbed DiCoTTA. DiCoTTA aims to learn feature representation to be\ninvariant to both current and previous test domains on the fly during testing.\nTo this end, we propose a new model architecture and a test-time adaptation\nstrategy dedicated to learning domain-invariant features without corrupting\nsemantic contents, along with a new data structure and optimization algorithm\nfor effectively managing information from previous test domains. DiCoTTA\nachieved state-of-the-art performance on four public CTTA benchmarks. Moreover,\nit showed superior generalization to unseen test domains.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05258", "pdf": "https://arxiv.org/pdf/2504.05258", "abs": "https://arxiv.org/abs/2504.05258", "authors": ["Adri√°n Bazaga", "Rexhina Blloshmi", "Bill Byrne", "Adri√† de Gispert"], "title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05298", "pdf": "https://arxiv.org/pdf/2504.05298", "abs": "https://arxiv.org/abs/2504.05298", "authors": ["Karan Dalal", "Daniel Koceja", "Gashon Hussein", "Jiarui Xu", "Yue Zhao", "Youjin Song", "Shihao Han", "Ka Chun Cheung", "Jan Kautz", "Carlos Guestrin", "Tatsunori Hashimoto", "Sanmi Koyejo", "Yejin Choi", "Yu Sun", "Xiaolong Wang"], "title": "One-Minute Video Generation with Test-Time Training", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Transformers today still struggle to generate one-minute videos because\nself-attention layers are inefficient for long context. Alternatives such as\nMamba layers struggle with complex multi-scene stories because their hidden\nstates are less expressive. We experiment with Test-Time Training (TTT) layers,\nwhose hidden states themselves can be neural networks, therefore more\nexpressive. Adding TTT layers into a pre-trained Transformer enables it to\ngenerate one-minute videos from text storyboards. For proof of concept, we\ncurate a dataset based on Tom and Jerry cartoons. Compared to baselines such as\nMamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers\ngenerate much more coherent videos that tell complex stories, leading by 34 Elo\npoints in a human evaluation of 100 videos per method. Although promising,\nresults still contain artifacts, likely due to the limited capability of the\npre-trained 5B model. The efficiency of our implementation can also be\nimproved. We have only experimented with one-minute videos due to resource\nconstraints, but the approach can be extended to longer videos and more complex\nstories. Sample videos, code and annotations are available at:\nhttps://test-time-training.github.io/video-dit", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time training"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03739", "pdf": "https://arxiv.org/pdf/2504.03739", "abs": "https://arxiv.org/abs/2504.03739", "authors": ["Mingyan Liu"], "title": "A Unified Virtual Mixture-of-Experts Framework:Enhanced Inference and Hallucination Mitigation in Single-Model System", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative models, such as GPT and BERT, have significantly improved\nperformance in tasks like text generation and summarization. However,\nhallucinations \"where models generate non-factual or misleading content\" are\nespecially problematic in smaller-scale architectures, limiting their\nreal-world applicability.In this paper, we propose a unified Virtual\nMixture-of-Experts (MoE) fusion strategy that enhances inference performance\nand mitigates hallucinations in a single Qwen 1.5 0.5B model without increasing\nthe parameter count. Our method leverages multiple domain-specific expert\nprompts (with the number of experts being adjustable) to guide the model from\ndifferent perspectives. We apply a statistical outlier truncation strategy\nbased on the mean and standard deviation to filter out abnormally high\nprobability predictions, and we inject noise into the embedding space to\npromote output diversity. To clearly assess the contribution of each module, we\nadopt a fixed voting mechanism rather than a dynamic gating network, thereby\navoiding additional confounding factors. We provide detailed theoretical\nderivations from both statistical and ensemble learning perspectives to\ndemonstrate how our method reduces output variance and suppresses\nhallucinations. Extensive ablation experiments on dialogue generation tasks\nshow that our approach significantly improves inference accuracy and robustness\nin small models. Additionally, we discuss methods for evaluating the\northogonality of virtual experts and outline the potential for future work\ninvolving dynamic expert weight allocation using gating networks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "summarization", "dialogue"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03794", "pdf": "https://arxiv.org/pdf/2504.03794", "abs": "https://arxiv.org/abs/2504.03794", "authors": ["Liangwei Yang", "Yuhui Xu", "Juntao Tan", "Doyen Sahoo", "Silvio Savarese", "Caiming Xiong", "Huan Wang", "Shelby Heinecke"], "title": "Entropy-Based Block Pruning for Efficient Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures", "summary": "As large language models continue to scale, their growing computational and\nstorage demands pose significant challenges for real-world deployment. In this\nwork, we investigate redundancy within Transformer-based models and propose an\nentropy-based pruning strategy to enhance efficiency while maintaining\nperformance. Empirical analysis reveals that the entropy of hidden\nrepresentations decreases in the early blocks but progressively increases\nacross most subsequent blocks. This trend suggests that entropy serves as a\nmore effective measure of information richness within computation blocks.\nUnlike cosine similarity, which primarily captures geometric relationships,\nentropy directly quantifies uncertainty and information content, making it a\nmore reliable criterion for pruning. Extensive experiments demonstrate that our\nentropy-based pruning approach surpasses cosine similarity-based methods in\nreducing model size while preserving accuracy, offering a promising direction\nfor efficient model deployment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03931", "pdf": "https://arxiv.org/pdf/2504.03931", "abs": "https://arxiv.org/abs/2504.03931", "authors": ["Zixuan Ke", "Yifei Ming", "Shafiq Joty"], "title": "Adaptation of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Tutorial Proposal for NAACL2025", "summary": "This tutorial on adaptation of LLMs is designed to address the growing demand\nfor models that go beyond the static capabilities of generic LLMs by providing\nan overview of dynamic, domain-specific, and task-adaptive LLM adaptation\ntechniques. While general LLMs have demonstrated strong generalization across a\nvariety of tasks, they often struggle to perform well in specialized domains\nsuch as finance, healthcare, and code generation for underrepresented\nlanguages. Additionally, their static nature limits their ability to evolve\nwith the changing world, and they are often extremely large in size, making\nthem impractical and costly to deploy at scale. As a result, the adaptation of\nLLMs has drawn much attention since the birth of LLMs and is of core\nimportance, both for industry, which focuses on serving its targeted users, and\nacademia, which can greatly benefit from small but powerful LLMs. To address\nthis gap, this tutorial aims to provide an overview of the LLM adaptation\ntechniques. We start with an introduction to LLM adaptation, from both the data\nperspective and the model perspective. We then emphasize how the evaluation\nmetrics and benchmarks are different from other techniques. After establishing\nthe problems, we explore various adaptation techniques. We categorize\nadaptation techniques into two main families. The first is parametric knowledge\nadaptation, which focuses on updating the parametric knowledge within LLMs.\nAdditionally, we will discuss real-time adaptation techniques, including model\nediting, which allows LLMs to be updated dynamically in production\nenvironments. The second kind of adaptation is semi-parametric knowledge\nadaptation, where the goal is to update LLM parameters to better leverage\nexternal knowledge or tools through techniques like retrieval-augmented\ngeneration (RAG) and agent-based systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "code generation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03821", "pdf": "https://arxiv.org/pdf/2504.03821", "abs": "https://arxiv.org/abs/2504.03821", "authors": ["Andrew Kiruluta", "Andreas Lemos"], "title": "A Hybrid Wavelet-Fourier Method for Next-Generation Conditional Diffusion Models", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "We present a novel generative modeling framework,Wavelet-Fourier-Diffusion,\nwhich adapts the diffusion paradigm to hybrid frequency representations in\norder to synthesize high-quality, high-fidelity images with improved spatial\nlocalization. In contrast to conventional diffusion models that rely\nexclusively on additive noise in pixel space, our approach leverages a\nmulti-transform that combines wavelet sub-band decomposition with partial\nFourier steps. This strategy progressively degrades and then reconstructs\nimages in a hybrid spectral domain during the forward and reverse diffusion\nprocesses. By supplementing traditional Fourier-based analysis with the spatial\nlocalization capabilities of wavelets, our model can capture both global\nstructures and fine-grained features more effectively. We further extend the\napproach to conditional image generation by integrating embeddings or\nconditional features via cross-attention. Experimental evaluations on CIFAR-10,\nCelebA-HQ, and a conditional ImageNet subset illustrate that our method\nachieves competitive or superior performance relative to baseline diffusion\nmodels and state-of-the-art GANs, as measured by Fr\\'echet Inception Distance\n(FID) and Inception Score (IS). We also show how the hybrid frequency-based\nrepresentation improves control over global coherence and fine texture\nsynthesis, paving the way for new directions in multi-scale generative\nmodeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03964", "pdf": "https://arxiv.org/pdf/2504.03964", "abs": "https://arxiv.org/abs/2504.03964", "authors": ["Simon A. Lee", "Anthony Wu", "Jeffrey N. Chiang"], "title": "Clinical ModernBERT: An efficient and long context encoder for biomedical text", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Manuscript writeup corresponding to the Clinical ModernBERT\n  pre-trained encoder (https://huggingface.co/Simonlee711/Clinical_ModernBERT)", "summary": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on\nlarge scale biomedical literature, clinical notes, and medical ontologies,\nincorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with\ntheir textual descriptions. Building on ModernBERT the current state of the art\nnatural language text encoder featuring architectural upgrades such as rotary\npositional embeddings (RoPE), Flash Attention, and extended context length up\nto 8,192 tokens our model adapts these innovations specifically for biomedical\nand clinical domains. Clinical ModernBERT excels at producing semantically rich\nrepresentations tailored for long context tasks. We validate this both by\nanalyzing its pretrained weights and through empirical evaluation on a\ncomprehensive suite of clinical NLP benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03991", "pdf": "https://arxiv.org/pdf/2504.03991", "abs": "https://arxiv.org/abs/2504.03991", "authors": ["Siddharth Srikanth", "Varun Bhatt", "Boshen Zhang", "Werner Hager", "Charles Michael Lewis", "Katia P. Sycara", "Aaquib Tabrez", "Stefanos Nikolaidis"], "title": "Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "Understanding how humans collaborate and communicate in teams is essential\nfor improving human-agent teaming and AI-assisted decision-making. However,\nrelying solely on data from large-scale user studies is impractical due to\nlogistical, ethical, and practical constraints, necessitating synthetic models\nof multiple diverse human behaviors. Recently, agents powered by Large Language\nModels (LLMs) have been shown to emulate human-like behavior in social\nsettings. But, obtaining a large set of diverse behaviors requires manual\neffort in the form of designing prompts. On the other hand, Quality Diversity\n(QD) optimization has been shown to be capable of generating diverse\nReinforcement Learning (RL) agent behavior. In this work, we combine QD\noptimization with LLM-powered agents to iteratively search for prompts that\ngenerate diverse team behavior in a long-horizon, multi-step collaborative\nenvironment. We first show, through a human-subjects experiment (n=54\nparticipants), that humans exhibit diverse coordination and communication\nbehavior in this domain. We then show that our approach can effectively\nreplicate trends from human teaming data and also capture behaviors that are\nnot easily observed without collecting large amounts of data. Our findings\nhighlight the combination of QD and LLM-powered agents as an effective tool for\nstudying teaming and communication strategies in multi-agent collaboration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04022", "pdf": "https://arxiv.org/pdf/2504.04022", "abs": "https://arxiv.org/abs/2504.04022", "authors": ["Essential AI", ":", "Darsh J Shah", "Peter Rushton", "Somanshu Singla", "Mohit Parmar", "Kurt Smith", "Yash Vanjani", "Ashish Vaswani", "Adarsh Chaluvaraju", "Andrew Hojel", "Andrew Ma", "Anil Thomas", "Anthony Polloreno", "Ashish Tanwer", "Burhan Drak Sibai", "Divya S Mansingka", "Divya Shivaprasad", "Ishaan Shah", "Karl Stratos", "Khoi Nguyen", "Michael Callahan", "Michael Pust", "Mrinal Iyer", "Philip Monk", "Platon Mazarakis", "Ritvik Kapila", "Saurabh Srivastava", "Tim Romanski"], "title": "Rethinking Reflection in Pre-Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A language model's ability to reflect on its own reasoning provides a key\nadvantage for solving complex problems. While most recent research has focused\non how this ability develops during reinforcement learning, we show that it\nactually begins to emerge much earlier - during the model's pre-training. To\nstudy this, we introduce deliberate errors into chains-of-thought and test\nwhether the model can still arrive at the correct answer by recognizing and\ncorrecting these mistakes. By tracking performance across different stages of\npre-training, we observe that this self-correcting ability appears early and\nimproves steadily over time. For instance, an OLMo2-7B model pre-trained on 4\ntrillion tokens displays self-correction on our six self-reflection tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03894", "pdf": "https://arxiv.org/pdf/2504.03894", "abs": "https://arxiv.org/abs/2504.03894", "authors": ["Haiqing Li", "Yuzhi Guo", "Feng Jiang", "Qifeng Zhou", "Hehuan Ma", "Junzhou Huang"], "title": "Leveraging Gait Patterns as Biomarkers: An attention-guided Deep Multiple Instance Learning Network for Scoliosis Classification", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 3 figures", "summary": "Scoliosis is a spinal curvature disorder that is difficult to detect early\nand can compress the chest cavity, impacting respiratory function and cardiac\nhealth. Especially for adolescents, delayed detection and treatment result in\nworsening compression. Traditional scoliosis detection methods heavily rely on\nclinical expertise, and X-ray imaging poses radiation risks, limiting\nlarge-scale early screening. We propose an Attention-Guided Deep Multi-Instance\nLearning method (Gait-MIL) to effectively capture discriminative features from\ngait patterns, which is inspired by ScoNet-MT's pioneering use of gait patterns\nfor scoliosis detection. We evaluate our method on the first large-scale\ndataset based on gait patterns for scoliosis classification. The results\ndemonstrate that our study improves the performance of using gait as a\nbiomarker for scoliosis detection, significantly enhances detection accuracy\nfor the particularly challenging Neutral cases, where subtle indicators are\noften overlooked. Our Gait-MIL also performs robustly in imbalanced scenarios,\nmaking it a promising tool for large-scale scoliosis screening.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04131", "pdf": "https://arxiv.org/pdf/2504.04131", "abs": "https://arxiv.org/abs/2504.04131", "authors": ["Michael J Bommarito", "Daniel Martin Katz", "Jillian Bommarito"], "title": "Precise Legal Sentence Boundary Detection for Retrieval at Scale: NUPunkt and CharBoundary", "categories": ["cs.CL"], "comment": "12 pages, 5 figures, 6 tables", "summary": "We present NUPunkt and CharBoundary, two sentence boundary detection\nlibraries optimized for high-precision, high-throughput processing of legal\ntext in large-scale applications such as due diligence, e-discovery, and legal\nresearch. These libraries address the critical challenges posed by legal\ndocuments containing specialized citations, abbreviations, and complex sentence\nstructures that confound general-purpose sentence boundary detectors.\n  Our experimental evaluation on five diverse legal datasets comprising over\n25,000 documents and 197,000 annotated sentence boundaries demonstrates that\nNUPunkt achieves 91.1% precision while processing 10 million characters per\nsecond with modest memory requirements (432 MB). CharBoundary models offer\nbalanced and adjustable precision-recall tradeoffs, with the large model\nachieving the highest F1 score (0.782) among all tested methods.\n  Notably, NUPunkt provides a 29-32% precision improvement over general-purpose\ntools while maintaining exceptional throughput, processing multi-million\ndocument collections in minutes rather than hours. Both libraries run\nefficiently on standard CPU hardware without requiring specialized\naccelerators. NUPunkt is implemented in pure Python with zero external\ndependencies, while CharBoundary relies only on scikit-learn and optional ONNX\nruntime integration for optimized performance. Both libraries are available\nunder the MIT license, can be installed via PyPI, and can be interactively\ntested at https://sentences.aleainstitute.ai/.\n  These libraries address critical precision issues in retrieval-augmented\ngeneration systems by preserving coherent legal concepts across sentences,\nwhere each percentage improvement in precision yields exponentially greater\nreductions in context fragmentation, creating cascading benefits throughout\nretrieval pipelines and significantly enhancing downstream reasoning quality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04012", "pdf": "https://arxiv.org/pdf/2504.04012", "abs": "https://arxiv.org/abs/2504.04012", "authors": ["Houzhang Fang", "Xiaolin Wang", "Zengyang Li", "Lu Wang", "Qingshan Li", "Yi Chang", "Luxin Yan"], "title": "Detection-Friendly Nonuniformity Correction: A Union Framework for Infrared UAVTarget Detection", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by CVPR2025", "summary": "Infrared unmanned aerial vehicle (UAV) images captured using thermal\ndetectors are often affected by temperature dependent low-frequency\nnonuniformity, which significantly reduces the contrast of the images.\nDetecting UAV targets under nonuniform conditions is crucial in UAV\nsurveillance applications. Existing methods typically treat infrared\nnonuniformity correction (NUC) as a preprocessing step for detection, which\nleads to suboptimal performance. Balancing the two tasks while enhancing\ndetection beneficial information remains challenging. In this paper, we present\na detection-friendly union framework, termed UniCD, that simultaneously\naddresses both infrared NUC and UAV target detection tasks in an end-to-end\nmanner. We first model NUC as a small number of parameter estimation problem\njointly driven by priors and data to generate detection-conducive images. Then,\nwe incorporate a new auxiliary loss with target mask supervision into the\nbackbone of the infrared UAV target detection network to strengthen target\nfeatures while suppressing the background. To better balance correction and\ndetection, we introduce a detection-guided self-supervised loss to reduce\nfeature discrepancies between the two tasks, thereby enhancing detection\nrobustness to varying nonuniformity levels. Additionally, we construct a new\nbenchmark composed of 50,000 infrared images in various nonuniformity types,\nmulti-scale UAV targets and rich backgrounds with target annotations, called\nIRBFD. Extensive experiments on IRBFD demonstrate that our UniCD is a robust\nunion framework for NUC and UAV target detection while achieving real-time\nprocessing capabilities. Dataset can be available at\nhttps://github.com/IVPLaboratory/UniCD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04025", "pdf": "https://arxiv.org/pdf/2504.04025", "abs": "https://arxiv.org/abs/2504.04025", "authors": ["Daniel Rivera", "Jacob Huddin", "Alexander Banerjee", "Rongzhen Zhang", "Brenda Mai", "Hanadi El Achi", "Jacob Armstrong", "Amer Wahed", "Andy Nguyen"], "title": "Artificial intelligence application in lymphoma diagnosis: from Convolutional Neural Network to Vision Transformer", "categories": ["cs.CV", "cs.LG"], "comment": "14 pages, 6 figures, 1 table", "summary": "Recently, vision transformers were shown to be capable of outperforming\nconvolutional neural networks when pretrained on sufficiently large datasets.\nVision transformer models show good accuracy on large scale datasets, with\nfeatures of multi-modal training. Due to their promising feature detection, we\naim to explore vision transformer models for diagnosis of anaplastic large cell\nlymphoma versus classical Hodgkin lymphoma using pathology whole slide images\nof HE slides. We compared the classification performance of the vision\ntransformer to our previously designed convolutional neural network on the same\ndataset. The dataset includes whole slide images of HE slides for 20 cases,\nincluding 10 cases in each diagnostic category. From each whole slide image, 60\nimage patches having size of 100 by 100 pixels and at magnification of 20 were\nobtained to yield 1200 image patches, from which 90 percent were used for\ntraining, 9 percent for validation, and 10 percent for testing. The test\nresults from the convolutional neural network model had previously shown an\nexcellent diagnostic accuracy of 100 percent. The test results from the vision\ntransformer model also showed a comparable accuracy at 100 percent. To the best\nof the authors' knowledge, this is the first direct comparison of predictive\nperformance between a vision transformer model and a convolutional neural\nnetwork model using the same dataset of lymphoma. Overall, convolutional neural\nnetwork has a more mature architecture than vision transformer and is usually\nthe best choice when large scale pretraining is not an available option.\nNevertheless, our current study shows comparable and excellent accuracy of\nvision transformer compared to that of convolutional neural network even with a\nrelatively small dataset of anaplastic large cell lymphoma and classical\nHodgkin lymphoma.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04034", "pdf": "https://arxiv.org/pdf/2504.04034", "abs": "https://arxiv.org/abs/2504.04034", "authors": ["Dianshuo Li", "Li Chen", "Yunxiang Cao", "Kai Zhu", "Jun Cheng"], "title": "UCS: A Universal Model for Curvilinear Structure Segmentation", "categories": ["cs.CV"], "comment": "11 pages, 9 figures", "summary": "Curvilinear structure segmentation (CSS) is vital in various domains,\nincluding medical imaging, landscape analysis, industrial surface inspection,\nand plant analysis. While existing methods achieve high performance within\nspecific domains, their generalizability is limited. On the other hand,\nlarge-scale models such as Segment Anything Model (SAM) exhibit strong\ngeneralization but are not optimized for curvilinear structures. Existing\nadaptations of SAM primarily focus on general object segmentation and lack\nspecialized design for CSS tasks. To bridge this gap, we propose the Universal\nCurvilinear structure Segmentation (\\textit{UCS}) model, which adapts SAM to\nCSS tasks while enhancing its generalization. \\textit{UCS} features a novel\nencoder architecture integrating a pretrained SAM encoder with two innovations:\na Sparse Adapter, strategically inserted to inherit the pre-trained SAM\nencoder's generalization capability while minimizing the number of fine-tuning\nparameters, and a Prompt Generation module, which leverages Fast Fourier\nTransform with a high-pass filter to generate curve-specific prompts.\nFurthermore, the \\textit{UCS} incorporates a mask decoder that eliminates\nreliance on manual interaction through a dual-compression module: a\nHierarchical Feature Compression module, which aggregates the outputs of the\nsampled encoder to enhance detail preservation, and a Guidance Feature\nCompression module, which extracts and compresses image-driven guidance\nfeatures. Evaluated on a comprehensive multi-domain dataset, including an\nin-house dataset covering eight natural curvilinear structures, \\textit{UCS}\ndemonstrates state-of-the-art generalization and open-set segmentation\nperformance across medical, engineering, natural, and plant imagery,\nestablishing a new benchmark for universal CSS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04045", "pdf": "https://arxiv.org/pdf/2504.04045", "abs": "https://arxiv.org/abs/2504.04045", "authors": ["Conghao Xiong", "Hao Chen", "Joseph J. Y. Sung"], "title": "A Survey of Pathology Foundation Model: Progress and Future Directions", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Computational pathology, analyzing whole slide images for automated cancer\ndiagnosis, relies on the multiple instance learning framework where performance\nheavily depends on the feature extractor and aggregator. Recent Pathology\nFoundation Models (PFMs), pretrained on large-scale histopathology data, have\nsignificantly enhanced capabilities of extractors and aggregators but lack\nsystematic analysis frameworks. This survey presents a hierarchical taxonomy\norganizing PFMs through a top-down philosophy that can be utilized to analyze\nFMs in any domain: model scope, model pretraining, and model design.\nAdditionally, we systematically categorize PFM evaluation tasks into\nslide-level, patch-level, multimodal, and biological tasks, providing\ncomprehensive benchmarking criteria. Our analysis identifies critical\nchallenges in both PFM development (pathology-specific methodology, end-to-end\npretraining, data-model scalability) and utilization (effective adaptation,\nmodel maintenance), paving the way for future directions in this promising\nfield. Resources referenced in this survey are available at\nhttps://github.com/BearCleverProud/AwesomeWSI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "criteria"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04124", "pdf": "https://arxiv.org/pdf/2504.04124", "abs": "https://arxiv.org/abs/2504.04124", "authors": ["Muhammad Ahmed Ullah Khan", "Abdul Hannan Khan", "Andreas Dengel"], "title": "EMF: Event Meta Formers for Event-based Real-time Traffic Object Detection", "categories": ["cs.CV"], "comment": "10 pages, 2 figures", "summary": "Event cameras have higher temporal resolution, and require less storage and\nbandwidth compared to traditional RGB cameras. However, due to relatively\nlagging performance of event-based approaches, event cameras have not yet\nreplace traditional cameras in performance-critical applications like\nautonomous driving. Recent approaches in event-based object detection try to\nbridge this gap by employing computationally expensive transformer-based\nsolutions. However, due to their resource-intensive components, these solutions\nfail to exploit the sparsity and higher temporal resolution of event cameras\nefficiently. Moreover, these solutions are adopted from the vision domain,\nlacking specificity to the event cameras. In this work, we explore efficient\nand performant alternatives to recurrent vision transformer models and propose\na novel event-based object detection backbone. The proposed backbone employs a\nnovel Event Progression Extractor module, tailored specifically for event data,\nand uses Metaformer concept with convolution-based efficient components. We\nevaluate the resultant model on well-established traffic object detection\nbenchmarks and conduct cross-dataset evaluation to test its ability to\ngeneralize. The proposed model outperforms the state-of-the-art on Prophesee\nGen1 dataset by 1.6 mAP while reducing inference time by 14%. Our proposed EMF\nbecomes the fastest DNN-based architecture in the domain by outperforming most\nefficient event-based object detectors. Moreover, the proposed model shows\nbetter ability to generalize to unseen data and scales better with the\nabundance of data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04130", "pdf": "https://arxiv.org/pdf/2504.04130", "abs": "https://arxiv.org/abs/2504.04130", "authors": ["Andrei-Alexandru Preda", "Iulian-Marius TƒÉiatu", "Dumitru-Clementin Cercel"], "title": "Scaling Federated Learning Solutions with Kubernetes for Synthesizing Histopathology Images", "categories": ["cs.CV"], "comment": null, "summary": "In the field of deep learning, large architectures often obtain the best\nperformance for many tasks, but also require massive datasets. In the\nhistological domain, tissue images are expensive to obtain and constitute\nsensitive medical information, raising concerns about data scarcity and\nprivacy. Vision Transformers are state-of-the-art computer vision models that\nhave proven helpful in many tasks, including image classification. In this\nwork, we combine vision Transformers with generative adversarial networks to\ngenerate histopathological images related to colorectal cancer and test their\nquality by augmenting a training dataset, leading to improved classification\naccuracy. Then, we replicate this performance using the federated learning\ntechnique and a realistic Kubernetes setup with multiple nodes, simulating a\nscenario where the training dataset is split among several hospitals unable to\nshare their information directly due to privacy concerns.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04158", "pdf": "https://arxiv.org/pdf/2504.04158", "abs": "https://arxiv.org/abs/2504.04158", "authors": ["Yunlong Lin", "Zixu Lin", "Haoyu Chen", "Panwang Pan", "Chenxin Li", "Sixiang Chen", "Yeying Jin", "Wenbo Li", "Xinghao Ding"], "title": "JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration", "categories": ["cs.CV"], "comment": "25 pages, 15 figures", "summary": "Vision-centric perception systems struggle with unpredictable and coupled\nweather degradations in the wild. Current solutions are often limited, as they\neither depend on specific degradation priors or suffer from significant domain\ngaps. To enable robust and autonomous operation in real-world conditions, we\npropose JarvisIR, a VLM-powered agent that leverages the VLM as a controller to\nmanage multiple expert restoration models. To further enhance system\nrobustness, reduce hallucinations, and improve generalizability in real-world\nadverse weather, JarvisIR employs a novel two-stage framework consisting of\nsupervised fine-tuning and human feedback alignment. Specifically, to address\nthe lack of paired data in real-world scenarios, the human feedback alignment\nenables the VLM to be fine-tuned effectively on large-scale real-world data in\nan unsupervised manner. To support the training and evaluation of JarvisIR, we\nintroduce CleanBench, a comprehensive dataset consisting of high-quality and\nlarge-scale instruction-responses pairs, including 150K synthetic entries and\n80K real entries. Extensive experiments demonstrate that JarvisIR exhibits\nsuperior decision-making and restoration capabilities. Compared with existing\nmethods, it achieves a 50% improvement in the average of all perception metrics\non CleanBench-Real. Project page: https://cvpr2025-jarvisir.github.io/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04185", "pdf": "https://arxiv.org/pdf/2504.04185", "abs": "https://arxiv.org/abs/2504.04185", "authors": ["Dong Liu", "Yuanchao Wu", "Bowen Tong", "Jiansong Deng"], "title": "SDEIT: Semantic-Driven Electrical Impedance Tomography", "categories": ["cs.CV"], "comment": null, "summary": "Regularization methods using prior knowledge are essential in solving\nill-posed inverse problems such as Electrical Impedance Tomography (EIT).\nHowever, designing effective regularization and integrating prior information\ninto EIT remains challenging due to the complexity and variability of\nanatomical structures. In this work, we introduce SDEIT, a novel\nsemantic-driven framework that integrates Stable Diffusion 3.5 into EIT,\nmarking the first use of large-scale text-to-image generation models in EIT.\nSDEIT employs natural language prompts as semantic priors to guide the\nreconstruction process. By coupling an implicit neural representation (INR)\nnetwork with a plug-and-play optimization scheme that leverages SD-generated\nimages as generative priors, SDEIT improves structural consistency and recovers\nfine details. Importantly, this method does not rely on paired training\ndatasets, increasing its adaptability to varied EIT scenarios. Extensive\nexperiments on both simulated and experimental data demonstrate that SDEIT\noutperforms state-of-the-art techniques, offering superior accuracy and\nrobustness. This work opens a new pathway for integrating multimodal priors\ninto ill-posed inverse problems like EIT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04271", "pdf": "https://arxiv.org/pdf/2504.04271", "abs": "https://arxiv.org/abs/2504.04271", "authors": ["Mete Ahishali", "Anis Ur Rahman", "Einari Heinaro", "Samuli Junttila"], "title": "ADA-Net: Attention-Guided Domain Adaptation Network with Contrastive Learning for Standing Dead Tree Segmentation Using Aerial Imagery", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Information on standing dead trees is important for understanding forest\necosystem functioning and resilience but has been lacking over large geographic\nregions. Climate change has caused large-scale tree mortality events that can\nremain undetected due to limited data. In this study, we propose a novel method\nfor segmenting standing dead trees using aerial multispectral orthoimages.\nBecause access to annotated datasets has been a significant problem in forest\nremote sensing due to the need for forest expertise, we introduce a method for\ndomain transfer by leveraging domain adaptation to learn a transformation from\na source domain X to target domain Y. In this Image-to-Image translation task,\nwe aim to utilize available annotations in the target domain by pre-training a\nsegmentation network. When images from a new study site without annotations are\nintroduced (source domain X), these images are transformed into the target\ndomain. Then, transfer learning is applied by inferring the pre-trained network\non domain-adapted images. In addition to investigating the feasibility of\ncurrent domain adaptation approaches for this objective, we propose a novel\napproach called the Attention-guided Domain Adaptation Network (ADA-Net) with\nenhanced contrastive learning. Accordingly, the ADA-Net approach provides new\nstate-of-the-art domain adaptation performance levels outperforming existing\napproaches. We have evaluated the proposed approach using two datasets from\nFinland and the US. The USA images are converted to the Finland domain, and we\nshow that the synthetic USA2Finland dataset exhibits similar characteristics to\nthe Finland domain images. The software implementation is shared at\nhttps://github.com/meteahishali/ADA-Net. The data is publicly available at\nhttps://www.kaggle.com/datasets/meteahishali/aerial-imagery-for-standing-dead-tree-segmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04348", "pdf": "https://arxiv.org/pdf/2504.04348", "abs": "https://arxiv.org/abs/2504.04348", "authors": ["Shihao Wang", "Zhiding Yu", "Xiaohui Jiang", "Shiyi Lan", "Min Shi", "Nadine Chang", "Jan Kautz", "Ying Li", "Jose M. Alvarez"], "title": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04569", "pdf": "https://arxiv.org/pdf/2504.04569", "abs": "https://arxiv.org/abs/2504.04569", "authors": ["Chitranshu Harbola", "Anupam Purwar"], "title": "KnowsLM: A framework for evaluation of small language models for knowledge augmentation and humanised conversations", "categories": ["cs.CL"], "comment": null, "summary": "In the evolving landscape of conversational AI, generating concise,\ncontext-aware, and human-like dialogue using small and medium-sized language\nmodels (LLMs) remains a complex challenge. This study investigates the\ninfluence of LoRA rank, dataset scale, and prompt prefix design on both\nknowledge retention and stylistic alignment. While fine-tuning improves fluency\nand enables stylistic customization, its ability to integrate unseen knowledge\nis constrained -- particularly with smaller datasets. Conversely, RAG-augmented\nmodels, equipped to incorporate external documents at inference, demonstrated\nsuperior factual accuracy on out-of-distribution prompts, though they lacked\nthe stylistic consistency achieved by fine-tuning. Evaluations by LLM-based\njudges across knowledge accuracy, conversational quality, and conciseness\nsuggest that fine-tuning is best suited for tone adaptation, whereas RAG excels\nat real-time knowledge augmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "accuracy", "dialogue"], "score": 5}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04700", "pdf": "https://arxiv.org/pdf/2504.04700", "abs": "https://arxiv.org/abs/2504.04700", "authors": ["Hyunseo Shin", "Wonseok Hwang"], "title": "Causal Retrieval with Semantic Consideration", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced the performance of conversational AI systems. To extend their\ncapabilities to knowledge-intensive domains such as biomedical and legal\nfields, where the accuracy is critical, LLMs are often combined with\ninformation retrieval (IR) systems to generate responses based on retrieved\ndocuments. However, for IR systems to effectively support such applications,\nthey must go beyond simple semantic matching and accurately capture diverse\nquery intents, including causal relationships. Existing IR models primarily\nfocus on retrieving documents based on surface-level semantic similarity,\noverlooking deeper relational structures such as causality. To address this, we\npropose CAWAI, a retrieval model that is trained with dual objectives: semantic\nand causal relations. Our extensive experiments demonstrate that CAWAI\noutperforms various models on diverse causal retrieval tasks especially under\nlarge-scale retrieval settings. We also show that CAWAI exhibits strong\nzero-shot generalization across scientific domain QA tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04737", "pdf": "https://arxiv.org/pdf/2504.04737", "abs": "https://arxiv.org/abs/2504.04737", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Shivam Mishra", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "title": "TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "In the landscape of Fact-based Judgment Prediction and Explanation (FJPE),\nreliance on factual data is essential for developing robust and realistic\nAI-driven decision-making tools. This paper introduces TathyaNyaya, the largest\nannotated dataset for FJPE tailored to the Indian legal context, encompassing\njudgments from the Supreme Court of India and various High Courts. Derived from\nthe Hindi terms \"Tathya\" (fact) and \"Nyaya\" (justice), the TathyaNyaya dataset\nis uniquely designed to focus on factual statements rather than complete legal\ntexts, reflecting real-world judicial processes where factual data drives\noutcomes. Complementing this dataset, we present FactLegalLlama, an\ninstruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM),\noptimized for generating high-quality explanations in FJPE tasks. Finetuned on\nthe factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy\nwith coherent, contextually relevant explanations, addressing the critical need\nfor transparency and interpretability in AI-assisted legal systems. Our\nmethodology combines transformers for binary judgment prediction with\nFactLegalLlama for explanation generation, creating a robust framework for\nadvancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses\nexisting datasets in scale and diversity but also establishes a benchmark for\nbuilding explainable AI systems in legal analysis. The findings underscore the\nimportance of factual precision and domain-specific tuning in enhancing\npredictive performance and interpretability, positioning TathyaNyaya and\nFactLegalLlama as foundational resources for AI-assisted legal decision-making.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04823", "pdf": "https://arxiv.org/pdf/2504.04823", "abs": "https://arxiv.org/abs/2504.04823", "authors": ["Ruikang Liu", "Yuxuan Sun", "Manyi Zhang", "Haoli Bai", "Xianzhi Yu", "Tiezheng Yu", "Chun Yuan", "Lu Hou"], "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04861", "pdf": "https://arxiv.org/pdf/2504.04861", "abs": "https://arxiv.org/abs/2504.04861", "authors": ["Hongtao Wang", "Renchi Yang", "Hewen Wang", "Haoran Zheng", "Jianliang Xu"], "title": "SAFT: Structure-aware Transformers for Textual Interaction Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Textual interaction networks (TINs) are an omnipresent data structure used to\nmodel the interplay between users and items on e-commerce websites, social\nnetworks, etc., where each interaction is associated with a text description.\nClassifying such textual interactions (TIC) finds extensive use in detecting\nspam reviews in e-commerce, fraudulent transactions in finance, and so on.\nExisting TIC solutions either (i) fail to capture the rich text semantics due\nto the use of context-free text embeddings, and/or (ii) disregard the bipartite\nstructure and node heterogeneity of TINs, leading to compromised TIC\nperformance. In this work, we propose SAFT, a new architecture that integrates\nlanguage- and graph-based modules for the effective fusion of textual and\nstructural semantics in the representation learning of interactions. In\nparticular, line graph attention (LGA)/gated attention units (GAUs) and\npretrained language models (PLMs) are capitalized on to model the\ninteraction-level and token-level signals, which are further coupled via the\nproxy token in an iterative and contextualized fashion. Additionally, an\nefficient and theoretically-grounded approach is developed to encode the local\nand global topology information pertaining to interactions into structural\nembeddings. The resulting embeddings not only inject the structural features\nunderlying TINs into the textual interaction encoding but also facilitate the\ndesign of graph sampling strategies. Extensive empirical evaluations on\nmultiple real TIN datasets demonstrate the superiority of SAFT over the\nstate-of-the-art baselines in TIC accuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04994", "pdf": "https://arxiv.org/pdf/2504.04994", "abs": "https://arxiv.org/abs/2504.04994", "authors": ["Ling Hu", "Yuemei Xu", "Xiaoyang Gu", "Letao Han"], "title": "Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the impressive performance of large language models (LLMs), they can\npresent unintended biases and harmful behaviors driven by encoded values,\nemphasizing the urgent need to understand the value mechanisms behind them.\nHowever, current research primarily evaluates these values through external\nresponses with a focus on AI safety, lacking interpretability and failing to\nassess social values in real-world contexts. In this paper, we propose a novel\nframework called ValueExploration, which aims to explore the behavior-driven\nmechanisms of National Social Values within LLMs at the neuron level. As a case\nstudy, we focus on Chinese Social Values and first construct C-voice, a\nlarge-scale bilingual benchmark for identifying and evaluating Chinese Social\nValues in LLMs. By leveraging C-voice, we then identify and locate the neurons\nresponsible for encoding these values according to activation difference.\nFinally, by deactivating these neurons, we analyze shifts in model behavior,\nuncovering the internal mechanism by which values influence LLM\ndecision-making. Extensive experiments on four representative LLMs validate the\nefficacy of our framework. The benchmark and code will be available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04653", "pdf": "https://arxiv.org/pdf/2504.04653", "abs": "https://arxiv.org/abs/2504.04653", "authors": ["Yimu Wang", "Mozhgan Nasr Azadani", "Sean Sedwards", "Krzysztof Czarnecki"], "title": "LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Redundancy of visual tokens in multi-modal large language models (MLLMs)\nsignificantly reduces their computational efficiency. Recent approaches, such\nas resamplers and summarizers, have sought to reduce the number of visual\ntokens, but at the cost of visual reasoning ability. To address this, we\npropose LEO-MINI, a novel MLLM that significantly reduces the number of visual\ntokens and simultaneously boosts visual reasoning capabilities. For efficiency,\nLEO-MINI incorporates CoTR, a novel token reduction module to consolidate a\nlarge number of visual tokens into a smaller set of tokens, using the\nsimilarity between visual tokens, text tokens, and a compact learnable query.\nFor effectiveness, to scale up the model's ability with minimal computational\noverhead, LEO-MINI employs MMoE, a novel mixture of multi-modal experts module.\nMMOE employs a set of LoRA experts with a novel router to switch between them\nbased on the input text and visual tokens instead of only using the input\nhidden state. MMoE also includes a general LoRA expert that is always activated\nto learn general knowledge for LLM reasoning. For extracting richer visual\nfeatures, MMOE employs a set of vision experts trained on diverse\ndomain-specific data. To demonstrate LEO-MINI's improved efficiency and\nperformance, we evaluate it against existing efficient MLLMs on various\nbenchmark vision-language tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04687", "pdf": "https://arxiv.org/pdf/2504.04687", "abs": "https://arxiv.org/abs/2504.04687", "authors": ["Yicheng Leng", "Chaowei Fang", "Junye Chen", "Yixiang Fang", "Sheng Li", "Guanbin Li"], "title": "Bridging Knowledge Gap Between Image Inpainting and Large-Area Visible Watermark Removal", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV", "I.2.10; I.4.4; I.4.5"], "comment": "To be published in AAAI 2025", "summary": "Visible watermark removal which involves watermark cleaning and background\ncontent restoration is pivotal to evaluate the resilience of watermarks.\nExisting deep neural network (DNN)-based models still struggle with large-area\nwatermarks and are overly dependent on the quality of watermark mask\nprediction. To overcome these challenges, we introduce a novel feature adapting\nframework that leverages the representation modeling capacity of a pre-trained\nimage inpainting model. Our approach bridges the knowledge gap between image\ninpainting and watermark removal by fusing information of the residual\nbackground content beneath watermarks into the inpainting backbone model. We\nestablish a dual-branch system to capture and embed features from the residual\nbackground content, which are merged into intermediate features of the\ninpainting backbone model via gated feature fusion modules. Moreover, for\nrelieving the dependence on high-quality watermark masks, we introduce a new\ntraining paradigm by utilizing coarse watermark masks to guide the inference\nprocess. This contributes to a visible image removal model which is insensitive\nto the quality of watermark mask during testing. Extensive experiments on both\na large-scale synthesized dataset and a real-world dataset demonstrate that our\napproach significantly outperforms existing state-of-the-art methods. The\nsource code is available in the supplementary materials.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04708", "pdf": "https://arxiv.org/pdf/2504.04708", "abs": "https://arxiv.org/abs/2504.04708", "authors": ["Minchul Kim", "Dingqiang Ye", "Yiyang Su", "Feng Liu", "Xiaoming Liu"], "title": "SapiensID: Foundation for Human Recognition", "categories": ["cs.CV"], "comment": "To appear in CVPR2025", "summary": "Existing human recognition systems often rely on separate, specialized models\nfor face and body analysis, limiting their effectiveness in real-world\nscenarios where pose, visibility, and context vary widely. This paper\nintroduces SapiensID, a unified model that bridges this gap, achieving robust\nperformance across diverse settings. SapiensID introduces (i) Retina Patch\n(RP), a dynamic patch generation scheme that adapts to subject scale and\nensures consistent tokenization of regions of interest, (ii) a masked\nrecognition model (MRM) that learns from variable token length, and (iii)\nSemantic Attention Head (SAH), an module that learns pose-invariant\nrepresentations by pooling features around key body parts. To facilitate\ntraining, we introduce WebBody4M, a large-scale dataset capturing diverse poses\nand scale variations. Extensive experiments demonstrate that SapiensID achieves\nstate-of-the-art results on various body ReID benchmarks, outperforming\nspecialized models in both short-term and long-term scenarios while remaining\ncompetitive with dedicated face recognition systems. Furthermore, SapiensID\nestablishes a strong baseline for the newly introduced challenge of Cross\nPose-Scale ReID, demonstrating its ability to generalize to complex, real-world\nconditions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05104", "pdf": "https://arxiv.org/pdf/2504.05104", "abs": "https://arxiv.org/abs/2504.05104", "authors": ["Saeid Ario Vaghefi", "Aymane Hachcham", "Veronica Grasso", "Jiska Manicus", "Nakiete Msemo", "Chiara Colesanti Senni", "Markus Leippold"], "title": "AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments", "categories": ["cs.CL"], "comment": null, "summary": "Tracking financial investments in climate adaptation is a complex and\nexpertise-intensive task, particularly for Early Warning Systems (EWS), which\nlack standardized financial reporting across multilateral development banks\n(MDBs) and funds. To address this challenge, we introduce an LLM-based agentic\nAI system that integrates contextual retrieval, fine-tuning, and multi-step\nreasoning to extract relevant financial data, classify investments, and ensure\ncompliance with funding guidelines. Our study focuses on a real-world\napplication: tracking EWS investments in the Climate Risk and Early Warning\nSystems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple\nAI-driven classification methods, including zero-shot and few-shot learning,\nfine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and\nan agent-based retrieval-augmented generation (RAG) approach. Our results show\nthat the agent-based RAG approach significantly outperforms other methods,\nachieving 87\\% accuracy, 89\\% precision, and 83\\% recall. Additionally, we\ncontribute a benchmark dataset and expert-annotated corpus, providing a\nvaluable resource for future research in AI-driven financial tracking and\nclimate finance transparency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04728", "pdf": "https://arxiv.org/pdf/2504.04728", "abs": "https://arxiv.org/abs/2504.04728", "authors": ["Sheng Zheng", "Chaoning Zhang", "Dongshen Han", "Fachrina Dewi Puspitasari", "Xinhong Hao", "Yang Yang", "Heng Tao Shen"], "title": "Exploring Kernel Transformations for Implicit Neural Representations", "categories": ["cs.CV"], "comment": "Accepted at IEEE Transactions on Multimedia (TMM) on December 20,\n  2024 (To appear on IEEE Website soon)", "summary": "Implicit neural representations (INRs), which leverage neural networks to\nrepresent signals by mapping coordinates to their corresponding attributes,\nhave garnered significant attention. They are extensively utilized for image\nrepresentation, with pixel coordinates as input and pixel values as output. In\ncontrast to prior works focusing on investigating the effect of the model's\ninside components (activation function, for instance), this work pioneers the\nexploration of the effect of kernel transformation of input/output while\nkeeping the model itself unchanged. A byproduct of our findings is a simple yet\neffective method that combines scale and shift to significantly boost INR with\nnegligible computation overhead. Moreover, we present two perspectives, depth\nand normalization, to interpret the performance benefits caused by scale and\nshift transformation. Overall, our work provides a new avenue for future works\nto understand and improve INR through the lens of kernel transformation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05226", "pdf": "https://arxiv.org/pdf/2504.05226", "abs": "https://arxiv.org/abs/2504.05226", "authors": ["Jungyeul Park"], "title": "Proposing TAGbank as a Corpus of Tree-Adjoining Grammar Derivations", "categories": ["cs.CL"], "comment": null, "summary": "The development of lexicalized grammars, particularly Tree-Adjoining Grammar\n(TAG), has significantly advanced our understanding of syntax and semantics in\nnatural language processing (NLP). While existing syntactic resources like the\nPenn Treebank and Universal Dependencies offer extensive annotations for\nphrase-structure and dependency parsing, there is a lack of large-scale corpora\ngrounded in lexicalized grammar formalisms. To address this gap, we introduce\nTAGbank, a corpus of TAG derivations automatically extracted from existing\nsyntactic treebanks. This paper outlines a methodology for mapping\nphrase-structure annotations to TAG derivations, leveraging the generative\npower of TAG to support parsing, grammar induction, and semantic analysis. Our\napproach builds on the work of CCGbank, extending it to incorporate the unique\nstructural properties of TAG, including its transparent derivation trees and\nits ability to capture long-distance dependencies. We also discuss the\nchallenges involved in the extraction process, including ensuring consistency\nacross treebank schemes and dealing with language-specific syntactic\nidiosyncrasies. Finally, we propose the future extension of TAGbank to include\nmultilingual corpora, focusing on the Penn Korean and Penn Chinese Treebanks,\nto explore the cross-linguistic application of TAG's formalism. By providing a\nrobust, derivation-based resource, TAGbank aims to support a wide range of\ncomputational tasks and contribute to the theoretical understanding of TAG's\ngenerative capacity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05262", "pdf": "https://arxiv.org/pdf/2504.05262", "abs": "https://arxiv.org/abs/2504.05262", "authors": ["Yang Yan", "Yu Lu", "Renjun Xu", "Zhenzhong Lan"], "title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n($0$ to $2^{64}$), probing two core properties: commutativity ($A+B=B+A$) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., $7\n\\rightarrow y$). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to $\\leq$7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of $A+B \\neq B+A$) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04781", "pdf": "https://arxiv.org/pdf/2504.04781", "abs": "https://arxiv.org/abs/2504.04781", "authors": ["Chaoyi Wang", "Baoqing Li", "Xinhan Di"], "title": "OCC-MLLM-CoT-Alpha: Towards Multi-stage Occlusion Recognition Based on Large Language Models via 3D-Aware Supervision and Chain-of-Thoughts Guidance", "categories": ["cs.CV", "I.2.10; I.4.8"], "comment": "This work has been accepted to the Multimodal Algorithmic Reasoning\n  (MAR) Workshop at CVPR 2025", "summary": "Comprehending occluded objects are not well studied in existing large-scale\nvisual-language multi-modal models. Current state-of-the-art multi-modal large\nmodels struggles to provide satisfactory results in understanding occluded\nobjects through universal visual encoders and supervised learning strategies.\nTherefore, we propose OCC-MLLM-CoT-Alpha, a multi-modal large vision language\nframework that integrates 3D-aware supervision and Chain-of-Thoughts guidance.\nParticularly, (1) we build a multi-modal large vision-language model framework\nwhich is consisted of a large multi-modal vision-language model and a 3D\nreconstruction expert model. (2) the corresponding multi-modal\nChain-of-Thoughts is learned through a combination of supervised and\nreinforcement training strategies, allowing the multi-modal vision-language\nmodel to enhance the recognition ability with learned multi-modal\nchain-of-thoughts guidance. (3) A large-scale multi-modal chain-of-thoughts\nreasoning dataset, consisting of $110k$ samples of occluded objects held in\nhand, is built. In the evaluation, the proposed methods demonstrate decision\nscore improvement of 15.75%,15.30%,16.98%,14.62%, and 4.42%,3.63%,6.94%,10.70%\nfor two settings of a variety of state-of-the-art models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04834", "pdf": "https://arxiv.org/pdf/2504.04834", "abs": "https://arxiv.org/abs/2504.04834", "authors": ["Pengju Sun", "Banglei Guan", "Zhenbao Yu", "Yang Shang", "Qifeng Yu", "Daniel Barath"], "title": "Learning Affine Correspondences by Integrating Geometric Constraints", "categories": ["cs.CV"], "comment": null, "summary": "Affine correspondences have received significant attention due to their\nbenefits in tasks like image matching and pose estimation. Existing methods for\nextracting affine correspondences still have many limitations in terms of\nperformance; thus, exploring a new paradigm is crucial. In this paper, we\npresent a new pipeline designed for extracting accurate affine correspondences\nby integrating dense matching and geometric constraints. Specifically, a novel\nextraction framework is introduced, with the aid of dense matching and a novel\nkeypoint scale and orientation estimator. For this purpose, we propose loss\nfunctions based on geometric constraints, which can effectively improve\naccuracy by supervising neural networks to learn feature geometry. The\nexperimental show that the accuracy and robustness of our method outperform the\nexisting ones in image matching tasks. To further demonstrate the effectiveness\nof the proposed method, we applied it to relative pose estimation. Affine\ncorrespondences extracted by our method lead to more accurate poses than the\nbaselines on a range of real-world datasets. The code is available at\nhttps://github.com/stilcrad/DenseAffine.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04030", "pdf": "https://arxiv.org/pdf/2504.04030", "abs": "https://arxiv.org/abs/2504.04030", "authors": ["Wasi Uddin Ahmad", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Vahid Noroozi", "Somshubra Majumdar", "Boris Ginsburg"], "title": "OpenCodeInstruct: A Large-scale Instruction Tuning Dataset for Code LLMs", "categories": ["cs.SE", "cs.CL"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) have transformed software development by\nenabling code generation, automated debugging, and complex reasoning. However,\ntheir continued advancement is constrained by the scarcity of high-quality,\npublicly available supervised fine-tuning (SFT) datasets tailored for coding\ntasks. To bridge this gap, we introduce OpenCodeInstruct, the largest\nopen-access instruction tuning dataset, comprising 5 million diverse samples.\nEach sample includes a programming question, solution, test cases, execution\nfeedback, and LLM-generated quality assessments. We fine-tune various base\nmodels, including LLaMA and Qwen, across multiple scales (1B+, 3B+, and 7B+)\nusing our dataset. Comprehensive evaluations on popular benchmarks (HumanEval,\nMBPP, LiveCodeBench, and BigCodeBench) demonstrate substantial performance\nimprovements achieved by SFT with OpenCodeInstruct. We also present a detailed\nmethodology encompassing seed data curation, synthetic instruction and solution\ngeneration, and filtering.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "code generation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04835", "pdf": "https://arxiv.org/pdf/2504.04835", "abs": "https://arxiv.org/abs/2504.04835", "authors": ["Shanshan Wang", "Haixiang Xu", "Hui Feng", "Xiaoqian Wang", "Pei Song", "Sijie Liu", "Jianhua He"], "title": "Inland Waterway Object Detection in Multi-environment: Dataset and Approach", "categories": ["cs.CV"], "comment": "37 pages,11 figures,5 tables", "summary": "The success of deep learning in intelligent ship visual perception relies\nheavily on rich image data. However, dedicated datasets for inland waterway\nvessels remain scarce, limiting the adaptability of visual perception systems\nin complex environments. Inland waterways, characterized by narrow channels,\nvariable weather, and urban interference, pose significant challenges to object\ndetection systems based on existing datasets. To address these issues, this\npaper introduces the Multi-environment Inland Waterway Vessel Dataset (MEIWVD),\ncomprising 32,478 high-quality images from diverse scenarios, including sunny,\nrainy, foggy, and artificial lighting conditions. MEIWVD covers common vessel\ntypes in the Yangtze River Basin, emphasizing diversity, sample independence,\nenvironmental complexity, and multi-scale characteristics, making it a robust\nbenchmark for vessel detection. Leveraging MEIWVD, this paper proposes a\nscene-guided image enhancement module to improve water surface images based on\nenvironmental conditions adaptively. Additionally, a parameter-limited dilated\nconvolution enhances the representation of vessel features, while a multi-scale\ndilated residual fusion method integrates multi-scale features for better\ndetection. Experiments show that MEIWVD provides a more rigorous benchmark for\nobject detection algorithms, and the proposed methods significantly improve\ndetector performance, especially in complex multi-environment scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04308", "pdf": "https://arxiv.org/pdf/2504.04308", "abs": "https://arxiv.org/abs/2504.04308", "authors": ["Yingcong Li", "Davoud Ataee Tarzanagh", "Ankit Singh Rawat", "Maryam Fazel", "Samet Oymak"], "title": "Gating is Weighting: Understanding Gated Linear Attention through In-context Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC"], "comment": null, "summary": "Linear attention methods offer a compelling alternative to softmax attention\ndue to their efficiency in recurrent decoding. Recent research has focused on\nenhancing standard linear attention by incorporating gating while retaining its\ncomputational benefits. Such Gated Linear Attention (GLA) architectures include\ncompetitive models such as Mamba and RWKV. In this work, we investigate the\nin-context learning capabilities of the GLA model and make the following\ncontributions. We show that a multilayer GLA can implement a general class of\nWeighted Preconditioned Gradient Descent (WPGD) algorithms with data-dependent\nweights. These weights are induced by the gating mechanism and the input,\nenabling the model to control the contribution of individual tokens to\nprediction. To further understand the mechanics of this weighting, we introduce\na novel data model with multitask prompts and characterize the optimization\nlandscape of learning a WPGD algorithm. Under mild conditions, we establish the\nexistence and uniqueness (up to scaling) of a global minimum, corresponding to\na unique WPGD solution. Finally, we translate these findings to explore the\noptimization landscape of GLA and shed light on how gating facilitates\ncontext-aware learning and when it is provably better than vanilla linear\nattention.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04383", "pdf": "https://arxiv.org/pdf/2504.04383", "abs": "https://arxiv.org/abs/2504.04383", "authors": ["Ximing Lu", "Seungju Han", "David Acuna", "Hyunwoo Kim", "Jaehun Jung", "Shrimai Prabhumoye", "Niklas Muennighoff", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Yejin Choi"], "title": "Retro-Search: Exploring Untaken Paths for Deeper and Efficient Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Code and data will be publicly released upon internal approval", "summary": "Large reasoning models exhibit remarkable reasoning capabilities via long,\nelaborate reasoning trajectories. Supervised fine-tuning on such reasoning\ntraces, also known as distillation, can be a cost-effective way to boost\nreasoning capabilities of student models. However, empirical observations\nreveal that these reasoning trajectories are often suboptimal, switching\nexcessively between different lines of thought, resulting in under-thinking,\nover-thinking, and even degenerate responses. We introduce Retro-Search, an\nMCTS-inspired search algorithm, for distilling higher quality reasoning paths\nfrom large reasoning models. Retro-Search retrospectively revises reasoning\npaths to discover better, yet shorter traces, which can then lead to student\nmodels with enhanced reasoning capabilities with shorter, thus faster\ninference. Our approach can enable two use cases: self-improvement, where\nmodels are fine-tuned on their own Retro-Search-ed thought traces, and\nweak-to-strong improvement, where a weaker model revises stronger model's\nthought traces via Retro-Search. For self-improving, R1-distill-7B, fine-tuned\non its own Retro-Search-ed traces, reduces the average reasoning length by\n31.2% while improving performance by 7.7% across seven math benchmarks. For\nweak-to-strong improvement, we retrospectively revise R1-671B's traces from the\nOpenThoughts dataset using R1-distill-32B as the Retro-Search-er, a model 20x\nsmaller. Qwen2.5-32B, fine-tuned on this refined data, achieves performance\ncomparable to R1-distill-32B, yielding an 11.3% reduction in reasoning length\nand a 2.4% performance improvement compared to fine-tuning on the original\nOpenThoughts data. Our work counters recently emergent viewpoints that question\nthe relevance of search algorithms in the era of large reasoning models, by\ndemonstrating that there are still opportunities for algorithmic advancements,\neven for frontier models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["MCTS"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04911", "pdf": "https://arxiv.org/pdf/2504.04911", "abs": "https://arxiv.org/abs/2504.04911", "authors": ["Ziyun Liang", "Xiaoqing Guo", "Wentian Xu", "Yasin Ibrahim", "Natalie Voets", "Pieter M Pretorius", "J. Alison Noble", "Konstantinos Kamnitsas"], "title": "IterMask3D: Unsupervised Anomaly Detection and Segmentation with Test-Time Iterative Mask Refinement in 3D Brain MR", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Unsupervised anomaly detection and segmentation methods train a model to\nlearn the training distribution as 'normal'. In the testing phase, they\nidentify patterns that deviate from this normal distribution as 'anomalies'. To\nlearn the `normal' distribution, prevailing methods corrupt the images and\ntrain a model to reconstruct them. During testing, the model attempts to\nreconstruct corrupted inputs based on the learned 'normal' distribution.\nDeviations from this distribution lead to high reconstruction errors, which\nindicate potential anomalies. However, corrupting an input image inevitably\ncauses information loss even in normal regions, leading to suboptimal\nreconstruction and an increased risk of false positives. To alleviate this, we\npropose IterMask3D, an iterative spatial mask-refining strategy designed for 3D\nbrain MRI. We iteratively spatially mask areas of the image as corruption and\nreconstruct them, then shrink the mask based on reconstruction error. This\nprocess iteratively unmasks 'normal' areas to the model, whose information\nfurther guides reconstruction of 'normal' patterns under the mask to be\nreconstructed accurately, reducing false positives. In addition, to achieve\nbetter reconstruction performance, we also propose using high-frequency image\ncontent as additional structural information to guide the reconstruction of the\nmasked area. Extensive experiments on the detection of both synthetic and\nreal-world imaging artifacts, as well as segmentation of various pathological\nlesions across multiple MRI sequences, consistently demonstrate the\neffectiveness of our proposed method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04653", "pdf": "https://arxiv.org/pdf/2504.04653", "abs": "https://arxiv.org/abs/2504.04653", "authors": ["Yimu Wang", "Mozhgan Nasr Azadani", "Sean Sedwards", "Krzysztof Czarnecki"], "title": "LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Redundancy of visual tokens in multi-modal large language models (MLLMs)\nsignificantly reduces their computational efficiency. Recent approaches, such\nas resamplers and summarizers, have sought to reduce the number of visual\ntokens, but at the cost of visual reasoning ability. To address this, we\npropose LEO-MINI, a novel MLLM that significantly reduces the number of visual\ntokens and simultaneously boosts visual reasoning capabilities. For efficiency,\nLEO-MINI incorporates CoTR, a novel token reduction module to consolidate a\nlarge number of visual tokens into a smaller set of tokens, using the\nsimilarity between visual tokens, text tokens, and a compact learnable query.\nFor effectiveness, to scale up the model's ability with minimal computational\noverhead, LEO-MINI employs MMoE, a novel mixture of multi-modal experts module.\nMMOE employs a set of LoRA experts with a novel router to switch between them\nbased on the input text and visual tokens instead of only using the input\nhidden state. MMoE also includes a general LoRA expert that is always activated\nto learn general knowledge for LLM reasoning. For extracting richer visual\nfeatures, MMOE employs a set of vision experts trained on diverse\ndomain-specific data. To demonstrate LEO-MINI's improved efficiency and\nperformance, we evaluate it against existing efficient MLLMs on various\nbenchmark vision-language tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04935", "pdf": "https://arxiv.org/pdf/2504.04935", "abs": "https://arxiv.org/abs/2504.04935", "authors": ["Peng Liu", "Heng-Chao Li", "Sen Lei", "Nanqing Liu", "Bin Feng", "Xiao Wu"], "title": "RCCFormer: A Robust Crowd Counting Network Based on Transformer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Crowd counting, which is a key computer vision task, has emerged as a\nfundamental technology in crowd analysis and public safety management. However,\nchallenges such as scale variations and complex backgrounds significantly\nimpact the accuracy of crowd counting. To mitigate these issues, this paper\nproposes a robust Transformer-based crowd counting network, termed RCCFormer,\nspecifically designed for background suppression and scale awareness. The\nproposed method incorporates a Multi-level Feature Fusion Module (MFFM), which\nmeticulously integrates features extracted at diverse stages of the backbone\narchitecture. It establishes a strong baseline capable of capturing intricate\nand comprehensive feature representations, surpassing traditional baselines.\nFurthermore, the introduced Detail-Embedded Attention Block (DEAB) captures\ncontextual information and local details through global self-attention and\nlocal attention along with a learnable manner for efficient fusion. This\nenhances the model's ability to focus on foreground regions while effectively\nmitigating background noise interference. Additionally, we develop an Adaptive\nScale-Aware Module (ASAM), with our novel Input-dependent Deformable\nConvolution (IDConv) as its fundamental building block. This module dynamically\nadapts to changes in head target shapes and scales, significantly improving the\nnetwork's capability to accommodate large-scale variations. The effectiveness\nof the proposed method is validated on the ShanghaiTech Part_A and Part_B,\nNWPU-Crowd, and QNRF datasets. The results demonstrate that our RCCFormer\nachieves excellent performance across all four datasets, showcasing\nstate-of-the-art outcomes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04699", "pdf": "https://arxiv.org/pdf/2504.04699", "abs": "https://arxiv.org/abs/2504.04699", "authors": ["Martin Weyssow", "Chengran Yang", "Junkai Chen", "Yikun Li", "Huihui Huang", "Ratnadira Widyasari", "Han Wei Ang", "Frank Liauw", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "title": "R2Vul: Learning to Reason about Software Vulnerabilities with Reinforcement Learning and Structured Reasoning Distillation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown promising performance in software\nvulnerability detection (SVD), yet their reasoning capabilities remain\nunreliable. Existing approaches relying on chain-of-thought (CoT) struggle to\nprovide relevant and actionable security assessments. Additionally, effective\nSVD requires not only generating coherent reasoning but also differentiating\nbetween well-founded and misleading yet plausible security assessments, an\naspect overlooked in prior work. To this end, we introduce R2Vul, a novel\napproach that distills structured reasoning into small LLMs using reinforcement\nlearning from AI feedback (RLAIF). Through RLAIF, R2Vul enables LLMs to produce\nstructured, security-aware reasoning that is actionable and reliable while\nexplicitly learning to distinguish valid assessments from misleading ones. We\nevaluate R2Vul across five languages against SAST tools, CoT, instruction\ntuning, and classification-based baselines. Our results show that R2Vul with\nstructured reasoning distillation enables a 1.5B student LLM to rival larger\nmodels while improving generalization to out-of-distribution vulnerabilities.\nBeyond model improvements, we contribute a large-scale, multilingual preference\ndataset featuring structured reasoning to support future research in SVD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "RLAIF", "AI feedback"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04974", "pdf": "https://arxiv.org/pdf/2504.04974", "abs": "https://arxiv.org/abs/2504.04974", "authors": ["Ming Li", "Ruiyi Zhang", "Jian Chen", "Jiuxiang Gu", "Yufan Zhou", "Franck Dernoncourt", "Wanrong Zhu", "Tianyi Zhou", "Tong Sun"], "title": "Towards Visual Text Grounding of Multimodal Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04736", "pdf": "https://arxiv.org/pdf/2504.04736", "abs": "https://arxiv.org/abs/2504.04736", "authors": ["Anna Goldie", "Azalia Mirhoseini", "Hao Zhou", "Irene Cai", "Christopher D. Manning"], "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning has been shown to improve the performance of large\nlanguage models. However, traditional approaches like RLHF or RLAIF treat the\nproblem as single-step. As focus shifts toward more complex reasoning and\nagentic tasks, language models must take multiple steps of text generation,\nreasoning and environment interaction before generating a solution. We propose\na synthetic data generation and RL methodology targeting multi-step\noptimization scenarios. This approach, called Step-Wise Reinforcement Learning\n(SWiRL), iteratively generates multi-step reasoning and tool use data, and then\nlearns from that data. It employs a simple step-wise decomposition that breaks\neach multi-step trajectory into multiple sub-trajectories corresponding to each\naction by the original model. It then applies synthetic data filtering and RL\noptimization on these sub-trajectories. We evaluated SWiRL on a number of\nmulti-step tool use, question answering, and mathematical reasoning tasks. Our\nexperiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%,\n14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA,\nMuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits\ngeneralization across tasks: for example, training only on HotPotQA (text\nquestion-answering) improves zero-shot performance on GSM8K (a math dataset) by\na relative 16.9%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning", "RLAIF"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "question answering", "mathematical reasoning"], "score": 4}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04974", "pdf": "https://arxiv.org/pdf/2504.04974", "abs": "https://arxiv.org/abs/2504.04974", "authors": ["Ming Li", "Ruiyi Zhang", "Jian Chen", "Jiuxiang Gu", "Yufan Zhou", "Franck Dernoncourt", "Wanrong Zhu", "Tianyi Zhou", "Tong Sun"], "title": "Towards Visual Text Grounding of Multimodal Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05046", "pdf": "https://arxiv.org/pdf/2504.05046", "abs": "https://arxiv.org/abs/2504.05046", "authors": ["Shenghao Ren", "Yi Lu", "Jiayi Huang", "Jiayi Zhao", "He Zhang", "Tao Yu", "Qiu Shen", "Xun Cao"], "title": "MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond", "categories": ["cs.CV"], "comment": null, "summary": "Existing human Motion Capture (MoCap) methods mostly focus on the visual\nsimilarity while neglecting the physical plausibility. As a result, downstream\ntasks such as driving virtual human in 3D scene or humanoid robots in real\nworld suffer from issues such as timing drift and jitter, spatial problems like\nsliding and penetration, and poor global trajectory accuracy. In this paper, we\nrevisit human MoCap from the perspective of interaction between human body and\nphysical world by exploring the role of pressure. Firstly, we construct a\nlarge-scale human Motion capture dataset with Pressure, RGB and Optical sensors\n(named MotionPRO), which comprises 70 volunteers performing 400 types of\nmotion, encompassing a total of 12.4M pose frames. Secondly, we examine both\nthe necessity and effectiveness of the pressure signal through two challenging\ntasks: (1) pose and trajectory estimation based solely on pressure: We propose\na network that incorporates a small kernel decoder and a long-short-term\nattention module, and proof that pressure could provide accurate global\ntrajectory and plausible lower body pose. (2) pose and trajectory estimation by\nfusing pressure and RGB: We impose constraints on orthographic similarity along\nthe camera axis and whole-body contact along the vertical axis to enhance the\ncross-attention strategy to fuse pressure and RGB feature maps. Experiments\ndemonstrate that fusing pressure with RGB features not only significantly\nimproves performance in terms of objective metrics, but also plausibly drives\nvirtual humans (SMPL) in 3D scene. Furthermore, we demonstrate that\nincorporating physical perception enables humanoid robots to perform more\nprecise and stable actions, which is highly beneficial for the development of\nembodied artificial intelligence. Project page is available at:\nhttps://nju-cite-mocaphumanoid.github.io/MotionPRO/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05062", "pdf": "https://arxiv.org/pdf/2504.05062", "abs": "https://arxiv.org/abs/2504.05062", "authors": ["Chenfeng Xu"], "title": "LDGNet: A Lightweight Difference Guiding Network for Remote Sensing Change Detection", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of deep learning, the field of change detection\n(CD) in remote sensing imagery has achieved remarkable progress. Existing\nchange detection methods primarily focus on achieving higher accuracy with\nincreased computational costs and parameter sizes, leaving development of\nlightweight methods for rapid real-world processing an underexplored challenge.\nTo address this challenge, we propose a Lightweight Difference Guiding Network\n(LDGNet), leveraging absolute difference image to guide optical remote sensing\nchange detection. First, to enhance the feature representation capability of\nthe lightweight backbone network, we propose the Difference Guiding Module\n(DGM), which leverages multi-scale features extracted from the absolute\ndifference image to progressively influence the original image encoder at each\nlayer, thereby reinforcing feature extraction. Second, we propose the\nDifference-Aware Dynamic Fusion (DADF) module with Visual State Space Model\n(VSSM) for lightweight long-range dependency modeling. The module first uses\nfeature absolute differences to guide VSSM's global contextual modeling of\nchange regions, then employs difference attention to dynamically fuse these\nlong-range features with feature differences, enhancing change semantics while\nsuppressing noise and background. Extensive experiments on multiple datasets\ndemonstrate that our method achieves comparable or superior performance to\ncurrent state-of-the-art (SOTA) methods requiring several times more\ncomputation, while maintaining only 3.43M parameters and 1.12G FLOPs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05220", "pdf": "https://arxiv.org/pdf/2504.05220", "abs": "https://arxiv.org/abs/2504.05220", "authors": ["Hengran Zhang", "Minghao Tang", "Keping Bi", "Jiafeng Guo", "Shihao Liu", "Daiting Shi", "Dawei Yin", "Xueqi Cheng"], "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "12 pages, 4 figures", "summary": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05112", "pdf": "https://arxiv.org/pdf/2504.05112", "abs": "https://arxiv.org/abs/2504.05112", "authors": ["Ronghui Zhang", "Dakang Lyu", "Tengfei Li", "Yunfan Wu", "Ujjal Manandhar", "Benfei Wang", "Junzhou Chen", "Bolin Gao", "Danwei Wang", "Yiqiu Tan"], "title": "ABCDWaveNet: Advancing Robust Road Ponding Detection in Fog through Dynamic Frequency-Spatial Synergy", "categories": ["cs.CV"], "comment": null, "summary": "Road ponding presents a significant threat to vehicle safety, particularly in\nadverse fog conditions, where reliable detection remains a persistent challenge\nfor Advanced Driver Assistance Systems (ADAS). To address this, we propose\nABCDWaveNet, a novel deep learning framework leveraging Dynamic\nFrequency-Spatial Synergy for robust ponding detection in fog. The core of\nABCDWaveNet achieves this synergy by integrating dynamic convolution for\nadaptive feature extraction across varying visibilities with a wavelet-based\nmodule for synergistic frequency-spatial feature enhancement, significantly\nimproving robustness against fog interference. Building on this foundation,\nABCDWaveNet captures multi-scale structural and contextual information,\nsubsequently employing an Adaptive Attention Coupling Gate (AACG) to adaptively\nfuse global and local features for enhanced accuracy. To facilitate realistic\nevaluations under combined adverse conditions, we introduce the Foggy Low-Light\nPuddle dataset. Extensive experiments demonstrate that ABCDWaveNet establishes\nnew state-of-the-art performance, achieving significant Intersection over Union\n(IoU) gains of 3.51%, 1.75%, and 1.03% on the Foggy-Puddle, Puddle-1000, and\nour Foggy Low-Light Puddle datasets, respectively. Furthermore, its processing\nspeed of 25.48 FPS on an NVIDIA Jetson AGX Orin confirms its suitability for\nADAS deployment. These findings underscore the effectiveness of the proposed\nDynamic Frequency-Spatial Synergy within ABCDWaveNet, offering valuable\ninsights for developing proactive road safety solutions capable of operating\nreliably in challenging weather conditions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "accuracy"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05170", "pdf": "https://arxiv.org/pdf/2504.05170", "abs": "https://arxiv.org/abs/2504.05170", "authors": ["Bonan Ding", "Jin Xie", "Jing Nie", "Jiale Cao"], "title": "SSLFusion: Scale & Space Aligned Latent Fusion Model for Multimodal 3D Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by AAAI 2025", "summary": "Multimodal 3D object detection based on deep neural networks has indeed made\nsignificant progress. However, it still faces challenges due to the\nmisalignment of scale and spatial information between features extracted from\n2D images and those derived from 3D point clouds. Existing methods usually\naggregate multimodal features at a single stage. However, leveraging\nmulti-stage cross-modal features is crucial for detecting objects of various\nscales. Therefore, these methods often struggle to integrate features across\ndifferent scales and modalities effectively, thereby restricting the accuracy\nof detection. Additionally, the time-consuming Query-Key-Value-based\n(QKV-based) cross-attention operations often utilized in existing methods aid\nin reasoning the location and existence of objects by capturing non-local\ncontexts. However, this approach tends to increase computational complexity. To\naddress these challenges, we present SSLFusion, a novel Scale & Space Aligned\nLatent Fusion Model, consisting of a scale-aligned fusion strategy (SAF), a\n3D-to-2D space alignment module (SAM), and a latent cross-modal fusion module\n(LFM). SAF mitigates scale misalignment between modalities by aggregating\nfeatures from both images and point clouds across multiple levels. SAM is\ndesigned to reduce the inter-modal gap between features from images and point\nclouds by incorporating 3D coordinate information into 2D image features.\nAdditionally, LFM captures cross-modal non-local contexts in the latent space\nwithout utilizing the QKV-based attention operations, thus mitigating\ncomputational complexity. Experiments on the KITTI and DENSE datasets\ndemonstrate that our SSLFusion outperforms state-of-the-art methods. Our\napproach obtains an absolute gain of 2.15% in 3D AP, compared with the\nstate-of-art method GraphAlign on the moderate level of the KITTI test set.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05178", "pdf": "https://arxiv.org/pdf/2504.05178", "abs": "https://arxiv.org/abs/2504.05178", "authors": ["Hao Fang", "Runmin Cong", "Xiankai Lu", "Zhiyang Chen", "Wei Zhang"], "title": "The 1st Solution for 4th PVUW MeViS Challenge: Unleashing the Potential of Large Multimodal Models for Referring Video Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Motion expression video segmentation is designed to segment objects in\naccordance with the input motion expressions. In contrast to the conventional\nReferring Video Object Segmentation (RVOS), it places emphasis on motion as\nwell as multi-object expressions, making it more arduous. Recently, Large\nMultimodal Models (LMMs) have begun to shine in RVOS due to their powerful\nvision-language perception capabilities. In this work, we propose a simple and\neffective inference optimization method to fully unleash the potential of LMMs\nin referring video segmentation. Firstly, we use Sa2VA as our baseline, which\nis a unified LMM for dense grounded understanding of both images and videos.\nSecondly, we uniformly sample the video frames during the inference process to\nenhance the model's understanding of the entire video. Finally, we integrate\nthe results of multiple expert models to mitigate the erroneous predictions of\na single model. Our solution achieved 61.98% J&F on the MeViS test set and\nranked 1st place in the 4th PVUW Challenge MeViS Track at CVPR 2025.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference optimization"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05184", "pdf": "https://arxiv.org/pdf/2504.05184", "abs": "https://arxiv.org/abs/2504.05184", "authors": ["Rayan Merghani Ahmed", "Adnan Iltaf", "Bin Li", "Shoujun Zhou"], "title": "MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised Prototypical Contrastive Loss for Coronary DSA Image Segmentation", "categories": ["cs.CV"], "comment": "Work in progress", "summary": "The accurate segmentation of coronary Digital Subtraction Angiography (DSA)\nimages is essential for diagnosing and treating coronary artery diseases.\nDespite advances in deep learning-based segmentation, challenges such as low\ncontrast, noise, overlapping structures, high intra-class variance, and class\nimbalance limit precise vessel delineation. To overcome these limitations, we\npropose the MSA-UNet3+: a Multi-Scale Attention enhanced UNet3+ architecture\nfor coronary DSA image segmentation. The framework combined Multi-Scale Dilated\nBottleneck (MSD-Bottleneck) with Contextual Attention Fusion Module (CAFM),\nwhich not only enhances multi-scale feature extraction but also preserve\nfine-grained details, and improve contextual understanding. Furthermore, we\npropose a new Supervised Prototypical Contrastive Loss (SPCL), which combines\nsupervised and prototypical contrastive learning to minimize class imbalance\nand high intra-class variance by focusing on hard-to-classified background\nsamples. Experiments carried out on a private coronary DSA dataset demonstrate\nthat MSA-UNet3+ outperforms state-of-the-art methods, achieving a Dice\ncoefficient of 87.73%, an F1-score of 87.78%, and significantly reduced Average\nSurface Distance (ASD) and Average Contour Distance (ACD). The developed\nframework provides clinicians with precise vessel segmentation, enabling\naccurate identification of coronary stenosis and supporting informed diagnostic\nand therapeutic decisions. The code will be released at the following GitHub\nprofile link https://github.com/rayanmerghani/MSA-UNet3plus.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05227", "pdf": "https://arxiv.org/pdf/2504.05227", "abs": "https://arxiv.org/abs/2504.05227", "authors": ["Julio Silva-Rodr√≠guez", "Jose Dolz", "Ismail Ben Ayed"], "title": "A Reality Check of Vision-Language Pre-training in Radiology: Have We Progressed Using Text?", "categories": ["cs.CV"], "comment": "IPMI 2025. Code and weights: https://github.com/jusiro/DLILP", "summary": "Vision-language pre-training has recently gained popularity as it allows\nlearning rich feature representations using large-scale data sources. This\nparadigm has quickly made its way into the medical image analysis community. In\nparticular, there is an impressive amount of recent literature developing\nvision-language models for radiology. However, the available medical datasets\nwith image-text supervision are scarce, and medical concepts are fine-grained,\ninvolving expert knowledge that existing vision-language models struggle to\nencode. In this paper, we propose to take a prudent step back from the\nliterature and revisit supervised, unimodal pre-training, using fine-grained\nlabels instead. We conduct an extensive comparison demonstrating that unimodal\npre-training is highly competitive and better suited to integrating\nheterogeneous data sources. Our results also question the potential of recent\nvision-language models for open-vocabulary generalization, which have been\nevaluated using optimistic experimental settings. Finally, we study novel\nalternatives to better integrate fine-grained labels and noisy text\nsupervision.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05249", "pdf": "https://arxiv.org/pdf/2504.05249", "abs": "https://arxiv.org/abs/2504.05249", "authors": ["Wenzhao Tang", "Weihang Li", "Xiucheng Liang", "Olaf Wysocki", "Filip Biljecki", "Christoph Holst", "Boris Jutzi"], "title": "Texture2LoD3: Enabling LoD3 Building Reconstruction With Panoramic Images", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for CVPRW '25", "summary": "Despite recent advancements in surface reconstruction, Level of Detail (LoD)\n3 building reconstruction remains an unresolved challenge. The main issue\npertains to the object-oriented modelling paradigm, which requires\ngeoreferencing, watertight geometry, facade semantics, and low-poly\nrepresentation -- Contrasting unstructured mesh-oriented models. In\nTexture2LoD3, we introduce a novel method leveraging the ubiquity of 3D\nbuilding model priors and panoramic street-level images, enabling the\nreconstruction of LoD3 building models. We observe that prior low-detail\nbuilding models can serve as valid planar targets for ortho-rectifying\nstreet-level panoramic images. Moreover, deploying segmentation on accurately\ntextured low-level building surfaces supports maintaining essential\ngeoreferencing, watertight geometry, and low-poly representation for LoD3\nreconstruction. In the absence of LoD3 validation data, we additionally\nintroduce the ReLoD3 dataset, on which we experimentally demonstrate that our\nmethod leads to improved facade segmentation accuracy by 11% and can replace\ncostly manual projections. We believe that Texture2LoD3 can scale the adoption\nof LoD3 models, opening applications in estimating building solar potential or\nenhancing autonomous driving simulations. The project website, code, and data\nare available here: https://wenzhaotang.github.io/Texture2LoD3/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05253", "pdf": "https://arxiv.org/pdf/2504.05253", "abs": "https://arxiv.org/abs/2504.05253", "authors": ["Ben Lonnqvist", "Elsa Scialom", "Abdulkadir Gokce", "Zehra Merchant", "Michael H. Herzog", "Martin Schrimpf"], "title": "Contour Integration Underlies Human-Like Vision", "categories": ["cs.CV"], "comment": null, "summary": "Despite the tremendous success of deep learning in computer vision, models\nstill fall behind humans in generalizing to new input distributions. Existing\nbenchmarks do not investigate the specific failure points of models by\nanalyzing performance under many controlled conditions. Our study\nsystematically dissects where and why models struggle with contour integration\n-- a hallmark of human vision -- by designing an experiment that tests object\nrecognition under various levels of object fragmentation. Humans (n=50) perform\nat high accuracy, even with few object contours present. This is in contrast to\nmodels which exhibit substantially lower sensitivity to increasing object\ncontours, with most of the over 1,000 models we tested barely performing above\nchance. Only at very large scales ($\\sim5B$ training dataset size) do models\nbegin to approach human performance. Importantly, humans exhibit an integration\nbias -- a preference towards recognizing objects made up of directional\nfragments over directionless fragments. We find that not only do models that\nshare this property perform better at our task, but that this bias also\nincreases with model training dataset size, and training models to exhibit\ncontour integration leads to high shape bias. Taken together, our results\nsuggest that contour integration is a hallmark of object vision that underlies\nobject recognition performance, and may be a mechanism learned from data at\nscale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05305", "pdf": "https://arxiv.org/pdf/2504.05305", "abs": "https://arxiv.org/abs/2504.05305", "authors": ["Sangbeom Lim", "Junwan Kim", "Heeji Yoon", "Jaewoo Jung", "Seungryong Kim"], "title": "URECA: Unique Region Caption Anything", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://cvlab-kaist.github.io/URECA Code:\n  https://github.com/cvlab-kaist/URECA", "summary": "Region-level captioning aims to generate natural language descriptions for\nspecific image regions while highlighting their distinguishing features.\nHowever, existing methods struggle to produce unique captions across\nmulti-granularity, limiting their real-world applicability. To address the need\nfor detailed region-level understanding, we introduce URECA dataset, a\nlarge-scale dataset tailored for multi-granularity region captioning. Unlike\nprior datasets that focus primarily on salient objects, URECA dataset ensures a\nunique and consistent mapping between regions and captions by incorporating a\ndiverse set of objects, parts, and background elements. Central to this is a\nstage-wise data curation pipeline, where each stage incrementally refines\nregion selection and caption generation. By leveraging Multimodal Large\nLanguage Models (MLLMs) at each stage, our pipeline produces distinctive and\ncontextually grounded captions with improved accuracy and semantic diversity.\nBuilding upon this dataset, we present URECA, a novel captioning model designed\nto effectively encode multi-granularity regions. URECA maintains essential\nspatial properties such as position and shape through simple yet impactful\nmodifications to existing MLLMs, enabling fine-grained and semantically rich\nregion descriptions. Our approach introduces dynamic mask modeling and a\nhigh-resolution mask encoder to enhance caption uniqueness. Experiments show\nthat URECA achieves state-of-the-art performance on URECA dataset and\ngeneralizes well to existing region-level captioning benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04153", "pdf": "https://arxiv.org/pdf/2504.04153", "abs": "https://arxiv.org/abs/2504.04153", "authors": ["Yikai Wang", "Guangce Liu", "Xinzhou Wang", "Zilong Chen", "Jiafang Li", "Xin Liang", "Fuchun Sun", "Jun Zhu"], "title": "Video4DGen: Enhancing Video and 4D Generation through Mutual Optimization", "categories": ["cs.GR", "cs.CV"], "comment": "Published in TPAMI 2025. Code: https://github.com/yikaiw/Vidu4D,\n  Project page: https://video4dgen.github.io", "summary": "The advancement of 4D (i.e., sequential 3D) generation opens up new\npossibilities for lifelike experiences in various applications, where users can\nexplore dynamic objects or characters from any viewpoint. Meanwhile, video\ngenerative models are receiving particular attention given their ability to\nproduce realistic and imaginative frames. These models are also observed to\nexhibit strong 3D consistency, indicating the potential to act as world\nsimulators. In this work, we present Video4DGen, a novel framework that excels\nin generating 4D representations from single or multiple generated videos as\nwell as generating 4D-guided videos. This framework is pivotal for creating\nhigh-fidelity virtual contents that maintain both spatial and temporal\ncoherence. The 4D outputs generated by Video4DGen are represented using our\nproposed Dynamic Gaussian Surfels (DGS), which optimizes time-varying warping\nfunctions to transform Gaussian surfels (surface elements) from a static state\nto a dynamically warped state. We design warped-state geometric regularization\nand refinements on Gaussian surfels, to preserve the structural integrity and\nfine-grained appearance details. To perform 4D generation from multiple videos\nand capture representation across spatial, temporal, and pose dimensions, we\ndesign multi-video alignment, root pose optimization, and pose-guided frame\nsampling strategies. The leveraging of continuous warping fields also enables a\nprecise depiction of pose, motion, and deformation over per-video frames.\nFurther, to improve the overall fidelity from the observation of all camera\nposes, Video4DGen performs novel-view video generation guided by the 4D\ncontent, with the proposed confidence-filtered DGS to enhance the quality of\ngenerated sequences. With the ability of 4D and video generation, Video4DGen\noffers a powerful tool for applications in virtual reality, animation, and\nbeyond.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04338", "pdf": "https://arxiv.org/pdf/2504.04338", "abs": "https://arxiv.org/abs/2504.04338", "authors": ["Alexander Naumann", "Xunjiang Gu", "Tolga Dimlioglu", "Mariusz Bojarski", "Alperen Degirmenci", "Alexander Popov", "Devansh Bisla", "Marco Pavone", "Urs M√ºller", "Boris Ivanovic"], "title": "Data Scaling Laws for End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "15 pages, 11 figures, 4 tables, CVPR 2025 Workshop on Autonomous\n  Driving", "summary": "Autonomous vehicle (AV) stacks have traditionally relied on decomposed\napproaches, with separate modules handling perception, prediction, and\nplanning. However, this design introduces information loss during inter-module\ncommunication, increases computational overhead, and can lead to compounding\nerrors. To address these challenges, recent works have proposed architectures\nthat integrate all components into an end-to-end differentiable model, enabling\nholistic system optimization. This shift emphasizes data engineering over\nsoftware integration, offering the potential to enhance system performance by\nsimply scaling up training resources. In this work, we evaluate the performance\nof a simple end-to-end driving architecture on internal driving datasets\nranging in size from 16 to 8192 hours with both open-loop metrics and\nclosed-loop simulations. Specifically, we investigate how much additional\ntraining data is needed to achieve a target performance gain, e.g., a 5%\nimprovement in motion prediction accuracy. By understanding the relationship\nbetween model performance and training dataset size, we aim to provide insights\nfor data-driven decision-making in autonomous driving development.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04458", "pdf": "https://arxiv.org/pdf/2504.04458", "abs": "https://arxiv.org/abs/2504.04458", "authors": ["Bashir Alam", "Masa Cirkovic", "Mete Harun Akcay", "Md Kaf Shahrier", "Sebastien Lafond", "Hergys Rexha", "Kurt Benke", "Sepinoud Azimi", "Janan Arslan"], "title": "CALF: A Conditionally Adaptive Loss Function to Mitigate Class-Imbalanced Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Imbalanced datasets pose a considerable challenge in training deep learning\n(DL) models for medical diagnostics, particularly for segmentation tasks.\nImbalance may be associated with annotation quality limited annotated datasets,\nrare cases, or small-scale regions of interest (ROIs). These conditions\nadversely affect model training and performance, leading to segmentation\nboundaries which deviate from the true ROIs. Traditional loss functions, such\nas Binary Cross Entropy, replicate annotation biases and limit model\ngeneralization. We propose a novel, statistically driven, conditionally\nadaptive loss function (CALF) tailored to accommodate the conditions of\nimbalanced datasets in DL training. It employs a data-driven methodology by\nestimating imbalance severity using statistical methods of skewness and\nkurtosis, then applies an appropriate transformation to balance the training\ndataset while preserving data heterogeneity. This transformative approach\nintegrates a multifaceted process, encompassing preprocessing, dataset\nfiltering, and dynamic loss selection to achieve optimal outcomes. We benchmark\nour method against conventional loss functions using qualitative and\nquantitative evaluations. Experiments using large-scale open-source datasets\n(i.e., UPENN-GBM, UCSF, LGG, and BraTS) validate our approach, demonstrating\nsubstantial segmentation improvements. Code availability:\nhttps://anonymous.4open.science/r/MICCAI-Submission-43F9/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04634", "pdf": "https://arxiv.org/pdf/2504.04634", "abs": "https://arxiv.org/abs/2504.04634", "authors": ["Foram Niravbhai Shah", "Parshwa Shah", "Muhammad Usama Saleem", "Ekkasit Pinyoanuntapong", "Pu Wang", "Hongfei Xue", "Ahmed Helmy"], "title": "DanceMosaic: High-Fidelity Dance Generation with Multimodal Editability", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advances in dance generation have enabled automatic synthesis of 3D\ndance motions. However, existing methods still struggle to produce\nhigh-fidelity dance sequences that simultaneously deliver exceptional realism,\nprecise dance-music synchronization, high motion diversity, and physical\nplausibility. Moreover, existing methods lack the flexibility to edit dance\nsequences according to diverse guidance signals, such as musical prompts, pose\nconstraints, action labels, and genre descriptions, significantly restricting\ntheir creative utility and adaptability. Unlike the existing approaches,\nDanceMosaic enables fast and high-fidelity dance generation, while allowing\nmultimodal motion editing. Specifically, we propose a multimodal masked motion\nmodel that fuses the text-to-motion model with music and pose adapters to learn\nprobabilistic mapping from diverse guidance signals to high-quality dance\nmotion sequences via progressive generative masking training. To further\nenhance the motion generation quality, we propose multimodal classifier-free\nguidance and inference-time optimization mechanism that further enforce the\nalignment between the generated motions and the multimodal guidance. Extensive\nexperiments demonstrate that our method establishes a new state-of-the-art\nperformance in dance generation, significantly advancing the quality and\neditability achieved by existing approaches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04831", "pdf": "https://arxiv.org/pdf/2504.04831", "abs": "https://arxiv.org/abs/2504.04831", "authors": ["Sanjeev Muralikrishnan", "Niladri Shekhar Dutt", "Niloy J. Mitra"], "title": "SMF: Template-free and Rig-free Animation Transfer using Kinetic Codes", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Animation retargeting involves applying a sparse motion description (e.g.,\n2D/3D keypoint sequences) to a given character mesh to produce a semantically\nplausible and temporally coherent full-body motion. Existing approaches come\nwith a mix of restrictions - they require annotated training data, assume\naccess to template-based shape priors or artist-designed deformation rigs,\nsuffer from limited generalization to unseen motion and/or shapes, or exhibit\nmotion jitter. We propose Self-supervised Motion Fields (SMF) as a\nself-supervised framework that can be robustly trained with sparse motion\nrepresentations, without requiring dataset specific annotations, templates, or\nrigs. At the heart of our method are Kinetic Codes, a novel autoencoder-based\nsparse motion encoding, that exposes a semantically rich latent space\nsimplifying large-scale training. Our architecture comprises dedicated spatial\nand temporal gradient predictors, which are trained end-to-end. The resultant\nnetwork, regularized by the Kinetic Codes's latent space, has good\ngeneralization across shapes and motions. We evaluated our method on unseen\nmotion sampled from AMASS, D4D, Mixamo, and raw monocular video for animation\ntransfer on various characters with varying shapes and topology. We report a\nnew SoTA on the AMASS dataset in the context of generalization to unseen\nmotion. Project webpage at https://motionfields.github.io/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05231", "pdf": "https://arxiv.org/pdf/2504.05231", "abs": "https://arxiv.org/abs/2504.05231", "authors": ["C√©sar Leblanc", "Lukas Picek", "Benjamin Deneu", "Pierre Bonnet", "Maximilien Servajean", "R√©mi Palard", "Alexis Joly"], "title": "Mapping biodiversity at very-high resolution in Europe", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper describes a cascading multimodal pipeline for high-resolution\nbiodiversity mapping across Europe, integrating species distribution modeling,\nbiodiversity indicators, and habitat classification. The proposed pipeline\nfirst predicts species compositions using a deep-SDM, a multimodal model\ntrained on remote sensing, climate time series, and species occurrence data at\n50x50m resolution. These predictions are then used to generate biodiversity\nindicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM\ndesigned for species-to-habitat mapping. With this approach, continental-scale\nspecies distribution maps, biodiversity indicator maps, and habitat maps are\nproduced, providing fine-grained ecological insights. Unlike traditional\nmethods, this framework enables joint modeling of interspecies dependencies,\nbias-aware training with heterogeneous presence-absence data, and large-scale\ninference from multi-source remote sensing inputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
