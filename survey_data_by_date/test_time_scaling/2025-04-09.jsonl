{"id": "2504.05732", "pdf": "https://arxiv.org/pdf/2504.05732", "abs": "https://arxiv.org/abs/2504.05732", "authors": ["Haoyu Wang", "Yujia Fu", "Zhu Zhang", "Shuo Wang", "Zirui Ren", "Xiaorong Wang", "Zhili Li", "Chaoqun He", "Bo An", "Zhiyuan Liu", "Maosong Sun"], "title": "LLM$\\times$MapReduce-V2: Entropy-Driven Convolutional Test-Time Scaling for Generating Long-Form Articles from Extremely Long Resources", "categories": ["cs.CL"], "comment": null, "summary": "Long-form generation is crucial for a wide range of practical applications,\ntypically categorized into short-to-long and long-to-long generation. While\nshort-to-long generations have received considerable attention, generating long\ntexts from extremely long resources remains relatively underexplored. The\nprimary challenge in long-to-long generation lies in effectively integrating\nand analyzing relevant information from extensive inputs, which remains\ndifficult for current large language models (LLMs). In this paper, we propose\nLLM$\\times$MapReduce-V2, a novel test-time scaling strategy designed to enhance\nthe ability of LLMs to process extremely long inputs. Drawing inspiration from\nconvolutional neural networks, which iteratively integrate local features into\nhigher-level global representations, LLM$\\times$MapReduce-V2 utilizes stacked\nconvolutional scaling layers to progressively expand the understanding of input\nmaterials. Both quantitative and qualitative experimental results demonstrate\nthat our approach substantially enhances the ability of LLMs to process long\ninputs and generate coherent, informative long-form articles, outperforming\nseveral representative baselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05759", "pdf": "https://arxiv.org/pdf/2504.05759", "abs": "https://arxiv.org/abs/2504.05759", "authors": ["Nathanaël Beau", "Benoît Crabbé"], "title": "RETROcode: Leveraging a Code Database for Improved Natural Language to Code Generation", "categories": ["cs.CL"], "comment": null, "summary": "As text and code resources have expanded, large-scale pre-trained models have\nshown promising capabilities in code generation tasks, typically employing\nsupervised fine-tuning with problem statement-program pairs. However,\nincreasing model size and data volume for performance gains also raises\ncomputational demands and risks of overfitting. Addressing these challenges, we\npresent RETROcode, a novel adaptation of the RETRO architecture \\cite{RETRO}\nfor sequence-to-sequence models, utilizing a large code database as an\nauxiliary scaling method. This approach, diverging from simply enlarging model\nand dataset sizes, allows RETROcode to leverage a vast code database for\nprediction, enhancing the model's efficiency by integrating extensive memory.\nOur findings indicate that RETROcode not only outperforms similar-sized\ntraditional architectures on test sets but also approaches the effectiveness of\nthe much larger Codex model, despite being trained from scratch on a\nsubstantially smaller dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "code generation"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06214", "pdf": "https://arxiv.org/pdf/2504.06214", "abs": "https://arxiv.org/abs/2504.06214", "authors": ["Chejian Xu", "Wei Ping", "Peng Xu", "Zihan Liu", "Boxin Wang", "Mohammad Shoeybi", "Bo Li", "Bryan Catanzaro"], "title": "From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Long-context capabilities are essential for a wide range of applications,\nincluding document and video understanding, in-context learning, and\ninference-time scaling, all of which require models to process and reason over\nlong sequences of text and multimodal data. In this work, we introduce a\nefficient training recipe for building ultra-long context LLMs from aligned\ninstruct model, pushing the boundaries of context lengths from 128K to 1M, 2M,\nand 4M tokens. Our approach leverages efficient continued pretraining\nstrategies to extend the context window and employs effective instruction\ntuning to maintain the instruction-following and reasoning abilities. Our\nUltraLong-8B, built on Llama3.1-Instruct with our recipe, achieves\nstate-of-the-art performance across a diverse set of long-context benchmarks.\nImportantly, models trained with our approach maintain competitive performance\non standard benchmarks, demonstrating balanced improvements for both long and\nshort context tasks. We further provide an in-depth analysis of key design\nchoices, highlighting the impacts of scaling strategies and data composition.\nOur findings establish a robust framework for efficiently scaling context\nlengths while preserving general model capabilities. We release all model\nweights at: https://ultralong.github.io/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05410", "pdf": "https://arxiv.org/pdf/2504.05410", "abs": "https://arxiv.org/abs/2504.05410", "authors": ["Benjamin Lipkin", "Benjamin LeBrun", "Jacob Hoover Vigly", "João Loula", "David R. MacIver", "Li Du", "Jason Eisner", "Ryan Cotterell", "Vikash Mansinghka", "Timothy J. O'Donnell", "Alexander K. Lew", "Tim Vieira"], "title": "Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The dominant approach to generating from language models subject to some\nconstraint is locally constrained decoding (LCD), incrementally sampling tokens\nat each time step such that the constraint is never violated. Typically, this\nis achieved through token masking: looping over the vocabulary and excluding\nnon-conforming tokens. There are two important problems with this approach. (i)\nEvaluating the constraint on every token can be prohibitively expensive -- LM\nvocabularies often exceed $100,000$ tokens. (ii) LCD can distort the global\ndistribution over strings, sampling tokens based only on local information,\neven if they lead down dead-end paths. This work introduces a new algorithm\nthat addresses both these problems. First, to avoid evaluating a constraint on\nthe full vocabulary at each step of generation, we propose an adaptive\nrejection sampling algorithm that typically requires orders of magnitude fewer\nconstraint evaluations. Second, we show how this algorithm can be extended to\nproduce low-variance, unbiased estimates of importance weights at a very small\nadditional cost -- estimates that can be soundly used within previously\nproposed sequential Monte Carlo algorithms to correct for the myopic behavior\nof local constraint enforcement. Through extensive empirical evaluation in\ntext-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON\ndomains, we show that our approach is superior to state-of-the-art baselines,\nsupporting a broader class of constraints and improving both runtime and\nperformance. Additional theoretical and empirical analyses show that our\nmethod's runtime efficiency is driven by its dynamic use of computation,\nscaling with the divergence between the unconstrained and constrained LM, and\nas a consequence, runtime improvements are greater for better models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05411", "pdf": "https://arxiv.org/pdf/2504.05411", "abs": "https://arxiv.org/abs/2504.05411", "authors": ["Lingzhi Shen", "Yunfei Long", "Xiaohao Cai", "Guanming Chen", "Imran Razzak", "Shoaib Jameel"], "title": "Less but Better: Parameter-Efficient Fine-Tuning of Large Language Models for Personality Detection", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Personality detection automatically identifies an individual's personality\nfrom various data sources, such as social media texts. However, as the\nparameter scale of language models continues to grow, the computational cost\nbecomes increasingly difficult to manage. Fine-tuning also grows more complex,\nmaking it harder to justify the effort and reliably predict outcomes. We\nintroduce a novel parameter-efficient fine-tuning framework, PersLLM, to\naddress these challenges. In PersLLM, a large language model (LLM) extracts\nhigh-dimensional representations from raw data and stores them in a dynamic\nmemory layer. PersLLM then updates the downstream layers with a replaceable\noutput network, enabling flexible adaptation to various personality detection\nscenarios. By storing the features in the memory layer, we eliminate the need\nfor repeated complex computations by the LLM. Meanwhile, the lightweight output\nnetwork serves as a proxy for evaluating the overall effectiveness of the\nframework, improving the predictability of results. Experimental results on key\nbenchmark datasets like Kaggle and Pandora show that PersLLM significantly\nreduces computational cost while maintaining competitive performance and strong\nadaptability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05400", "pdf": "https://arxiv.org/pdf/2504.05400", "abs": "https://arxiv.org/abs/2504.05400", "authors": ["Sihang Li", "Zeyu Jiang", "Grace Chen", "Chenyang Xu", "Siqi Tan", "Xue Wang", "Irving Fang", "Kristof Zyskowski", "Shannon P. McPherron", "Radu Iovita", "Chen Feng", "Jing Zhang"], "title": "GARF: Learning Generalizable 3D Reassembly for Real-World Fractures", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 11 figures. Project Page https://ai4ce.github.io/GARF/", "summary": "3D reassembly is a challenging spatial intelligence task with broad\napplications across scientific domains. While large-scale synthetic datasets\nhave fueled promising learning-based approaches, their generalizability to\ndifferent domains is limited. Critically, it remains uncertain whether models\ntrained on synthetic datasets can generalize to real-world fractures where\nbreakage patterns are more complex. To bridge this gap, we propose GARF, a\ngeneralizable 3D reassembly framework for real-world fractures. GARF leverages\nfracture-aware pretraining to learn fracture features from individual\nfragments, with flow matching enabling precise 6-DoF alignments. At inference\ntime, we introduce one-step preassembly, improving robustness to unseen objects\nand varying numbers of fractures. In collaboration with archaeologists,\npaleoanthropologists, and ornithologists, we curate Fractura, a diverse dataset\nfor vision and learning communities, featuring real-world fracture types across\nceramics, bones, eggshells, and lithics. Comprehensive experiments have shown\nour approach consistently outperforms state-of-the-art methods on both\nsynthetic and real-world datasets, achieving 82.87\\% lower rotation error and\n25.15\\% higher part accuracy. This sheds light on training on synthetic data to\nadvance real-world 3D puzzle solving, demonstrating its strong generalization\nacross unseen object shapes and diverse fracture types.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05451", "pdf": "https://arxiv.org/pdf/2504.05451", "abs": "https://arxiv.org/abs/2504.05451", "authors": ["Arjun Somayazulu", "Efi Mavroudi", "Changan Chen", "Lorenzo Torresani", "Kristen Grauman"], "title": "Learning Activity View-invariance Under Extreme Viewpoint Changes via Curriculum Knowledge Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Traditional methods for view-invariant learning from video rely on controlled\nmulti-view settings with minimal scene clutter. However, they struggle with\nin-the-wild videos that exhibit extreme viewpoint differences and share little\nvisual content. We introduce a method for learning rich video representations\nin the presence of such severe view-occlusions. We first define a\ngeometry-based metric that ranks views at a fine-grained temporal scale by\ntheir likely occlusion level. Then, using those rankings, we formulate a\nknowledge distillation objective that preserves action-centric semantics with a\nnovel curriculum learning procedure that pairs incrementally more challenging\nviews over time, thereby allowing smooth adaptation to extreme viewpoint\ndifferences. We evaluate our approach on two tasks, outperforming SOTA models\non both temporal keystep grounding and fine-grained keystep recognition\nbenchmarks - particularly on views that exhibit severe occlusion.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05535", "pdf": "https://arxiv.org/pdf/2504.05535", "abs": "https://arxiv.org/abs/2504.05535", "authors": ["M-A-P Team", "Siwei Wu", "Jincheng Ren", "Xinrun Du", "Shuyue Guo", "Xingwei Qu", "Yiming Liang", "Jie Liu", "Yunwen Li", "Tianyu Zheng", "Boyu Feng", "Huaqing Yuan", "Zenith Wang", "Jiaheng Liu", "Wenhao Huang", "Chenglin Cai", "Haoran Que", "Jian Yang", "Yuelin Bai", "Zekun Moore Wang", "Zhouliang Yu", "Qunshu Lin", "Ding Pan", "Yuchen Jiang", "Tiannan Wang", "Wangchunshu Zhou", "Shenzhi Wang", "Xingyuan Bu", "Minghao Liu", "Guoyin Wang", "Ge Zhang", "Chenghua Lin"], "title": "COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values", "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences has achieved\nremarkable success. However, existing Chinese preference datasets are limited\nby small scale, narrow domain coverage, and lack of rigorous data validation.\nAdditionally, the reliance on human annotators for instruction and response\nlabeling significantly constrains the scalability of human preference datasets.\nTo address these challenges, we design an LLM-based Chinese preference dataset\nannotation pipeline with no human intervention. Specifically, we crawled and\ncarefully filtered 92k high-quality Chinese queries and employed 15 mainstream\nLLMs to generate and score chosen-rejected response pairs. Based on it, we\nintroduce COIG-P (Chinese Open Instruction Generalist - Preference), a\nhigh-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese\npreference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel,\nand Role. Building upon COIG-P, to reduce the overhead of using LLMs for\nscoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously\nconstructed a Chinese Reward Benchmark (CRBench). Evaluation results based on\nAlignBench \\citep{liu2024alignbenchbenchmarkingchinesealignment} show that that\nCOIG-P significantly outperforms other Chinese preference datasets, and it\nbrings significant performance improvements ranging from 2% to 12% for the\nQwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results\non CRBench demonstrate that our CRM has a strong and robust scoring ability. We\napply it to filter chosen-rejected response pairs in a test split of COIG-P,\nand our experiments show that it is comparable to GPT-4o in identifying\nlow-quality samples while maintaining efficiency and cost-effectiveness. Our\ncodes and data are released in\nhttps://github.com/multimodal-art-projection/COIG-P.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "preference dataset", "human preference", "annotation"], "score": 6}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05456", "pdf": "https://arxiv.org/pdf/2504.05456", "abs": "https://arxiv.org/abs/2504.05456", "authors": ["Omar De Mitri", "Ruyu Wang", "Marco F. Huber"], "title": "Generative Adversarial Networks with Limited Data: A Survey and Benchmarking", "categories": ["cs.CV"], "comment": null, "summary": "Generative Adversarial Networks (GANs) have shown impressive results in\nvarious image synthesis tasks. Vast studies have demonstrated that GANs are\nmore powerful in feature and expression learning compared to other generative\nmodels and their latent space encodes rich semantic information. However, the\ntremendous performance of GANs heavily relies on the access to large-scale\ntraining data and deteriorates rapidly when the amount of data is limited. This\npaper aims to provide an overview of GANs, its variants and applications in\nvarious vision tasks, focusing on addressing the limited data issue. We analyze\nstate-of-the-art GANs in limited data regime with designed experiments, along\nwith presenting various methods attempt to tackle this problem from different\nperspectives. Finally, we further elaborate on remaining challenges and trends\nfor future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05468", "pdf": "https://arxiv.org/pdf/2504.05468", "abs": "https://arxiv.org/abs/2504.05468", "authors": ["Thanos Delatolas", "Vicky Kalogeiton", "Dim P. Papadopoulos"], "title": "Studying Image Diffusion Features for Zero-Shot Video Object Segmentation", "categories": ["cs.CV"], "comment": "Accepted to CVPRW2025", "summary": "This paper investigates the use of large-scale diffusion models for Zero-Shot\nVideo Object Segmentation (ZS-VOS) without fine-tuning on video data or\ntraining on any image segmentation data. While diffusion models have\ndemonstrated strong visual representations across various tasks, their direct\napplication to ZS-VOS remains underexplored. Our goal is to find the optimal\nfeature extraction process for ZS-VOS by identifying the most suitable time\nstep and layer from which to extract features. We further analyze the affinity\nof these features and observe a strong correlation with point correspondences.\nThrough extensive experiments on DAVIS-17 and MOSE, we find that diffusion\nmodels trained on ImageNet outperform those trained on larger, more diverse\ndatasets for ZS-VOS. Additionally, we highlight the importance of point\ncorrespondences in achieving high segmentation accuracy, and we yield\nstate-of-the-art results in ZS-VOS. Finally, our approach performs on par with\nmodels trained on expensive image segmentation datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05491", "pdf": "https://arxiv.org/pdf/2504.05491", "abs": "https://arxiv.org/abs/2504.05491", "authors": ["Sakib Reza", "Xiyun Song", "Heather Yu", "Zongfang Lin", "Mohsen Moghaddam", "Octavia Camps"], "title": "REEF: Relevance-Aware and Efficient LLM Adapter for Video Understanding", "categories": ["cs.CV"], "comment": "Accepted at CVPRW'25", "summary": "Integrating vision models into large language models (LLMs) has sparked\nsignificant interest in creating vision-language foundation models, especially\nfor video understanding. Recent methods often utilize memory banks to handle\nuntrimmed videos for video-level understanding. However, they typically\ncompress visual memory using similarity-based greedy approaches, which can\noverlook the contextual importance of individual tokens. To address this, we\nintroduce an efficient LLM adapter designed for video-level understanding of\nuntrimmed videos that prioritizes the contextual relevance of spatio-temporal\ntokens. Our framework leverages scorer networks to selectively compress the\nvisual memory bank and filter spatial tokens based on relevance, using a\ndifferentiable Top-K operator for end-to-end training. Across three key\nvideo-level understanding tasks$\\unicode{x2013}$ untrimmed video\nclassification, video question answering, and video\ncaptioning$\\unicode{x2013}$our method achieves competitive or superior results\non four large-scale datasets while reducing computational overhead by up to\n34%. The code will be available soon on GitHub.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05632", "pdf": "https://arxiv.org/pdf/2504.05632", "abs": "https://arxiv.org/abs/2504.05632", "authors": ["Sanchit Kabra", "Akshita Jha", "Chandan Reddy"], "title": "Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "17 pages", "summary": "Recent advances in large-scale generative language models have shown that\nreasoning capabilities can significantly improve model performance across a\nvariety of tasks. However, the impact of reasoning on a model's ability to\nmitigate stereotypical responses remains largely underexplored. In this work,\nwe investigate the crucial relationship between a model's reasoning ability and\nfairness, and ask whether improved reasoning capabilities can mitigate harmful\nstereotypical responses, especially those arising due to shallow or flawed\nreasoning. We conduct a comprehensive evaluation of multiple open-source LLMs,\nand find that larger models with stronger reasoning abilities exhibit\nsubstantially lower stereotypical bias on existing fairness benchmarks.\nBuilding on this insight, we introduce ReGiFT -- Reasoning Guided Fine-Tuning,\na novel approach that extracts structured reasoning traces from advanced\nreasoning models and infuses them into models that lack such capabilities. We\nuse only general-purpose reasoning and do not require any fairness-specific\nsupervision for bias mitigation. Notably, we see that models fine-tuned using\nReGiFT not only improve fairness relative to their non-reasoning counterparts\nbut also outperform advanced reasoning models on fairness benchmarks. We also\nanalyze how variations in the correctness of the reasoning traces and their\nlength influence model fairness and their overall performance. Our findings\nhighlight that enhancing reasoning capabilities is an effective,\nfairness-agnostic strategy for mitigating stereotypical bias caused by\nreasoning flaws.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05499", "pdf": "https://arxiv.org/pdf/2504.05499", "abs": "https://arxiv.org/abs/2504.05499", "authors": ["Ruoyu Xue", "Jingyi Xu", "Sounak Mondal", "Hieu Le", "Gregory Zelinsky", "Minh Hoai", "Dimitris Samaras"], "title": "Few-shot Personalized Scanpath Prediction", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025,20 pages, 10 figures", "summary": "A personalized model for scanpath prediction provides insights into the\nvisual preferences and attention patterns of individual subjects. However,\nexisting methods for training scanpath prediction models are data-intensive and\ncannot be effectively personalized to new individuals with only a few available\nexamples. In this paper, we propose few-shot personalized scanpath prediction\ntask (FS-PSP) and a novel method to address it, which aims to predict scanpaths\nfor an unseen subject using minimal support data of that subject's scanpath\nbehavior. The key to our method's adaptability is the Subject-Embedding Network\n(SE-Net), specifically designed to capture unique, individualized\nrepresentations for each subject's scanpaths. SE-Net generates subject\nembeddings that effectively distinguish between subjects while minimizing\nvariability among scanpaths from the same individual. The personalized scanpath\nprediction model is then conditioned on these subject embeddings to produce\naccurate, personalized results. Experiments on multiple eye-tracking datasets\ndemonstrate that our method excels in FS-PSP settings and does not require any\nfine-tuning steps at test time. Code is available at:\nhttps://github.com/cvlab-stonybrook/few-shot-scanpath", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05693", "pdf": "https://arxiv.org/pdf/2504.05693", "abs": "https://arxiv.org/abs/2504.05693", "authors": ["Aniket Deroy", "Subhankar Maity"], "title": "STRIVE: A Think & Improve Approach with Iterative Refinement for Enhancing Question Quality Estimation", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 6 figures", "summary": "Automatically assessing question quality is crucial for educators as it saves\ntime, ensures consistency, and provides immediate feedback for refining\nteaching materials. We propose a novel methodology called STRIVE (Structured\nThinking and Refinement with multiLLMs for Improving Verified Question\nEstimation) using a series of Large Language Models (LLMs) for automatic\nquestion evaluation. This approach aims to improve the accuracy and depth of\nquestion quality assessment, ultimately supporting diverse learners and\nenhancing educational practices. The method estimates question quality in an\nautomated manner by generating multiple evaluations based on the strengths and\nweaknesses of the provided question and then choosing the best solution\ngenerated by the LLM. Then the process is improved by iterative review and\nresponse with another LLM until the evaluation metric values converge. This\nsophisticated method of evaluating question quality improves the estimation of\nquestion quality by automating the task of question quality evaluation.\nCorrelation scores show that using this proposed method helps to improve\ncorrelation with human judgments compared to the baseline method. Error\nanalysis shows that metrics like relevance and appropriateness improve\nsignificantly relative to human judgments by using STRIVE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05579", "pdf": "https://arxiv.org/pdf/2504.05579", "abs": "https://arxiv.org/abs/2504.05579", "authors": ["Artem Zholus", "Carl Doersch", "Yi Yang", "Skanda Koppula", "Viorica Patraucean", "Xu Owen He", "Ignacio Rocco", "Mehdi S. M. Sajjadi", "Sarath Chandar", "Ross Goroshin"], "title": "TAPNext: Tracking Any Point (TAP) as Next Token Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Tracking Any Point (TAP) in a video is a challenging computer vision problem\nwith many demonstrated applications in robotics, video editing, and 3D\nreconstruction. Existing methods for TAP rely heavily on complex\ntracking-specific inductive biases and heuristics, limiting their generality\nand potential for scaling. To address these challenges, we present TAPNext, a\nnew approach that casts TAP as sequential masked token decoding. Our model is\ncausal, tracks in a purely online fashion, and removes tracking-specific\ninductive biases. This enables TAPNext to run with minimal latency, and removes\nthe temporal windowing required by many existing state of art trackers. Despite\nits simplicity, TAPNext achieves a new state-of-the-art tracking performance\namong both online and offline trackers. Finally, we present evidence that many\nwidely used tracking heuristics emerge naturally in TAPNext through end-to-end\ntraining.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05747", "pdf": "https://arxiv.org/pdf/2504.05747", "abs": "https://arxiv.org/abs/2504.05747", "authors": ["Raymond Ng", "Thanh Ngan Nguyen", "Yuli Huang", "Ngee Chia Tai", "Wai Yi Leong", "Wei Qi Leong", "Xianbin Yong", "Jian Gang Ngui", "Yosephine Susanto", "Nicholas Cheng", "Hamsawardhini Rengarajan", "Peerat Limkonchotiwat", "Adithya Venkatadri Hulagadri", "Kok Wai Teng", "Yeo Yeow Tong", "Bryan Siow", "Wei Yi Teo", "Wayne Lau", "Choon Meng Tan", "Brandon Ong", "Zhi Hao Ong", "Jann Railey Montalan", "Adwin Chan", "Sajeban Antonyrex", "Ren Lee", "Esther Choa", "David Ong Tat-Wee", "Bing Jie Darius Liu", "William Chandra Tjhi", "Erik Cambria", "Leslie Teo"], "title": "SEA-LION: Southeast Asian Languages in One Network", "categories": ["cs.CL"], "comment": "We released our model at\n  https://huggingface.co/collections/aisingapore/sea-lionv3-672589a39cdadd6a5b199581", "summary": "Recently, Large Language Models (LLMs) have dominated much of the artificial\nintelligence scene with their ability to process and generate natural\nlanguages. However, the majority of LLM research and development remains\nEnglish-centric, leaving low-resource languages such as those in the Southeast\nAsian (SEA) region under-represented. To address this representation gap, we\nintroduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge\nmultilingual LLMs designed for SEA languages. The SEA-LION family of LLMs\nsupports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese,\nMalay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages\nlarge-scale multilingual continued pre-training with a comprehensive\npost-training regime involving multiple stages of instruction fine-tuning,\nalignment, and model merging. Evaluation results on multilingual benchmarks\nindicate that our models achieve state-of-the-art performance across LLMs\nsupporting SEA languages. We open-source the models to benefit the wider SEA\ncommunity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05599", "pdf": "https://arxiv.org/pdf/2504.05599", "abs": "https://arxiv.org/abs/2504.05599", "authors": ["Yi Peng", "Chris", "Xiaokun Wang", "Yichen Wei", "Jiangbo Pei", "Weijie Qiu", "Ai Jian", "Yunzhuo Hao", "Jiachun Pan", "Tianyidan Xie", "Li Ge", "Rongxian Zhuang", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce Skywork R1V, a multimodal reasoning model extending the an\nR1-series Large language models (LLM) to visual modalities via an efficient\nmultimodal transfer method. Leveraging a lightweight visual projector, Skywork\nR1V facilitates seamless multimodal adaptation without necessitating retraining\nof either the foundational language model or the vision encoder. To strengthen\nvisual-text alignment, we propose a hybrid optimization strategy that combines\nIterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO), significantly enhancing cross-modal integration efficiency.\nAdditionally, we introduce an adaptive-length Chain-of-Thought distillation\napproach for reasoning data generation. This approach dynamically optimizes\nreasoning chain lengths, thereby enhancing inference efficiency and preventing\nexcessive reasoning overthinking. Empirical evaluations demonstrate that\nSkywork R1V, with only 38B parameters, delivers competitive performance,\nachieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista.\nMeanwhile, it maintains robust textual reasoning performance, evidenced by\nimpressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model\nweights have been publicly released to promote openness and reproducibility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05764", "pdf": "https://arxiv.org/pdf/2504.05764", "abs": "https://arxiv.org/abs/2504.05764", "authors": ["Jiho Gwak", "Yuchul Jung"], "title": "Layer-Aware Embedding Fusion for LLMs in Text Classifications", "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "11 pages, 3 figures, Preprint", "summary": "Embedding fusion has emerged as an effective approach for enhancing\nperformance across various NLP tasks. However, systematic guidelines for\nselecting optimal layers and developing effective fusion strategies for the\nintegration of LLMs remain underexplored. In this study, we propose a\nlayer-aware embedding selection method and investigate how to quantitatively\nevaluate different layers to identify the most important ones for downstream\nNLP tasks, showing that the critical layers vary depending on the dataset. We\nalso explore how combining embeddings from multiple LLMs, without requiring\nmodel fine-tuning, can improve performance. Experiments on four English text\nclassification datasets (SST-2, MR, R8, and R52) demonstrate that different\nlayers in LLMs exhibit varying degrees of representational strength for\nclassification, and that combining embeddings from different models can enhance\nperformance if the models exhibit complementary characteristics. Additionally,\nwe discuss resources overhead (memory and inference time) to provide a balanced\nperspective on the real world feasibility of embedding fusion. Future work will\nexplore multilingual and domain specific datasets, as well as techniques for\nautomating layer selection, to improve both performance and scalability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05601", "pdf": "https://arxiv.org/pdf/2504.05601", "abs": "https://arxiv.org/abs/2504.05601", "authors": ["Zhenteng Li", "Sheng Lian", "Dengfeng Pan", "Youlin Wang", "Wei Liu"], "title": "AD-Det: Boosting Object Detection in UAV Images with Focused Small Objects and Balanced Tail Classes", "categories": ["cs.CV"], "comment": null, "summary": "Object detection in Unmanned Aerial Vehicle (UAV) images poses significant\nchallenges due to complex scale variations and class imbalance among objects.\nExisting methods often address these challenges separately, overlooking the\nintricate nature of UAV images and the potential synergy between them. In\nresponse, this paper proposes AD-Det, a novel framework employing a coherent\ncoarse-to-fine strategy that seamlessly integrates two pivotal components:\nAdaptive Small Object Enhancement (ASOE) and Dynamic Class-balanced Copy-paste\n(DCC). ASOE utilizes a high-resolution feature map to identify and cluster\nregions containing small objects. These regions are subsequently enlarged and\nprocessed by a fine-grained detector. On the other hand, DCC conducts\nobject-level resampling by dynamically pasting tail classes around the cluster\ncenters obtained by ASOE, main-taining a dynamic memory bank for each tail\nclass. This approach enables AD-Det to not only extract regions with small\nobjects for precise detection but also dynamically perform reasonable\nresampling for tail-class objects. Consequently, AD-Det enhances the overall\ndetection performance by addressing the challenges of scale variations and\nclass imbalance in UAV images through a synergistic and adaptive framework. We\nextensively evaluate our approach on two public datasets, i.e., VisDrone and\nUAVDT, and demonstrate that AD-Det significantly outperforms existing\ncompetitive alternatives. Notably, AD-Det achieves a 37.5% Average Precision\n(AP) on the VisDrone dataset, surpassing its counterparts by at least 3.1%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05824", "pdf": "https://arxiv.org/pdf/2504.05824", "abs": "https://arxiv.org/abs/2504.05824", "authors": ["Zhang Dong", "Songhang deng", "Mingbang Wang", "Le Dai", "Jiyuan Li", "Xingzu Liu", "Ruilin Nong"], "title": "End-to-End Dialog Neural Coreference Resolution: Balancing Efficiency and Accuracy in Large-Scale Systems", "categories": ["cs.CL"], "comment": "submission of acl 2025", "summary": "Large-scale coreference resolution presents a significant challenge in\nnatural language processing, necessitating a balance between efficiency and\naccuracy. In response to this challenge, we introduce an End-to-End Neural\nCoreference Resolution system tailored for large-scale applications. Our system\nefficiently identifies and resolves coreference links in text, ensuring minimal\ncomputational overhead without compromising on performance. By utilizing\nadvanced neural network architectures, we incorporate various contextual\nembeddings and attention mechanisms, which enhance the quality of predictions\nfor coreference pairs. Furthermore, we apply optimization strategies to\naccelerate processing speeds, making the system suitable for real-world\ndeployment. Extensive evaluations conducted on benchmark datasets demonstrate\nthat our model achieves improved accuracy compared to existing approaches,\nwhile effectively maintaining rapid inference times. Rigorous testing confirms\nthe ability of our system to deliver precise coreference resolutions\nefficiently, thereby establishing a benchmark for future advancements in this\nfield.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05995", "pdf": "https://arxiv.org/pdf/2504.05995", "abs": "https://arxiv.org/abs/2504.05995", "authors": ["Firoj Alam", "Md Arid Hasan", "Sahinur Rahman Laskar", "Mucahid Kutlu", "Shammur Absar Chowdhury"], "title": "NativQA Framework: Enabling LLMs with Native, Local, and Everyday Knowledge", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "comment": "LLMs, Native, Multilingual, Language Diversity, Contextual\n  Understanding, Minority Languages, Culturally Informed, Foundation Models,\n  Large Language Models", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout cultural bias, fairness, and their applicability in diverse linguistic\nand underrepresented regional contexts. To enhance and benchmark the\ncapabilities of LLMs, there is a need to develop large-scale resources focused\non multilingual, local, and cultural contexts. In this study, we propose a\nframework, NativQA, that can seamlessly construct large-scale, culturally and\nregionally aligned QA datasets in native languages. The framework utilizes\nuser-defined seed queries and leverages search engines to collect\nlocation-specific, everyday information. It has been evaluated across 39\nlocations in 24 countries and in 7 languages, ranging from extremely\nlow-resource to high-resource languages, which resulted over 300K Question\nAnswer (QA) pairs. The developed resources can be used for LLM benchmarking and\nfurther fine-tuning. The framework has been made publicly available for the\ncommunity (https://gitlab.com/nativqa/nativqa-framework).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06011", "pdf": "https://arxiv.org/pdf/2504.06011", "abs": "https://arxiv.org/abs/2504.06011", "authors": ["Monojit Choudhury", "Shivam Chauhan", "Rocktim Jyoti Das", "Dhruv Sahnan", "Xudong Han", "Haonan Li", "Aaryamonvikram Singh", "Alok Anil Jadhav", "Utkarsh Agarwal", "Mukund Choudhary", "Debopriyo Banerjee", "Fajri Koto", "Junaid Bhat", "Awantika Shukla", "Samujjwal Ghosh", "Samta Kamboj", "Onkar Pandit", "Lalit Pradhan", "Rahul Pal", "Sunil Sahu", "Soundar Doraiswamy", "Parvez Mullah", "Ali El Filali", "Neha Sengupta", "Gokul Ramakrishnan", "Rituraj Joshi", "Gurpreet Gosal", "Avraham Sheinin", "Natalia Vassilieva", "Preslav Nakov"], "title": "Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for Hindi", "categories": ["cs.CL"], "comment": null, "summary": "Developing high-quality large language models (LLMs) for moderately resourced\nlanguages presents unique challenges in data availability, model adaptation,\nand evaluation. We introduce Llama-3-Nanda-10B-Chat, or Nanda for short, a\nstate-of-the-art Hindi-centric instruction-tuned generative LLM, designed to\npush the boundaries of open-source Hindi language models. Built upon\nLlama-3-8B, Nanda incorporates continuous pre-training with expanded\ntransformer blocks, leveraging the Llama Pro methodology. A key challenge was\nthe limited availability of high-quality Hindi text data; we addressed this\nthrough rigorous data curation, augmentation, and strategic bilingual training,\nbalancing Hindi and English corpora to optimize cross-linguistic knowledge\ntransfer. With 10 billion parameters, Nanda stands among the top-performing\nopen-source Hindi and multilingual models of similar scale, demonstrating\nsignificant advantages over many existing models. We provide an in-depth\ndiscussion of training strategies, fine-tuning techniques, safety alignment,\nand evaluation metrics, demonstrating how these approaches enabled Nanda to\nachieve state-of-the-art results. By open-sourcing Nanda, we aim to advance\nresearch in Hindi LLMs and support a wide range of real-world applications\nacross academia, industry, and public services.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06036", "pdf": "https://arxiv.org/pdf/2504.06036", "abs": "https://arxiv.org/abs/2504.06036", "authors": ["Qitong Wang", "Mohammed J. Zaki", "Georgios Kollias", "Vasileios Kalantzis"], "title": "Multi-Sense Embeddings for Language Models and Knowledge Distillation", "categories": ["cs.CL"], "comment": "16 pages, 4 figures", "summary": "Transformer-based large language models (LLMs) rely on contextual embeddings\nwhich generate different (continuous) representations for the same token\ndepending on its surrounding context. Nonetheless, words and tokens typically\nhave a limited number of senses (or meanings). We propose multi-sense\nembeddings as a drop-in replacement for each token in order to capture the\nrange of their uses in a language. To construct a sense embedding dictionary,\nwe apply a clustering algorithm to embeddings generated by an LLM and consider\nthe cluster centers as representative sense embeddings. In addition, we propose\na novel knowledge distillation method that leverages the sense dictionary to\nlearn a smaller student model that mimics the senses from the much larger base\nLLM model, offering significant space and inference time savings, while\nmaintaining competitive performance. Via thorough experiments on various\nbenchmarks, we showcase the effectiveness of our sense embeddings and knowledge\ndistillation approach. We share our code at\nhttps://github.com/Qitong-Wang/SenseDict", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06160", "pdf": "https://arxiv.org/pdf/2504.06160", "abs": "https://arxiv.org/abs/2504.06160", "authors": ["Rijul Magu", "Arka Dutta", "Sean Kim", "Ashiqur R. KhudaBukhsh", "Munmun De Choudhury"], "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI", "J.4; K.4.1; K.4.2"], "comment": null, "summary": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05706", "pdf": "https://arxiv.org/pdf/2504.05706", "abs": "https://arxiv.org/abs/2504.05706", "authors": ["Fida Mohammad Thoker", "Letian Jiang", "Chen Zhao", "Piyush Bagad", "Hazel Doughty", "Bernard Ghanem", "Cees G. M. Snoek"], "title": "SEVERE++: Evaluating Benchmark Sensitivity in Generalization of Video Representation Learning", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Continued advances in self-supervised learning have led to significant\nprogress in video representation learning, offering a scalable alternative to\nsupervised approaches by removing the need for manual annotations. Despite\nstrong performance on standard action recognition benchmarks, video\nself-supervised learning methods are largely evaluated under narrow protocols,\ntypically pretraining on Kinetics-400 and fine-tuning on similar datasets,\nlimiting our understanding of their generalization in real world scenarios. In\nthis work, we present a comprehensive evaluation of modern video\nself-supervised models, focusing on generalization across four key downstream\nfactors: domain shift, sample efficiency, action granularity, and task\ndiversity. Building on our prior work analyzing benchmark sensitivity in\nCNN-based contrastive learning, we extend the study to cover state-of-the-art\ntransformer-based video-only and video-text models. Specifically, we benchmark\n12 transformer-based methods (7 video-only, 5 video-text) and compare them to\n10 CNN-based methods, totaling over 1100 experiments across 8 datasets and 7\ndownstream tasks. Our analysis shows that, despite architectural advances,\ntransformer-based models remain sensitive to downstream conditions. No method\ngeneralizes consistently across all factors, video-only transformers perform\nbetter under domain shifts, CNNs outperform for fine-grained tasks, and\nvideo-text models often underperform despite large scale pretraining. We also\nfind that recent transformer models do not consistently outperform earlier\napproaches. Our findings provide a detailed view of the strengths and\nlimitations of current video SSL methods and offer a unified benchmark for\nevaluating generalization in video representation learning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05774", "pdf": "https://arxiv.org/pdf/2504.05774", "abs": "https://arxiv.org/abs/2504.05774", "authors": ["Enming Zhang", "Zhengyu Li", "Yanru Wu", "Jingge Wang", "Yang Tan", "Ruizhe Zhao", "Guan Wang", "Yang Li"], "title": "Transferable Mask Transformer: Cross-domain Semantic Segmentation with Region-adaptive Transferability Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in Vision Transformers (ViTs) have set new benchmarks in\nsemantic segmentation. However, when adapting pretrained ViTs to new target\ndomains, significant performance degradation often occurs due to distribution\nshifts, resulting in suboptimal global attention. Since self-attention\nmechanisms are inherently data-driven, they may fail to effectively attend to\nkey objects when source and target domains exhibit differences in texture,\nscale, or object co-occurrence patterns. While global and patch-level domain\nadaptation methods provide partial solutions, region-level adaptation with\ndynamically shaped regions is crucial due to spatial heterogeneity in\ntransferability across different image areas. We present Transferable Mask\nTransformer (TMT), a novel region-level adaptation framework for semantic\nsegmentation that aligns cross-domain representations through spatial\ntransferability analysis. TMT consists of two key components: (1) An Adaptive\nCluster-based Transferability Estimator (ACTE) that dynamically segments images\ninto structurally and semantically coherent regions for localized\ntransferability assessment, and (2) A Transferable Masked Attention (TMA)\nmodule that integrates region-specific transferability maps into ViTs'\nattention mechanisms, prioritizing adaptation in regions with low\ntransferability and high semantic uncertainty. Comprehensive evaluations across\n20 cross-domain pairs demonstrate TMT's superiority, achieving an average 2%\nMIoU improvement over vanilla fine-tuning and a 1.28% increase compared to\nstate-of-the-art baselines. The source code will be publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05419", "pdf": "https://arxiv.org/pdf/2504.05419", "abs": "https://arxiv.org/abs/2504.05419", "authors": ["Anqi Zhang", "Yulin Chen", "Jane Pan", "Chen Zhao", "Aurojit Panda", "Jinyang Li", "He He"], "title": "Reasoning Models Know When They're Right: Probing Hidden States for Self-Verification", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning models have achieved remarkable performance on tasks like math and\nlogical reasoning thanks to their ability to search during reasoning. However,\nthey still suffer from overthinking, often performing unnecessary reasoning\nsteps even after reaching the correct answer. This raises the question: can\nmodels evaluate the correctness of their intermediate answers during reasoning?\nIn this work, we study whether reasoning models encode information about answer\ncorrectness through probing the model's hidden states. The resulting probe can\nverify intermediate answers with high accuracy and produces highly calibrated\nscores. Additionally, we find models' hidden states encode correctness of\nfuture answers, enabling early prediction of the correctness before the\nintermediate answer is fully formulated. We then use the probe as a verifier to\ndecide whether to exit reasoning at intermediate answers during inference,\nreducing the number of inference tokens by 24\\% without compromising\nperformance. These findings confirm that reasoning models do encode a notion of\ncorrectness yet fail to exploit it, revealing substantial untapped potential to\nenhance their efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-verification"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05794", "pdf": "https://arxiv.org/pdf/2504.05794", "abs": "https://arxiv.org/abs/2504.05794", "authors": ["Leiye Liu", "Miao Zhang", "Jihao Yin", "Tingwei Liu", "Wei Ji", "Yongri Piao", "Huchuan Lu"], "title": "DefMamba: Deformable Visual State Space Model", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Recently, state space models (SSM), particularly Mamba, have attracted\nsignificant attention from scholars due to their ability to effectively balance\ncomputational efficiency and performance. However, most existing visual Mamba\nmethods flatten images into 1D sequences using predefined scan orders, which\nresults the model being less capable of utilizing the spatial structural\ninformation of the image during the feature extraction process. To address this\nissue, we proposed a novel visual foundation model called DefMamba. This model\nincludes a multi-scale backbone structure and deformable mamba (DM) blocks,\nwhich dynamically adjust the scanning path to prioritize important information,\nthus enhancing the capture and processing of relevant input features. By\ncombining a deformable scanning(DS) strategy, this model significantly improves\nits ability to learn image structures and detects changes in object details.\nNumerous experiments have shown that DefMamba achieves state-of-the-art\nperformance in various visual tasks, including image classification, object\ndetection, instance segmentation, and semantic segmentation. The code is open\nsource on DefMamba.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05800", "pdf": "https://arxiv.org/pdf/2504.05800", "abs": "https://arxiv.org/abs/2504.05800", "authors": ["Jaskirat Singh", "Junshen Kevin Chen", "Jonas Kohler", "Michael Cohen"], "title": "Storybooth: Training-free Multi-Subject Consistency for Improved Visual Storytelling", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Training-free consistent text-to-image generation depicting the same subjects\nacross different images is a topic of widespread recent interest. Existing\nworks in this direction predominantly rely on cross-frame self-attention; which\nimproves subject-consistency by allowing tokens in each frame to pay attention\nto tokens in other frames during self-attention computation. While useful for\nsingle subjects, we find that it struggles when scaling to multiple characters.\nIn this work, we first analyze the reason for these limitations. Our\nexploration reveals that the primary-issue stems from self-attention-leakage,\nwhich is exacerbated when trying to ensure consistency across\nmultiple-characters. This happens when tokens from one subject pay attention to\nother characters, causing them to appear like each other (e.g., a dog appearing\nlike a duck). Motivated by these findings, we propose StoryBooth: a\ntraining-free approach for improving multi-character consistency. In\nparticular, we first leverage multi-modal chain-of-thought reasoning and\nregion-based generation to apriori localize the different subjects across the\ndesired story outputs. The final outputs are then generated using a modified\ndiffusion model which consists of two novel layers: 1) a bounded cross-frame\nself-attention layer for reducing inter-character attention leakage, and 2)\ntoken-merging layer for improving consistency of fine-grain subject details.\nThrough both qualitative and quantitative results we find that the proposed\napproach surpasses prior state-of-the-art, exhibiting improved consistency\nacross both multiple-characters and fine-grain subject details.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05599", "pdf": "https://arxiv.org/pdf/2504.05599", "abs": "https://arxiv.org/abs/2504.05599", "authors": ["Yi Peng", "Chris", "Xiaokun Wang", "Yichen Wei", "Jiangbo Pei", "Weijie Qiu", "Ai Jian", "Yunzhuo Hao", "Jiachun Pan", "Tianyidan Xie", "Li Ge", "Rongxian Zhuang", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce Skywork R1V, a multimodal reasoning model extending the an\nR1-series Large language models (LLM) to visual modalities via an efficient\nmultimodal transfer method. Leveraging a lightweight visual projector, Skywork\nR1V facilitates seamless multimodal adaptation without necessitating retraining\nof either the foundational language model or the vision encoder. To strengthen\nvisual-text alignment, we propose a hybrid optimization strategy that combines\nIterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO), significantly enhancing cross-modal integration efficiency.\nAdditionally, we introduce an adaptive-length Chain-of-Thought distillation\napproach for reasoning data generation. This approach dynamically optimizes\nreasoning chain lengths, thereby enhancing inference efficiency and preventing\nexcessive reasoning overthinking. Empirical evaluations demonstrate that\nSkywork R1V, with only 38B parameters, delivers competitive performance,\nachieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista.\nMeanwhile, it maintains robust textual reasoning performance, evidenced by\nimpressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model\nweights have been publicly released to promote openness and reproducibility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05605", "pdf": "https://arxiv.org/pdf/2504.05605", "abs": "https://arxiv.org/abs/2504.05605", "authors": ["Gejian Zhao", "Hanzhou Wu", "Xinpeng Zhang", "Athanasios V. Vasilakos"], "title": "ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs", "categories": ["cs.CR", "cs.CL"], "comment": "Zhao et al., 16 pages, 2025, uploaded by Hanzhou Wu, Shanghai\n  University", "summary": "Chain-of-Thought (CoT) enhances an LLM's ability to perform complex reasoning\ntasks, but it also introduces new security issues. In this work, we present\nShadowCoT, a novel backdoor attack framework that targets the internal\nreasoning mechanism of LLMs. Unlike prior token-level or prompt-based attacks,\nShadowCoT directly manipulates the model's cognitive reasoning path, enabling\nit to hijack multi-step reasoning chains and produce logically coherent but\nadversarial outcomes. By conditioning on internal reasoning states, ShadowCoT\nlearns to recognize and selectively disrupt key reasoning steps, effectively\nmounting a self-reflective cognitive attack within the target model. Our\napproach introduces a lightweight yet effective multi-stage injection pipeline,\nwhich selectively rewires attention pathways and perturbs intermediate\nrepresentations with minimal parameter overhead (only 0.15% updated). ShadowCoT\nfurther leverages reinforcement learning and reasoning chain pollution (RCP) to\nautonomously synthesize stealthy adversarial CoTs that remain undetectable to\nadvanced defenses. Extensive experiments across diverse reasoning benchmarks\nand LLMs show that ShadowCoT consistently achieves high Attack Success Rate\n(94.4%) and Hijacking Success Rate (88.4%) while preserving benign performance.\nThese results reveal an emergent class of cognition-level threats and highlight\nthe urgent need for defenses beyond shallow surface-level consistency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05830", "pdf": "https://arxiv.org/pdf/2504.05830", "abs": "https://arxiv.org/abs/2504.05830", "authors": ["Shiao Wang", "Xiao Wang", "Bo Jiang", "Lin Zhu", "Guoqi Li", "Yaowei Wang", "Yonghong Tian", "Jin Tang"], "title": "Human Activity Recognition using RGB-Event based Sensors: A Multi-modal Heat Conduction Model and A Benchmark Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "Journal Extension of HARDVS (AAAI 2024)", "summary": "Human Activity Recognition (HAR) primarily relied on traditional RGB cameras\nto achieve high-performance activity recognition. However, the challenging\nfactors in real-world scenarios, such as insufficient lighting and rapid\nmovements, inevitably degrade the performance of RGB cameras. To address these\nchallenges, biologically inspired event cameras offer a promising solution to\novercome the limitations of traditional RGB cameras. In this work, we rethink\nhuman activity recognition by combining the RGB and event cameras. The first\ncontribution is the proposed large-scale multi-modal RGB-Event human activity\nrecognition benchmark dataset, termed HARDVS 2.0, which bridges the dataset\ngaps. It contains 300 categories of everyday real-world actions with a total of\n107,646 paired videos covering various challenging scenarios. Inspired by the\nphysics-informed heat conduction model, we propose a novel multi-modal heat\nconduction operation framework for effective activity recognition, termed\nMMHCO-HAR. More in detail, given the RGB frames and event streams, we first\nextract the feature embeddings using a stem network. Then, multi-modal Heat\nConduction blocks are designed to fuse the dual features, the key module of\nwhich is the multi-modal Heat Conduction Operation layer. We integrate RGB and\nevent embeddings through a multi-modal DCT-IDCT layer while adaptively\nincorporating the thermal conductivity coefficient via FVEs into this module.\nAfter that, we propose an adaptive fusion module based on a policy routing\nstrategy for high-performance classification. Comprehensive experiments\ndemonstrate that our method consistently performs well, validating its\neffectiveness and robustness. The source code and benchmark dataset will be\nreleased on https://github.com/Event-AHU/HARDVS/tree/HARDVSv2", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06261", "pdf": "https://arxiv.org/pdf/2504.06261", "abs": "https://arxiv.org/abs/2504.06261", "authors": ["Gleb Rodionov", "Roman Garipov", "Alina Shutova", "George Yakushev", "Vage Egiazarian", "Anton Sinitsin", "Denis Kuznedelev", "Dan Alistarh"], "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention", "categories": ["cs.LG", "cs.CL"], "comment": "Preprint, work in progress", "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05977", "pdf": "https://arxiv.org/pdf/2504.05977", "abs": "https://arxiv.org/abs/2504.05977", "authors": ["Jakob Lønborg Christensen", "Morten Rieger Hannemose", "Anders Bjorholm Dahl", "Vedrana Andersen Dahl"], "title": "Diffusion Based Ambiguous Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted at SCIA25", "summary": "Medical image segmentation often involves inherent uncertainty due to\nvariations in expert annotations. Capturing this uncertainty is an important\ngoal and previous works have used various generative image models for the\npurpose of representing the full distribution of plausible expert ground\ntruths. In this work, we explore the design space of diffusion models for\ngenerative segmentation, investigating the impact of noise schedules,\nprediction types, and loss weightings. Notably, we find that making the noise\nschedule harder with input scaling significantly improves performance. We\nconclude that x- and v-prediction outperform epsilon-prediction, likely because\nthe diffusion process is in the discrete segmentation domain. Many loss\nweightings achieve similar performance as long as they give enough weight to\nthe end of the diffusion process. We base our experiments on the LIDC-IDRI lung\nlesion dataset and obtain state-of-the-art (SOTA) performance. Additionally, we\nintroduce a randomly cropped variant of the LIDC-IDRI dataset that is better\nsuited for uncertainty in image segmentation. Our model also achieves SOTA in\nthis harder setting.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05979", "pdf": "https://arxiv.org/pdf/2504.05979", "abs": "https://arxiv.org/abs/2504.05979", "authors": ["Sixiang Chen", "Jinbin Bai", "Zhuoran Zhao", "Tian Ye", "Qingyu Shi", "Donghao Zhou", "Wenhao Chai", "Xin Lin", "Jianzong Wu", "Chao Tang", "Shilin Xu", "Tao Zhang", "Haobo Yuan", "Yikang Zhou", "Wei Chow", "Linfeng Li", "Xiangtai Li", "Lei Zhu", "Lu Qi"], "title": "An Empirical Study of GPT-4o Image Generation Capabilities", "categories": ["cs.CV"], "comment": null, "summary": "The landscape of image generation has rapidly evolved, from early GAN-based\napproaches to diffusion models and, most recently, to unified generative\narchitectures that seek to bridge understanding and generation tasks. Recent\nadvances, especially the GPT-4o, have demonstrated the feasibility of\nhigh-fidelity multimodal generation, their architectural design remains\nmysterious and unpublished. This prompts the question of whether image and text\ngeneration have already been successfully integrated into a unified framework\nfor those methods. In this work, we conduct an empirical study of GPT-4o's\nimage generation capabilities, benchmarking it against leading open-source and\ncommercial models. Our evaluation covers four main categories, including\ntext-to-image, image-to-image, image-to-3D, and image-to-X generation, with\nmore than 20 tasks. Our analysis highlights the strengths and limitations of\nGPT-4o under various settings, and situates it within the broader evolution of\ngenerative modeling. Through this investigation, we identify promising\ndirections for future unified generative models, emphasizing the role of\narchitectural design and data scaling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06010", "pdf": "https://arxiv.org/pdf/2504.06010", "abs": "https://arxiv.org/abs/2504.06010", "authors": ["Stefanos-Iordanis Papadopoulos", "Christos Koutlis", "Symeon Papadopoulos", "Panagiotis C. Petrantonakis"], "title": "Latent Multimodal Reconstruction for Misinformation Detection", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Multimodal misinformation, such as miscaptioned images, where captions\nmisrepresent an image's origin, context, or meaning, poses a growing challenge\nin the digital age. To support fact-checkers, researchers have been focusing on\ncreating datasets and developing methods for multimodal misinformation\ndetection (MMD). Due to the scarcity of large-scale annotated MMD datasets,\nrecent studies leverage synthetic training data via out-of-context\nimage-caption pairs or named entity manipulations; altering names, dates, and\nlocations. However, these approaches often produce simplistic misinformation\nthat fails to reflect real-world complexity, limiting the robustness of\ndetection models trained on them. Meanwhile, despite recent advancements, Large\nVision-Language Models (LVLMs) remain underutilized for generating diverse,\nrealistic synthetic training data for MMD. To address this gap, we introduce\n\"MisCaption This!\", a training dataset comprising LVLM-generated miscaptioned\nimages. Additionally, we introduce \"Latent Multimodal Reconstruction\" (LAMAR),\na network trained to reconstruct the embeddings of truthful captions, providing\na strong auxiliary signal to the detection process. To optimize LAMAR, we\nexplore different training strategies (end-to-end training and large-scale\npre-training) and integration approaches (direct, mask, gate, and attention).\nExtensive experiments show that models trained on \"MisCaption This!\" generalize\nbetter on real-world misinformation, while LAMAR sets new state-of-the-art on\nboth NewsCLIPpings and VERITE benchmarks; highlighting the potential of\nLVLM-generated data and reconstruction-based approaches for advancing MMD. We\nrelease our code at:\nhttps://github.com/stevejpapad/miscaptioned-image-reconstruction", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06021", "pdf": "https://arxiv.org/pdf/2504.06021", "abs": "https://arxiv.org/abs/2504.06021", "authors": ["Dahyun Kang", "Ahmet Iscen", "Eunchan Jo", "Sua Choi", "Minsu Cho", "Cordelia Schmid"], "title": "Memory-Modular Classification: Learning to Generalize with Memory Replacement", "categories": ["cs.CV"], "comment": "Accepted to TMLR. Code available: https://github.com/dahyun-kang/mml", "summary": "We propose a novel memory-modular learner for image classification that\nseparates knowledge memorization from reasoning. Our model enables effective\ngeneralization to new classes by simply replacing the memory contents, without\nthe need for model retraining. Unlike traditional models that encode both world\nknowledge and task-specific skills into their weights during training, our\nmodel stores knowledge in the external memory of web-crawled image and text\ndata. At inference time, the model dynamically selects relevant content from\nthe memory based on the input image, allowing it to adapt to arbitrary classes\nby simply replacing the memory contents. The key differentiator that our\nlearner meta-learns to perform classification tasks with noisy web data from\nunseen classes, resulting in robust performance across various classification\nscenarios. Experimental results demonstrate the promising performance and\nversatility of our approach in handling diverse classification tasks, including\nzero-shot/few-shot classification of unseen classes, fine-grained\nclassification, and class-incremental classification.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06131", "pdf": "https://arxiv.org/pdf/2504.06131", "abs": "https://arxiv.org/abs/2504.06131", "authors": ["Sudipta Banerjee", "Anubhav Jain", "Chinmay Hegde", "Nasir Memon"], "title": "FaceCloak: Learning to Protect Face Templates", "categories": ["cs.CV"], "comment": "Accepted in IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG 2025)", "summary": "Generative models can reconstruct face images from encoded representations\n(templates) bearing remarkable likeness to the original face raising security\nand privacy concerns. We present FaceCloak, a neural network framework that\nprotects face templates by generating smart, renewable binary cloaks. Our\nmethod proactively thwarts inversion attacks by cloaking face templates with\nunique disruptors synthesized from a single face template on the fly while\nprovably retaining biometric utility and unlinkability. Our cloaked templates\ncan suppress sensitive attributes while generalizing to novel feature\nextraction schemes and outperforms leading baselines in terms of biometric\nmatching and resiliency to reconstruction attacks. FaceCloak-based matching is\nextremely fast (inference time cost=0.28ms) and light-weight (0.57MB).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06144", "pdf": "https://arxiv.org/pdf/2504.06144", "abs": "https://arxiv.org/abs/2504.06144", "authors": ["Jihun Park", "Jongmin Gim", "Kyoungmin Lee", "Minseok Oh", "Minwoo Choi", "Jaeyeul Kim", "Woo Chool Park", "Sunghoon Im"], "title": "A Training-Free Style-aligned Image Generation with Scale-wise Autoregressive Model", "categories": ["cs.CV"], "comment": "17 pages, 15 figures", "summary": "We present a training-free style-aligned image generation method that\nleverages a scale-wise autoregressive model. While large-scale text-to-image\n(T2I) models, particularly diffusion-based methods, have demonstrated\nimpressive generation quality, they often suffer from style misalignment across\ngenerated image sets and slow inference speeds, limiting their practical\nusability. To address these issues, we propose three key components: initial\nfeature replacement to ensure consistent background appearance, pivotal feature\ninterpolation to align object placement, and dynamic style injection, which\nreinforces style consistency using a schedule function. Unlike previous methods\nrequiring fine-tuning or additional training, our approach maintains fast\ninference while preserving individual content details. Extensive experiments\nshow that our method achieves generation quality comparable to competing\napproaches, significantly improves style alignment, and delivers inference\nspeeds over six times faster than the fastest model.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06153", "pdf": "https://arxiv.org/pdf/2504.06153", "abs": "https://arxiv.org/abs/2504.06153", "authors": ["Akash Kumar", "Ashlesha Kumar", "Vibhav Vineet", "Yogesh S Rawat"], "title": "A Large-Scale Analysis on Contextual Self-Supervised Video Representation Learning", "categories": ["cs.CV"], "comment": "CVPR'25 Workshop: 6th Data-Efficient Workshop", "summary": "Self-supervised learning has emerged as a powerful paradigm for label-free\nmodel pretraining, particularly in the video domain, where manual annotation is\ncostly and time-intensive. However, existing self-supervised approaches employ\ndiverse experimental setups, making direct comparisons challenging due to the\nabsence of a standardized benchmark. In this work, we establish a unified\nbenchmark that enables fair comparisons across different methods. Additionally,\nwe systematically investigate five critical aspects of self-supervised learning\nin videos: (1) dataset size, (2) model complexity, (3) data distribution, (4)\ndata noise, and (5) feature representations. To facilitate this study, we\nevaluate six self-supervised learning methods across six network architectures,\nconducting extensive experiments on five benchmark datasets and assessing\nperformance on two distinct downstream tasks. Our analysis reveals key insights\ninto the interplay between pretraining strategies, dataset characteristics,\npretext tasks, and model architectures. Furthermore, we extend these findings\nto Video Foundation Models (ViFMs), demonstrating their relevance in\nlarge-scale video representation learning. Finally, leveraging these insights,\nwe propose a novel approach that significantly reduces training data\nrequirements while surpassing state-of-the-art methods that rely on 10% more\npretraining data. We believe this work will guide future research toward a\ndeeper understanding of self-supervised video representation learning and its\nbroader implications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06178", "pdf": "https://arxiv.org/pdf/2504.06178", "abs": "https://arxiv.org/abs/2504.06178", "authors": ["Yujia Hu", "Songhua Liu", "Xingyi Yang", "Xinchao Wang"], "title": "Flash Sculptor: Modular 3D Worlds from Objects", "categories": ["cs.CV"], "comment": null, "summary": "Existing text-to-3D and image-to-3D models often struggle with complex scenes\ninvolving multiple objects and intricate interactions. Although some recent\nattempts have explored such compositional scenarios, they still require an\nextensive process of optimizing the entire layout, which is highly cumbersome\nif not infeasible at all. To overcome these challenges, we propose Flash\nSculptor in this paper, a simple yet effective framework for compositional 3D\nscene/object reconstruction from a single image. At the heart of Flash Sculptor\nlies a divide-and-conquer strategy, which decouples compositional scene\nreconstruction into a sequence of sub-tasks, including handling the appearance,\nrotation, scale, and translation of each individual instance. Specifically, for\nrotation, we introduce a coarse-to-fine scheme that brings the best of both\nworlds--efficiency and accuracy--while for translation, we develop an\noutlier-removal-based algorithm that ensures robust and precise parameters in a\nsingle step, without any iterative optimization. Extensive experiments\ndemonstrate that Flash Sculptor achieves at least a 3 times speedup over\nexisting compositional 3D methods, while setting new benchmarks in\ncompositional 3D reconstruction performance. Codes are available at\nhttps://github.com/YujiaHu1109/Flash-Sculptor.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05316", "pdf": "https://arxiv.org/pdf/2504.05316", "abs": "https://arxiv.org/abs/2504.05316", "authors": ["Yinan Zhou", "Yaxiong Wang", "Haokun Lin", "Chen Ma", "Li Zhu", "Zhedong Zheng"], "title": "Scale Up Composed Image Retrieval Learning via Modification Text Generation", "categories": ["cs.IR", "cs.AI", "cs.CV"], "comment": "12 pages, 8 figures", "summary": "Composed Image Retrieval (CIR) aims to search an image of interest using a\ncombination of a reference image and modification text as the query. Despite\nrecent advancements, this task remains challenging due to limited training data\nand laborious triplet annotation processes. To address this issue, this paper\nproposes to synthesize the training triplets to augment the training resource\nfor the CIR problem. Specifically, we commence by training a modification text\ngenerator exploiting large-scale multimodal models and scale up the CIR\nlearning throughout both the pretraining and fine-tuning stages. During\npretraining, we leverage the trained generator to directly create Modification\nText-oriented Synthetic Triplets(MTST) conditioned on pairs of images. For\nfine-tuning, we first synthesize reverse modification text to connect the\ntarget image back to the reference image. Subsequently, we devise a two-hop\nalignment strategy to incrementally close the semantic gap between the\nmultimodal pair and the target image. We initially learn an implicit prototype\nutilizing both the original triplet and its reversed version in a cycle manner,\nfollowed by combining the implicit prototype feature with the modification text\nto facilitate accurate alignment with the target image. Extensive experiments\nvalidate the efficacy of the generated triplets and confirm that our proposed\nmethodology attains competitive recall on both the CIRR and FashionIQ\nbenchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05692", "pdf": "https://arxiv.org/pdf/2504.05692", "abs": "https://arxiv.org/abs/2504.05692", "authors": ["Songyan Zhang", "Yongtao Ge", "Jinyuan Tian", "Guangkai Xu", "Hao Chen", "Chen Lv", "Chunhua Shen"], "title": "POMATO: Marrying Pointmap Matching with Temporal Motion for Dynamic 3D Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "code: https://github.com/wyddmw/POMATO", "summary": "3D reconstruction in dynamic scenes primarily relies on the combination of\ngeometry estimation and matching modules where the latter task is pivotal for\ndistinguishing dynamic regions which can help to mitigate the interference\nintroduced by camera and object motion. Furthermore, the matching module\nexplicitly models object motion, enabling the tracking of specific targets and\nadvancing motion understanding in complex scenarios. Recently, the proposed\nrepresentation of pointmap in DUSt3R suggests a potential solution to unify\nboth geometry estimation and matching in 3D space, but it still struggles with\nambiguous matching in dynamic regions, which may hamper further improvement. In\nthis work, we present POMATO, a unified framework for dynamic 3D reconstruction\nby marrying pointmap matching with temporal motion. Specifically, our method\nfirst learns an explicit matching relationship by mapping RGB pixels from both\ndynamic and static regions across different views to 3D pointmaps within a\nunified coordinate system. Furthermore, we introduce a temporal motion module\nfor dynamic motions that ensures scale consistency across different frames and\nenhances performance in tasks requiring both precise geometry and reliable\nmatching, most notably 3D point tracking. We show the effectiveness of the\nproposed pointmap matching and temporal fusion paradigm by demonstrating the\nremarkable performance across multiple downstream tasks, including video depth\nestimation, 3D point tracking, and pose estimation. Code and models are\npublicly available at https://github.com/wyddmw/POMATO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05740", "pdf": "https://arxiv.org/pdf/2504.05740", "abs": "https://arxiv.org/abs/2504.05740", "authors": ["Jee Won Lee", "Hansol Lim", "Sooyeun Yang", "Jongseong Choi"], "title": "Micro-splatting: Maximizing Isotropic Constraints for Refined Optimization in 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advancements in 3D Gaussian Splatting have achieved impressive\nscalability and real-time rendering for large-scale scenes but often fall short\nin capturing fine-grained details. Conventional approaches that rely on\nrelatively large covariance parameters tend to produce blurred representations,\nwhile directly reducing covariance sizes leads to sparsity. In this work, we\nintroduce Micro-splatting (Maximizing Isotropic Constraints for Refined\nOptimization in 3D Gaussian Splatting), a novel framework designed to overcome\nthese limitations. Our approach leverages a covariance regularization term to\npenalize excessively large Gaussians to ensure each splat remains compact and\nisotropic. This work implements an adaptive densification strategy that\ndynamically refines regions with high image gradients by lowering the splitting\nthreshold, followed by loss function enhancement. This strategy results in a\ndenser and more detailed gaussian means where needed, without sacrificing\nrendering efficiency. Quantitative evaluations using metrics such as L1, L2,\nPSNR, SSIM, and LPIPS, alongside qualitative comparisons demonstrate that our\nmethod significantly enhances fine-details in 3D reconstructions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05990", "pdf": "https://arxiv.org/pdf/2504.05990", "abs": "https://arxiv.org/abs/2504.05990", "authors": ["Heather M. Whitney", "Hui Li", "Karen Drukker", "Elbert Huang", "Maryellen L. Giger"], "title": "AI analysis of medical images at scale as a health disparities probe: a feasibility demonstration using chest radiographs", "categories": ["physics.med-ph", "cs.CV"], "comment": "21 pages, 4 figures", "summary": "Health disparities (differences in non-genetic conditions that influence\nhealth) can be associated with differences in burden of disease by groups\nwithin a population. Social determinants of health (SDOH) are domains such as\nhealth care access, dietary access, and economics frequently studied for\npotential association with health disparities. Evaluating SDOH-related\nphenotypes using routine medical images as data sources may enhance health\ndisparities research. We developed a pipeline for using quantitative measures\nautomatically extracted from medical images as inputs into health disparities\nindex calculations. Our study focused on the use case of two SDOH demographic\ncorrelates (sex and race) and data extracted from chest radiographs of 1,571\nunique patients. The likelihood of severe disease within the lung parenchyma\nfrom each image type, measured using an established deep learning model, was\nmerged into a single numerical image-based phenotype for each patient. Patients\nwere then separated into phenogroups by unsupervised clustering of the\nimage-based phenotypes. The health rate for each phenogroup was defined as the\nmedian image-based phenotype for each SDOH used as inputs to four\nimaging-derived health disparities indices (iHDIs): one absolute measure\n(between-group variance) and three relative measures (index of disparity, Theil\nindex, and mean log deviation). The iHDI measures demonstrated feasible values\nfor each SDOH demographic correlate, showing potential for medical images to\nserve as a novel probe for health disparities. Large-scale AI analysis of\nmedical images can serve as a probe for a novel data source for health\ndisparities research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06084", "pdf": "https://arxiv.org/pdf/2504.06084", "abs": "https://arxiv.org/abs/2504.06084", "authors": ["Alexey Gavryushin", "Xi Wang", "Robert J. S. Malate", "Chenyu Yang", "Xiangyi Jia", "Shubh Goel", "Davide Liconti", "René Zurbrügg", "Robert K. Katzschmann", "Marc Pollefeys"], "title": "MAPLE: Encoding Dexterous Robotic Manipulation Priors Learned From Egocentric Videos", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Large-scale egocentric video datasets capture diverse human activities across\na wide range of scenarios, offering rich and detailed insights into how humans\ninteract with objects, especially those that require fine-grained dexterous\ncontrol. Such complex, dexterous skills with precise controls are crucial for\nmany robotic manipulation tasks, yet are often insufficiently addressed by\ntraditional data-driven approaches to robotic manipulation. To address this\ngap, we leverage manipulation priors learned from large-scale egocentric video\ndatasets to improve policy learning for dexterous robotic manipulation tasks.\nWe present MAPLE, a novel method for dexterous robotic manipulation that\nexploits rich manipulation priors to enable efficient policy learning and\nbetter performance on diverse, complex manipulation tasks. Specifically, we\npredict hand-object contact points and detailed hand poses at the moment of\nhand-object contact and use the learned features to train policies for\ndownstream manipulation tasks. Experimental results demonstrate the\neffectiveness of MAPLE across existing simulation benchmarks, as well as a\nnewly designed set of challenging simulation tasks, which require fine-grained\nobject control and complex dexterous skills. The benefits of MAPLE are further\nhighlighted in real-world experiments using a dexterous robotic hand, whereas\nsimultaneous evaluation across both simulation and real-world experiments has\nremained underexplored in prior work.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
