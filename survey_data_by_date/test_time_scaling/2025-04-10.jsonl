{"id": "2504.06908", "pdf": "https://arxiv.org/pdf/2504.06908", "abs": "https://arxiv.org/abs/2504.06908", "authors": ["Emmanuelle Bourigault", "Amir Jamaludin", "Abdullah Hamdi"], "title": "UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "preprint", "summary": "In medical imaging, the primary challenge is collecting large-scale labeled\ndata due to privacy concerns, logistics, and high labeling costs. In this work,\nwe present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset\nof body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D\nimages) and more than 1.37 billion 2D segmentation masks of 72 organs, all\nbased on the UK Biobank MRI dataset. We utilize automatic labeling, introduce\nan automated label cleaning pipeline with organ-specific filters, and manually\nannotate a subset of 300 MRIs with 11 abdominal classes to validate the quality\n(referred to as UKBOB-manual). This approach allows for scaling up the dataset\ncollection while maintaining confidence in the labels. We further confirm the\nvalidity of the labels by demonstrating zero-shot generalization of trained\nmodels on the filtered UKBOB to other small labeled datasets from similar\ndomains (e.g., abdominal MRI). To further mitigate the effect of noisy labels,\nwe propose a novel method called Entropy Test-time Adaptation (ETTA) to refine\nthe segmentation output. We use UKBOB to train a foundation model, Swin-BOB,\nfor 3D medical image segmentation based on the Swin-UNetr architecture,\nachieving state-of-the-art results in several benchmarks in 3D medical imaging,\nincluding the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the\nBTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained\nmodels and the code are available at https://emmanuelleb985.github.io/ukbob ,\nand the filtered labels will be made available with the UK Biobank.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale", "test-time adaptation"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06514", "pdf": "https://arxiv.org/pdf/2504.06514", "abs": "https://arxiv.org/abs/2504.06514", "authors": ["Chenrui Fan", "Ming Li", "Lichao Sun", "Tianyi Zhou"], "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scaling law"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06438", "pdf": "https://arxiv.org/pdf/2504.06438", "abs": "https://arxiv.org/abs/2504.06438", "authors": ["Yuehan Qin", "Shawn Li", "Yi Nian", "Xinyan Velocity Yu", "Yue Zhao", "Xuezhe Ma"], "title": "Don't Let It Hallucinate: Premise Verification via Retrieval-Augmented Logical Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown substantial capacity for generating\nfluent, contextually appropriate responses. However, they can produce\nhallucinated outputs, especially when a user query includes one or more false\npremises-claims that contradict established facts. Such premises can mislead\nLLMs into offering fabricated or misleading details. Existing approaches\ninclude pretraining, fine-tuning, and inference-time techniques that often rely\non access to logits or address hallucinations after they occur. These methods\ntend to be computationally expensive, require extensive training data, or lack\nproactive mechanisms to prevent hallucination before generation, limiting their\nefficiency in real-time applications. We propose a retrieval-based framework\nthat identifies and addresses false premises before generation. Our method\nfirst transforms a user's query into a logical representation, then applies\nretrieval-augmented generation (RAG) to assess the validity of each premise\nusing factual sources. Finally, we incorporate the verification results into\nthe LLM's prompt to maintain factual consistency in the final output.\nExperiments show that this approach effectively reduces hallucinations,\nimproves factual accuracy, and does not require access to model logits or\nlarge-scale fine-tuning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06460", "pdf": "https://arxiv.org/pdf/2504.06460", "abs": "https://arxiv.org/abs/2504.06460", "authors": ["Sai Adith Senthil Kumar", "Hao Yan", "Saipavan Perepa", "Murong Yue", "Ziyu Yao"], "title": "Can LLMs Simulate Personas with Reversed Performance? A Benchmark for Counterfactual Instruction Following", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are now increasingly widely used to simulate\npersonas in virtual environments, leveraging their instruction-following\ncapability. However, we discovered that even state-of-the-art LLMs cannot\nsimulate personas with reversed performance (e.g., student personas with low\nproficiency in educational settings), which impairs the simulation diversity\nand limits the practical applications of the simulated environments. In this\nwork, using mathematical reasoning as a representative scenario, we propose the\nfirst benchmark dataset for evaluating LLMs on simulating personas with\nreversed performance, a capability that we dub \"counterfactual instruction\nfollowing\". We evaluate both open-weight and closed-source LLMs on this task\nand find that LLMs, including the OpenAI o1 reasoning model, all struggle to\nfollow counterfactual instructions for simulating reversedly performing\npersonas. Intersectionally simulating both the performance level and the race\npopulation of a persona worsens the effect even further. These results\nhighlight the challenges of counterfactual instruction following and the need\nfor further research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1", "reasoning model"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06606", "pdf": "https://arxiv.org/pdf/2504.06606", "abs": "https://arxiv.org/abs/2504.06606", "authors": ["Minghe Gao", "Xuqi Liu", "Zhongqi Yue", "Yang Wu", "Shuang Chen", "Juncheng Li", "Siliang Tang", "Fei Wu", "Tat-Seng Chua", "Yueting Zhuang"], "title": "Benchmarking Multimodal CoT Reward Model Stepwise by Visual Program", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in reward signal usage for Large Language Models (LLMs)\nare remarkable. However, significant challenges exist when transitioning reward\nsignal to the multimodal domain, including labor-intensive annotations,\nover-reliance on one-step rewards, and inadequate evaluation. To address these\nissues, we propose SVIP, a novel approach to train a step-level\nmulti-dimensional Chain-of-Thought~(CoT) reward model automatically. It\ngenerates code for solving visual tasks and transforms the analysis of code\nblocks into the evaluation of CoT step as training samples. Then, we train\nSVIP-Reward model using a multi-head attention mechanism called TriAtt-CoT. The\nadvantages of SVIP-Reward are evident throughout the entire process of MLLM. We\nalso introduce a benchmark for CoT reward model training and testing.\nExperimental results demonstrate that SVIP-Reward improves MLLM performance\nacross training and inference-time scaling, yielding better results on\nbenchmarks while reducing hallucinations and enhancing reasoning ability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "multi-dimensional"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07081", "pdf": "https://arxiv.org/pdf/2504.07081", "abs": "https://arxiv.org/abs/2504.07081", "authors": ["Gabriel Grand", "Joshua B. Tenenbaum", "Vikash K. Mansinghka", "Alexander K. Lew", "Jacob Andreas"], "title": "Self-Steering Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While test-time reasoning enables language models to tackle complex tasks,\nsearching or planning in natural language can be slow, costly, and error-prone.\nBut even when LMs struggle to emulate the precise reasoning steps needed to\nsolve a problem, they often excel at describing its abstract structure--both\nhow to verify solutions and how to search for them. This paper introduces\nDisCIPL, a method for \"self-steering\" LMs where a Planner model generates a\ntask-specific inference program that is executed by a population of Follower\nmodels. Our approach equips LMs with the ability to write recursive search\nprocedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models,\nincluding GPT-4o and o1, on challenging constrained generation tasks. In\ndecoupling planning from execution, our work opens up a design space of\nhighly-parallelized Monte Carlo inference strategies that outperform standard\nbest-of-N sampling, require no finetuning, and can be implemented automatically\nby existing LMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "o1"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06982", "pdf": "https://arxiv.org/pdf/2504.06982", "abs": "https://arxiv.org/abs/2504.06982", "authors": ["Yuhang Yang", "Fengqi Liu", "Yixing Lu", "Qin Zhao", "Pingyu Wu", "Wei Zhai", "Ran Yi", "Yang Cao", "Lizhuang Ma", "Zheng-Jun Zha", "Junting Dong"], "title": "SIGMAN:Scaling 3D Human Gaussian Generation with Millions of Assets", "categories": ["cs.CV"], "comment": "project page:https://yyvhang.github.io/SIGMAN_3D/", "summary": "3D human digitization has long been a highly pursued yet challenging task.\nExisting methods aim to generate high-quality 3D digital humans from single or\nmultiple views, but remain primarily constrained by current paradigms and the\nscarcity of 3D human assets. Specifically, recent approaches fall into several\nparadigms: optimization-based and feed-forward (both single-view regression and\nmulti-view generation with reconstruction). However, they are limited by slow\nspeed, low quality, cascade reasoning, and ambiguity in mapping low-dimensional\nplanes to high-dimensional space due to occlusion and invisibility,\nrespectively. Furthermore, existing 3D human assets remain small-scale,\ninsufficient for large-scale training. To address these challenges, we propose\na latent space generation paradigm for 3D human digitization, which involves\ncompressing multi-view images into Gaussians via a UV-structured VAE, along\nwith DiT-based conditional generation, we transform the ill-posed\nlow-to-high-dimensional mapping problem into a learnable distribution shift,\nwhich also supports end-to-end inference. In addition, we employ the multi-view\noptimization approach combined with synthetic data to construct the HGS-1M\ndataset, which contains $1$ million 3D Gaussian assets to support the\nlarge-scale training. Experimental results demonstrate that our paradigm,\npowered by large-scale training, produces high-quality 3D human Gaussians with\nintricate textures, facial details, and loose clothing deformation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06330", "pdf": "https://arxiv.org/pdf/2504.06330", "abs": "https://arxiv.org/abs/2504.06330", "authors": ["Hicham Talaoubrid", "Anissa Mokraoui", "Ismail Ben Ayed", "Axel Prouvost", "Sonimith Hang", "Monit Korn", "Rémi Harvey"], "title": "Analyzing the Impact of Low-Rank Adaptation for Cross-Domain Few-Shot Object Detection in Aerial Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper investigates the application of Low-Rank Adaptation (LoRA) to\nsmall models for cross-domain few-shot object detection in aerial images.\nOriginally designed for large-scale models, LoRA helps mitigate overfitting,\nmaking it a promising approach for resource-constrained settings. We integrate\nLoRA into DiffusionDet, and evaluate its performance on the DOTA and DIOR\ndatasets. Our results show that LoRA applied after an initial fine-tuning\nslightly improves performance in low-shot settings (e.g., 1-shot and 5-shot),\nwhile full fine-tuning remains more effective in higher-shot configurations.\nThese findings highlight LoRA's potential for efficient adaptation in aerial\nobject detection, encouraging further research into parameter-efficient\nfine-tuning strategies for few-shot learning. Our code is available here:\nhttps://github.com/HichTala/LoRA-DiffusionDet.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06564", "pdf": "https://arxiv.org/pdf/2504.06564", "abs": "https://arxiv.org/abs/2504.06564", "authors": ["Qingcheng Zeng", "Weihao Xuan", "Leyang Cui", "Rob Voigt"], "title": "Do Reasoning Models Show Better Verbalized Calibration?", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Large reasoning models (LRMs) have recently shown impressive capabilities in\ncomplex reasoning by leveraging increased test-time computation and exhibiting\nbehaviors akin to human-like deliberation. Despite these advances, it remains\nan open question whether LRMs are better calibrated - particularly in their\nverbalized confidence - compared to instruction-tuned counterparts. In this\npaper, we investigate the calibration properties of LRMs trained via supervised\nfine-tuning distillation on long reasoning traces (henceforth SFT reasoning\nmodels) and outcome-based reinforcement learning for reasoning (henceforth RL\nreasoning models) across diverse domains. Our findings reveal that LRMs\nsignificantly outperform instruction-tuned models on complex reasoning tasks in\nboth accuracy and confidence calibration. In contrast, we find surprising\ntrends in the domain of factuality in particular. On factuality tasks, while\nDeepseek-R1 shows strong calibration behavior, smaller QwQ-32B shows no\nimprovement over instruct models; moreover, SFT reasoning models display worse\ncalibration (greater overconfidence) compared to instruct models. Our results\nprovide evidence for a potentially critical role of reasoning-oriented RL\ntraining in improving LLMs' capacity for generating trustworthy, self-aware\noutputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06464", "pdf": "https://arxiv.org/pdf/2504.06464", "abs": "https://arxiv.org/abs/2504.06464", "authors": ["José A. Pilartes-Congo", "Matthew Kastl", "Michael J. Starek", "Marina Vicens-Miquel", "Philippe Tissot"], "title": "Implementation of a Zed 2i Stereo Camera for High-Frequency Shoreline Change and Coastal Elevation Monitoring", "categories": ["cs.CV"], "comment": "Published in IGARSS 2023 - 2023 IEEE International Geoscience and\n  Remote Sensing Symposium", "summary": "The increasing population, thus financial interests, in coastal areas have\nincreased the need to monitor coastal elevation and shoreline change. Though\nseveral resources exist to obtain this information, they often lack the\nrequired temporal resolution for short-term monitoring (e.g., every hour). To\naddress this issue, this study implements a low-cost ZED 2i stereo camera\nsystem and close-range photogrammetry to collect images for generating 3D point\nclouds, digital surface models (DSMs) of beach elevation, and georectified\nimagery at a localized scale and high temporal resolution. The main\ncontributions of this study are (i) intrinsic camera calibration, (ii)\ngeorectification and registration of acquired imagery and point cloud, (iii)\ngeneration of the DSM of the beach elevation, and (iv) a comparison of derived\nproducts against those from uncrewed aircraft system structure-from-motion\nphotogrammetry. Preliminary results show that despite its limitations, the ZED\n2i can provide the desired mapping products at localized and high temporal\nscales. The system achieved a mean reprojection error of 0.20 px, a point cloud\nregistration of 27 cm, a vertical error of 37.56 cm relative to ground truth,\nand georectification root mean square errors of 2.67 cm and 2.81 cm for x and\ny.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06792", "pdf": "https://arxiv.org/pdf/2504.06792", "abs": "https://arxiv.org/abs/2504.06792", "authors": ["Zican Dong", "Han Peng", "Peiyu Liu", "Wayne Xin Zhao", "Dong Wu", "Feng Xiao", "Zhifeng Wang"], "title": "Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Mixture-of-Experts (MoE) models achieve a favorable trade-off between\nperformance and inference efficiency by activating only a subset of experts.\nHowever, the memory overhead of storing all experts remains a major limitation,\nespecially in large-scale MoE models such as DeepSeek-R1 (671B). In this study,\nwe investigate domain specialization and expert redundancy in large-scale MoE\nmodels and uncover a consistent behavior we term few-shot expert localization,\nwith only a few demonstrations, the model consistently activates a sparse and\nstable subset of experts. Building on this observation, we propose a simple yet\neffective pruning framework, EASY-EP, that leverages a few domain-specific\ndemonstrations to identify and retain only the most relevant experts. EASY-EP\ncomprises two key components: output-aware expert importance assessment and\nexpert-level token contribution estimation. The former evaluates the importance\nof each expert for the current token by considering the gating scores and\nmagnitudes of the outputs of activated experts, while the latter assesses the\ncontribution of tokens based on representation similarities after and before\nrouted experts. Experiments show that our method can achieve comparable\nperformances and $2.99\\times$ throughput under the same memory budget with full\nDeepSeek-R1 with only half the experts. Our code is available at\nhttps://github.com/RUCAIBox/EASYEP.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06823", "pdf": "https://arxiv.org/pdf/2504.06823", "abs": "https://arxiv.org/abs/2504.06823", "authors": ["Xiaotian Ye", "Mengqi Zhang", "Shu Wu"], "title": "Open Problems and a Hypothetical Path Forward in LLM Knowledge Paradigms", "categories": ["cs.CL"], "comment": "Blog post preprint, work in progress", "summary": "Knowledge is fundamental to the overall capabilities of Large Language Models\n(LLMs). The knowledge paradigm of a model, which dictates how it encodes and\nutilizes knowledge, significantly affects its performance. Despite the\ncontinuous development of LLMs under existing knowledge paradigms, issues\nwithin these frameworks continue to constrain model potential.\n  This blog post highlight three critical open problems limiting model\ncapabilities: (1) challenges in knowledge updating for LLMs, (2) the failure of\nreverse knowledge generalization (the reversal curse), and (3) conflicts in\ninternal knowledge. We review recent progress made in addressing these issues\nand discuss potential general solutions. Based on observations in these areas,\nwe propose a hypothetical paradigm based on Contextual Knowledge Scaling, and\nfurther outline implementation pathways that remain feasible within\ncontemporary techniques. Evidence suggests this approach holds potential to\naddress current shortcomings, serving as our vision for future model paradigms.\n  This blog post aims to provide researchers with a brief overview of progress\nin LLM knowledge systems, while provide inspiration for the development of\nnext-generation model architectures.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06843", "pdf": "https://arxiv.org/pdf/2504.06843", "abs": "https://arxiv.org/abs/2504.06843", "authors": ["Angela Lopez-Cardona", "Sebastian Idesis", "Ioannis Arapakis"], "title": "Integrating Cognitive Processing Signals into Language Models: A Review of Advances, Applications and Future Directions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, the integration of cognitive neuroscience in Natural Language\nProcessing (NLP) has gained significant attention. This article provides a\ncritical and timely overview of recent advancements in leveraging cognitive\nsignals, particularly Eye-tracking (ET) signals, to enhance Language Models\n(LMs) and Multimodal Large Language Models (MLLMs). By incorporating\nuser-centric cognitive signals, these approaches address key challenges,\nincluding data scarcity and the environmental costs of training large-scale\nmodels. Cognitive signals enable efficient data augmentation, faster\nconvergence, and improved human alignment. The review emphasises the potential\nof ET data in tasks like Visual Question Answering (VQA) and mitigating\nhallucinations in MLLMs, and concludes by discussing emerging challenges and\nresearch trends.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "human alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06629", "pdf": "https://arxiv.org/pdf/2504.06629", "abs": "https://arxiv.org/abs/2504.06629", "authors": ["MinKyu Lee", "Sangeek Hyun", "Woojin Jun", "Hyunjun Kim", "Jiwoo Chung", "Jae-Pil Heo"], "title": "Rethinking LayerNorm in Image Restoration Transformers", "categories": ["cs.CV"], "comment": null, "summary": "This work investigates abnormal feature behaviors observed in image\nrestoration (IR) Transformers. Specifically, we identify two critical issues:\nfeature entropy becoming excessively small and feature magnitudes diverging up\nto a million-fold scale. We pinpoint the root cause to the per-token\nnormalization aspect of conventional LayerNorm, which disrupts essential\nspatial correlations and internal feature statistics. To address this, we\npropose a simple normalization strategy tailored for IR Transformers. Our\napproach applies normalization across the entire spatio-channel dimension,\neffectively preserving spatial correlations. Additionally, we introduce an\ninput-adaptive rescaling method that aligns feature statistics to the unique\nstatistical requirements of each input. Experimental results verify that this\ncombined strategy effectively resolves feature divergence, significantly\nenhancing both the stability and performance of IR Transformers across various\nIR tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07070", "pdf": "https://arxiv.org/pdf/2504.07070", "abs": "https://arxiv.org/abs/2504.07070", "authors": ["Zhouhang Xie", "Junda Wu", "Yiran Shen", "Yu Xia", "Xintong Li", "Aaron Chang", "Ryan Rossi", "Sachin Kumar", "Bodhisattwa Prasad Majumder", "Jingbo Shang", "Prithviraj Ammanabrolu", "Julian McAuley"], "title": "A Survey on Personalized and Pluralistic Preference Alignment in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Personalized preference alignment for large language models (LLMs), the\nprocess of tailoring LLMs to individual users' preferences, is an emerging\nresearch direction spanning the area of NLP and personalization. In this\nsurvey, we present an analysis of works on personalized alignment and modeling\nfor LLMs. We introduce a taxonomy of preference alignment techniques, including\ntraining time, inference time, and additionally, user-modeling based methods.\nWe provide analysis and discussion on the strengths and limitations of each\ngroup of techniques and then cover evaluation, benchmarks, as well as open\nproblems in the field.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07072", "pdf": "https://arxiv.org/pdf/2504.07072", "abs": "https://arxiv.org/abs/2504.07072", "authors": ["Israfel Salazar", "Manuel Fernández Burda", "Shayekh Bin Islam", "Arshia Soltani Moakhar", "Shivalika Singh", "Fabian Farestam", "Angelika Romanou", "Danylo Boiko", "Dipika Khullar", "Mike Zhang", "Dominik Krzemiński", "Jekaterina Novikova", "Luísa Shimabucoro", "Joseph Marvin Imperial", "Rishabh Maheshwary", "Sharad Duwal", "Alfonso Amayuelas", "Swati Rajwal", "Jebish Purbey", "Ahmed Ruby", "Nicholas Popovič", "Marek Suppa", "Azmine Toushik Wasi", "Ram Mohan Rao Kadiyala", "Olga Tsymboi", "Maksim Kostritsya", "Bardia Soltani Moakhar", "Gabriel da Costa Merlin", "Otávio Ferracioli Coletti", "Maral Jabbari Shiviari", "MohammadAmin farahani fard", "Silvia Fernandez", "María Grandury", "Dmitry Abulkhanov", "Drishti Sharma", "Andre Guarnier De Mitri", "Leticia Bossatto Marchezi", "Johan Obando-Ceron", "Nazar Kohut", "Beyza Ermis", "Desmond Elliott", "Enzo Ferrante", "Sara Hooker", "Marzieh Fadaee"], "title": "Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The evaluation of vision-language models (VLMs) has mainly relied on\nEnglish-language benchmarks, leaving significant gaps in both multilingual and\nmulticultural coverage. While multilingual benchmarks have expanded, both in\nsize and languages, many rely on translations of English datasets, failing to\ncapture cultural nuances. In this work, we propose Kaleidoscope, as the most\ncomprehensive exam benchmark to date for the multilingual evaluation of\nvision-language models. Kaleidoscope is a large-scale, in-language multimodal\nbenchmark designed to evaluate VLMs across diverse languages and visual inputs.\nKaleidoscope covers 18 languages and 14 different subjects, amounting to a\ntotal of 20,911 multiple-choice questions. Built through an open science\ncollaboration with a diverse group of researchers worldwide, Kaleidoscope\nensures linguistic and cultural authenticity. We evaluate top-performing\nmultilingual vision-language models and find that they perform poorly on\nlow-resource languages and in complex multimodal scenarios. Our results\nhighlight the need for progress on culturally inclusive multimodal evaluation\nframeworks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06741", "pdf": "https://arxiv.org/pdf/2504.06741", "abs": "https://arxiv.org/abs/2504.06741", "authors": ["Constantin Ulrich", "Tassilo Wald", "Fabian Isensee", "Klaus H. Maier-Hein"], "title": "Large Scale Supervised Pretraining For Traumatic Brain Injury Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The segmentation of lesions in Moderate to Severe Traumatic Brain Injury\n(msTBI) presents a significant challenge in neuroimaging due to the diverse\ncharacteristics of these lesions, which vary in size, shape, and distribution\nacross brain regions and tissue types. This heterogeneity complicates\ntraditional image processing techniques, resulting in critical errors in tasks\nsuch as image registration and brain parcellation. To address these challenges,\nthe AIMS-TBI Segmentation Challenge 2024 aims to advance innovative\nsegmentation algorithms specifically designed for T1-weighted MRI data, the\nmost widely utilized imaging modality in clinical practice. Our proposed\nsolution leverages a large-scale multi-dataset supervised pretraining approach\ninspired by the MultiTalent method. We train a Resenc L network on a\ncomprehensive collection of datasets covering various anatomical and\npathological structures, which equips the model with a robust understanding of\nbrain anatomy and pathology. Following this, the model is fine-tuned on\nmsTBI-specific data to optimize its performance for the unique characteristics\nof T1-weighted MRI scans and outperforms the baseline without pretraining up to\n2 Dice points.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06785", "pdf": "https://arxiv.org/pdf/2504.06785", "abs": "https://arxiv.org/abs/2504.06785", "authors": ["Shuoshuo Xu", "Kai Zhao", "James Loney", "Zili Li", "Andrea Visentin"], "title": "Zero-Shot Image-Based Large Language Model Approach to Road Pavement Monitoring", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Effective and rapid evaluation of pavement surface condition is critical for\nprioritizing maintenance, ensuring transportation safety, and minimizing\nvehicle wear and tear. While conventional manual inspections suffer from\nsubjectivity, existing machine learning-based methods are constrained by their\nreliance on large and high-quality labeled datasets, which require significant\nresources and limit adaptability across varied road conditions. The\nrevolutionary advancements in Large Language Models (LLMs) present significant\npotential for overcoming these challenges. In this study, we propose an\ninnovative automated zero-shot learning approach that leverages the image\nrecognition and natural language understanding capabilities of LLMs to assess\nroad conditions effectively. Multiple LLM-based assessment models were\ndeveloped, employing prompt engineering strategies aligned with the Pavement\nSurface Condition Index (PSCI) standards. These models' accuracy and\nreliability were evaluated against official PSCI results, with an optimized\nmodel ultimately selected. Extensive tests benchmarked the optimized model\nagainst evaluations from various levels experts using Google Street View road\nimages. The results reveal that the LLM-based approach can effectively assess\nroad conditions, with the optimized model -employing comprehensive and\nstructured prompt engineering strategies -outperforming simpler configurations\nby achieving high accuracy and consistency, even surpassing expert evaluations.\nMoreover, successfully applying the optimized model to Google Street View\nimages demonstrates its potential for future city-scale deployments. These\nfindings highlight the transformative potential of LLMs in automating road\ndamage evaluations and underscore the pivotal role of detailed prompt\nengineering in achieving reliable assessments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "consistency", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06801", "pdf": "https://arxiv.org/pdf/2504.06801", "abs": "https://arxiv.org/abs/2504.06801", "authors": ["Rishubh Parihar", "Srinjay Sarkar", "Sarthak Vora", "Jogendra Kundu", "R. Venkatesh Babu"], "title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection", "categories": ["cs.CV"], "comment": "https://rishubhpar.github.io/monoplace3D", "summary": "Current monocular 3D detectors are held back by the limited diversity and\nscale of real-world datasets. While data augmentation certainly helps, it's\nparticularly difficult to generate realistic scene-aware augmented data for\noutdoor settings. Most current approaches to synthetic data generation focus on\nrealistic object appearance through improved rendering techniques. However, we\nshow that where and how objects are positioned is just as crucial for training\neffective 3D monocular detectors. The key obstacle lies in automatically\ndetermining realistic object placement parameters - including position,\ndimensions, and directional alignment when introducing synthetic objects into\nactual scenes. To address this, we introduce MonoPlace3D, a novel system that\nconsiders the 3D scene content to create realistic augmentations. Specifically,\ngiven a background scene, MonoPlace3D learns a distribution over plausible 3D\nbounding boxes. Subsequently, we render realistic objects and place them\naccording to the locations sampled from the learned distribution. Our\ncomprehensive evaluation on two standard datasets KITTI and NuScenes,\ndemonstrates that MonoPlace3D significantly improves the accuracy of multiple\nexisting monocular 3D detectors while being highly data efficient.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06704", "pdf": "https://arxiv.org/pdf/2504.06704", "abs": "https://arxiv.org/abs/2504.06704", "authors": ["Yoshihiro Yamada"], "title": "CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Transformers have driven remarkable breakthroughs in natural language\nprocessing and computer vision, yet their standard attention mechanism still\nimposes O(N^2) complexity, hindering scalability to longer sequences. We\nintroduce Circular-convolutional ATtention (CAT), a Fourier-based approach that\nefficiently applies circular convolutions to reduce complexity without\nsacrificing representational power. CAT achieves O(NlogN) computations,\nrequires fewer learnable parameters by streamlining fully-connected layers, and\nintroduces no heavier operations, resulting in consistent accuracy improvements\nand about a 10% speedup in naive PyTorch implementations on large-scale\nbenchmarks such as ImageNet-1k and WikiText-103. Grounded in an\nengineering-isomorphism framework, CAT's design not only offers practical\nefficiency and ease of implementation but also provides insights to guide the\ndevelopment of next-generation, high-performance Transformer architectures.\nFinally, our ablation studies highlight the key conditions underlying CAT's\nsuccess, shedding light on broader principles for scalable attention\nmechanisms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07086", "pdf": "https://arxiv.org/pdf/2504.07086", "abs": "https://arxiv.org/abs/2504.07086", "authors": ["Andreas Hochlehnert", "Hardik Bhatnagar", "Vishaal Udandarao", "Samuel Albanie", "Ameya Prabhu", "Matthias Bethge"], "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility", "categories": ["cs.LG", "cs.CL"], "comment": "Technical Report", "summary": "Reasoning has emerged as the next major frontier for language models (LMs),\nwith rapid advances from both academic and industrial labs. However, this\nprogress often outpaces methodological rigor, with many evaluations relying on\nbenchmarking practices that lack transparency, robustness, or statistical\ngrounding. In this work, we conduct a comprehensive empirical study and find\nthat current mathematical reasoning benchmarks are highly sensitive to subtle\nimplementation choices - including decoding parameters, random seeds, prompt\nformatting, and even hardware and software-framework configurations.\nPerformance gains reported in recent studies frequently hinge on unclear\ncomparisons or unreported sources of variance. To address these issues, we\npropose a standardized evaluation framework with clearly defined best practices\nand reporting standards. Using this framework, we reassess recent methods and\nfind that reinforcement learning (RL) approaches yield only modest improvements\n- far below prior claims - and are prone to overfitting, especially on\nsmall-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)\nmethods show consistently stronger generalization. To foster reproducibility,\nwe release all code, prompts, and model outputs, for reasoning benchmarks,\nestablishing more rigorous foundations for future work.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06920", "pdf": "https://arxiv.org/pdf/2504.06920", "abs": "https://arxiv.org/abs/2504.06920", "authors": ["Masquil Elías", "Marí Roger", "Ehret Thibaud", "Meinhardt-Llopis Enric", "Musé Pablo", "Facciolo Gabriele"], "title": "S-EO: A Large-Scale Dataset for Geometry-Aware Shadow Detection in Remote Sensing Applications", "categories": ["cs.CV"], "comment": "Accepted at Earthvision 2025 (CVPR Workshop)", "summary": "We introduce the S-EO dataset: a large-scale, high-resolution dataset,\ndesigned to advance geometry-aware shadow detection. Collected from diverse\npublic-domain sources, including challenge datasets and government providers\nsuch as USGS, our dataset comprises 702 georeferenced tiles across the USA,\neach covering 500x500 m. Each tile includes multi-date, multi-angle WorldView-3\npansharpened RGB images, panchromatic images, and a ground-truth DSM of the\narea obtained from LiDAR scans. For each image, we provide a shadow mask\nderived from geometry and sun position, a vegetation mask based on the NDVI\nindex, and a bundle-adjusted RPC model. With approximately 20,000 images, the\nS-EO dataset establishes a new public resource for shadow detection in remote\nsensing imagery and its applications to 3D reconstruction. To demonstrate the\ndataset's impact, we train and evaluate a shadow detector, showcasing its\nability to generalize, even to aerial images. Finally, we extend EO-NeRF - a\nstate-of-the-art NeRF approach for satellite imagery - to leverage our shadow\npredictions for improved 3D reconstructions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06965", "pdf": "https://arxiv.org/pdf/2504.06965", "abs": "https://arxiv.org/abs/2504.06965", "authors": ["Teng Xiao", "Qi Hu", "Qingsong Yan", "Wei Liu", "Zhiwei Ye", "Fei Deng"], "title": "A Deep Single Image Rectification Approach for Pan-Tilt-Zoom Cameras", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025", "summary": "Pan-Tilt-Zoom (PTZ) cameras with wide-angle lenses are widely used in\nsurveillance but often require image rectification due to their inherent\nnonlinear distortions. Current deep learning approaches typically struggle to\nmaintain fine-grained geometric details, resulting in inaccurate rectification.\nThis paper presents a Forward Distortion and Backward Warping Network\n(FDBW-Net), a novel framework for wide-angle image rectification. It begins by\nusing a forward distortion model to synthesize barrel-distorted images,\nreducing pixel redundancy and preventing blur. The network employs a pyramid\ncontext encoder with attention mechanisms to generate backward warping flows\ncontaining geometric details. Then, a multi-scale decoder is used to restore\ndistorted features and output rectified images. FDBW-Net's performance is\nvalidated on diverse datasets: public benchmarks, AirSim-rendered PTZ camera\nimagery, and real-scene PTZ camera datasets. It demonstrates that FDBW-Net\nachieves SOTA performance in distortion rectification, boosting the\nadaptability of PTZ cameras for practical visual applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06978", "pdf": "https://arxiv.org/pdf/2504.06978", "abs": "https://arxiv.org/abs/2504.06978", "authors": ["Daiwei Zhang", "Joaquin Gajardo", "Tomislav Medic", "Isinsu Katircioglu", "Mike Boss", "Norbert Kirchgessner", "Achim Walter", "Lukas Roth"], "title": "Wheat3DGS: In-field 3D Reconstruction, Instance Segmentation and Phenotyping of Wheat Heads with Gaussian Splatting", "categories": ["cs.CV"], "comment": "Copyright 2025 IEEE. This is the author's version of the work. It is\n  posted here for your personal use. Not for redistribution. The definitive\n  version is published in the 2025 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition Workshops (CVPRW)", "summary": "Automated extraction of plant morphological traits is crucial for supporting\ncrop breeding and agricultural management through high-throughput field\nphenotyping (HTFP). Solutions based on multi-view RGB images are attractive due\nto their scalability and affordability, enabling volumetric measurements that\n2D approaches cannot directly capture. While advanced methods like Neural\nRadiance Fields (NeRFs) have shown promise, their application has been limited\nto counting or extracting traits from only a few plants or organs. Furthermore,\naccurately measuring complex structures like individual wheat heads-essential\nfor studying crop yields-remains particularly challenging due to occlusions and\nthe dense arrangement of crop canopies in field conditions. The recent\ndevelopment of 3D Gaussian Splatting (3DGS) offers a promising alternative for\nHTFP due to its high-quality reconstructions and explicit point-based\nrepresentation. In this paper, we present Wheat3DGS, a novel approach that\nleverages 3DGS and the Segment Anything Model (SAM) for precise 3D instance\nsegmentation and morphological measurement of hundreds of wheat heads\nautomatically, representing the first application of 3DGS to HTFP. We validate\nthe accuracy of wheat head extraction against high-resolution laser scan data,\nobtaining per-instance mean absolute percentage errors of 15.1%, 18.3%, and\n40.2% for length, width, and volume. We provide additional comparisons to\nNeRF-based approaches and traditional Muti-View Stereo (MVS), demonstrating\nsuperior results. Our approach enables rapid, non-destructive measurements of\nkey yield-related traits at scale, with significant implications for\naccelerating crop breeding and improving our understanding of wheat\ndevelopment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07029", "pdf": "https://arxiv.org/pdf/2504.07029", "abs": "https://arxiv.org/abs/2504.07029", "authors": ["Ran Zhang", "Xuanhua He", "Ke Cao", "Liu Liu", "Li Zhang", "Man Zhou", "Jie Zhang"], "title": "Distilling Textual Priors from LLM to Efficient Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modality image fusion aims to synthesize a single, comprehensive image\nfrom multiple source inputs. Traditional approaches, such as CNNs and GANs,\noffer efficiency but struggle to handle low-quality or complex inputs. Recent\nadvances in text-guided methods leverage large model priors to overcome these\nlimitations, but at the cost of significant computational overhead, both in\nmemory and inference time. To address this challenge, we propose a novel\nframework for distilling large model priors, eliminating the need for text\nguidance during inference while dramatically reducing model size. Our framework\nutilizes a teacher-student architecture, where the teacher network incorporates\nlarge model priors and transfers this knowledge to a smaller student network\nvia a tailored distillation process. Additionally, we introduce spatial-channel\ncross-fusion module to enhance the model's ability to leverage textual priors\nacross both spatial and channel dimensions. Our method achieves a favorable\ntrade-off between computational efficiency and fusion quality. The distilled\nnetwork, requiring only 10\\% of the parameters and inference time of the\nteacher network, retains 90\\% of its performance and outperforms existing SOTA\nmethods. Extensive experiments demonstrate the effectiveness of our approach.\nThe implementation will be made publicly available as an open-source resource.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07083", "pdf": "https://arxiv.org/pdf/2504.07083", "abs": "https://arxiv.org/abs/2504.07083", "authors": ["Mengchen Zhang", "Tong Wu", "Jing Tan", "Ziwei Liu", "Gordon Wetzstein", "Dahua Lin"], "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography", "categories": ["cs.CV"], "comment": null, "summary": "Camera trajectory design plays a crucial role in video production, serving as\na fundamental tool for conveying directorial intent and enhancing visual\nstorytelling. In cinematography, Directors of Photography meticulously craft\ncamera movements to achieve expressive and intentional framing. However,\nexisting methods for camera trajectory generation remain limited: Traditional\napproaches rely on geometric optimization or handcrafted procedural systems,\nwhile recent learning-based methods often inherit structural biases or lack\ntextual alignment, constraining creative synthesis. In this work, we introduce\nan auto-regressive model inspired by the expertise of Directors of Photography\nto generate artistic and expressive camera trajectories. We first introduce\nDataDoP, a large-scale multi-modal dataset containing 29K real-world shots with\nfree-moving camera trajectories, depth maps, and detailed captions in specific\nmovements, interaction with the scene, and directorial intent. Thanks to the\ncomprehensive and diverse database, we further train an auto-regressive,\ndecoder-only Transformer for high-quality, context-aware camera movement\ngeneration based on text guidance and RGBD inputs, named GenDoP. Extensive\nexperiments demonstrate that compared to existing methods, GenDoP offers better\ncontrollability, finer-grained trajectory adjustments, and higher motion\nstability. We believe our approach establishes a new standard for\nlearning-based cinematography, paving the way for future advancements in camera\ncontrol and filmmaking. Our project website:\nhttps://kszpxxzmc.github.io/GenDoP/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06301", "pdf": "https://arxiv.org/pdf/2504.06301", "abs": "https://arxiv.org/abs/2504.06301", "authors": ["Mohsen Jenadeleh", "Jon Sneyers", "Panqi Jia", "Shima Mohammadi", "Joao Ascenso", "Dietmar Saupe"], "title": "Subjective Visual Quality Assessment for High-Fidelity Learning-Based Image Compression", "categories": ["eess.IV", "cs.CV"], "comment": "7 pages, 5 figures, 3 tables, submitted to QoMEX 2025", "summary": "Learning-based image compression methods have recently emerged as promising\nalternatives to traditional codecs, offering improved rate-distortion\nperformance and perceptual quality. JPEG AI represents the latest standardized\nframework in this domain, leveraging deep neural networks for high-fidelity\nimage reconstruction. In this study, we present a comprehensive subjective\nvisual quality assessment of JPEG AI-compressed images using the JPEG AIC-3\nmethodology, which quantifies perceptual differences in terms of Just\nNoticeable Difference (JND) units. We generated a dataset of 50 compressed\nimages with fine-grained distortion levels from five diverse sources. A\nlarge-scale crowdsourced experiment collected 96,200 triplet responses from 459\nparticipants. We reconstructed JND-based quality scales using a unified model\nbased on boosted and plain triplet comparisons. Additionally, we evaluated the\nalignment of multiple objective image quality metrics with human perception in\nthe high-fidelity range. The CVVDP metric achieved the overall highest\nperformance; however, most metrics including CVVDP were overly optimistic in\npredicting the quality of JPEG AI-compressed images. These findings emphasize\nthe necessity for rigorous subjective evaluations in the development and\nbenchmarking of modern image codecs, particularly in the high-fidelity range.\nAnother technical contribution is the introduction of the well-known\nMeng-Rosenthal-Rubin statistical test to the field of Quality of Experience\nresearch. This test can reliably assess the significance of difference in\nperformance of quality metrics in terms of correlation between metrics and\nground truth. The complete dataset, including all subjective scores, is\npublicly available at https://github.com/jpeg-aic/dataset-JPEG-AI-SDR25.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "fine-grained"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06410", "pdf": "https://arxiv.org/pdf/2504.06410", "abs": "https://arxiv.org/abs/2504.06410", "authors": ["Huzaifa Arif", "Keerthiram Murugesan", "Payel Das", "Alex Gittens", "Pin-Yu Chen"], "title": "PEEL the Layers and Find Yourself: Revisiting Inference-time Data Leakage for Residual Neural Networks", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": null, "summary": "This paper explores inference-time data leakage risks of deep neural networks\n(NNs), where a curious and honest model service provider is interested in\nretrieving users' private data inputs solely based on the model inference\nresults. Particularly, we revisit residual NNs due to their popularity in\ncomputer vision and our hypothesis that residual blocks are a primary cause of\ndata leakage owing to the use of skip connections. By formulating\ninference-time data leakage as a constrained optimization problem, we propose a\nnovel backward feature inversion method, \\textbf{PEEL}, which can effectively\nrecover block-wise input features from the intermediate output of residual NNs.\nThe surprising results in high-quality input data recovery can be explained by\nthe intuition that the output from these residual blocks can be considered as a\nnoisy version of the input and thus the output retains sufficient information\nfor input recovery. We demonstrate the effectiveness of our layer-by-layer\nfeature inversion method on facial image datasets and pre-trained classifiers.\nOur results show that PEEL outperforms the state-of-the-art recovery methods by\nan order of magnitude when evaluated by mean squared error (MSE). The code is\navailable at\n\\href{https://github.com/Huzaifa-Arif/PEEL}{https://github.com/Huzaifa-Arif/PEEL}", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06704", "pdf": "https://arxiv.org/pdf/2504.06704", "abs": "https://arxiv.org/abs/2504.06704", "authors": ["Yoshihiro Yamada"], "title": "CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Transformers have driven remarkable breakthroughs in natural language\nprocessing and computer vision, yet their standard attention mechanism still\nimposes O(N^2) complexity, hindering scalability to longer sequences. We\nintroduce Circular-convolutional ATtention (CAT), a Fourier-based approach that\nefficiently applies circular convolutions to reduce complexity without\nsacrificing representational power. CAT achieves O(NlogN) computations,\nrequires fewer learnable parameters by streamlining fully-connected layers, and\nintroduces no heavier operations, resulting in consistent accuracy improvements\nand about a 10% speedup in naive PyTorch implementations on large-scale\nbenchmarks such as ImageNet-1k and WikiText-103. Grounded in an\nengineering-isomorphism framework, CAT's design not only offers practical\nefficiency and ease of implementation but also provides insights to guide the\ndevelopment of next-generation, high-performance Transformer architectures.\nFinally, our ablation studies highlight the key conditions underlying CAT's\nsuccess, shedding light on broader principles for scalable attention\nmechanisms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06866", "pdf": "https://arxiv.org/pdf/2504.06866", "abs": "https://arxiv.org/abs/2504.06866", "authors": ["Seunghyeok Back", "Joosoon Lee", "Kangmin Kim", "Heeseon Rho", "Geonhyup Lee", "Raeyoung Kang", "Sangbeom Lee", "Sangjun Noh", "Youngjin Lee", "Taeyeop Lee", "Kyoobin Lee"], "title": "GraspClutter6D: A Large-scale Real-world Dataset for Robust Perception and Grasping in Cluttered Scenes", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Robust grasping in cluttered environments remains an open challenge in\nrobotics. While benchmark datasets have significantly advanced deep learning\nmethods, they mainly focus on simplistic scenes with light occlusion and\ninsufficient diversity, limiting their applicability to practical scenarios. We\npresent GraspClutter6D, a large-scale real-world grasping dataset featuring:\n(1) 1,000 highly cluttered scenes with dense arrangements (14.1 objects/scene,\n62.6\\% occlusion), (2) comprehensive coverage across 200 objects in 75\nenvironment configurations (bins, shelves, and tables) captured using four\nRGB-D cameras from multiple viewpoints, and (3) rich annotations including 736K\n6D object poses and 9.3B feasible robotic grasps for 52K RGB-D images. We\nbenchmark state-of-the-art segmentation, object pose estimation, and grasping\ndetection methods to provide key insights into challenges in cluttered\nenvironments. Additionally, we validate the dataset's effectiveness as a\ntraining resource, demonstrating that grasping networks trained on\nGraspClutter6D significantly outperform those trained on existing datasets in\nboth simulation and real-world experiments. The dataset, toolkit, and\nannotation tools are publicly available on our project website:\nhttps://sites.google.com/view/graspclutter6d.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06961", "pdf": "https://arxiv.org/pdf/2504.06961", "abs": "https://arxiv.org/abs/2504.06961", "authors": ["Yu Qi", "Yuanchen Ju", "Tianming Wei", "Chi Chu", "Lawson L. S. Wong", "Huazhe Xu"], "title": "Two by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CVPR 2025 (Conference on Computer Vision and Pattern\n  Recognition)", "summary": "3D assembly tasks, such as furniture assembly and component fitting, play a\ncrucial role in daily life and represent essential capabilities for future home\nrobots. Existing benchmarks and datasets predominantly focus on assembling\ngeometric fragments or factory parts, which fall short in addressing the\ncomplexities of everyday object interactions and assemblies. To bridge this\ngap, we present 2BY2, a large-scale annotated dataset for daily pairwise\nobjects assembly, covering 18 fine-grained tasks that reflect real-life\nscenarios, such as plugging into sockets, arranging flowers in vases, and\ninserting bread into toasters. 2BY2 dataset includes 1,034 instances and 517\npairwise objects with pose and symmetry annotations, requiring approaches that\nalign geometric shapes while accounting for functional and spatial\nrelationships between objects. Leveraging the 2BY2 dataset, we propose a\ntwo-step SE(3) pose estimation method with equivariant features for assembly\nconstraints. Compared to previous shape assembly methods, our approach achieves\nstate-of-the-art performance across all 18 tasks in the 2BY2 dataset.\nAdditionally, robot experiments further validate the reliability and\ngeneralization ability of our method for complex 3D assembly tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability", "fine-grained"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07072", "pdf": "https://arxiv.org/pdf/2504.07072", "abs": "https://arxiv.org/abs/2504.07072", "authors": ["Israfel Salazar", "Manuel Fernández Burda", "Shayekh Bin Islam", "Arshia Soltani Moakhar", "Shivalika Singh", "Fabian Farestam", "Angelika Romanou", "Danylo Boiko", "Dipika Khullar", "Mike Zhang", "Dominik Krzemiński", "Jekaterina Novikova", "Luísa Shimabucoro", "Joseph Marvin Imperial", "Rishabh Maheshwary", "Sharad Duwal", "Alfonso Amayuelas", "Swati Rajwal", "Jebish Purbey", "Ahmed Ruby", "Nicholas Popovič", "Marek Suppa", "Azmine Toushik Wasi", "Ram Mohan Rao Kadiyala", "Olga Tsymboi", "Maksim Kostritsya", "Bardia Soltani Moakhar", "Gabriel da Costa Merlin", "Otávio Ferracioli Coletti", "Maral Jabbari Shiviari", "MohammadAmin farahani fard", "Silvia Fernandez", "María Grandury", "Dmitry Abulkhanov", "Drishti Sharma", "Andre Guarnier De Mitri", "Leticia Bossatto Marchezi", "Johan Obando-Ceron", "Nazar Kohut", "Beyza Ermis", "Desmond Elliott", "Enzo Ferrante", "Sara Hooker", "Marzieh Fadaee"], "title": "Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The evaluation of vision-language models (VLMs) has mainly relied on\nEnglish-language benchmarks, leaving significant gaps in both multilingual and\nmulticultural coverage. While multilingual benchmarks have expanded, both in\nsize and languages, many rely on translations of English datasets, failing to\ncapture cultural nuances. In this work, we propose Kaleidoscope, as the most\ncomprehensive exam benchmark to date for the multilingual evaluation of\nvision-language models. Kaleidoscope is a large-scale, in-language multimodal\nbenchmark designed to evaluate VLMs across diverse languages and visual inputs.\nKaleidoscope covers 18 languages and 14 different subjects, amounting to a\ntotal of 20,911 multiple-choice questions. Built through an open science\ncollaboration with a diverse group of researchers worldwide, Kaleidoscope\nensures linguistic and cultural authenticity. We evaluate top-performing\nmultilingual vision-language models and find that they perform poorly on\nlow-resource languages and in complex multimodal scenarios. Our results\nhighlight the need for progress on culturally inclusive multimodal evaluation\nframeworks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
