{"id": "2504.08010", "pdf": "https://arxiv.org/pdf/2504.08010", "abs": "https://arxiv.org/abs/2504.08010", "authors": ["Shuaicheng Niu", "Guohao Chen", "Peilin Zhao", "Tianyi Wang", "Pengcheng Wu", "Zhiqi Shen"], "title": "Self-Bootstrapping for Versatile Test-Time Adaptation", "categories": ["cs.CV", "cs.LG"], "comment": "16 pages, 10 tables, 4 figures", "summary": "In this paper, we seek to develop a versatile test-time adaptation (TTA)\nobjective for a variety of tasks - classification and regression across image-,\nobject-, and pixel-level predictions. We achieve this through a\nself-bootstrapping scheme that optimizes prediction consistency between the\ntest image (as target) and its deteriorated view. The key challenge lies in\ndevising effective augmentations/deteriorations that: i) preserve the image's\ngeometric information, e.g., object sizes and locations, which is crucial for\nTTA on object/pixel-level tasks, and ii) provide sufficient learning signals\nfor TTA. To this end, we analyze how common distribution shifts affect the\nimage's information power across spatial frequencies in the Fourier domain, and\nreveal that low-frequency components carry high power and masking these\ncomponents supplies more learning signals, while masking high-frequency\ncomponents can not. In light of this, we randomly mask the low-frequency\namplitude of an image in its Fourier domain for augmentation. Meanwhile, we\nalso augment the image with noise injection to compensate for missing learning\nsignals at high frequencies, by enhancing the information power there.\nExperiments show that, either independently or as a plug-and-play module, our\nmethod achieves superior results across classification, segmentation, and 3D\nmonocular detection tasks with both transformer and CNN models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08259", "pdf": "https://arxiv.org/pdf/2504.08259", "abs": "https://arxiv.org/abs/2504.08259", "authors": ["Ruohao Zhan", "Yijin Li", "Yisheng He", "Shuo Chen", "Yichen Shen", "Xinyu Chen", "Zilong Dong", "Zhaoyang Huang", "Guofeng Zhang"], "title": "CoProSketch: Controllable and Progressive Sketch Generation with Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 9 figures", "summary": "Sketches serve as fundamental blueprints in artistic creation because sketch\nediting is easier and more intuitive than pixel-level RGB image editing for\npainting artists, yet sketch generation remains unexplored despite advancements\nin generative models. We propose a novel framework CoProSketch, providing\nprominent controllability and details for sketch generation with diffusion\nmodels. A straightforward method is fine-tuning a pretrained image generation\ndiffusion model with binarized sketch images. However, we find that the\ndiffusion models fail to generate clear binary images, which makes the produced\nsketches chaotic. We thus propose to represent the sketches by unsigned\ndistance field (UDF), which is continuous and can be easily decoded to sketches\nthrough a lightweight network. With CoProSketch, users generate a rough sketch\nfrom a bounding box and a text prompt. The rough sketch can be manually edited\nand fed back into the model for iterative refinement and will be decoded to a\ndetailed sketch as the final result. Additionally, we curate the first\nlarge-scale text-sketch paired dataset as the training data. Experiments\ndemonstrate superior semantic consistency and controllability over baselines,\noffering a practical solution for integrating user feedback into generative\nworkflows.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "iterative refinement"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08348", "pdf": "https://arxiv.org/pdf/2504.08348", "abs": "https://arxiv.org/abs/2504.08348", "authors": ["Josef Bengtson", "David Nilsson", "Fredrik Kahl"], "title": "Geometric Consistency Refinement for Single Image Novel View Synthesis via Test-Time Adaptation of Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 EDGE Workshop. Project page:\n  https://gc-ref.github.io/", "summary": "Diffusion models for single image novel view synthesis (NVS) can generate\nhighly realistic and plausible images, but they are limited in the geometric\nconsistency to the given relative poses. The generated images often show\nsignificant errors with respect to the epipolar constraints that should be\nfulfilled, as given by the target pose. In this paper we address this issue by\nproposing a methodology to improve the geometric correctness of images\ngenerated by a diffusion model for single image NVS. We formulate a loss\nfunction based on image matching and epipolar constraints, and optimize the\nstarting noise in a diffusion sampling process such that the generated image\nshould both be a realistic image and fulfill geometric constraints derived from\nthe given target pose. Our method does not require training data or fine-tuning\nof the diffusion models, and we show that we can apply it to multiple\nstate-of-the-art models for single image NVS. The method is evaluated on the\nMegaScenes dataset and we show that geometric consistency is improved compared\nto the baseline models while retaining the quality of the generated images.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08736", "pdf": "https://arxiv.org/pdf/2504.08736", "abs": "https://arxiv.org/abs/2504.08736", "authors": ["Tianwei Xiong", "Jun Hao Liew", "Zilong Huang", "Jiashi Feng", "Xihui Liu"], "title": "GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation", "categories": ["cs.CV"], "comment": "project page: https://silentview.github.io/GigaTok", "summary": "In autoregressive (AR) image generation, visual tokenizers compress images\ninto compact discrete latent tokens, enabling efficient training of downstream\nautoregressive models for visual generation via next-token prediction. While\nscaling visual tokenizers improves image reconstruction quality, it often\ndegrades downstream generation quality -- a challenge not adequately addressed\nin existing literature. To address this, we introduce GigaTok, the first\napproach to simultaneously improve image reconstruction, generation, and\nrepresentation learning when scaling visual tokenizers. We identify the growing\ncomplexity of latent space as the key factor behind the reconstruction vs.\ngeneration dilemma. To mitigate this, we propose semantic regularization, which\naligns tokenizer features with semantically consistent features from a\npre-trained visual encoder. This constraint prevents excessive latent space\ncomplexity during scaling, yielding consistent improvements in both\nreconstruction and downstream autoregressive generation. Building on semantic\nregularization, we explore three key practices for scaling tokenizers:(1) using\n1D tokenizers for better scalability, (2) prioritizing decoder scaling when\nexpanding both encoder and decoder, and (3) employing entropy loss to stabilize\ntraining for billion-scale tokenizers. By scaling to $\\bf{3 \\space billion}$\nparameters, GigaTok achieves state-of-the-art performance in reconstruction,\ndownstream AR generation, and downstream AR representation quality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.07986", "pdf": "https://arxiv.org/pdf/2504.07986", "abs": "https://arxiv.org/abs/2504.07986", "authors": ["Runjin Chen", "Zhenyu Zhang", "Junyuan Hong", "Souvik Kundu", "Zhangyang Wang"], "title": "SEAL: Steerable Reasoning Calibration of Large Language Models for Free", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated\ncompelling capabilities for complex reasoning tasks via the extended\nchain-of-thought (CoT) reasoning mechanism. However, recent studies reveal\nsubstantial redundancy in the CoT reasoning traces, which not only increases\ninference latency but also negatively impacts model performance by diverting\nattention to unnecessary reasoning paths. To address this issue, we investigate\nthe internal reasoning structures of LLMs and categorize them into three\nprimary thought types: execution, reflection, and transition thoughts.\nMoreover, our analysis reveals that excessive reflection and transition\nthoughts are strongly correlated with failure cases and these thought\ncategories exhibit clear separation in the latent space. Based on these, we\nintroduce SEAL (Steerable reasoning calibration), a training-free approach that\nseamlessly calibrates the CoT process, improving accuracy while demonstrating\nsignificant efficiency gains. SEAL consists of an offline stage for extracting\nthe reasoning steering vector in the latent space, followed by an on-the-fly\ncalibration of the reasoning trace through representation intervention using\nthe steering vector. Notably, the steering vector exhibits strong\ntransferability across various tasks. Extensive experiments across multiple\nmodels (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500,\nGSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11%\nimprovement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our\ncode is publicly available at https://github.com/VITA-Group/SEAL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08110", "pdf": "https://arxiv.org/pdf/2504.08110", "abs": "https://arxiv.org/abs/2504.08110", "authors": ["Muhammad Saif Ullah Khan", "Stephan Krauß", "Didier Stricker"], "title": "Towards Unconstrained 2D Pose Estimation of the Human Spine", "categories": ["cs.CV"], "comment": "Accepted for publication in CVPRW 2025", "summary": "We present SpineTrack, the first comprehensive dataset for 2D spine pose\nestimation in unconstrained settings, addressing a crucial need in sports\nanalytics, healthcare, and realistic animation. Existing pose datasets often\nsimplify the spine to a single rigid segment, overlooking the nuanced\narticulation required for accurate motion analysis. In contrast, SpineTrack\nannotates nine detailed spinal keypoints across two complementary subsets: a\nsynthetic set comprising 25k annotations created using Unreal Engine with\nbiomechanical alignment through OpenSim, and a real-world set comprising over\n33k annotations curated via an active learning pipeline that iteratively\nrefines automated annotations with human feedback. This integrated approach\nensures anatomically consistent labels at scale, even for challenging,\nin-the-wild images. We further introduce SpinePose, extending state-of-the-art\nbody pose estimators using knowledge distillation and an anatomical\nregularization strategy to jointly predict body and spine keypoints. Our\nexperiments in both general and sports-specific contexts validate the\neffectiveness of SpineTrack for precise spine pose estimation, establishing a\nrobust foundation for future research in advanced biomechanical analysis and 3D\nspine reconstruction in the wild.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08212", "pdf": "https://arxiv.org/pdf/2504.08212", "abs": "https://arxiv.org/abs/2504.08212", "authors": ["Guangcong Zheng", "Teng Li", "Xianpan Zhou", "Xi Li"], "title": "RealCam-Vid: High-resolution Video Dataset with Dynamic Scenes and Metric-scale Camera Movements", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in camera-controllable video generation have been constrained\nby the reliance on static-scene datasets with relative-scale camera\nannotations, such as RealEstate10K. While these datasets enable basic viewpoint\ncontrol, they fail to capture dynamic scene interactions and lack metric-scale\ngeometric consistency-critical for synthesizing realistic object motions and\nprecise camera trajectories in complex environments. To bridge this gap, we\nintroduce the first fully open-source, high-resolution dynamic-scene dataset\nwith metric-scale camera annotations in https://github.com/ZGCTroy/RealCam-Vid.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08222", "pdf": "https://arxiv.org/pdf/2504.08222", "abs": "https://arxiv.org/abs/2504.08222", "authors": ["Zhaoyu Liu", "Kan Jiang", "Murong Ma", "Zhe Hou", "Yun Lin", "Jin Song Dong"], "title": "F$^3$Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos", "categories": ["cs.CV", "cs.AI"], "comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)", "summary": "Analyzing Fast, Frequent, and Fine-grained (F$^3$) events presents a\nsignificant challenge in video analytics and multi-modal LLMs. Current methods\nstruggle to identify events that satisfy all the F$^3$ criteria with high\naccuracy due to challenges such as motion blur and subtle visual discrepancies.\nTo advance research in video understanding, we introduce F$^3$Set, a benchmark\nthat consists of video datasets for precise F$^3$ event detection. Datasets in\nF$^3$Set are characterized by their extensive scale and comprehensive detail,\nusually encompassing over 1,000 event types with precise timestamps and\nsupporting multi-level granularity. Currently, F$^3$Set contains several sports\ndatasets, and this framework may be extended to other applications as well. We\nevaluated popular temporal action understanding methods on F$^3$Set, revealing\nsubstantial challenges for existing techniques. Additionally, we propose a new\nmethod, F$^3$ED, for F$^3$ event detections, achieving superior performance.\nThe dataset, model, and benchmark code are available at\nhttps://github.com/F3Set/F3Set.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "fine-grained", "criteria"], "score": 5}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08527", "pdf": "https://arxiv.org/pdf/2504.08527", "abs": "https://arxiv.org/abs/2504.08527", "authors": ["Taisei Kanda", "Mingzhe Jin", "Wataru Zaitsu"], "title": "Integrated ensemble of BERT- and features-based models for authorship attribution in Japanese literary works", "categories": ["cs.CL"], "comment": null, "summary": "Traditionally, authorship attribution (AA) tasks relied on statistical data\nanalysis and classification based on stylistic features extracted from texts.\nIn recent years, pre-trained language models (PLMs) have attracted significant\nattention in text classification tasks. However, although they demonstrate\nexcellent performance on large-scale short-text datasets, their effectiveness\nremains under-explored for small samples, particularly in AA tasks.\nAdditionally, a key challenge is how to effectively leverage PLMs in\nconjunction with traditional feature-based methods to advance AA research. In\nthis study, we aimed to significantly improve performance using an integrated\nintegrative ensemble of traditional feature-based and modern PLM-based methods\non an AA task in a small sample. For the experiment, we used two corpora of\nliterary works to classify 10 authors each. The results indicate that BERT is\neffective, even for small-sample AA tasks. Both BERT-based and classifier\nensembles outperformed their respective stand-alone models, and the integrated\nensemble approach further improved the scores significantly. For the corpus\nthat was not included in the pre-training data, the integrated ensemble\nimproved the F1 score by approximately 14 points, compared to the\nbest-performing single model. Our methodology provides a viable solution for\nthe efficient use of the ever-expanding array of data processing tools in the\nforeseeable future.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08280", "pdf": "https://arxiv.org/pdf/2504.08280", "abs": "https://arxiv.org/abs/2504.08280", "authors": ["Xiong Li", "Shulei Liu", "Xingning Chen", "Yisong Wu", "Dong Zhu"], "title": "PNE-SGAN: Probabilistic NDT-Enhanced Semantic Graph Attention Network for LiDAR Loop Closure Detection", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "LiDAR loop closure detection (LCD) is crucial for consistent Simultaneous\nLocalization and Mapping (SLAM) but faces challenges in robustness and\naccuracy. Existing methods, including semantic graph approaches, often suffer\nfrom coarse geometric representations and lack temporal robustness against\nnoise, dynamics, and viewpoint changes. We introduce PNE-SGAN, a Probabilistic\nNDT-Enhanced Semantic Graph Attention Network, to overcome these limitations.\nPNE-SGAN enhances semantic graphs by using Normal Distributions Transform (NDT)\ncovariance matrices as rich, discriminative geometric node features, processed\nvia a Graph Attention Network (GAT). Crucially, it integrates graph similarity\nscores into a probabilistic temporal filtering framework (modeled as an\nHMM/Bayes filter), incorporating uncertain odometry for motion modeling and\nutilizing forward-backward smoothing to effectively handle ambiguities.\nEvaluations on challenging KITTI sequences (00 and 08) demonstrate\nstate-of-the-art performance, achieving Average Precision of 96.2\\% and 95.1\\%,\nrespectively. PNE-SGAN significantly outperforms existing methods, particularly\nin difficult bidirectional loop scenarios where others falter. By synergizing\ndetailed NDT geometry with principled probabilistic temporal reasoning,\nPNE-SGAN offers a highly accurate and robust solution for LiDAR LCD, enhancing\nSLAM reliability in complex, large-scale environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08590", "pdf": "https://arxiv.org/pdf/2504.08590", "abs": "https://arxiv.org/abs/2504.08590", "authors": ["Nicola Horst", "Davide Mazzaccara", "Antonia Schmidt", "Michael Sullivan", "Filippo Momentè", "Luca Franceschetti", "Philipp Sadler", "Sherzod Hakimov", "Alberto Testoni", "Raffaella Bernardi", "Raquel Fernández", "Alexander Koller", "Oliver Lemon", "David Schlangen", "Mario Giulianelli", "Alessandro Suglia"], "title": "Playpen: An Environment for Exploring Learning Through Conversational Interaction", "categories": ["cs.CL"], "comment": "Source code: https://github.com/lm-playpen/playpen Please send\n  correspodence to: lm-playschool@googlegroups.com", "summary": "Are we running out of learning signal? Predicting the next word in an\nexisting text has turned out to be a powerful signal, at least at scale. But\nthere are signs that we are running out of this resource. In recent months,\ninteraction between learner and feedback-giver has come into focus, both for\n\"alignment\" (with a reward model judging the quality of instruction following\nattempts) and for improving \"reasoning\" (process- and outcome-based verifiers\njudging reasoning steps). In this paper, we explore to what extent synthetic\ninteraction in what we call Dialogue Games -- goal-directed and rule-governed\nactivities driven predominantly by verbal actions -- can provide a learning\nsignal, and how this signal can be used. We introduce an environment for\nproducing such interaction data (with the help of a Large Language Model as\ncounterpart to the learner model), both offline and online. We investigate the\neffects of supervised fine-tuning on this data, as well as reinforcement\nlearning setups such as DPO, and GRPO; showing that all of these approaches\nachieve some improvements in in-domain games, but only GRPO demonstrates the\nability to generalise to out-of-domain games as well as retain competitive\nperformance in reference-based tasks. We release the framework and the baseline\ntraining setups in the hope that this can foster research in this promising new\ndirection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "alignment", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08596", "pdf": "https://arxiv.org/pdf/2504.08596", "abs": "https://arxiv.org/abs/2504.08596", "authors": ["Gaya Mehenni", "Amal Zouaq"], "title": "MedHal: An Evaluation Dataset for Medical Hallucination Detection", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "We present MedHal, a novel large-scale dataset specifically designed to\nevaluate if models can detect hallucinations in medical texts. Current\nhallucination detection methods face significant limitations when applied to\nspecialized domains like medicine, where they can have disastrous consequences.\nExisting medical datasets are either too small, containing only a few hundred\nsamples, or focus on a single task like Question Answering or Natural Language\nInference. MedHal addresses these gaps by: (1) incorporating diverse medical\ntext sources and tasks; (2) providing a substantial volume of annotated samples\nsuitable for training medical hallucination detection models; and (3) including\nexplanations for factual inconsistencies to guide model learning. We\ndemonstrate MedHal's utility by training and evaluating a baseline medical\nhallucination detection model, showing improvements over general-purpose\nhallucination detection approaches. This resource enables more efficient\nevaluation of medical text generation systems while reducing reliance on costly\nexpert review, potentially accelerating the development of medical AI research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "question answering"], "score": 3}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08672", "pdf": "https://arxiv.org/pdf/2504.08672", "abs": "https://arxiv.org/abs/2504.08672", "authors": ["Fangzhi Xu", "Hang Yan", "Chang Ma", "Haiteng Zhao", "Qiushi Sun", "Kanzhi Cheng", "Junxian He", "Jun Liu", "Zhiyong Wu"], "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08358", "pdf": "https://arxiv.org/pdf/2504.08358", "abs": "https://arxiv.org/abs/2504.08358", "authors": ["Jiarui Wang", "Huiyu Duan", "Yu Zhao", "Juntong Wang", "Guangtao Zhai", "Xiongkuo Min"], "title": "LMM4LMM: Benchmarking and Evaluating Large-multimodal Image Generation with LMMs", "categories": ["cs.CV"], "comment": null, "summary": "Recent breakthroughs in large multimodal models (LMMs) have significantly\nadvanced both text-to-image (T2I) generation and image-to-text (I2T)\ninterpretation. However, many generated images still suffer from issues related\nto perceptual quality and text-image alignment. Given the high cost and\ninefficiency of manual evaluation, an automatic metric that aligns with human\npreferences is desirable. To this end, we present EvalMi-50K, a comprehensive\ndataset and benchmark for evaluating large-multimodal image generation, which\nfeatures (i) comprehensive tasks, encompassing 2,100 extensive prompts across\n20 fine-grained task dimensions, and (ii) large-scale human-preference\nannotations, including 100K mean-opinion scores (MOSs) and 50K\nquestion-answering (QA) pairs annotated on 50,400 images generated from 24 T2I\nmodels. Based on EvalMi-50K, we propose LMM4LMM, an LMM-based metric for\nevaluating large multimodal T2I generation from multiple dimensions including\nperception, text-image correspondence, and task-specific accuracy. Extensive\nexperimental results show that LMM4LMM achieves state-of-the-art performance on\nEvalMi-50K, and exhibits strong generalization ability on other AI-generated\nimage evaluation benchmark datasets, manifesting the generality of both the\nEvalMi-50K dataset and LMM4LMM metric. Both EvalMi-50K and LMM4LMM will be\nreleased at https://github.com/IntMeGroup/LMM4LMM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08361", "pdf": "https://arxiv.org/pdf/2504.08361", "abs": "https://arxiv.org/abs/2504.08361", "authors": ["Yi Chen", "Tianchen Deng", "Wentao Zhao", "Xiaoning Wang", "Wenqian Xi", "Weidong Chen", "Jingchuan Wang"], "title": "SN-LiDAR: Semantic Neural Fields for Novel Space-time View LiDAR Synthesis", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Recent research has begun exploring novel view synthesis (NVS) for LiDAR\npoint clouds, aiming to generate realistic LiDAR scans from unseen viewpoints.\nHowever, most existing approaches do not reconstruct semantic labels, which are\ncrucial for many downstream applications such as autonomous driving and robotic\nperception. Unlike images, which benefit from powerful segmentation models,\nLiDAR point clouds lack such large-scale pre-trained models, making semantic\nannotation time-consuming and labor-intensive. To address this challenge, we\npropose SN-LiDAR, a method that jointly performs accurate semantic\nsegmentation, high-quality geometric reconstruction, and realistic LiDAR\nsynthesis. Specifically, we employ a coarse-to-fine planar-grid feature\nrepresentation to extract global features from multi-frame point clouds and\nleverage a CNN-based encoder to extract local semantic features from the\ncurrent frame point cloud. Extensive experiments on SemanticKITTI and KITTI-360\ndemonstrate the superiority of SN-LiDAR in both semantic and geometric\nreconstruction, effectively handling dynamic objects and large-scale scenes.\nCodes will be available on https://github.com/dtc111111/SN-Lidar.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08719", "pdf": "https://arxiv.org/pdf/2504.08719", "abs": "https://arxiv.org/abs/2504.08719", "authors": ["Krishna C. Puvvada", "Faisal Ladhak", "Santiago Akle Serrano", "Cheng-Ping Hsieh", "Shantanu Acharya", "Somshubra Majumdar", "Fei Jia", "Samuel Kriman", "Simeng Sun", "Dima Rekesh", "Boris Ginsburg"], "title": "SWAN-GPT: An Efficient and Scalable Approach for Long-Context Language Modeling", "categories": ["cs.CL"], "comment": null, "summary": "We present a decoder-only Transformer architecture that robustly generalizes\nto sequence lengths substantially longer than those seen during training. Our\nmodel, SWAN-GPT, interleaves layers without positional encodings (NoPE) and\nsliding-window attention layers equipped with rotary positional encodings\n(SWA-RoPE). Experiments demonstrate strong performance on sequence lengths\nsignificantly longer than the training length without the need for additional\nlong-context training. This robust length extrapolation is achieved through our\nnovel architecture, enhanced by a straightforward dynamic scaling of attention\nscores during inference. In addition, SWAN-GPT is more computationally\nefficient than standard GPT architectures, resulting in cheaper training and\nhigher throughput. Further, we demonstrate that existing pre-trained\ndecoder-only models can be efficiently converted to the SWAN architecture with\nminimal continued training, enabling longer contexts. Overall, our work\npresents an effective approach for scaling language models to longer contexts\nin a robust and efficient manner.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08044", "pdf": "https://arxiv.org/pdf/2504.08044", "abs": "https://arxiv.org/abs/2504.08044", "authors": ["Tanmay Laud", "Akadia Kacha-Ochana", "Steven A. Sumner", "Vikram Krishnasamy", "Royal Law", "Lyna Schieber", "Munmun De Choudhury", "Mai ElSherief"], "title": "Large-Scale Analysis of Online Questions Related to Opioid Use Disorder on Reddit", "categories": ["cs.SI", "cs.CL"], "comment": "Accepted to ICWSM 2025", "summary": "Opioid use disorder (OUD) is a leading health problem that affects individual\nwell-being as well as general public health. Due to a variety of reasons,\nincluding the stigma faced by people using opioids, online communities for\nrecovery and support were formed on different social media platforms. In these\ncommunities, people share their experiences and solicit information by asking\nquestions to learn about opioid use and recovery. However, these communities do\nnot always contain clinically verified information. In this paper, we study\nnatural language questions asked in the context of OUD-related discourse on\nReddit. We adopt transformer-based question detection along with hierarchical\nclustering across 19 subreddits to identify six coarse-grained categories and\n69 fine-grained categories of OUD-related questions. Our analysis uncovers ten\nareas of information seeking from Reddit users in the context of OUD: drug\nsales, specific drug-related questions, OUD treatment, drug uses, side effects,\nwithdrawal, lifestyle, drug testing, pain management and others, during the\nstudy period of 2018-2021. Our work provides a major step in improving the\nunderstanding of OUD-related questions people ask unobtrusively on Reddit. We\nfinally discuss technological interventions and public health harm reduction\ntechniques based on the topics of these questions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08247", "pdf": "https://arxiv.org/pdf/2504.08247", "abs": "https://arxiv.org/abs/2504.08247", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "Millions of States: Designing a Scalable MoE Architecture with RWKV-7 Meta-learner", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "State-based sequence models like RWKV-7 offer a compelling alternative to\nTransformer architectures, achieving linear complexity while demonstrating\ngreater expressive power in short-context scenarios and enabling state tracking\nbeyond the \\(\\text{TC}^0\\) complexity class. However, RWKV-7 lacks mechanisms\nfor token-parameter interactions and native scalability, limiting its\nadaptability and growth without retraining. In this paper, we propose\n\\textbf{Meta-State}, a novel extension to RWKV-7 that replaces attention\nmechanisms with a fully state-driven approach, integrating token-parameter\ninteractions through a \\textbf{Self-State Encoder} (SSE) mechanism. The SSE\nrepurposes a portion of the RWKV-7 Weighted Key-Value (WKV) state as\ntransformation weights to encode token-parameter interactions in a linear,\nstate-driven manner without introducing new trainable matrices or softmax\noperations, while preserving the autoregressive property of token processing.\nMeta-State supports progressive model scaling by expanding the WKV state and\nparameter tokens, reusing existing parameters without retraining. Our approach\nbridges the gap between state-based modeling, token-parameter interactions, and\nscalable architectures, offering a flexible framework for efficient and\nadaptable sequence modeling with linear complexity and constant memory usage.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08641", "pdf": "https://arxiv.org/pdf/2504.08641", "abs": "https://arxiv.org/abs/2504.08641", "authors": ["Jialu Li", "Shoubin Yu", "Han Lin", "Jaemin Cho", "Jaehong Yoon", "Mohit Bansal"], "title": "Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Website: https://video-msg.github.io; The first three authors\n  contributed equally", "summary": "Recent advancements in text-to-video (T2V) diffusion models have\nsignificantly enhanced the visual quality of the generated videos. However,\neven recent T2V models find it challenging to follow text descriptions\naccurately, especially when the prompt requires accurate control of spatial\nlayouts or object trajectories. A recent line of research uses layout guidance\nfor T2V models that require fine-tuning or iterative manipulation of the\nattention map during inference time. This significantly increases the memory\nrequirement, making it difficult to adopt a large T2V model as a backbone. To\naddress this, we introduce Video-MSG, a training-free Guidance method for T2V\ngeneration based on Multimodal planning and Structured noise initialization.\nVideo-MSG consists of three steps, where in the first two steps, Video-MSG\ncreates Video Sketch, a fine-grained spatio-temporal plan for the final video,\nspecifying background, foreground, and object trajectories, in the form of\ndraft video frames. In the last step, Video-MSG guides a downstream T2V\ndiffusion model with Video Sketch through noise inversion and denoising.\nNotably, Video-MSG does not need fine-tuning or attention manipulation with\nadditional memory during inference time, making it easier to adopt large T2V\nmodels. Video-MSG demonstrates its effectiveness in enhancing text alignment\nwith multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V\ngeneration benchmarks (T2VCompBench and VBench). We provide comprehensive\nablation studies about noise inversion ratio, different background generators,\nbackground object detection, and foreground object segmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08714", "pdf": "https://arxiv.org/pdf/2504.08714", "abs": "https://arxiv.org/abs/2504.08714", "authors": ["Xinyi Gu", "Jiayuan Mao"], "title": "Generating Fine Details of Entity Interactions", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project Page: https://concepts-ai.com/p/detailscribe/", "summary": "Images not only depict objects but also encapsulate rich interactions between\nthem. However, generating faithful and high-fidelity images involving multiple\nentities interacting with each other, is a long-standing challenge. While\npre-trained text-to-image models are trained on large-scale datasets to follow\ndiverse text instructions, they struggle to generate accurate interactions,\nlikely due to the scarcity of training data for uncommon object interactions.\nThis paper introduces InterActing, an interaction-focused dataset with 1000\nfine-grained prompts covering three key scenarios: (1) functional and\naction-based interactions, (2) compositional spatial relationships, and (3)\nmulti-subject interactions. To address interaction generation challenges, we\npropose a decomposition-augmented refinement procedure. Our approach,\nDetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose\ninteractions into finer-grained concepts, uses a VLM to critique generated\nimages, and applies targeted interventions within the diffusion process in\nrefinement. Automatic and human evaluations show significantly improved image\nquality, demonstrating the potential of enhanced inference strategies. Our\ndataset and code are available at https://concepts-ai.com/p/detailscribe/ to\nfacilitate future exploration of interaction-rich image generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08584", "pdf": "https://arxiv.org/pdf/2504.08584", "abs": "https://arxiv.org/abs/2504.08584", "authors": ["Mahshad Lotfinia", "Arash Tayebiarasteh", "Samaneh Samiei", "Mehdi Joodaki", "Soroosh Tayebi Arasteh"], "title": "Boosting multi-demographic federated learning for chest x-ray analysis using general-purpose self-supervised representations", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Reliable artificial intelligence (AI) models for medical image analysis often\ndepend on large and diverse labeled datasets. Federated learning (FL) offers a\ndecentralized and privacy-preserving approach to training but struggles in\nhighly non-independent and identically distributed (non-IID) settings, where\ninstitutions with more representative data may experience degraded performance.\nMoreover, existing large-scale FL studies have been limited to adult datasets,\nneglecting the unique challenges posed by pediatric data, which introduces\nadditional non-IID variability. To address these limitations, we analyzed\nn=398,523 adult chest radiographs from diverse institutions across multiple\ncountries and n=9,125 pediatric images, leveraging transfer learning from\ngeneral-purpose self-supervised image representations to classify pneumonia and\ncases with no abnormality. Using state-of-the-art vision transformers, we found\nthat FL improved performance only for smaller adult datasets (P<0.001) but\ndegraded performance for larger datasets (P<0.064) and pediatric cases\n(P=0.242). However, equipping FL with self-supervised weights significantly\nenhanced outcomes across pediatric cases (P=0.031) and most adult datasets\n(P<0.008), except the largest dataset (P=0.052). These findings underscore the\npotential of easily deployable general-purpose self-supervised image\nrepresentations to address non-IID challenges in clinical FL applications and\nhighlight their promise for enhancing patient outcomes and advancing pediatric\nhealthcare, where data scarcity and variability remain persistent obstacles.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08641", "pdf": "https://arxiv.org/pdf/2504.08641", "abs": "https://arxiv.org/abs/2504.08641", "authors": ["Jialu Li", "Shoubin Yu", "Han Lin", "Jaemin Cho", "Jaehong Yoon", "Mohit Bansal"], "title": "Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Website: https://video-msg.github.io; The first three authors\n  contributed equally", "summary": "Recent advancements in text-to-video (T2V) diffusion models have\nsignificantly enhanced the visual quality of the generated videos. However,\neven recent T2V models find it challenging to follow text descriptions\naccurately, especially when the prompt requires accurate control of spatial\nlayouts or object trajectories. A recent line of research uses layout guidance\nfor T2V models that require fine-tuning or iterative manipulation of the\nattention map during inference time. This significantly increases the memory\nrequirement, making it difficult to adopt a large T2V model as a backbone. To\naddress this, we introduce Video-MSG, a training-free Guidance method for T2V\ngeneration based on Multimodal planning and Structured noise initialization.\nVideo-MSG consists of three steps, where in the first two steps, Video-MSG\ncreates Video Sketch, a fine-grained spatio-temporal plan for the final video,\nspecifying background, foreground, and object trajectories, in the form of\ndraft video frames. In the last step, Video-MSG guides a downstream T2V\ndiffusion model with Video Sketch through noise inversion and denoising.\nNotably, Video-MSG does not need fine-tuning or attention manipulation with\nadditional memory during inference time, making it easier to adopt large T2V\nmodels. Video-MSG demonstrates its effectiveness in enhancing text alignment\nwith multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V\ngeneration benchmarks (T2VCompBench and VBench). We provide comprehensive\nablation studies about noise inversion ratio, different background generators,\nbackground object detection, and foreground object segmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08714", "pdf": "https://arxiv.org/pdf/2504.08714", "abs": "https://arxiv.org/abs/2504.08714", "authors": ["Xinyi Gu", "Jiayuan Mao"], "title": "Generating Fine Details of Entity Interactions", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project Page: https://concepts-ai.com/p/detailscribe/", "summary": "Images not only depict objects but also encapsulate rich interactions between\nthem. However, generating faithful and high-fidelity images involving multiple\nentities interacting with each other, is a long-standing challenge. While\npre-trained text-to-image models are trained on large-scale datasets to follow\ndiverse text instructions, they struggle to generate accurate interactions,\nlikely due to the scarcity of training data for uncommon object interactions.\nThis paper introduces InterActing, an interaction-focused dataset with 1000\nfine-grained prompts covering three key scenarios: (1) functional and\naction-based interactions, (2) compositional spatial relationships, and (3)\nmulti-subject interactions. To address interaction generation challenges, we\npropose a decomposition-augmented refinement procedure. Our approach,\nDetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose\ninteractions into finer-grained concepts, uses a VLM to critique generated\nimages, and applies targeted interventions within the diffusion process in\nrefinement. Automatic and human evaluations show significantly improved image\nquality, demonstrating the potential of enhanced inference strategies. Our\ndataset and code are available at https://concepts-ai.com/p/detailscribe/ to\nfacilitate future exploration of interaction-rich image generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08718", "pdf": "https://arxiv.org/pdf/2504.08718", "abs": "https://arxiv.org/abs/2504.08718", "authors": ["Haohang Jian", "Jinlu Zhang", "Junyi Wu", "Zhigang Tu"], "title": "EMO-X: Efficient Multi-Person Pose and Shape Estimation in One-Stage", "categories": ["cs.CV"], "comment": null, "summary": "Expressive Human Pose and Shape Estimation (EHPS) aims to jointly estimate\nhuman pose, hand gesture, and facial expression from monocular images. Existing\nmethods predominantly rely on Transformer-based architectures, which suffer\nfrom quadratic complexity in self-attention, leading to substantial\ncomputational overhead, especially in multi-person scenarios. Recently, Mamba\nhas emerged as a promising alternative to Transformers due to its efficient\nglobal modeling capability. However, it remains limited in capturing\nfine-grained local dependencies, which are essential for precise EHPS. To\naddress these issues, we propose EMO-X, the Efficient Multi-person One-stage\nmodel for multi-person EHPS. Specifically, we explore a Scan-based Global-Local\nDecoder (SGLD) that integrates global context with skeleton-aware local\nfeatures to iteratively enhance human tokens. Our EMO-X leverages the superior\nglobal modeling capability of Mamba and designs a local bidirectional scan\nmechanism for skeleton-aware local refinement. Comprehensive experiments\ndemonstrate that EMO-X strikes an excellent balance between efficiency and\naccuracy. Notably, it achieves a significant reduction in computational\ncomplexity, requiring 69.8% less inference time compared to state-of-the-art\n(SOTA) methods, while outperforming most of them in accuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08177", "pdf": "https://arxiv.org/pdf/2504.08177", "abs": "https://arxiv.org/abs/2504.08177", "authors": ["Sourya Sengupta", "Satrajit Chakrabarty", "Keerthi Sravan Ravi", "Gopal Avinash", "Ravi Soni"], "title": "SynthFM: Training Modality-agnostic Foundation Models for Medical Image Segmentation without Real Medical Data", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Foundation models like the Segment Anything Model (SAM) excel in zero-shot\nsegmentation for natural images but struggle with medical image segmentation\ndue to differences in texture, contrast, and noise. Annotating medical images\nis costly and requires domain expertise, limiting large-scale annotated data\navailability. To address this, we propose SynthFM, a synthetic data generation\nframework that mimics the complexities of medical images, enabling foundation\nmodels to adapt without real medical data. Using SAM's pretrained encoder and\ntraining the decoder from scratch on SynthFM's dataset, we evaluated our method\non 11 anatomical structures across 9 datasets (CT, MRI, and Ultrasound).\nSynthFM outperformed zero-shot baselines like SAM and MedSAM, achieving\nsuperior results under different prompt settings and on out-of-distribution\ndatasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08431", "pdf": "https://arxiv.org/pdf/2504.08431", "abs": "https://arxiv.org/abs/2504.08431", "authors": ["Jiafan Lu", "Dongcheng Hu", "Yitian Ye", "Anqi Liu", "Zixian Zhang", "Xin Peng"], "title": "The Composite Visual-Laser Navigation Method Applied in Indoor Poultry Farming Environments", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Indoor poultry farms require inspection robots to maintain precise\nenvironmental control, which is crucial for preventing the rapid spread of\ndisease and large-scale bird mortality. However, the complex conditions within\nthese facilities, characterized by areas of intense illumination and water\naccumulation, pose significant challenges. Traditional navigation methods that\nrely on a single sensor often perform poorly in such environments, resulting in\nissues like laser drift and inaccuracies in visual navigation line extraction.\nTo overcome these limitations, we propose a novel composite navigation method\nthat integrates both laser and vision technologies. This approach dynamically\ncomputes a fused yaw angle based on the real-time reliability of each sensor\nmodality, thereby eliminating the need for physical navigation lines.\nExperimental validation in actual poultry house environments demonstrates that\nour method not only resolves the inherent drawbacks of single-sensor systems,\nbut also significantly enhances navigation precision and operational\nefficiency. As such, it presents a promising solution for improving the\nperformance of inspection robots in complex indoor poultry farming settings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08541", "pdf": "https://arxiv.org/pdf/2504.08541", "abs": "https://arxiv.org/abs/2504.08541", "authors": ["Zhao Dong", "Ka Chen", "Zhaoyang Lv", "Hong-Xing Yu", "Yunzhi Zhang", "Cheng Zhang", "Yufeng Zhu", "Stephen Tian", "Zhengqin Li", "Geordie Moffatt", "Sean Christofferson", "James Fort", "Xiaqing Pan", "Mingfei Yan", "Jiajun Wu", "Carl Yuheng Ren", "Richard Newcombe"], "title": "Digital Twin Catalog: A Large-Scale Photorealistic 3D Object Digital Twin Dataset", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.RO"], "comment": "accepted to CVPR 2025 highlights", "summary": "We introduce Digital Twin Catalog (DTC), a new large-scale photorealistic 3D\nobject digital twin dataset. A digital twin of a 3D object is a highly\ndetailed, virtually indistinguishable representation of a physical object,\naccurately capturing its shape, appearance, physical properties, and other\nattributes. Recent advances in neural-based 3D reconstruction and inverse\nrendering have significantly improved the quality of 3D object reconstruction.\nDespite these advancements, there remains a lack of a large-scale, digital twin\nquality real-world dataset and benchmark that can quantitatively assess and\ncompare the performance of different reconstruction methods, as well as improve\nreconstruction quality through training or fine-tuning. Moreover, to\ndemocratize 3D digital twin creation, it is essential to integrate creation\ntechniques with next-generation egocentric computing platforms, such as AR\nglasses. Currently, there is no dataset available to evaluate 3D object\nreconstruction using egocentric captured images. To address these gaps, the DTC\ndataset features 2,000 scanned digital twin-quality 3D objects, along with\nimage sequences captured under different lighting conditions using DSLR cameras\nand egocentric AR glasses. This dataset establishes the first comprehensive\nreal-world evaluation benchmark for 3D digital twin creation tasks, offering a\nrobust foundation for comparing and improving existing reconstruction methods.\nThe DTC dataset is already released at\nhttps://www.projectaria.com/datasets/dtc/ and we will also make the baseline\nevaluations open-source.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08603", "pdf": "https://arxiv.org/pdf/2504.08603", "abs": "https://arxiv.org/abs/2504.08603", "authors": ["Sebastián Barbas Laina", "Simon Boche", "Sotiris Papatheodorou", "Simon Schaefer", "Jaehyung Jung", "Stefan Leutenegger"], "title": "FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "11 pages, 5 figures", "summary": "Geometrically accurate and semantically expressive map representations have\nproven invaluable to facilitate robust and safe mobile robot navigation and\ntask planning. Nevertheless, real-time, open-vocabulary semantic understanding\nof large-scale unknown environments is still an open problem. In this paper we\npresent FindAnything, an open-world mapping and exploration framework that\nincorporates vision-language information into dense volumetric submaps. Thanks\nto the use of vision-language features, FindAnything bridges the gap between\npure geometric and open-vocabulary semantic information for a higher level of\nunderstanding while allowing to explore any environment without the help of any\nexternal source of ground-truth pose information. We represent the environment\nas a series of volumetric occupancy submaps, resulting in a robust and accurate\nmap representation that deforms upon pose updates when the underlying SLAM\nsystem corrects its drift, allowing for a locally consistent representation\nbetween submaps. Pixel-wise vision-language features are aggregated from\nefficient SAM (eSAM)-generated segments, which are in turn integrated into\nobject-centric volumetric submaps, providing a mapping from open-vocabulary\nqueries to 3D geometry that is scalable also in terms of memory usage. The\nopen-vocabulary map representation of FindAnything achieves state-of-the-art\nsemantic accuracy in closed-set evaluations on the Replica dataset. This level\nof scene understanding allows a robot to explore environments based on objects\nor areas of interest selected via natural language queries. Our system is the\nfirst of its kind to be deployed on resource-constrained devices, such as MAVs,\nleveraging vision-language information for real-world robotic tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-14.jsonl"}
