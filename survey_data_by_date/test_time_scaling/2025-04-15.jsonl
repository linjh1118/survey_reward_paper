{"id": "2504.09639", "pdf": "https://arxiv.org/pdf/2504.09639", "abs": "https://arxiv.org/abs/2504.09639", "authors": ["Haotian Wang", "Han Zhao", "Shuaiting Chen", "Xiaoyu Tian", "Sitong Zhao", "Yunjie Ji", "Yiping Peng", "Xiangang Li"], "title": "Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs), such as DeepSeek-R1 and\nOpenAI-o1, have demonstrated the significant effectiveness of test-time\nscaling, achieving substantial performance gains across various benchmarks.\nThese advanced models utilize deliberate \"thinking\" steps to systematically\nenhance answer quality. In this paper, we propose leveraging these high-quality\noutputs generated by reasoning-intensive models to improve less computationally\ndemanding, non-reasoning models. We explore and compare methodologies for\nutilizing the answers produced by reasoning models to train and improve\nnon-reasoning models. Through straightforward Supervised Fine-Tuning (SFT)\nexperiments on established benchmarks, we demonstrate consistent improvements\nacross various benchmarks, underscoring the potential of this approach for\nadvancing the ability of models to answer questions directly.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "o1", "reasoning model"], "score": 4}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09639", "pdf": "https://arxiv.org/pdf/2504.09639", "abs": "https://arxiv.org/abs/2504.09639", "authors": ["Haotian Wang", "Han Zhao", "Shuaiting Chen", "Xiaoyu Tian", "Sitong Zhao", "Yunjie Ji", "Yiping Peng", "Xiangang Li"], "title": "Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs), such as DeepSeek-R1 and\nOpenAI-o1, have demonstrated the significant effectiveness of test-time\nscaling, achieving substantial performance gains across various benchmarks.\nThese advanced models utilize deliberate \"thinking\" steps to systematically\nenhance answer quality. In this paper, we propose leveraging these high-quality\noutputs generated by reasoning-intensive models to improve less computationally\ndemanding, non-reasoning models. We explore and compare methodologies for\nutilizing the answers produced by reasoning models to train and improve\nnon-reasoning models. Through straightforward Supervised Fine-Tuning (SFT)\nexperiments on established benchmarks, we demonstrate consistent improvements\nacross various benchmarks, underscoring the potential of this approach for\nadvancing the ability of models to answer questions directly.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "o1", "reasoning model"], "score": 4}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09130", "pdf": "https://arxiv.org/pdf/2504.09130", "abs": "https://arxiv.org/abs/2504.09130", "authors": ["Yikun Wang", "Siyin Wang", "Qinyuan Cheng", "Zhaoye Fei", "Liang Ding", "Qipeng Guo", "Dacheng Tao", "Xipeng Qiu"], "title": "VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search", "categories": ["cs.CL"], "comment": "12 pages", "summary": "Recent advancements in Large Vision-Language Models have showcased remarkable\ncapabilities. However, they often falter when confronted with complex reasoning\ntasks that humans typically address through visual aids and deliberate,\nstep-by-step thinking. While existing methods have explored text-based slow\nthinking or rudimentary visual assistance, they fall short of capturing the\nintricate, interleaved nature of human visual-verbal reasoning processes. To\novercome these limitations and inspired by the mechanisms of slow thinking in\nhuman cognition, we introduce VisuoThink, a novel framework that seamlessly\nintegrates visuospatial and linguistic domains. VisuoThink facilitates\nmultimodal slow thinking by enabling progressive visual-textual reasoning and\nincorporates test-time scaling through look-ahead tree search. Extensive\nexperiments demonstrate that VisuoThink significantly enhances reasoning\ncapabilities via inference-time scaling, even without fine-tuning, achieving\nstate-of-the-art performance in tasks involving geometry and spatial reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time", "scaling"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09130", "pdf": "https://arxiv.org/pdf/2504.09130", "abs": "https://arxiv.org/abs/2504.09130", "authors": ["Yikun Wang", "Siyin Wang", "Qinyuan Cheng", "Zhaoye Fei", "Liang Ding", "Qipeng Guo", "Dacheng Tao", "Xipeng Qiu"], "title": "VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search", "categories": ["cs.CL"], "comment": "12 pages", "summary": "Recent advancements in Large Vision-Language Models have showcased remarkable\ncapabilities. However, they often falter when confronted with complex reasoning\ntasks that humans typically address through visual aids and deliberate,\nstep-by-step thinking. While existing methods have explored text-based slow\nthinking or rudimentary visual assistance, they fall short of capturing the\nintricate, interleaved nature of human visual-verbal reasoning processes. To\novercome these limitations and inspired by the mechanisms of slow thinking in\nhuman cognition, we introduce VisuoThink, a novel framework that seamlessly\nintegrates visuospatial and linguistic domains. VisuoThink facilitates\nmultimodal slow thinking by enabling progressive visual-textual reasoning and\nincorporates test-time scaling through look-ahead tree search. Extensive\nexperiments demonstrate that VisuoThink significantly enhances reasoning\ncapabilities via inference-time scaling, even without fine-tuning, achieving\nstate-of-the-art performance in tasks involving geometry and spatial reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time", "scaling"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09354", "pdf": "https://arxiv.org/pdf/2504.09354", "abs": "https://arxiv.org/abs/2504.09354", "authors": ["Duy-Cat Can", "Quang-Huy Tang", "Huong Ha", "Binh T. Nguyen", "Oliver Y. Chén"], "title": "REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Timely and accurate diagnosis of neurodegenerative disorders, such as\nAlzheimer's disease, is central to disease management. Existing deep learning\nmodels require large-scale annotated datasets and often function as \"black\nboxes\". Additionally, datasets in clinical practice are frequently small or\nunlabeled, restricting the full potential of deep learning methods. Here, we\nintroduce REMEMBER -- Retrieval-based Explainable Multimodal Evidence-guided\nModeling for Brain Evaluation and Reasoning -- a new machine learning framework\nthat facilitates zero- and few-shot Alzheimer's diagnosis using brain MRI scans\nthrough a reference-based reasoning process. Specifically, REMEMBER first\ntrains a contrastively aligned vision-text model using expert-annotated\nreference data and extends pseudo-text modalities that encode abnormality\ntypes, diagnosis labels, and composite clinical descriptions. Then, at\ninference time, REMEMBER retrieves similar, human-validated cases from a\ncurated dataset and integrates their contextual information through a dedicated\nevidence encoding module and attention-based inference head. Such an\nevidence-guided design enables REMEMBER to imitate real-world clinical\ndecision-making process by grounding predictions in retrieved imaging and\ntextual context. Specifically, REMEMBER outputs diagnostic predictions\nalongside an interpretable report, including reference images and explanations\naligned with clinical workflows. Experimental results demonstrate that REMEMBER\nachieves robust zero- and few-shot performance and offers a powerful and\nexplainable framework to neuroimaging-based diagnosis in the real world,\nespecially under limited data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09903", "pdf": "https://arxiv.org/pdf/2504.09903", "abs": "https://arxiv.org/abs/2504.09903", "authors": ["Bo-Wei Chen", "An-Zi Yen", "Chung-Chi Chen"], "title": "Refining Financial Consumer Complaints through Multi-Scale Model Interaction", "categories": ["cs.CL"], "comment": null, "summary": "Legal writing demands clarity, formality, and domain-specific\nprecision-qualities often lacking in documents authored by individuals without\nlegal training. To bridge this gap, this paper explores the task of legal text\nrefinement that transforms informal, conversational inputs into persuasive\nlegal arguments. We introduce FinDR, a Chinese dataset of financial dispute\nrecords, annotated with official judgments on claim reasonableness. Our\nproposed method, Multi-Scale Model Interaction (MSMI), leverages a lightweight\nclassifier to evaluate outputs and guide iterative refinement by Large Language\nModels (LLMs). Experimental results demonstrate that MSMI significantly\noutperforms single-pass prompting strategies. Additionally, we validate the\ngeneralizability of MSMI on several short-text benchmarks, showing improved\nadversarial robustness. Our findings reveal the potential of multi-model\ncollaboration for enhancing legal document generation and broader text\nrefinement tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "iterative refinement"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09641", "pdf": "https://arxiv.org/pdf/2504.09641", "abs": "https://arxiv.org/abs/2504.09641", "authors": ["Xingjian Zhang", "Siwei Wen", "Wenjun Wu", "Lei Huang"], "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recently, improving the reasoning ability of large multimodal models (LMMs)\nthrough reinforcement learning has made great progress. However, most existing\nworks are based on highly reasoning-intensive datasets such as mathematics and\ncode, and researchers generally choose large-scale models as the foundation. We\nargue that exploring small-scale models' reasoning capabilities remains\nvaluable for researchers with limited computational resources. Moreover,\nenabling models to explain their reasoning processes on general\nquestion-answering datasets is equally meaningful. Therefore, we present the\nsmall-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video,\na traceably trained video understanding model with no more than 4B parameters,\nit not only demonstrates significantly improved reasoning and thinking\ncapabilities after using reinforcement learning on general Video-QA datasets,\nbut also exhibits the emergent characteristic of \"aha moments\". Furthermore, we\nshare a series of experimental findings, aiming to provide practical insights\nfor future exploration of video reasoning (thinking) abilities in small-scale\nmodels. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "reasoning model"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10342", "pdf": "https://arxiv.org/pdf/2504.10342", "abs": "https://arxiv.org/abs/2504.10342", "authors": ["Yueqi Song", "Tianyue Ou", "Yibo Kong", "Zecheng Li", "Graham Neubig", "Xiang Yue"], "title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge", "categories": ["cs.CL"], "comment": "56 pages, 43 figures", "summary": "Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "inference compute"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "correlation"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09878", "pdf": "https://arxiv.org/pdf/2504.09878", "abs": "https://arxiv.org/abs/2504.09878", "authors": ["Yunpeng Tan", "Junlin Hao", "Jiangkai Wu", "Liming Liu", "Qingyang Li", "Xinggong Zhang"], "title": "MCBlock: Boosting Neural Radiance Field Training Speed by MCTS-based Dynamic-Resolution Ray Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Neural Radiance Field (NeRF) is widely known for high-fidelity novel view\nsynthesis. However, even the state-of-the-art NeRF model, Gaussian Splatting,\nrequires minutes for training, far from the real-time performance required by\nmultimedia scenarios like telemedicine. One of the obstacles is its inefficient\nsampling, which is only partially addressed by existing works. Existing\npoint-sampling algorithms uniformly sample simple-texture regions (easy to fit)\nand complex-texture regions (hard to fit), while existing ray-sampling\nalgorithms sample these regions all in the finest granularity (i.e. the pixel\nlevel), both wasting GPU training resources. Actually, regions with different\ntexture intensities require different sampling granularities. To this end, we\npropose a novel dynamic-resolution ray-sampling algorithm, MCBlock, which\nemploys Monte Carlo Tree Search (MCTS) to partition each training image into\npixel blocks with different sizes for active block-wise training. Specifically,\nthe trees are initialized according to the texture of training images to boost\nthe initialization speed, and an expansion/pruning module dynamically optimizes\nthe block partition. MCBlock is implemented in Nerfstudio, an open-source\ntoolset, and achieves a training acceleration of up to 2.33x, surpassing other\nray-sampling algorithms. We believe MCBlock can apply to any cone-tracing NeRF\nmodel and contribute to the multimedia community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search", "MCTS"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10481", "pdf": "https://arxiv.org/pdf/2504.10481", "abs": "https://arxiv.org/abs/2504.10481", "authors": ["Ding Chen", "Qingchen Yu", "Pengyuan Wang", "Wentao Zhang", "Bo Tang", "Feiyu Xiong", "Xinchi Li", "Minchuan Yang", "Zhiyu Li"], "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations", "categories": ["cs.CL"], "comment": "32 pages", "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1", "reasoning model"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation", "accuracy"], "score": 4}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08801", "pdf": "https://arxiv.org/pdf/2504.08801", "abs": "https://arxiv.org/abs/2504.08801", "authors": ["Andrew Kiruluta", "Priscilla Burity", "Samantha Williams"], "title": "Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformer architectures, underpinned by the self-attention mechanism, have\nachieved state-of-the-art results across numerous natural language processing\n(NLP) tasks by effectively modeling long-range dependencies. However, the\ncomputational complexity of self-attention, scaling quadratically with input\nsequence length, presents significant challenges for processing very long\nsequences or operating under resource constraints. This paper introduces the\nLearnable Multi-Scale Wavelet Transformer (LMWT), a novel architecture that\nreplaces the standard dot-product self-attention with a learnable multi-scale\nHaar wavelet transform module. Leveraging the intrinsic multi-resolution\nproperties of wavelets, the LMWT efficiently captures both local details and\nglobal context. Crucially, the parameters of the wavelet transform, including\nscale-specific coefficients, are learned end-to-end during training, allowing\nthe model to adapt its decomposition strategy to the data and task. We present\nthe detailed mathematical formulation of the learnable Haar wavelet module and\nits integration into the transformer framework, supplemented by an\narchitectural diagram. We conduct a comprehensive experimental evaluation on a\nstandard machine translation benchmark (WMT16 En-De), comparing the LMWT\nagainst a baseline self-attention transformer using metrics like BLEU score,\nperplexity, and token accuracy. Furthermore, we analyze the computational\ncomplexity, highlighting the linear scaling of our approach, discuss its\nnovelty in the context of related work, and explore the interpretability\noffered by visualizing the learned Haar coefficients. Our results indicate that\nthe LMWT achieves competitive performance while offering substantial\ncomputational advantages, positioning it as a promising and novel alternative\nfor efficient sequence modeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09967", "pdf": "https://arxiv.org/pdf/2504.09967", "abs": "https://arxiv.org/abs/2504.09967", "authors": ["Xun Zhu", "Fanbin Mo", "Zheng Zhang", "Jiaxi Wang", "Yiming Shi", "Ming Wu", "Chuang Zhang", "Miao Li", "Ji Wu"], "title": "Enhancing Multi-task Learning Capability of Medical Generalist Foundation Model via Image-centric Multi-annotation Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of medical generalist foundation models has revolutionized\nconventional task-specific model development paradigms, aiming to better handle\nmultiple tasks through joint training on large-scale medical datasets. However,\nrecent advances prioritize simple data scaling or architectural component\nenhancement, while neglecting to re-examine multi-task learning from a\ndata-centric perspective. Critically, simply aggregating existing data\nresources leads to decentralized image-task alignment, which fails to cultivate\ncomprehensive image understanding or align with clinical needs for\nmulti-dimensional image interpretation. In this paper, we introduce the\nimage-centric multi-annotation X-ray dataset (IMAX), the first attempt to\nenhance the multi-task learning capabilities of medical multi-modal large\nlanguage models (MLLMs) from the data construction level. To be specific, IMAX\nis featured from the following attributes: 1) High-quality data curation. A\ncomprehensive collection of more than 354K entries applicable to seven\ndifferent medical tasks. 2) Image-centric dense annotation. Each X-ray image is\nassociated with an average of 4.10 tasks and 7.46 training entries, ensuring\nmulti-task representation richness per image. Compared to the general\ndecentralized multi-annotation X-ray dataset (DMAX), IMAX consistently\ndemonstrates significant multi-task average performance gains ranging from\n3.20% to 21.05% across seven open-source state-of-the-art medical MLLMs.\nMoreover, we investigate differences in statistical patterns exhibited by IMAX\nand DMAX training processes, exploring potential correlations between\noptimization dynamics and multi-task performance. Finally, leveraging the core\nconcept of IMAX data construction, we propose an optimized DMAX-based training\nstrategy to alleviate the dilemma of obtaining high-quality IMAX data in\npractical scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "multi-dimensional"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09037", "pdf": "https://arxiv.org/pdf/2504.09037", "abs": "https://arxiv.org/abs/2504.09037", "authors": ["Zixuan Ke", "Fangkai Jiao", "Yifei Ming", "Xuan-Phi Nguyen", "Austin Xu", "Do Xuan Long", "Minzhi Li", "Chengwei Qin", "Peifeng Wang", "Silvio Savarese", "Caiming Xiong", "Shafiq Joty"], "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems", "categories": ["cs.AI", "cs.CL"], "comment": "72 pages, 6 figures", "summary": "Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ...", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09058", "pdf": "https://arxiv.org/pdf/2504.09058", "abs": "https://arxiv.org/abs/2504.09058", "authors": ["Chengyuan Liu", "Shihang Wang", "Lizhi Qing", "Kaisong Song", "Junjie Cao", "Jun Lin", "Ji Zhang", "Ang Li", "Kun Kuang", "Fei Wu"], "title": "Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection Improvement", "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Recently, stepwise supervision on Chain of Thoughts (CoTs) presents an\nenhancement on the logical reasoning tasks such as coding and math, with the\nhelp of Monte Carlo Tree Search (MCTS). However, its contribution to tasks\nrequiring domain-specific expertise and knowledge remains unexplored. Motivated\nby the interest, we identify several potential challenges of vanilla MCTS\nwithin this context, and propose the framework of Stepwise Domain\nKnowledge-Driven Reasoning Optimization, employing the MCTS algorithm to\ndevelop step-level supervision for problems that require essential\ncomprehension, reasoning, and specialized knowledge. Additionally, we also\nintroduce the Preference Optimization towards Reflection Paths, which\niteratively learns self-reflection on the reasoning thoughts from better\nperspectives. We have conducted extensive experiments to evaluate the advantage\nof the methodologies. Empirical results demonstrate the effectiveness on\nvarious legal-domain problems. We also report a diverse set of valuable\nfindings, hoping to encourage the enthusiasm to the research of domain-specific\nLLMs and MCTS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search", "MCTS"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09354", "pdf": "https://arxiv.org/pdf/2504.09354", "abs": "https://arxiv.org/abs/2504.09354", "authors": ["Duy-Cat Can", "Quang-Huy Tang", "Huong Ha", "Binh T. Nguyen", "Oliver Y. Chén"], "title": "REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Timely and accurate diagnosis of neurodegenerative disorders, such as\nAlzheimer's disease, is central to disease management. Existing deep learning\nmodels require large-scale annotated datasets and often function as \"black\nboxes\". Additionally, datasets in clinical practice are frequently small or\nunlabeled, restricting the full potential of deep learning methods. Here, we\nintroduce REMEMBER -- Retrieval-based Explainable Multimodal Evidence-guided\nModeling for Brain Evaluation and Reasoning -- a new machine learning framework\nthat facilitates zero- and few-shot Alzheimer's diagnosis using brain MRI scans\nthrough a reference-based reasoning process. Specifically, REMEMBER first\ntrains a contrastively aligned vision-text model using expert-annotated\nreference data and extends pseudo-text modalities that encode abnormality\ntypes, diagnosis labels, and composite clinical descriptions. Then, at\ninference time, REMEMBER retrieves similar, human-validated cases from a\ncurated dataset and integrates their contextual information through a dedicated\nevidence encoding module and attention-based inference head. Such an\nevidence-guided design enables REMEMBER to imitate real-world clinical\ndecision-making process by grounding predictions in retrieved imaging and\ntextual context. Specifically, REMEMBER outputs diagnostic predictions\nalongside an interpretable report, including reference images and explanations\naligned with clinical workflows. Experimental results demonstrate that REMEMBER\nachieves robust zero- and few-shot performance and offers a powerful and\nexplainable framework to neuroimaging-based diagnosis in the real world,\nespecially under limited data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10479", "pdf": "https://arxiv.org/pdf/2504.10479", "abs": "https://arxiv.org/abs/2504.10479", "authors": ["Jinguo Zhu", "Weiyun Wang", "Zhe Chen", "Zhaoyang Liu", "Shenglong Ye", "Lixin Gu", "Yuchen Duan", "Hao Tian", "Weijie Su", "Jie Shao", "Zhangwei Gao", "Erfei Cui", "Yue Cao", "Yangzhou Liu", "Weiye Xu", "Hao Li", "Jiahao Wang", "Han Lv", "Dengnian Chen", "Songze Li", "Yinan He", "Tan Jiang", "Jiapeng Luo", "Yi Wang", "Conghui He", "Botian Shi", "Xingcheng Zhang", "Wenqi Shao", "Junjun He", "Yingtong Xiong", "Wenwen Qu", "Peng Sun", "Penglong Jiao", "Lijun Wu", "Kaipeng Zhang", "Huipeng Deng", "Jiaye Ge", "Kai Chen", "Limin Wang", "Min Dou", "Lewei Lu", "Xizhou Zhu", "Tong Lu", "Dahua Lin", "Yu Qiao", "Jifeng Dai", "Wenhai Wang"], "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09354", "pdf": "https://arxiv.org/pdf/2504.09354", "abs": "https://arxiv.org/abs/2504.09354", "authors": ["Duy-Cat Can", "Quang-Huy Tang", "Huong Ha", "Binh T. Nguyen", "Oliver Y. Chén"], "title": "REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Timely and accurate diagnosis of neurodegenerative disorders, such as\nAlzheimer's disease, is central to disease management. Existing deep learning\nmodels require large-scale annotated datasets and often function as \"black\nboxes\". Additionally, datasets in clinical practice are frequently small or\nunlabeled, restricting the full potential of deep learning methods. Here, we\nintroduce REMEMBER -- Retrieval-based Explainable Multimodal Evidence-guided\nModeling for Brain Evaluation and Reasoning -- a new machine learning framework\nthat facilitates zero- and few-shot Alzheimer's diagnosis using brain MRI scans\nthrough a reference-based reasoning process. Specifically, REMEMBER first\ntrains a contrastively aligned vision-text model using expert-annotated\nreference data and extends pseudo-text modalities that encode abnormality\ntypes, diagnosis labels, and composite clinical descriptions. Then, at\ninference time, REMEMBER retrieves similar, human-validated cases from a\ncurated dataset and integrates their contextual information through a dedicated\nevidence encoding module and attention-based inference head. Such an\nevidence-guided design enables REMEMBER to imitate real-world clinical\ndecision-making process by grounding predictions in retrieved imaging and\ntextual context. Specifically, REMEMBER outputs diagnostic predictions\nalongside an interpretable report, including reference images and explanations\naligned with clinical workflows. Experimental results demonstrate that REMEMBER\nachieves robust zero- and few-shot performance and offers a powerful and\nexplainable framework to neuroimaging-based diagnosis in the real world,\nespecially under limited data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09903", "pdf": "https://arxiv.org/pdf/2504.09903", "abs": "https://arxiv.org/abs/2504.09903", "authors": ["Bo-Wei Chen", "An-Zi Yen", "Chung-Chi Chen"], "title": "Refining Financial Consumer Complaints through Multi-Scale Model Interaction", "categories": ["cs.CL"], "comment": null, "summary": "Legal writing demands clarity, formality, and domain-specific\nprecision-qualities often lacking in documents authored by individuals without\nlegal training. To bridge this gap, this paper explores the task of legal text\nrefinement that transforms informal, conversational inputs into persuasive\nlegal arguments. We introduce FinDR, a Chinese dataset of financial dispute\nrecords, annotated with official judgments on claim reasonableness. Our\nproposed method, Multi-Scale Model Interaction (MSMI), leverages a lightweight\nclassifier to evaluate outputs and guide iterative refinement by Large Language\nModels (LLMs). Experimental results demonstrate that MSMI significantly\noutperforms single-pass prompting strategies. Additionally, we validate the\ngeneralizability of MSMI on several short-text benchmarks, showing improved\nadversarial robustness. Our findings reveal the potential of multi-model\ncollaboration for enhancing legal document generation and broader text\nrefinement tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "iterative refinement"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09641", "pdf": "https://arxiv.org/pdf/2504.09641", "abs": "https://arxiv.org/abs/2504.09641", "authors": ["Xingjian Zhang", "Siwei Wen", "Wenjun Wu", "Lei Huang"], "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recently, improving the reasoning ability of large multimodal models (LMMs)\nthrough reinforcement learning has made great progress. However, most existing\nworks are based on highly reasoning-intensive datasets such as mathematics and\ncode, and researchers generally choose large-scale models as the foundation. We\nargue that exploring small-scale models' reasoning capabilities remains\nvaluable for researchers with limited computational resources. Moreover,\nenabling models to explain their reasoning processes on general\nquestion-answering datasets is equally meaningful. Therefore, we present the\nsmall-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video,\na traceably trained video understanding model with no more than 4B parameters,\nit not only demonstrates significantly improved reasoning and thinking\ncapabilities after using reinforcement learning on general Video-QA datasets,\nbut also exhibits the emergent characteristic of \"aha moments\". Furthermore, we\nshare a series of experimental findings, aiming to provide practical insights\nfor future exploration of video reasoning (thinking) abilities in small-scale\nmodels. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "reasoning model"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10342", "pdf": "https://arxiv.org/pdf/2504.10342", "abs": "https://arxiv.org/abs/2504.10342", "authors": ["Yueqi Song", "Tianyue Ou", "Yibo Kong", "Zecheng Li", "Graham Neubig", "Xiang Yue"], "title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge", "categories": ["cs.CL"], "comment": "56 pages, 43 figures", "summary": "Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "inference compute"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "correlation"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09878", "pdf": "https://arxiv.org/pdf/2504.09878", "abs": "https://arxiv.org/abs/2504.09878", "authors": ["Yunpeng Tan", "Junlin Hao", "Jiangkai Wu", "Liming Liu", "Qingyang Li", "Xinggong Zhang"], "title": "MCBlock: Boosting Neural Radiance Field Training Speed by MCTS-based Dynamic-Resolution Ray Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Neural Radiance Field (NeRF) is widely known for high-fidelity novel view\nsynthesis. However, even the state-of-the-art NeRF model, Gaussian Splatting,\nrequires minutes for training, far from the real-time performance required by\nmultimedia scenarios like telemedicine. One of the obstacles is its inefficient\nsampling, which is only partially addressed by existing works. Existing\npoint-sampling algorithms uniformly sample simple-texture regions (easy to fit)\nand complex-texture regions (hard to fit), while existing ray-sampling\nalgorithms sample these regions all in the finest granularity (i.e. the pixel\nlevel), both wasting GPU training resources. Actually, regions with different\ntexture intensities require different sampling granularities. To this end, we\npropose a novel dynamic-resolution ray-sampling algorithm, MCBlock, which\nemploys Monte Carlo Tree Search (MCTS) to partition each training image into\npixel blocks with different sizes for active block-wise training. Specifically,\nthe trees are initialized according to the texture of training images to boost\nthe initialization speed, and an expansion/pruning module dynamically optimizes\nthe block partition. MCBlock is implemented in Nerfstudio, an open-source\ntoolset, and achieves a training acceleration of up to 2.33x, surpassing other\nray-sampling algorithms. We believe MCBlock can apply to any cone-tracing NeRF\nmodel and contribute to the multimedia community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search", "MCTS"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10481", "pdf": "https://arxiv.org/pdf/2504.10481", "abs": "https://arxiv.org/abs/2504.10481", "authors": ["Ding Chen", "Qingchen Yu", "Pengyuan Wang", "Wentao Zhang", "Bo Tang", "Feiyu Xiong", "Xinchi Li", "Minchuan Yang", "Zhiyu Li"], "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations", "categories": ["cs.CL"], "comment": "32 pages", "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1", "reasoning model"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation", "accuracy"], "score": 4}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08801", "pdf": "https://arxiv.org/pdf/2504.08801", "abs": "https://arxiv.org/abs/2504.08801", "authors": ["Andrew Kiruluta", "Priscilla Burity", "Samantha Williams"], "title": "Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformer architectures, underpinned by the self-attention mechanism, have\nachieved state-of-the-art results across numerous natural language processing\n(NLP) tasks by effectively modeling long-range dependencies. However, the\ncomputational complexity of self-attention, scaling quadratically with input\nsequence length, presents significant challenges for processing very long\nsequences or operating under resource constraints. This paper introduces the\nLearnable Multi-Scale Wavelet Transformer (LMWT), a novel architecture that\nreplaces the standard dot-product self-attention with a learnable multi-scale\nHaar wavelet transform module. Leveraging the intrinsic multi-resolution\nproperties of wavelets, the LMWT efficiently captures both local details and\nglobal context. Crucially, the parameters of the wavelet transform, including\nscale-specific coefficients, are learned end-to-end during training, allowing\nthe model to adapt its decomposition strategy to the data and task. We present\nthe detailed mathematical formulation of the learnable Haar wavelet module and\nits integration into the transformer framework, supplemented by an\narchitectural diagram. We conduct a comprehensive experimental evaluation on a\nstandard machine translation benchmark (WMT16 En-De), comparing the LMWT\nagainst a baseline self-attention transformer using metrics like BLEU score,\nperplexity, and token accuracy. Furthermore, we analyze the computational\ncomplexity, highlighting the linear scaling of our approach, discuss its\nnovelty in the context of related work, and explore the interpretability\noffered by visualizing the learned Haar coefficients. Our results indicate that\nthe LMWT achieves competitive performance while offering substantial\ncomputational advantages, positioning it as a promising and novel alternative\nfor efficient sequence modeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09967", "pdf": "https://arxiv.org/pdf/2504.09967", "abs": "https://arxiv.org/abs/2504.09967", "authors": ["Xun Zhu", "Fanbin Mo", "Zheng Zhang", "Jiaxi Wang", "Yiming Shi", "Ming Wu", "Chuang Zhang", "Miao Li", "Ji Wu"], "title": "Enhancing Multi-task Learning Capability of Medical Generalist Foundation Model via Image-centric Multi-annotation Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of medical generalist foundation models has revolutionized\nconventional task-specific model development paradigms, aiming to better handle\nmultiple tasks through joint training on large-scale medical datasets. However,\nrecent advances prioritize simple data scaling or architectural component\nenhancement, while neglecting to re-examine multi-task learning from a\ndata-centric perspective. Critically, simply aggregating existing data\nresources leads to decentralized image-task alignment, which fails to cultivate\ncomprehensive image understanding or align with clinical needs for\nmulti-dimensional image interpretation. In this paper, we introduce the\nimage-centric multi-annotation X-ray dataset (IMAX), the first attempt to\nenhance the multi-task learning capabilities of medical multi-modal large\nlanguage models (MLLMs) from the data construction level. To be specific, IMAX\nis featured from the following attributes: 1) High-quality data curation. A\ncomprehensive collection of more than 354K entries applicable to seven\ndifferent medical tasks. 2) Image-centric dense annotation. Each X-ray image is\nassociated with an average of 4.10 tasks and 7.46 training entries, ensuring\nmulti-task representation richness per image. Compared to the general\ndecentralized multi-annotation X-ray dataset (DMAX), IMAX consistently\ndemonstrates significant multi-task average performance gains ranging from\n3.20% to 21.05% across seven open-source state-of-the-art medical MLLMs.\nMoreover, we investigate differences in statistical patterns exhibited by IMAX\nand DMAX training processes, exploring potential correlations between\noptimization dynamics and multi-task performance. Finally, leveraging the core\nconcept of IMAX data construction, we propose an optimized DMAX-based training\nstrategy to alleviate the dilemma of obtaining high-quality IMAX data in\npractical scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "multi-dimensional"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09037", "pdf": "https://arxiv.org/pdf/2504.09037", "abs": "https://arxiv.org/abs/2504.09037", "authors": ["Zixuan Ke", "Fangkai Jiao", "Yifei Ming", "Xuan-Phi Nguyen", "Austin Xu", "Do Xuan Long", "Minzhi Li", "Chengwei Qin", "Peifeng Wang", "Silvio Savarese", "Caiming Xiong", "Shafiq Joty"], "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems", "categories": ["cs.AI", "cs.CL"], "comment": "72 pages, 6 figures", "summary": "Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ...", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09058", "pdf": "https://arxiv.org/pdf/2504.09058", "abs": "https://arxiv.org/abs/2504.09058", "authors": ["Chengyuan Liu", "Shihang Wang", "Lizhi Qing", "Kaisong Song", "Junjie Cao", "Jun Lin", "Ji Zhang", "Ang Li", "Kun Kuang", "Fei Wu"], "title": "Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection Improvement", "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Recently, stepwise supervision on Chain of Thoughts (CoTs) presents an\nenhancement on the logical reasoning tasks such as coding and math, with the\nhelp of Monte Carlo Tree Search (MCTS). However, its contribution to tasks\nrequiring domain-specific expertise and knowledge remains unexplored. Motivated\nby the interest, we identify several potential challenges of vanilla MCTS\nwithin this context, and propose the framework of Stepwise Domain\nKnowledge-Driven Reasoning Optimization, employing the MCTS algorithm to\ndevelop step-level supervision for problems that require essential\ncomprehension, reasoning, and specialized knowledge. Additionally, we also\nintroduce the Preference Optimization towards Reflection Paths, which\niteratively learns self-reflection on the reasoning thoughts from better\nperspectives. We have conducted extensive experiments to evaluate the advantage\nof the methodologies. Empirical results demonstrate the effectiveness on\nvarious legal-domain problems. We also report a diverse set of valuable\nfindings, hoping to encourage the enthusiasm to the research of domain-specific\nLLMs and MCTS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search", "MCTS"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09354", "pdf": "https://arxiv.org/pdf/2504.09354", "abs": "https://arxiv.org/abs/2504.09354", "authors": ["Duy-Cat Can", "Quang-Huy Tang", "Huong Ha", "Binh T. Nguyen", "Oliver Y. Chén"], "title": "REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Timely and accurate diagnosis of neurodegenerative disorders, such as\nAlzheimer's disease, is central to disease management. Existing deep learning\nmodels require large-scale annotated datasets and often function as \"black\nboxes\". Additionally, datasets in clinical practice are frequently small or\nunlabeled, restricting the full potential of deep learning methods. Here, we\nintroduce REMEMBER -- Retrieval-based Explainable Multimodal Evidence-guided\nModeling for Brain Evaluation and Reasoning -- a new machine learning framework\nthat facilitates zero- and few-shot Alzheimer's diagnosis using brain MRI scans\nthrough a reference-based reasoning process. Specifically, REMEMBER first\ntrains a contrastively aligned vision-text model using expert-annotated\nreference data and extends pseudo-text modalities that encode abnormality\ntypes, diagnosis labels, and composite clinical descriptions. Then, at\ninference time, REMEMBER retrieves similar, human-validated cases from a\ncurated dataset and integrates their contextual information through a dedicated\nevidence encoding module and attention-based inference head. Such an\nevidence-guided design enables REMEMBER to imitate real-world clinical\ndecision-making process by grounding predictions in retrieved imaging and\ntextual context. Specifically, REMEMBER outputs diagnostic predictions\nalongside an interpretable report, including reference images and explanations\naligned with clinical workflows. Experimental results demonstrate that REMEMBER\nachieves robust zero- and few-shot performance and offers a powerful and\nexplainable framework to neuroimaging-based diagnosis in the real world,\nespecially under limited data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10479", "pdf": "https://arxiv.org/pdf/2504.10479", "abs": "https://arxiv.org/abs/2504.10479", "authors": ["Jinguo Zhu", "Weiyun Wang", "Zhe Chen", "Zhaoyang Liu", "Shenglong Ye", "Lixin Gu", "Yuchen Duan", "Hao Tian", "Weijie Su", "Jie Shao", "Zhangwei Gao", "Erfei Cui", "Yue Cao", "Yangzhou Liu", "Weiye Xu", "Hao Li", "Jiahao Wang", "Han Lv", "Dengnian Chen", "Songze Li", "Yinan He", "Tan Jiang", "Jiapeng Luo", "Yi Wang", "Conghui He", "Botian Shi", "Xingcheng Zhang", "Wenqi Shao", "Junjun He", "Yingtong Xiong", "Wenwen Qu", "Peng Sun", "Penglong Jiao", "Lijun Wu", "Kaipeng Zhang", "Huipeng Deng", "Jiaye Ge", "Kai Chen", "Limin Wang", "Min Dou", "Lewei Lu", "Xizhou Zhu", "Tong Lu", "Dahua Lin", "Yu Qiao", "Jifeng Dai", "Wenhai Wang"], "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08959", "pdf": "https://arxiv.org/pdf/2504.08959", "abs": "https://arxiv.org/abs/2504.08959", "authors": ["Yilin Wang", "Chuan Guo", "Yuxuan Mu", "Muhammad Gohar Javed", "Xinxin Zuo", "Juwei Lu", "Hai Jiang", "Li Cheng"], "title": "MotionDreamer: One-to-Many Motion Synthesis with Localized Generative Masked Transformer", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "ICLR 2025 acceptance", "summary": "Generative masked transformers have demonstrated remarkable success across\nvarious content generation tasks, primarily due to their ability to effectively\nmodel large-scale dataset distributions with high consistency. However, in the\nanimation domain, large datasets are not always available. Applying generative\nmasked modeling to generate diverse instances from a single MoCap reference may\nlead to overfitting, a challenge that remains unexplored. In this work, we\npresent MotionDreamer, a localized masked modeling paradigm designed to learn\ninternal motion patterns from a given motion with arbitrary topology and\nduration. By embedding the given motion into quantized tokens with a novel\ndistribution regularization method, MotionDreamer constructs a robust and\ninformative codebook for local motion patterns. Moreover, a sliding window\nlocal attention is introduced in our masked transformer, enabling the\ngeneration of natural yet diverse animations that closely resemble the\nreference motion patterns. As demonstrated through comprehensive experiments,\nMotionDreamer outperforms the state-of-the-art methods that are typically GAN\nor Diffusion-based in both faithfulness and diversity. Thanks to the\nconsistency and robustness of the quantization-based approach, MotionDreamer\ncan also effectively perform downstream tasks such as temporal motion editing,\n\\textcolor{update}{crowd animation}, and beat-aligned dance generation, all\nusing a single reference motion. Visit our project page:\nhttps://motiondreamer.github.io/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08966", "pdf": "https://arxiv.org/pdf/2504.08966", "abs": "https://arxiv.org/abs/2504.08966", "authors": ["Mohamed Dhouib", "Davide Buscaldi", "Sonia Vanier", "Aymen Shabou"], "title": "PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Visual Language Models require substantial computational resources for\ninference due to the additional input tokens needed to represent visual\ninformation. However, these visual tokens often contain redundant and\nunimportant information, resulting in an unnecessarily high number of tokens.\nTo address this, we introduce PACT, a method that reduces inference time and\nmemory usage by pruning irrelevant tokens and merging visually redundant ones\nat an early layer of the language model. Our approach uses a novel importance\nmetric to identify unimportant tokens without relying on attention scores,\nmaking it compatible with FlashAttention. We also propose a novel clustering\nalgorithm, called Distance Bounded Density Peak Clustering, which efficiently\nclusters visual tokens while constraining the distances between elements within\na cluster by a predefined threshold. We demonstrate the effectiveness of PACT\nthrough extensive experiments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09048", "pdf": "https://arxiv.org/pdf/2504.09048", "abs": "https://arxiv.org/abs/2504.09048", "authors": ["Yongchang Wu", "Zipeng Qi", "Zhenwei Shi", "Zhengxia Zou"], "title": "BlockGaussian: Efficient Large-Scale Scene NovelView Synthesis via Adaptive Block-Based Gaussian Splatting", "categories": ["cs.CV"], "comment": "https://github.com/SunshineWYC/BlockGaussian", "summary": "The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated\nremarkable potential in novel view synthesis tasks. The divide-and-conquer\nparadigm has enabled large-scale scene reconstruction, but significant\nchallenges remain in scene partitioning, optimization, and merging processes.\nThis paper introduces BlockGaussian, a novel framework incorporating a\ncontent-aware scene partition strategy and visibility-aware block optimization\nto achieve efficient and high-quality large-scale scene reconstruction.\nSpecifically, our approach considers the content-complexity variation across\ndifferent regions and balances computational load during scene partitioning,\nenabling efficient scene reconstruction. To tackle the supervision mismatch\nissue during independent block optimization, we introduce auxiliary points\nduring individual block optimization to align the ground-truth supervision,\nwhich enhances the reconstruction quality. Furthermore, we propose a\npseudo-view geometry constraint that effectively mitigates rendering\ndegradation caused by airspace floaters during block merging. Extensive\nexperiments on large-scale scenes demonstrate that our approach achieves\nstate-of-the-art performance in both reconstruction efficiency and rendering\nquality, with a 5x speedup in optimization and an average PSNR improvement of\n1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces\ncomputational requirements, enabling large-scale scene reconstruction on a\nsingle 24GB VRAM device. The project page is available at\nhttps://github.com/SunshineWYC/BlockGaussian", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09106", "pdf": "https://arxiv.org/pdf/2504.09106", "abs": "https://arxiv.org/abs/2504.09106", "authors": ["Yonghao Huang", "Leiting Chen", "Chuan Zhou"], "title": "Multi-modal and Multi-view Fundus Image Fusion for Retinopathy Diagnosis via Multi-scale Cross-attention and Shifted Window Self-attention", "categories": ["cs.CV"], "comment": null, "summary": "The joint interpretation of multi-modal and multi-view fundus images is\ncritical for retinopathy prevention, as different views can show the complete\n3D eyeball field and different modalities can provide complementary lesion\nareas. Compared with single images, the sequence relationships in multi-modal\nand multi-view fundus images contain long-range dependencies in lesion\nfeatures. By modeling the long-range dependencies in these sequences, lesion\nareas can be more comprehensively mined, and modality-specific lesions can be\ndetected. To learn the long-range dependency relationship and fuse\ncomplementary multi-scale lesion features between different fundus modalities,\nwe design a multi-modal fundus image fusion method based on multi-scale\ncross-attention, which solves the static receptive field problem in previous\nmulti-modal medical fusion methods based on attention. To capture multi-view\nrelative positional relationships between different views and fuse\ncomprehensive lesion features between different views, we design a multi-view\nfundus image fusion method based on shifted window self-attention, which also\nsolves the computational complexity of the multi-view fundus fusion method\nbased on self-attention is quadratic to the size and number of multi-view\nfundus images. Finally, we design a multi-task retinopathy diagnosis framework\nto help ophthalmologists reduce workload and improve diagnostic accuracy by\ncombining the proposed two fusion methods. The experimental results of\nretinopathy classification and report generation tasks indicate our method's\npotential to improve the efficiency and reliability of retinopathy diagnosis in\nclinical practice, achieving a classification accuracy of 82.53\\% and a report\ngeneration BlEU-1 of 0.543.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09129", "pdf": "https://arxiv.org/pdf/2504.09129", "abs": "https://arxiv.org/abs/2504.09129", "authors": ["Jizong Peng", "Tze Ho Elden Tse", "Kai Xu", "Wenchao Gao", "Angela Yao"], "title": "A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a powerful reconstruction technique, but it\nneeds to be initialized from accurate camera poses and high-fidelity point\nclouds. Typically, the initialization is taken from Structure-from-Motion (SfM)\nalgorithms; however, SfM is time-consuming and restricts the application of\n3DGS in real-world scenarios and large-scale scene reconstruction. We introduce\na constrained optimization method for simultaneous camera pose estimation and\n3D reconstruction that does not require SfM support. Core to our approach is\ndecomposing a camera pose into a sequence of camera-to-(device-)center and\n(device-)center-to-world optimizations. To facilitate, we propose two\noptimization constraints conditioned to the sensitivity of each parameter group\nand restricts each parameter's search space. In addition, as we learn the scene\ngeometry directly from the noisy point clouds, we propose geometric constraints\nto improve the reconstruction quality. Experiments demonstrate that the\nproposed method significantly outperforms the existing (multi-modal) 3DGS\nbaseline and methods supplemented by COLMAP on both our collected dataset and\ntwo public benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09155", "pdf": "https://arxiv.org/pdf/2504.09155", "abs": "https://arxiv.org/abs/2504.09155", "authors": ["Zhanzhou Feng", "Shiliang Zhang"], "title": "Evolved Hierarchical Masking for Self-Supervised Learning", "categories": ["cs.CV"], "comment": null, "summary": "Existing Masked Image Modeling methods apply fixed mask patterns to guide the\nself-supervised training. As those mask patterns resort to different criteria\nto depict image contents, sticking to a fixed pattern leads to a limited vision\ncues modeling capability.This paper introduces an evolved hierarchical masking\nmethod to pursue general visual cues modeling in self-supervised learning. The\nproposed method leverages the vision model being trained to parse the input\nvisual cues into a hierarchy structure, which is hence adopted to generate\nmasks accordingly. The accuracy of hierarchy is on par with the capability of\nthe model being trained, leading to evolved mask patterns at different training\nstages. Initially, generated masks focus on low-level visual cues to grasp\nbasic textures, then gradually evolve to depict higher-level cues to reinforce\nthe learning of more complicated object semantics and contexts. Our method does\nnot require extra pre-trained models or annotations and ensures training\nefficiency by evolving the training difficulty. We conduct extensive\nexperiments on seven downstream tasks including partial-duplicate image\nretrieval relying on low-level details, as well as image classification and\nsemantic segmentation that require semantic parsing capability. Experimental\nresults demonstrate that it substantially boosts performance across these\ntasks. For instance, it surpasses the recent MAE by 1.1\\% in imageNet-1K\nclassification and 1.4\\% in ADE20K segmentation with the same training epochs.\nWe also align the proposed method with the current research focus on LLMs. The\nproposed approach bridges the gap with large-scale pre-training on semantic\ndemanding tasks and enhances intricate detail perception in tasks requiring\nlow-level feature recognition.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "criteria"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09184", "pdf": "https://arxiv.org/pdf/2504.09184", "abs": "https://arxiv.org/abs/2504.09184", "authors": ["Lennart Finke", "Thomas Dooms", "Mat Allen", "Juan Diego Rodriguez", "Noa Nabeshima", "Dan Braun"], "title": "Parameterized Synthetic Text Generation with SimpleStories", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present SimpleStories, a large synthetic story dataset in simple language,\nconsisting of 2 million stories each in English and Japanese. Our method\nemploys parametrization of prompts with features at multiple levels of\nabstraction, allowing for systematic control over story characteristics to\nensure broad syntactic and semantic diversity. Building on and addressing\nlimitations in the TinyStories dataset, our approach demonstrates that\nsimplicity and variety can be achieved simultaneously in synthetic text\ngeneration at scale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09203", "pdf": "https://arxiv.org/pdf/2504.09203", "abs": "https://arxiv.org/abs/2504.09203", "authors": ["Saikat Dutta", "Akhil Vasim", "Siddhant Gole", "Hamid Rezatofighi", "Biplab Banerjee"], "title": "AerOSeg: Harnessing SAM for Open-Vocabulary Segmentation in Remote Sensing Images", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at EarthVision workshop, CVPR 2025", "summary": "Image segmentation beyond predefined categories is a key challenge in remote\nsensing, where novel and unseen classes often emerge during inference.\nOpen-vocabulary image Segmentation addresses these generalization issues in\ntraditional supervised segmentation models while reducing reliance on extensive\nper-pixel annotations, which are both expensive and labor-intensive to obtain.\nMost Open-Vocabulary Segmentation (OVS) methods are designed for natural images\nbut struggle with remote sensing data due to scale variations, orientation\nchanges, and complex scene compositions. This necessitates the development of\nOVS approaches specifically tailored for remote sensing. In this context, we\npropose AerOSeg, a novel OVS approach for remote sensing data. First, we\ncompute robust image-text correlation features using multiple rotated versions\nof the input image and domain-specific prompts. These features are then refined\nthrough spatial and class refinement blocks. Inspired by the success of the\nSegment Anything Model (SAM) in diverse domains, we leverage SAM features to\nguide the spatial refinement of correlation features. Additionally, we\nintroduce a semantic back-projection module and loss to ensure the seamless\npropagation of SAM's semantic information throughout the segmentation pipeline.\nFinally, we enhance the refined correlation features using a multi-scale\nattention-aware decoder to produce the final segmentation map. We validate our\nSAM-guided Open-Vocabulary Remote Sensing Segmentation model on three benchmark\nremote sensing datasets: iSAID, DLRSD, and OpenEarthMap. Our model outperforms\nstate-of-the-art open-vocabulary segmentation methods, achieving an average\nimprovement of 2.54 h-mIoU.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09215", "pdf": "https://arxiv.org/pdf/2504.09215", "abs": "https://arxiv.org/abs/2504.09215", "authors": ["Zhicheng Zhang", "Hao Tang", "Jinhui Tang"], "title": "Multi-scale Activation, Refinement, and Aggregation: Exploring Diverse Cues for Fine-Grained Bird Recognition", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by AAAI2025", "summary": "Given the critical role of birds in ecosystems, Fine-Grained Bird Recognition\n(FGBR) has gained increasing attention, particularly in distinguishing birds\nwithin similar subcategories. Although Vision Transformer (ViT)-based methods\noften outperform Convolutional Neural Network (CNN)-based methods in FGBR,\nrecent studies reveal that the limited receptive field of plain ViT model\nhinders representational richness and makes them vulnerable to scale variance.\nThus, enhancing the multi-scale capabilities of existing ViT-based models to\novercome this bottleneck in FGBR is a worthwhile pursuit. In this paper, we\npropose a novel framework for FGBR, namely Multi-scale Diverse Cues Modeling\n(MDCM), which explores diverse cues at different scales across various stages\nof a multi-scale Vision Transformer (MS-ViT) in an\n\"Activation-Selection-Aggregation\" paradigm. Specifically, we first propose a\nmulti-scale cue activation module to ensure the discriminative cues learned at\ndifferent stage are mutually different. Subsequently, a multi-scale token\nselection mechanism is proposed to remove redundant noise and highlight\ndiscriminative, scale-specific cues at each stage. Finally, the selected tokens\nfrom each stage are independently utilized for bird recognition, and the\nrecognition results from multiple stages are adaptively fused through a\nmulti-scale dynamic aggregation mechanism for final model decisions. Both\nqualitative and quantitative results demonstrate the effectiveness of our\nproposed MDCM, which outperforms CNN- and ViT-based models on several\nwidely-used FGBR benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09223", "pdf": "https://arxiv.org/pdf/2504.09223", "abs": "https://arxiv.org/abs/2504.09223", "authors": ["Wenjin Ke", "Zhe Li", "Dong Li", "Lu Tian", "Emad Barsoum"], "title": "DL-QAT: Weight-Decomposed Low-Rank Quantization-Aware Training for Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Improving the efficiency of inference in Large Language Models (LLMs) is a\ncritical area of research. Post-training Quantization (PTQ) is a popular\ntechnique, but it often faces challenges at low-bit levels, particularly in\ndownstream tasks. Quantization-aware Training (QAT) can alleviate this problem,\nbut it requires significantly more computational resources. To tackle this, we\nintroduced Weight-Decomposed Low-Rank Quantization-Aware Training (DL-QAT),\nwhich merges the advantages of QAT while training only less than 1% of the\ntotal parameters. Specifically, we introduce a group-specific quantization\nmagnitude to adjust the overall scale of each quantization group. Within each\nquantization group, we use LoRA matrices to update the weight size and\ndirection in the quantization space. We validated the effectiveness of our\nmethod on the LLaMA and LLaMA2 model families. The results show significant\nimprovements over our baseline method across different quantization\ngranularities. For instance, for LLaMA-7B, our approach outperforms the\nprevious state-of-the-art method by 4.2% in MMLU on 3-bit LLaMA-7B model.\nAdditionally, our quantization results on pre-trained models also surpass\nprevious QAT methods, demonstrating the superior performance and efficiency of\nour approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09389", "pdf": "https://arxiv.org/pdf/2504.09389", "abs": "https://arxiv.org/abs/2504.09389", "authors": ["Vishakh Padmakumar", "Chen Yueh-Han", "Jane Pan", "Valerie Chen", "He He"], "title": "Beyond Memorization: Mapping the Originality-Quality Frontier of Language Models", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly used for ideation and\nscientific discovery, it is important to evaluate their ability to generate\nnovel output. Prior work evaluates novelty as the originality with respect to\ntraining data, but original outputs can be low quality. In contrast, non-expert\njudges may favor high-quality but memorized outputs, limiting the reliability\nof human preference as a metric. We propose a new novelty metric for LLM\ngenerations that balances originality and quality -- the harmonic mean of the\nfraction of \\ngrams unseen during training and a task-specific quality score.\nWe evaluate the novelty of generations from two families of open-data models\n(OLMo and Pythia) on three creative tasks: story completion, poetry writing,\nand creative tool use. We find that LLM generated text is less novel than human\nwritten text. To elicit more novel outputs, we experiment with various\ninference-time methods, which reveals a trade-off between originality and\nquality. While these methods can boost novelty, they do so by increasing\noriginality at the expense of quality. In contrast, increasing model size or\napplying post-training reliably shifts the Pareto frontier, highlighting that\nstarting with a stronger base model is a more effective way to improve novelty.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "reliability"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09255", "pdf": "https://arxiv.org/pdf/2504.09255", "abs": "https://arxiv.org/abs/2504.09255", "authors": ["Sijing Wu", "Yunhao Li", "Ziwen Xu", "Yixuan Gao", "Huiyu Duan", "Wei Sun", "Guangtao Zhai"], "title": "FVQ: A Large-Scale Dataset and A LMM-based Method for Face Video Quality Assessment", "categories": ["cs.CV"], "comment": null, "summary": "Face video quality assessment (FVQA) deserves to be explored in addition to\ngeneral video quality assessment (VQA), as face videos are the primary content\non social media platforms and human visual system (HVS) is particularly\nsensitive to human faces. However, FVQA is rarely explored due to the lack of\nlarge-scale FVQA datasets. To fill this gap, we present the first large-scale\nin-the-wild FVQA dataset, FVQ-20K, which contains 20,000 in-the-wild face\nvideos together with corresponding mean opinion score (MOS) annotations. Along\nwith the FVQ-20K dataset, we further propose a specialized FVQA method named\nFVQ-Rater to achieve human-like rating and scoring for face video, which is the\nfirst attempt to explore the potential of large multimodal models (LMMs) for\nthe FVQA task. Concretely, we elaborately extract multi-dimensional features\nincluding spatial features, temporal features, and face-specific features\n(i.e., portrait features and face embeddings) to provide comprehensive visual\ninformation, and take advantage of the LoRA-based instruction tuning technique\nto achieve quality-specific fine-tuning, which shows superior performance on\nboth FVQ-20K and CFVQA datasets. Extensive experiments and comprehensive\nanalysis demonstrate the significant potential of the FVQ-20K dataset and\nFVQ-Rater method in promoting the development of FVQA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "multi-dimensional"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09258", "pdf": "https://arxiv.org/pdf/2504.09258", "abs": "https://arxiv.org/abs/2504.09258", "authors": ["Jianyu Wu", "Hao Yang", "Xinhua Zeng", "Guibing He", "Zhiyu Chen", "Zihui Li", "Xiaochuan Zhang", "Yangyang Ma", "Run Fang", "Yang Liu"], "title": "PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for Pathology Visual-Language Tasks", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The diagnosis of pathological images is often limited by expert availability\nand regional disparities, highlighting the importance of automated diagnosis\nusing Vision-Language Models (VLMs). Traditional multimodal models typically\nemphasize outcomes over the reasoning process, compromising the reliability of\nclinical decisions. To address the weak reasoning abilities and lack of\nsupervised processes in pathological VLMs, we have innovatively proposed\nPathVLM-R1, a visual language model designed specifically for pathological\nimages. We have based our model on Qwen2.5-VL-7B-Instruct and enhanced its\nperformance for pathological tasks through meticulously designed post-training\nstrategies. Firstly, we conduct supervised fine-tuning guided by pathological\ndata to imbue the model with foundational pathological knowledge, forming a new\npathological base model. Subsequently, we introduce Group Relative Policy\nOptimization (GRPO) and propose a dual reward-driven reinforcement learning\noptimization, ensuring strict constraint on logical supervision of the\nreasoning process and accuracy of results via cross-modal process reward and\noutcome accuracy reward. In the pathological image question-answering tasks,\nthe testing results of PathVLM-R1 demonstrate a 14% improvement in accuracy\ncompared to baseline methods, and it demonstrated superior performance compared\nto the Qwen2.5-VL-32B version despite having a significantly smaller parameter\nsize. Furthermore, in out-domain data evaluation involving four medical imaging\nmodalities: Computed Tomography (CT), dermoscopy, fundus photography, and\nOptical Coherence Tomography (OCT) images: PathVLM-R1's transfer performance\nimproved by an average of 17.3% compared to traditional SFT methods. These\nresults clearly indicate that PathVLM-R1 not only enhances accuracy but also\npossesses broad applicability and expansion potential.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09398", "pdf": "https://arxiv.org/pdf/2504.09398", "abs": "https://arxiv.org/abs/2504.09398", "authors": ["Gaurav Kumar", "Murali Mohana Krishna Dandu"], "title": "Composable NLP Workflows for BERT-based Ranking and QA System", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures, 6 tables", "summary": "There has been a lot of progress towards building NLP models that scale to\nmultiple tasks. However, real-world systems contain multiple components and it\nis tedious to handle cross-task interaction with varying levels of text\ngranularity. In this work, we built an end-to-end Ranking and\nQuestion-Answering (QA) system using Forte, a toolkit that makes composable NLP\npipelines. We utilized state-of-the-art deep learning models such as BERT,\nRoBERTa in our pipeline, evaluated the performance on MS-MARCO and Covid-19\ndatasets using metrics such as BLUE, MRR, F1 and compared the results of\nranking and QA systems with their corresponding benchmark results. The modular\nnature of our pipeline and low latency of reranker makes it easy to build\ncomplex NLP applications easily.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09402", "pdf": "https://arxiv.org/pdf/2504.09402", "abs": "https://arxiv.org/abs/2504.09402", "authors": ["Feijiang Han", "Licheng Guo", "Hengtao Cui", "Zhiyuan Lyu"], "title": "Question Tokens Deserve More Attention: Enhancing Large Language Models without Training through Step-by-Step Reading and Question Attention Recalibration", "categories": ["cs.CL", "cs.AI"], "comment": "CIS 5300", "summary": "Large Language Models (LLMs) often struggle with tasks that require a deep\nunderstanding of complex questions, especially when faced with long-range\ndependencies or multi-step reasoning. This work investigates the limitations of\ncurrent LLMs in question comprehension and identifies three insights: (1)\nrepeating question tokens improves comprehension by increasing attention to\nquestion regions, (2) increased backward dependencies negatively affect\nperformance due to unidirectional attentional constraints, and (3)\nrecalibrating attentional mechanisms to prioritize question-relevant regions\nimproves performance.\n  Based on these findings, we first propose a family of prompt-based strategies\n- Step-by-Step Reading (SSR), SSR+, and SSR++ - that guide LLMs to\nincrementally process question tokens and align their reasoning with the input\nstructure. These methods significantly improve performance, with SSR++\nachieving state-of-the-art results on several benchmarks: 96.66% on GSM8K,\n94.61% on ASDiv, and 76.28% on AQuA. Second, we introduce a training-free\nattention recalibration mechanism that dynamically adjusts attention\ndistributions during inference to emphasize question-relevant regions. This\nmethod improves the accuracy of LLaMA 3.1-8B on AQuA by 5.17% without changing\nmodel parameters or input prompts.\n  Taken together, our results highlight the importance of structured prompt\ndesign and attention optimization in improving LLM comprehension, providing\nlightweight yet effective tools for improving performance in various NLP tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09291", "pdf": "https://arxiv.org/pdf/2504.09291", "abs": "https://arxiv.org/abs/2504.09291", "authors": ["Jiaying Qian", "Ziheng Jia", "Zicheng Zhang", "Zeyu Zhang", "Guangtao Zhai", "Xiongkuo Min"], "title": "Towards Explainable Partial-AIGC Image Quality Assessment", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The rapid advancement of AI-driven visual generation technologies has\ncatalyzed significant breakthroughs in image manipulation, particularly in\nachieving photorealistic localized editing effects on natural scene images\n(NSIs). Despite extensive research on image quality assessment (IQA) for\nAI-generated images (AGIs), most studies focus on fully AI-generated outputs\n(e.g., text-to-image generation), leaving the quality assessment of\npartial-AIGC images (PAIs)-images with localized AI-driven edits an almost\nunprecedented field. Motivated by this gap, we construct the first large-scale\nPAI dataset towards explainable partial-AIGC image quality assessment (EPAIQA),\nthe EPAIQA-15K, which includes 15K images with localized AI manipulation in\ndifferent regions and over 300K multi-dimensional human ratings. Based on this,\nwe leverage large multi-modal models (LMMs) and propose a three-stage model\ntraining paradigm. This paradigm progressively trains the LMM for editing\nregion grounding, quantitative quality scoring, and quality explanation.\nFinally, we develop the EPAIQA series models, which possess explainable quality\nfeedback capabilities. Our work represents a pioneering effort in the\nperceptual IQA field for comprehensive PAI quality assessment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "multi-dimensional"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09566", "pdf": "https://arxiv.org/pdf/2504.09566", "abs": "https://arxiv.org/abs/2504.09566", "authors": ["Chenghao Li", "Chaoning Zhang", "Yi Lu", "Jiaquan Zhang", "Qigan Sun", "Xudong Wang", "Jiwei Wei", "Guoqing Wang", "Yang Yang", "Heng Tao Shen"], "title": "Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting enhances the reasoning of large language\nmodels (LLMs) by decomposing problems into sequential steps, mimicking human\nlogic and reducing errors. However, complex tasks with vast solution spaces and\nvague constraints often exceed the capacity of a single reasoning chain.\nInspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic\ngeometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends\nCoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper\nlogical dependencies, enabling more robust and structured problem-solving. MFR\ndecomposes a module into a sequence of free modules with minimal rank,\nproviding a structured analytical approach to complex systems. This method\nintroduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\",\n\"Exactness\" and \"Minimality\", enabling the systematic decomposition of the\noriginal complex problem into logically complete minimal subproblems while\npreserving key problem features and reducing reasoning length. We tested SoT\nacross diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini,\nQwen2.5), achieving inference accuracy that matches or surpasses mainstream\nCoTs standards. Additionally, by aligning the sampling process with algebraic\nconstraints, our approach enhances the scalability of inference time in LLMs,\nensuring both transparent reasoning and high performance. Our code will be\npublicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09426", "pdf": "https://arxiv.org/pdf/2504.09426", "abs": "https://arxiv.org/abs/2504.09426", "authors": ["Shengao Wang", "Arjun Chandra", "Aoming Liu", "Venkatesh Saligrama", "Boqing Gong"], "title": "BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Human infants rapidly develop visual reasoning skills from minimal input,\nsuggesting that developmentally inspired pretraining could significantly\nenhance the efficiency of vision-language models (VLMs). Although recent\nefforts have leveraged infant-inspired datasets like SAYCam, existing\nevaluation benchmarks remain misaligned--they are either too simplistic,\nnarrowly scoped, or tailored for large-scale pretrained models. Additionally,\ntraining exclusively on infant data overlooks the broader, diverse input from\nwhich infants naturally learn. To address these limitations, we propose\nBabyVLM, a novel framework comprising comprehensive in-domain evaluation\nbenchmarks and a synthetic training dataset created via child-directed\ntransformations of existing datasets. We demonstrate that VLMs trained with our\nsynthetic dataset achieve superior performance on BabyVLM tasks compared to\nmodels trained solely on SAYCam or general-purpose data of the SAYCam size.\nBabyVLM thus provides a robust, developmentally aligned evaluation tool and\nillustrates how compact models trained on carefully curated data can generalize\neffectively, opening pathways toward data-efficient vision-language learning\nparadigms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09665", "pdf": "https://arxiv.org/pdf/2504.09665", "abs": "https://arxiv.org/abs/2504.09665", "authors": ["Liqiang Wen", "Guanming Xiong", "Tong Mo", "Bing Li", "Weiping Li", "Wen Zhao"], "title": "CLEAR-KGQA: Clarification-Enhanced Ambiguity Resolution for Knowledge Graph Question Answering", "categories": ["cs.CL"], "comment": "This work has been accepted by the IJCNN 2025 main track", "summary": "This study addresses the challenge of ambiguity in knowledge graph question\nanswering (KGQA). While recent KGQA systems have made significant progress,\nparticularly with the integration of large language models (LLMs), they\ntypically assume user queries are unambiguous, which is an assumption that\nrarely holds in real-world applications. To address these limitations, we\npropose a novel framework that dynamically handles both entity ambiguity (e.g.,\ndistinguishing between entities with similar names) and intent ambiguity (e.g.,\nclarifying different interpretations of user queries) through interactive\nclarification. Our approach employs a Bayesian inference mechanism to quantify\nquery ambiguity and guide LLMs in determining when and how to request\nclarification from users within a multi-turn dialogue framework. We further\ndevelop a two-agent interaction framework where an LLM-based user simulator\nenables iterative refinement of logical forms through simulated user feedback.\nExperimental results on the WebQSP and CWQ dataset demonstrate that our method\nsignificantly improves performance by effectively resolving semantic\nambiguities. Additionally, we contribute a refined dataset of disambiguated\nqueries, derived from interaction histories, to facilitate future research in\nthis direction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue", "question answering"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09687", "pdf": "https://arxiv.org/pdf/2504.09687", "abs": "https://arxiv.org/abs/2504.09687", "authors": ["Salman Faroz"], "title": "Domain-Adaptive Continued Pre-Training of Small Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Continued pre-training of small language models offers a promising path for\ndomain adaptation with limited computational resources. I've investigated this\napproach within educational domains, evaluating it as a resource-efficient\nalternative to training models from scratch. Using a 125M parameter model, I\ndemonstrate significant performance improvements through incremental training\non 400 million tokens, followed by further training to reach 1 billion tokens.\nMy approach includes comprehensive data preprocessing, memory-optimized\ntraining configurations, and benchmark-based evaluation. Results show notable\ngains in knowledge-intensive tasks (MMLU +8.1%) and contextual understanding\n(HellaSwag +7.6%), while revealing educational domain specialization\ntrade-offs. I analyze token efficiency, catastrophic forgetting mitigation\nstrategies, and scaling patterns. My findings suggest that thoughtful\npreprocessing and training methodologies enable meaningful improvements in\nlanguage model capabilities even with constrained computational resources,\nopening pathways for domain-specific adaptation of smaller language models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09696", "pdf": "https://arxiv.org/pdf/2504.09696", "abs": "https://arxiv.org/abs/2504.09696", "authors": ["Jixiao Zhang", "Chunsheng Zuo"], "title": "GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in R1-like reasoning models leveraging Group Relative Policy\nOptimization (GRPO) have significantly improved the performance of language\nmodels on mathematical reasoning tasks. However, current GRPO implementations\nencounter critical challenges, including reward sparsity due to binary accuracy\nmetrics, limited incentives for conciseness, and insufficient focus on complex\nreasoning tasks. To address these issues, we propose GRPO-LEAD, a suite of\nnovel enhancements tailored for mathematical reasoning. Specifically, GRPO-LEAD\nintroduces (1) a length-dependent accuracy reward to encourage concise and\nprecise solutions, (2) an explicit penalty mechanism for incorrect answers to\nsharpen decision boundaries, and (3) a difficulty-aware advantage reweighting\nstrategy that amplifies learning signals for challenging problems. Furthermore,\nwe systematically examine the impact of model scale and supervised fine-tuning\n(SFT) strategies, demonstrating that larger-scale base models and carefully\ncurated datasets significantly enhance reinforcement learning effectiveness.\nExtensive empirical evaluations and ablation studies confirm that GRPO-LEAD\nsubstantially mitigates previous shortcomings, resulting in language models\nthat produce more concise, accurate, and robust reasoning across diverse\nmathematical tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09448", "pdf": "https://arxiv.org/pdf/2504.09448", "abs": "https://arxiv.org/abs/2504.09448", "authors": ["Lin Zhu", "Yifeng Yang", "Zichao Nie", "Yuan Gao", "Jiarui Li", "Qinying Gu", "Xinbing Wang", "Chenghu Zhou", "Nanyang Ye"], "title": "InfoBound: A Provable Information-Bounds Inspired Framework for Both OoD Generalization and OoD Detection", "categories": ["cs.CV"], "comment": "Under Review", "summary": "In real-world scenarios, distribution shifts give rise to the importance of\ntwo problems: out-of-distribution (OoD) generalization, which focuses on\nmodels' generalization ability against covariate shifts (i.e., the changes of\nenvironments), and OoD detection, which aims to be aware of semantic shifts\n(i.e., test-time unseen classes). Real-world testing environments often involve\na combination of both covariate and semantic shifts. While numerous methods\nhave been proposed to address these critical issues, only a few works tackled\nthem simultaneously. Moreover, prior works often improve one problem but\nsacrifice the other. To overcome these limitations, we delve into boosting OoD\ndetection and OoD generalization from the perspective of information theory,\nwhich can be easily applied to existing models and different tasks. Building\nupon the theoretical bounds for mutual information and conditional entropy, we\nprovide a unified approach, composed of Mutual Information Minimization\n(MI-Min) and Conditional Entropy Maximizing (CE-Max). Extensive experiments and\ncomprehensive evaluations on multi-label image classification and object\ndetection have demonstrated the superiority of our method. It successfully\nmitigates trade-offs between the two challenges compared to competitive\nbaselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09802", "pdf": "https://arxiv.org/pdf/2504.09802", "abs": "https://arxiv.org/abs/2504.09802", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Training Small Reasoning LLMs with Cognitive Preference Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The reasoning capabilities of large language models (LLMs), such as OpenAI's\no1 and DeepSeek-R1, have seen substantial advancements through deep thinking.\nHowever, these enhancements come with significant resource demands,\nunderscoring the need to explore strategies to train effective reasoning LLMs\nwith far fewer parameters. A critical challenge is that smaller models have\ndifferent capacities and cognitive trajectories than their larger counterparts.\nHence, direct distillation of chain-of-thought (CoT) results from large LLMs to\nsmaller ones can be sometimes ineffective and requires a huge amount of\nannotated data. In this paper, we introduce a novel framework called\nCritique-Rethink-Verify (CRV), designed for training smaller yet powerful\nreasoning LLMs. Our CRV framework consists of multiple LLM agents, each\nspecializing in unique abilities: (i) critiquing the CoTs according to the\ncognitive capabilities of smaller models, (ii) rethinking and refining these\nCoTs based on the critiques, and (iii) verifying the correctness of the refined\nresults. We further propose the cognitive preference optimization (CogPO)\nalgorithm to enhance the reasoning abilities of smaller models by aligning\nthoughts of these models with their cognitive capacities. Comprehensive\nevaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV\nand CogPO, which outperforms other training methods by a large margin.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09507", "pdf": "https://arxiv.org/pdf/2504.09507", "abs": "https://arxiv.org/abs/2504.09507", "authors": ["Mengjiao Wang", "Junpei Zhang", "Xu Liu", "Yuting Yang", "Mengru Ma"], "title": "FVOS for MOSE Track of 4th PVUW Challenge: 3rd Place Solution", "categories": ["cs.CV"], "comment": "5 pages, 3 figures", "summary": "Video Object Segmentation (VOS) is one of the most fundamental and\nchallenging tasks in computer vision and has a wide range of applications. Most\nexisting methods rely on spatiotemporal memory networks to extract frame-level\nfeatures and have achieved promising results on commonly used datasets.\nHowever, these methods often struggle in more complex real-world scenarios.\nThis paper addresses this issue, aiming to achieve accurate segmentation of\nvideo objects in challenging scenes. We propose fine-tuning VOS (FVOS),\noptimizing existing methods for specific datasets through tailored training.\nAdditionally, we introduce a morphological post-processing strategy to address\nthe issue of excessively large gaps between adjacent objects in single-model\npredictions. Finally, we apply a voting-based fusion method on multi-scale\nsegmentation results to generate the final output. Our approach achieves J&F\nscores of 76.81% and 83.92% during the validation and testing stages,\nrespectively, securing third place overall in the MOSE Track of the 4th PVUW\nchallenge 2025.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09513", "pdf": "https://arxiv.org/pdf/2504.09513", "abs": "https://arxiv.org/abs/2504.09513", "authors": ["Puyu Han", "Jiaju Kang", "Yuhang Pan", "Erting Pan", "Zeyu Zhang", "Qunchao Jin", "Juntao Jiang", "Zhichen Liu", "Luqi Gong"], "title": "DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale pre-trained diffusion models have produced excellent results in\nthe field of conditional image generation. However, restoration of ancient\nmurals, as an important downstream task in this field, poses significant\nchallenges to diffusion model-based restoration methods due to its large\ndefective area and scarce training samples. Conditional restoration tasks are\nmore concerned with whether the restored part meets the aesthetic standards of\nmural restoration in terms of overall style and seam detail, and such metrics\nfor evaluating heuristic image complements are lacking in current research. We\ntherefore propose DiffuMural, a combined Multi-scale convergence and\nCollaborative Diffusion mechanism with ControlNet and cyclic consistency loss\nto optimise the matching between the generated images and the conditional\ncontrol. DiffuMural demonstrates outstanding capabilities in mural restoration,\nleveraging training data from 23 large-scale Dunhuang murals that exhibit\nconsistent visual aesthetics. The model excels in restoring intricate details,\nachieving a coherent overall appearance, and addressing the unique challenges\nposed by incomplete murals lacking factual grounding. Our evaluation framework\nincorporates four key metrics to quantitatively assess incomplete murals:\nfactual accuracy, textural detail, contextual semantics, and holistic visual\ncoherence. Furthermore, we integrate humanistic value assessments to ensure the\nrestored murals retain their cultural and artistic significance. Extensive\nexperiments validate that our method outperforms state-of-the-art (SOTA)\napproaches in both qualitative and quantitative metrics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09528", "pdf": "https://arxiv.org/pdf/2504.09528", "abs": "https://arxiv.org/abs/2504.09528", "authors": ["Xing Zi", "Tengjun Ni", "Xianjing Fan", "Xian Tao", "Jun Li", "Ali Braytee", "Mukesh Prasad"], "title": "AeroLite: Tag-Guided Lightweight Generation of Aerial Image Captions", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Accurate and automated captioning of aerial imagery is crucial for\napplications like environmental monitoring, urban planning, and disaster\nmanagement. However, this task remains challenging due to complex spatial\nsemantics and domain variability. To address these issues, we introduce\n\\textbf{AeroLite}, a lightweight, tag-guided captioning framework designed to\nequip small-scale language models (1--3B parameters) with robust and\ninterpretable captioning capabilities specifically for remote sensing images.\n\\textbf{AeroLite} leverages GPT-4o to generate a large-scale, semantically rich\npseudo-caption dataset by integrating multiple remote sensing benchmarks,\nincluding DLRSD, iSAID, LoveDA, WHU, and RSSCN7. To explicitly capture key\nsemantic elements such as orientation and land-use types, AeroLite employs\nnatural language processing techniques to extract relevant semantic tags. These\ntags are then learned by a dedicated multi-label CLIP encoder, ensuring precise\nsemantic predictions. To effectively fuse visual and semantic information, we\npropose a novel bridging multilayer perceptron (MLP) architecture, aligning\nsemantic tags with visual embeddings while maintaining minimal computational\noverhead. AeroLite's flexible design also enables seamless integration with\nvarious pretrained large language models. We adopt a two-stage LoRA-based\ntraining approach: the initial stage leverages our pseudo-caption dataset to\ncapture broad remote sensing semantics, followed by fine-tuning on smaller,\ncurated datasets like UCM and Sydney Captions to refine domain-specific\nalignment. Experimental evaluations demonstrate that AeroLite surpasses\nsignificantly larger models (e.g., 13B parameters) in standard captioning\nmetrics, including BLEU and METEOR, while maintaining substantially lower\ncomputational costs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09555", "pdf": "https://arxiv.org/pdf/2504.09555", "abs": "https://arxiv.org/abs/2504.09555", "authors": ["Jinhao Li", "Zijian Chen", "Runze Dong", "Tingzhu Chen", "Changbo Wang", "Guangtao Zhai"], "title": "Mitigating Long-tail Distribution in Oracle Bone Inscriptions: Dataset, Model, and Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "The oracle bone inscription (OBI) recognition plays a significant role in\nunderstanding the history and culture of ancient China. However, the existing\nOBI datasets suffer from a long-tail distribution problem, leading to biased\nperformance of OBI recognition models across majority and minority classes.\nWith recent advancements in generative models, OBI synthesis-based data\naugmentation has become a promising avenue to expand the sample size of\nminority classes. Unfortunately, current OBI datasets lack large-scale\nstructure-aligned image pairs for generative model training. To address these\nproblems, we first present the Oracle-P15K, a structure-aligned OBI dataset for\nOBI generation and denoising, consisting of 14,542 images infused with domain\nknowledge from OBI experts. Second, we propose a diffusion model-based pseudo\nOBI generator, called OBIDiff, to achieve realistic and controllable OBI\ngeneration. Given a clean glyph image and a target rubbing-style image, it can\neffectively transfer the noise style of the original rubbing to the glyph\nimage. Extensive experiments on OBI downstream tasks and user preference\nstudies show the effectiveness of the proposed Oracle-P15K dataset and\ndemonstrate that OBIDiff can accurately preserve inherent glyph structures\nwhile transferring authentic rubbing styles effectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10157", "pdf": "https://arxiv.org/pdf/2504.10157", "abs": "https://arxiv.org/abs/2504.10157", "authors": ["Xinnong Zhang", "Jiayu Lin", "Xinyi Mou", "Shiyue Yang", "Xiawei Liu", "Libo Sun", "Hanjia Lyu", "Yihang Yang", "Weihong Qi", "Yue Chen", "Guanying Li", "Ling Yan", "Yao Hu", "Siming Chen", "Yu Wang", "Jingxuan Huang", "Jiebo Luo", "Shiping Tang", "Libo Wu", "Baohua Zhou", "Zhongyu Wei"], "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users", "categories": ["cs.CL", "cs.CY"], "comment": "work in progress", "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09608", "pdf": "https://arxiv.org/pdf/2504.09608", "abs": "https://arxiv.org/abs/2504.09608", "authors": ["Xingke Song", "Xiaoying Yang", "Chenglin Yao", "Jianfeng Ren", "Ruibin Bai", "Xin Chen", "Xudong Jiang"], "title": "ERL-MPP: Evolutionary Reinforcement Learning with Multi-head Puzzle Perception for Solving Large-scale Jigsaw Puzzles of Eroded Gaps", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Solving jigsaw puzzles has been extensively studied. While most existing\nmodels focus on solving either small-scale puzzles or puzzles with no gap\nbetween fragments, solving large-scale puzzles with gaps presents distinctive\nchallenges in both image understanding and combinatorial optimization. To\ntackle these challenges, we propose a framework of Evolutionary Reinforcement\nLearning with Multi-head Puzzle Perception (ERL-MPP) to derive a better set of\nswapping actions for solving the puzzles. Specifically, to tackle the\nchallenges of perceiving the puzzle with gaps, a Multi-head Puzzle Perception\nNetwork (MPPN) with a shared encoder is designed, where multiple puzzlet heads\ncomprehensively perceive the local assembly status, and a discriminator head\nprovides a global assessment of the puzzle. To explore the large swapping\naction space efficiently, an Evolutionary Reinforcement Learning (EvoRL) agent\nis designed, where an actor recommends a set of suitable swapping actions from\na large action space based on the perceived puzzle status, a critic updates the\nactor using the estimated rewards and the puzzle status, and an evaluator\ncoupled with evolutionary strategies evolves the actions aligning with the\nhistorical assembly experience. The proposed ERL-MPP is comprehensively\nevaluated on the JPLEG-5 dataset with large gaps and the MIT dataset with\nlarge-scale puzzles. It significantly outperforms all state-of-the-art models\non both datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10160", "pdf": "https://arxiv.org/pdf/2504.10160", "abs": "https://arxiv.org/abs/2504.10160", "authors": ["Zhaopeng Feng", "Shaosheng Cao", "Jiahan Ren", "Jiayuan Su", "Ruizhe Chen", "Yan Zhang", "Zhe Xu", "Yao Hu", "Jian Wu", "Zuozhu Liu"], "title": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress. Our code is available at\n  https://github.com/fzp0424/MT-R1-Zero", "summary": "Large-scale reinforcement learning (RL) methods have proven highly effective\nin enhancing the reasoning abilities of large language models (LLMs),\nparticularly for tasks with verifiable solutions such as mathematics and\ncoding. However, applying this idea to machine translation (MT), where outputs\nare flexibly formatted and difficult to automatically evaluate with explicit\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\nguide LLMs towards improved translation quality via emergent reasoning. On the\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\ngeneralization capabilities on out-of-distribution MT tasks, robustly\nsupporting multilingual and low-resource settings. Extensive analysis of model\nbehavior across different initializations and reward metrics offers pioneering\ninsight into the critical role of reward design, LLM adaptability, training\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\nOur code is available at https://github.com/fzp0424/MT-R1-Zero.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09644", "pdf": "https://arxiv.org/pdf/2504.09644", "abs": "https://arxiv.org/abs/2504.09644", "authors": ["Kaiyu Li", "Zepeng Xin", "Li Pang", "Chao Pang", "Yupeng Deng", "Jing Yao", "Guisong Xia", "Deyu Meng", "Zhi Wang", "Xiangyong Cao"], "title": "SegEarth-R1: Geospatial Pixel Reasoning via Large Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing has become critical for understanding environmental dynamics,\nurban planning, and disaster management. However, traditional remote sensing\nworkflows often rely on explicit segmentation or detection methods, which\nstruggle to handle complex, implicit queries that require reasoning over\nspatial context, domain knowledge, and implicit user intent. Motivated by this,\nwe introduce a new task, \\ie, geospatial pixel reasoning, which allows implicit\nquerying and reasoning and generates the mask of the target region. To advance\nthis task, we construct and release the first large-scale benchmark dataset\ncalled EarthReason, which comprises 5,434 manually annotated image masks with\nover 30,000 implicit question-answer pairs. Moreover, we propose SegEarth-R1, a\nsimple yet effective language-guided segmentation baseline that integrates a\nhierarchical visual encoder, a large language model (LLM) for instruction\nparsing, and a tailored mask generator for spatial correlation. The design of\nSegEarth-R1 incorporates domain-specific adaptations, including aggressive\nvisual token compression to handle ultra-high-resolution remote sensing images,\na description projection module to fuse language and multi-scale features, and\na streamlined mask prediction pipeline that directly queries description\nembeddings. Extensive experiments demonstrate that SegEarth-R1 achieves\nstate-of-the-art performance on both reasoning and referring segmentation\ntasks, significantly outperforming traditional and LLM-based segmentation\nmethods. Our data and code will be released at\nhttps://github.com/earth-insights/SegEarth-R1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "correlation"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10187", "pdf": "https://arxiv.org/pdf/2504.10187", "abs": "https://arxiv.org/abs/2504.10187", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "title": "Deep Reasoning Translation via Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown\npromising performance in various complex tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation and taking cultural differences into account.\nThis task is still under-explored in deep reasoning LLMs. In this paper, we\nintroduce DeepTrans, a deep reasoning translation model that learns free\ntranslation via reinforcement learning. Specifically, we carefully build a\nreward model with pre-defined scoring criteria on both the translation results\nand the thought process. Given the source sentences, the reward model teaches\nthe deep translation model how to think and free-translate them during\nreinforcement learning. In this way, training DeepTrans does not need any\nlabeled translations, avoiding the human-intensive annotation or\nresource-intensive data synthesis. Experimental results show the effectiveness\nof DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance\nby 16.3% in literature translation, and outperforms strong deep reasoning\nbaselines as well as baselines that are fine-tuned with synthesized data.\nMoreover, we summarize the failures and interesting findings during our RL\nexploration. We hope this work could inspire other researchers in free\ntranslation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "criteria"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09666", "pdf": "https://arxiv.org/pdf/2504.09666", "abs": "https://arxiv.org/abs/2504.09666", "authors": ["Yao Yuan", "Pan Gao", "Qun Dai", "Jie Qin", "Wei Xiang"], "title": "Uncertainty Guided Refinement for Fine-Grained Salient Object Detection", "categories": ["cs.CV"], "comment": "IEEE Transactions on Image Processing 2025", "summary": "Recently, salient object detection (SOD) methods have achieved impressive\nperformance. However, salient regions predicted by existing methods usually\ncontain unsaturated regions and shadows, which limits the model for reliable\nfine-grained predictions. To address this, we introduce the uncertainty\nguidance learning approach to SOD, intended to enhance the model's perception\nof uncertain regions. Specifically, we design a novel Uncertainty Guided\nRefinement Attention Network (UGRAN), which incorporates three important\ncomponents, i.e., the Multilevel Interaction Attention (MIA) module, the Scale\nSpatial-Consistent Attention (SSCA) module, and the Uncertainty Refinement\nAttention (URA) module. Unlike conventional methods dedicated to enhancing\nfeatures, the proposed MIA facilitates the interaction and perception of\nmultilevel features, leveraging the complementary characteristics among\nmultilevel features. Then, through the proposed SSCA, the salient information\nacross diverse scales within the aggregated features can be integrated more\ncomprehensively and integrally. In the subsequent steps, we utilize the\nuncertainty map generated from the saliency prediction map to enhance the\nmodel's perception capability of uncertain regions, generating a\nhighly-saturated fine-grained saliency prediction map. Additionally, we devise\nan adaptive dynamic partition (ADP) mechanism to minimize the computational\noverhead of the URA module and improve the utilization of uncertainty guidance.\nExperiments on seven benchmark datasets demonstrate the superiority of the\nproposed UGRAN over the state-of-the-art methodologies. Codes will be released\nat https://github.com/I2-Multimedia-Lab/UGRAN.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09766", "pdf": "https://arxiv.org/pdf/2504.09766", "abs": "https://arxiv.org/abs/2504.09766", "authors": ["Diego Marcondes"], "title": "On the representation of stack operators by mathematical morphology", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces the class of grey-scale image stack operators as those\nthat (a) map binary-images into binary-images and (b) commute in average with\ncross-sectioning. We show that stack operators are 1-Lipchitz extensions of set\noperators which can be represented by applying a characteristic set operator to\nthe cross-sections of the image and summing. In particular, they are a\ngeneralisation of stack filters, for which the characteristic set operators are\nincreasing. Our main result is that stack operators inherit lattice properties\nof the characteristic set operators. We focus on the case of\ntranslation-invariant and locally defined stack operators and show the main\nresult by deducing the characteristic function, kernel, and basis\nrepresentation of stack operators. The results of this paper have implications\non the design of image operators, since imply that to solve some grey-scale\nimage processing problems it is enough to design an operator for performing the\ndesired transformation on binary images, and then considering its extension\ngiven by a stack operator. We leave many topics for future research regarding\nthe machine learning of stack operators and the characterisation of the image\nprocessing problems that can be solved by them.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10391", "pdf": "https://arxiv.org/pdf/2504.10391", "abs": "https://arxiv.org/abs/2504.10391", "authors": ["Varun Vasudevan", "Faezeh Akhavizadegan", "Abhinav Prakash", "Yokila Arora", "Jason Cho", "Tanya Mendiratta", "Sushant Kumar", "Kannan Achan"], "title": "LLM-driven Constrained Copy Generation through Iterative Refinement", "categories": ["cs.CL"], "comment": "10 pages, 2 figures, 7 Tables", "summary": "Crafting a marketing message (copy), or copywriting is a challenging\ngeneration task, as the copy must adhere to various constraints. Copy creation\nis inherently iterative for humans, starting with an initial draft followed by\nsuccessive refinements. However, manual copy creation is time-consuming and\nexpensive, resulting in only a few copies for each use case. This limitation\nrestricts our ability to personalize content to customers. Contrary to the\nmanual approach, LLMs can generate copies quickly, but the generated content\ndoes not consistently meet all the constraints on the first attempt (similar to\nhumans). While recent studies have shown promise in improving constrained\ngeneration through iterative refinement, they have primarily addressed tasks\nwith only a few simple constraints. Consequently, the effectiveness of\niterative refinement for tasks such as copy generation, which involves many\nintricate constraints, remains unclear. To address this gap, we propose an\nLLM-based end-to-end framework for scalable copy generation using iterative\nrefinement. To the best of our knowledge, this is the first study to address\nmultiple challenging constraints simultaneously in copy generation. Examples of\nthese constraints include length, topics, keywords, preferred lexical ordering,\nand tone of voice. We demonstrate the performance of our framework by creating\ncopies for e-commerce banners for three different use cases of varying\ncomplexity. Our results show that iterative refinement increases the copy\nsuccess rate by $16.25-35.91$% across use cases. Furthermore, the copies\ngenerated using our approach outperformed manually created content in multiple\npilot studies using a multi-armed bandit framework. The winning copy improved\nthe click-through rate by $38.5-45.21$%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09876", "pdf": "https://arxiv.org/pdf/2504.09876", "abs": "https://arxiv.org/abs/2504.09876", "authors": ["Tran Quoc Khanh Le", "Nguyen Lan Vi Vu", "Ha-Hieu Pham", "Xuan-Loc Huynh", "Tien-Huy Nguyen", "Minh Huu Nhat Le", "Quan Nguyen", "Hien D. Nguyen"], "title": "HDC: Hierarchical Distillation for Multi-level Noisy Consistency in Semi-Supervised Fetal Ultrasound Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Transvaginal ultrasound is a critical imaging modality for evaluating\ncervical anatomy and detecting physiological changes. However, accurate\nsegmentation of cervical structures remains challenging due to low contrast,\nshadow artifacts, and fuzzy boundaries. While convolutional neural networks\n(CNNs) have shown promising results in medical image segmentation, their\nperformance is often limited by the need for large-scale annotated datasets -\nan impractical requirement in clinical ultrasound imaging. Semi-supervised\nlearning (SSL) offers a compelling solution by leveraging unlabeled data, but\nexisting teacher-student frameworks often suffer from confirmation bias and\nhigh computational costs. We propose HDC, a novel semi-supervised segmentation\nframework that integrates Hierarchical Distillation and Consistency learning\nwithin a multi-level noise mean-teacher framework. Unlike conventional\napproaches that rely solely on pseudo-labeling, we introduce a hierarchical\ndistillation mechanism that guides feature-level learning via two novel\nobjectives: (1) Correlation Guidance Loss to align feature representations\nbetween the teacher and main student branch, and (2) Mutual Information Loss to\nstabilize representations between the main and noisy student branches. Our\nframework reduces model complexity while improving generalization. Extensive\nexperiments on two fetal ultrasound datasets, FUGC and PSFH, demonstrate that\nour method achieves competitive performance with significantly lower\ncomputational overhead than existing multi-teacher models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "consistency"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08744", "pdf": "https://arxiv.org/pdf/2504.08744", "abs": "https://arxiv.org/abs/2504.08744", "authors": ["Esmail Gumaan"], "title": "ExpertRAG: Efficient RAG with Mixture of Experts -- Optimizing Context Retrieval for Adaptive LLM Responses", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "30 pages, 4 figures", "summary": "ExpertRAG is a novel theoretical framework that integrates Mixture-of-Experts\n(MoE) architectures with Retrieval Augmented Generation (RAG) to advance the\nefficiency and accuracy of knowledge-intensive language modeling. We propose a\ndynamic retrieval gating mechanism coupled with expert routing, enabling the\nmodel to selectively consult an external knowledge store or rely on specialized\ninternal experts based on the query's needs. The paper lays out the theoretical\nfoundations of ExpertRAG, including a probabilistic formulation that treats\nretrieval and expert selection as latent decisions, and mathematical\njustifications for its efficiency in both computation and knowledge\nutilization. We derive formulae to quantify the expected computational cost\nsavings from selective retrieval and the capacity gains from sparse expert\nutilization. A comparative analysis positions ExpertRAG against standard RAG\n(with always-on retrieval) and pure MoE models (e.g., Switch Transformer,\nMixtral) to highlight its unique balance between parametric knowledge and\nnon-parametric retrieval. We also outline an experimental validation strategy,\nproposing benchmarks and evaluation protocols to test ExpertRAG's performance\non factual recall, generalization, and inference efficiency. The proposed\nframework, although presented theoretically, is supported by insights from\nprior work in RAG and MoE, and is poised to provide more factual, efficient,\nand adaptive generation by leveraging the best of both paradigms. In summary,\nExpertRAG contributes a new perspective on scaling and augmenting language\nmodels, backed by a thorough analysis and a roadmap for empirical validation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09897", "pdf": "https://arxiv.org/pdf/2504.09897", "abs": "https://arxiv.org/abs/2504.09897", "authors": ["Jaewoo Lee", "Keyang Xuan", "Chanakya Ekbote", "Sandeep Polisetty", "Yi R.", "Fung", "Paul Pu Liang"], "title": "TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language Models", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable versatility in\nunderstanding diverse multimodal data and tasks. However, these capabilities\ncome with an increased model scale. While post-training pruning reduces model\nsize in unimodal models, its application to MLLMs often yields limited success.\nOur analysis discovers that conventional methods fail to account for the unique\ntoken attributes across layers and modalities inherent to MLLMs. Inspired by\nthis observation, we propose TAMP, a simple yet effective pruning framework\ntailored for MLLMs, featuring two key components: (1) Diversity-Aware Sparsity,\nwhich adjusts sparsity ratio per layer based on diversities among multimodal\noutput tokens, preserving more parameters in high-diversity layers; and (2)\nAdaptive Multimodal Input Activation, which identifies representative\nmultimodal input tokens using attention scores to guide unstructured weight\npruning. We validate our method on two state-of-the-art MLLMs: LLaVA-NeXT,\ndesigned for vision-language tasks, and VideoLLaMA2, capable of processing\naudio, visual, and language modalities. Empirical experiments across various\nmultimodal evaluation benchmarks demonstrate that each component of our\napproach substantially outperforms existing pruning techniques.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09899", "pdf": "https://arxiv.org/pdf/2504.09899", "abs": "https://arxiv.org/abs/2504.09899", "authors": ["Ziwang Xu", "Lanqing Guo", "Satoshi Tsutsui", "Shuyan Zhang", "Alex C. Kot", "Bihan Wen"], "title": "Digital Staining with Knowledge Distillation: A Unified Framework for Unpaired and Paired-But-Misaligned Data", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to IEEE Transactions on Medical Imaging", "summary": "Staining is essential in cell imaging and medical diagnostics but poses\nsignificant challenges, including high cost, time consumption, labor intensity,\nand irreversible tissue alterations. Recent advances in deep learning have\nenabled digital staining through supervised model training. However, collecting\nlarge-scale, perfectly aligned pairs of stained and unstained images remains\ndifficult. In this work, we propose a novel unsupervised deep learning\nframework for digital cell staining that reduces the need for extensive paired\ndata using knowledge distillation. We explore two training schemes: (1)\nunpaired and (2) paired-but-misaligned settings. For the unpaired case, we\nintroduce a two-stage pipeline, comprising light enhancement followed by\ncolorization, as a teacher model. Subsequently, we obtain a student staining\ngenerator through knowledge distillation with hybrid non-reference losses. To\nleverage the pixel-wise information between adjacent sections, we further\nextend to the paired-but-misaligned setting, adding the Learning to Align\nmodule to utilize pixel-level information. Experiment results on our dataset\ndemonstrate that our proposed unsupervised deep staining method can generate\nstained images with more accurate positions and shapes of the cell targets in\nboth settings. Compared with competing methods, our method achieves improved\nresults both qualitatively and quantitatively (e.g., NIQE and PSNR).We applied\nour digital staining method to the White Blood Cell (WBC) dataset,\ninvestigating its potential for medical applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08777", "pdf": "https://arxiv.org/pdf/2504.08777", "abs": "https://arxiv.org/abs/2504.08777", "authors": ["Teo Susnjak", "Cole Palffy", "Tatiana Zimina", "Nazgul Altynbekova", "Kunal Garg", "Leona Gilbert"], "title": "The Lyme Disease Controversy: An AI-Driven Discourse Analysis of a Quarter Century of Academic Debate and Divides", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "The scientific discourse surrounding Chronic Lyme Disease (CLD) and\nPost-Treatment Lyme Disease Syndrome (PTLDS) has evolved over the past\ntwenty-five years into a complex and polarised debate, shaped by shifting\nresearch priorities, institutional influences, and competing explanatory\nmodels. This study presents the first large-scale, systematic examination of\nthis discourse using an innovative hybrid AI-driven methodology, combining\nlarge language models with structured human validation to analyse thousands of\nscholarly abstracts spanning 25 years. By integrating Large Language Models\n(LLMs) with expert oversight, we developed a quantitative framework for\ntracking epistemic shifts in contested medical fields, with applications to\nother content analysis domains. Our analysis revealed a progressive transition\nfrom infection-based models of Lyme disease to immune-mediated explanations for\npersistent symptoms. This study offers new empirical insights into the\nstructural and epistemic forces shaping Lyme disease research, providing a\nscalable and replicable methodology for analysing discourse, while underscoring\nthe value of AI-assisted methodologies in social science and medical research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08804", "pdf": "https://arxiv.org/pdf/2504.08804", "abs": "https://arxiv.org/abs/2504.08804", "authors": ["Pooya Razavi", "Sonya J. Powers"], "title": "Estimating Item Difficulty Using Large Language Models and Tree-Based Machine Learning Algorithms", "categories": ["cs.CY", "cs.CL", "cs.LG"], "comment": null, "summary": "Estimating item difficulty through field-testing is often resource-intensive\nand time-consuming. As such, there is strong motivation to develop methods that\ncan predict item difficulty at scale using only the item content. Large\nLanguage Models (LLMs) represent a new frontier for this goal. The present\nresearch examines the feasibility of using an LLM to predict item difficulty\nfor K-5 mathematics and reading assessment items (N = 5170). Two estimation\napproaches were implemented: (a) a direct estimation method that prompted the\nLLM to assign a single difficulty rating to each item, and (b) a feature-based\nstrategy where the LLM extracted multiple cognitive and linguistic features,\nwhich were then used in ensemble tree-based models (random forests and gradient\nboosting) to predict difficulty. Overall, direct LLM estimates showed moderate\nto strong correlations with true item difficulties. However, their accuracy\nvaried by grade level, often performing worse for early grades. In contrast,\nthe feature-based method yielded stronger predictive accuracy, with\ncorrelations as high as r = 0.87 and lower error estimates compared to both\ndirect LLM predictions and baseline regressors. These findings highlight the\npromise of LLMs in streamlining item development and reducing reliance on\nextensive field testing and underscore the importance of structured feature\nextraction. We provide a seven-step workflow for testing professionals who\nwould want to implement a similar item difficulty estimation approach with\ntheir item pool.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08949", "pdf": "https://arxiv.org/pdf/2504.08949", "abs": "https://arxiv.org/abs/2504.08949", "authors": ["Haokai Ma", "Yunshan Ma", "Ruobing Xie", "Lei Meng", "Jialie Shen", "Xingwu Sun", "Zhanhui Kang", "Tat-Seng Chua"], "title": "Large Language Model Empowered Recommendation Meets All-domain Continual Pre-Training", "categories": ["cs.IR", "cs.CL"], "comment": "In submission", "summary": "Recent research efforts have investigated how to integrate Large Language\nModels (LLMs) into recommendation, capitalizing on their semantic comprehension\nand open-world knowledge for user behavior understanding. These approaches\npredominantly employ supervised fine-tuning on single-domain user interactions\nto adapt LLMs for specific recommendation tasks. However, they typically\nencounter dual challenges: the mismatch between general language\nrepresentations and domain-specific preference patterns, as well as the limited\nadaptability to multi-domain recommendation scenarios. To bridge these gaps, we\nintroduce CPRec -- an All-domain Continual Pre-Training framework for\nRecommendation -- designed to holistically align LLMs with universal user\nbehaviors through the continual pre-training paradigm. Specifically, we first\ndesign a unified prompt template and organize users' multi-domain behaviors\ninto domain-specific behavioral sequences and all-domain mixed behavioral\nsequences that emulate real-world user decision logic. To optimize behavioral\nknowledge infusion, we devise a Warmup-Stable-Annealing learning rate schedule\ntailored for the continual pre-training paradigm in recommendation to\nprogressively enhance the LLM's capability in knowledge adaptation from\nopen-world knowledge to universal recommendation tasks. To evaluate the\neffectiveness of our CPRec, we implement it on a large-scale dataset covering\nseven domains and conduct extensive experiments on five real-world datasets\nfrom two distinct platforms. Experimental results confirm that our continual\npre-training paradigm significantly mitigates the semantic-behavioral\ndiscrepancy and achieves state-of-the-art performance in all recommendation\nscenarios. The source code will be released upon acceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09081", "pdf": "https://arxiv.org/pdf/2504.09081", "abs": "https://arxiv.org/abs/2504.09081", "authors": ["Prabhat Pandey", "Rupak Vignesh Swaminathan", "K V Vijay Girish", "Arunasish Sen", "Jian Xie", "Grant P. Strimel", "Andreas Schwarz"], "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10004", "pdf": "https://arxiv.org/pdf/2504.10004", "abs": "https://arxiv.org/abs/2504.10004", "authors": ["Matías Piqueras", "Alexandra Segerberg", "Matteo Magnani", "Måns Magnusson", "Nataša Sladoje"], "title": "An Image is Worth $K$ Topics: A Visual Structural Topic Model with Pretrained Image Embeddings", "categories": ["cs.CV", "cs.CY", "stat.AP", "stat.ME"], "comment": null, "summary": "Political scientists are increasingly interested in analyzing visual content\nat scale. However, the existing computational toolbox is still in need of\nmethods and models attuned to the specific challenges and goals of social and\npolitical inquiry. In this article, we introduce a visual Structural Topic\nModel (vSTM) that combines pretrained image embeddings with a structural topic\nmodel. This has important advantages compared to existing approaches. First,\npretrained embeddings allow the model to capture the semantic complexity of\nimages relevant to political contexts. Second, the structural topic model\nprovides the ability to analyze how topics and covariates are related, while\nmaintaining a nuanced representation of images as a mixture of multiple topics.\nIn our empirical application, we show that the vSTM is able to identify topics\nthat are interpretable, coherent, and substantively relevant to the study of\nonline political communication.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09265", "pdf": "https://arxiv.org/pdf/2504.09265", "abs": "https://arxiv.org/abs/2504.09265", "authors": ["Lei Kang", "Jia Li", "Mi Tian", "Hua Huang"], "title": "Mixture of Group Experts for Learning Invariant Representations", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Sparsely activated Mixture-of-Experts (MoE) models effectively increase the\nnumber of parameters while maintaining consistent computational costs per\ntoken. However, vanilla MoE models often suffer from limited diversity and\nspecialization among experts, constraining their performance and scalability,\nespecially as the number of experts increases. In this paper, we present a\nnovel perspective on vanilla MoE with top-$k$ routing inspired by sparse\nrepresentation. This allows us to bridge established theoretical insights from\nsparse representation into MoE models. Building on this foundation, we propose\na group sparse regularization approach for the input of top-$k$ routing, termed\nMixture of Group Experts (MoGE). MoGE indirectly regularizes experts by\nimposing structural constraints on the routing inputs, while preserving the\noriginal MoE architecture. Furthermore, we organize the routing input into a 2D\ntopographic map, spatially grouping neighboring elements. This structure\nenables MoGE to capture representations invariant to minor transformations,\nthereby significantly enhancing expert diversity and specialization.\nComprehensive evaluations across various Transformer models for image\nclassification and language modeling tasks demonstrate that MoGE substantially\noutperforms its MoE counterpart, with minimal additional memory and computation\noverhead. Our approach provides a simple yet effective solution to scale the\nnumber of experts and reduce redundancy among them. The source code is included\nin the supplementary material and will be publicly released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10018", "pdf": "https://arxiv.org/pdf/2504.10018", "abs": "https://arxiv.org/abs/2504.10018", "authors": ["Xiao Wang", "Haiyang Wang", "Shiao Wang", "Qiang Chen", "Jiandong Jin", "Haoyu Song", "Bo Jiang", "Chenglong Li"], "title": "RGB-Event based Pedestrian Attribute Recognition: A Benchmark Dataset and An Asymmetric RWKV Fusion Framework", "categories": ["cs.CV", "cs.AI"], "comment": "The First Benchmark Dataset for RGB-Event Multimodal Pedestrian\n  Attribute Recognition Task", "summary": "Existing pedestrian attribute recognition methods are generally developed\nbased on RGB frame cameras. However, these approaches are constrained by the\nlimitations of RGB cameras, such as sensitivity to lighting conditions and\nmotion blur, which hinder their performance. Furthermore, current attribute\nrecognition primarily focuses on analyzing pedestrians' external appearance and\nclothing, lacking an exploration of emotional dimensions. In this paper, we\nrevisit these issues and propose a novel multi-modal RGB-Event attribute\nrecognition task by drawing inspiration from the advantages of event cameras in\nlow-light, high-speed, and low-power consumption. Specifically, we introduce\nthe first large-scale multi-modal pedestrian attribute recognition dataset,\ntermed EventPAR, comprising 100K paired RGB-Event samples that cover 50\nattributes related to both appearance and six human emotions, diverse scenes,\nand various seasons. By retraining and evaluating mainstream PAR models on this\ndataset, we establish a comprehensive benchmark and provide a solid foundation\nfor future research in terms of data and algorithmic baselines. In addition, we\npropose a novel RWKV-based multi-modal pedestrian attribute recognition\nframework, featuring an RWKV visual encoder and an asymmetric RWKV fusion\nmodule. Extensive experiments are conducted on our proposed dataset as well as\ntwo simulated datasets (MARS-Attribute and DukeMTMC-VID-Attribute), achieving\nstate-of-the-art results. The source code and dataset will be released on\nhttps://github.com/Event-AHU/OpenPAR", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09426", "pdf": "https://arxiv.org/pdf/2504.09426", "abs": "https://arxiv.org/abs/2504.09426", "authors": ["Shengao Wang", "Arjun Chandra", "Aoming Liu", "Venkatesh Saligrama", "Boqing Gong"], "title": "BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Human infants rapidly develop visual reasoning skills from minimal input,\nsuggesting that developmentally inspired pretraining could significantly\nenhance the efficiency of vision-language models (VLMs). Although recent\nefforts have leveraged infant-inspired datasets like SAYCam, existing\nevaluation benchmarks remain misaligned--they are either too simplistic,\nnarrowly scoped, or tailored for large-scale pretrained models. Additionally,\ntraining exclusively on infant data overlooks the broader, diverse input from\nwhich infants naturally learn. To address these limitations, we propose\nBabyVLM, a novel framework comprising comprehensive in-domain evaluation\nbenchmarks and a synthetic training dataset created via child-directed\ntransformations of existing datasets. We demonstrate that VLMs trained with our\nsynthetic dataset achieve superior performance on BabyVLM tasks compared to\nmodels trained solely on SAYCam or general-purpose data of the SAYCam size.\nBabyVLM thus provides a robust, developmentally aligned evaluation tool and\nillustrates how compact models trained on carefully curated data can generalize\neffectively, opening pathways toward data-efficient vision-language learning\nparadigms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10070", "pdf": "https://arxiv.org/pdf/2504.10070", "abs": "https://arxiv.org/abs/2504.10070", "authors": ["Kiana Hoshanfar", "Alireza Hosseini", "Ahmad Kalhor", "Babak Nadjar Araabi"], "title": "DTFSal: Audio-Visual Dynamic Token Fusion for Video Saliency Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Audio-visual saliency prediction aims to mimic human visual attention by\nidentifying salient regions in videos through the integration of both visual\nand auditory information. Although visual-only approaches have significantly\nadvanced, effectively incorporating auditory cues remains challenging due to\ncomplex spatio-temporal interactions and high computational demands. To address\nthese challenges, we propose Dynamic Token Fusion Saliency (DFTSal), a novel\naudio-visual saliency prediction framework designed to balance accuracy with\ncomputational efficiency. Our approach features a multi-scale visual encoder\nequipped with two novel modules: the Learnable Token Enhancement Block (LTEB),\nwhich adaptively weights tokens to emphasize crucial saliency cues, and the\nDynamic Learnable Token Fusion Block (DLTFB), which employs a shifting\noperation to reorganize and merge features, effectively capturing long-range\ndependencies and detailed spatial information. In parallel, an audio branch\nprocesses raw audio signals to extract meaningful auditory features. Both\nvisual and audio features are integrated using our Adaptive Multimodal Fusion\nBlock (AMFB), which employs local, global, and adaptive fusion streams for\nprecise cross-modal fusion. The resulting fused features are processed by a\nhierarchical multi-decoder structure, producing accurate saliency maps.\nExtensive evaluations on six audio-visual benchmarks demonstrate that DFTSal\nachieves SOTA performance while maintaining computational efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09723", "pdf": "https://arxiv.org/pdf/2504.09723", "abs": "https://arxiv.org/abs/2504.09723", "authors": ["Dakuo Wang", "Ting-Yao Hsu", "Yuxuan Lu", "Limeng Cui", "Yaochen Xie", "William Headean", "Bingsheng Yao", "Akash Veeragouni", "Jiapeng Liu", "Sreyashi Nag", "Jessie Wang"], "title": "AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "A/B testing experiment is a widely adopted method for evaluating UI/UX design\ndecisions in modern web applications. Yet, traditional A/B testing remains\nconstrained by its dependence on the large-scale and live traffic of human\nparticipants, and the long time of waiting for the testing result. Through\nformative interviews with six experienced industry practitioners, we identified\ncritical bottlenecks in current A/B testing workflows. In response, we present\nAgentA/B, a novel system that leverages Large Language Model-based autonomous\nagents (LLM Agents) to automatically simulate user interaction behaviors with\nreal webpages. AgentA/B enables scalable deployment of LLM agents with diverse\npersonas, each capable of navigating the dynamic webpage and interactively\nexecuting multi-step interactions like search, clicking, filtering, and\npurchasing. In a demonstrative controlled experiment, we employ AgentA/B to\nsimulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and\ncompare agent behaviors with real human shopping behaviors at a scale. Our\nfindings suggest AgentA/B can emulate human-like behavior patterns.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09737", "pdf": "https://arxiv.org/pdf/2504.09737", "abs": "https://arxiv.org/abs/2504.09737", "authors": ["Nitya Thakkar", "Mert Yuksekgonul", "Jake Silberg", "Animesh Garg", "Nanyun Peng", "Fei Sha", "Rose Yu", "Carl Vondrick", "James Zou"], "title": "Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "30 pages, 7 figures", "summary": "Peer review at AI conferences is stressed by rapidly rising submission\nvolumes, leading to deteriorating review quality and increased author\ndissatisfaction. To address these issues, we developed Review Feedback Agent, a\nsystem leveraging multiple large language models (LLMs) to improve review\nclarity and actionability by providing automated feedback on vague comments,\ncontent misunderstandings, and unprofessional remarks to reviewers. Implemented\nat ICLR 2025 as a large randomized control study, our system provided optional\nfeedback to more than 20,000 randomly selected reviews. To ensure high-quality\nfeedback for reviewers at this scale, we also developed a suite of automated\nreliability tests powered by LLMs that acted as guardrails to ensure feedback\nquality, with feedback only being sent to reviewers if it passed all the tests.\nThe results show that 27% of reviewers who received feedback updated their\nreviews, and over 12,000 feedback suggestions from the agent were incorporated\nby those reviewers. This suggests that many reviewers found the AI-generated\nfeedback sufficiently helpful to merit updating their reviews. Incorporating AI\nfeedback led to significantly longer reviews (an average increase of 80 words\namong those who updated after receiving feedback) and more informative reviews,\nas evaluated by blinded researchers. Moreover, reviewers who were selected to\nreceive AI feedback were also more engaged during paper rebuttals, as seen in\nlonger author-reviewer discussions. This work demonstrates that carefully\ndesigned LLM-generated review feedback can enhance peer review quality by\nmaking reviews more specific and actionable while increasing engagement between\nreviewers and authors. The Review Feedback Agent is publicly available at\nhttps://github.com/zou-group/review_feedback_agent.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["AI feedback"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10090", "pdf": "https://arxiv.org/pdf/2504.10090", "abs": "https://arxiv.org/abs/2504.10090", "authors": ["I-Sheng Fang", "Jun-Cheng Chen"], "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09858", "pdf": "https://arxiv.org/pdf/2504.09858", "abs": "https://arxiv.org/abs/2504.09858", "authors": ["Wenjie Ma", "Jingxuan He", "Charlie Snell", "Tyler Griggs", "Sewon Min", "Matei Zaharia"], "title": "Reasoning Models Can Be Effective Without Thinking", "categories": ["cs.AI", "cs.CL"], "comment": "33 pages, 7 main figures, 2 tables", "summary": "Recent LLMs have significantly improved reasoning capabilities, primarily by\nincluding an explicit, lengthy Thinking process as part of generation. In this\npaper, we question whether this explicit thinking is necessary. Using the\nstate-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking\nprocess via simple prompting, denoted as NoThinking, can be surprisingly\neffective. When controlling for the number of tokens, NoThinking outperforms\nThinking across a diverse set of seven challenging reasoning\ndatasets--including mathematical problem solving, formal theorem proving, and\ncoding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with\n700 tokens. Notably, the performance of NoThinking becomes more competitive\nwith pass@k as k increases. Building on this observation, we demonstrate that a\nparallel scaling approach that uses NoThinking to generate N outputs\nindependently and aggregates them is highly effective. For aggregation, we use\ntask-specific verifiers when available, or we apply simple best-of-N strategies\nsuch as confidence-based selection. Our method outperforms a range of baselines\nwith similar latency using Thinking, and is comparable to Thinking with\nsignificantly longer latency (up to 9x). Together, our research encourages a\nreconsideration of the necessity of lengthy thinking processes, while also\nestablishing a competitive reference for achieving strong reasoning performance\nin low-budget settings or at low latency using parallel scaling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09946", "pdf": "https://arxiv.org/pdf/2504.09946", "abs": "https://arxiv.org/abs/2504.09946", "authors": ["Qian Wang", "Zhanzhi Lou", "Zhenheng Tang", "Nuo Chen", "Xuandong Zhao", "Wenxuan Zhang", "Dawn Song", "Bingsheng He"], "title": "Assessing Judging Bias in Large Reasoning Models: An Empirical Study", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have\ndemonstrated remarkable reasoning capabilities, raising important questions\nabout their biases in LLM-as-a-judge settings. We present a comprehensive\nbenchmark comparing judging biases between LLMs and LRMs across both subjective\npreference-alignment datasets and objective fact-based datasets. Through\ninvestigation of bandwagon, authority, position, and distraction biases, we\nuncover four key findings: (1) despite their advanced reasoning capabilities,\nLRMs remain susceptible to the above biases; (2) LRMs demonstrate better\nrobustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit\nnotable position bias, preferring options in later positions; and (4) we\nidentify a novel \"superficial reflection bias\" where phrases mimicking\nreasoning (e.g., \"wait, let me think...\") significantly influence model\njudgments. To address these biases, we design and evaluate three mitigation\nstrategies: specialized system prompts that reduce judging biases by up to 19\\%\nin preference alignment datasets and 14\\% in fact-related datasets, in-context\nlearning that provides up to 27\\% improvement on preference tasks but shows\ninconsistent results on factual tasks, and a self-reflection mechanism that\nreduces biases by up to 10\\% in preference datasets and 16\\% in fact-related\ndatasets, with self-reflection proving particularly effective for LRMs. Our\nwork provides crucial insights for developing more reliable LLM-as-a-Judge\nframeworks, especially as LRMs become increasingly deployed as automated\njudges.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10158", "pdf": "https://arxiv.org/pdf/2504.10158", "abs": "https://arxiv.org/abs/2504.10158", "authors": ["Jiansheng Li", "Xingxuan Zhang", "Hao Zou", "Yige Guo", "Renzhe Xu", "Yilong Liu", "Chuzhao Zhu", "Yue He", "Peng Cui"], "title": "COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current object detectors often suffer significant perfor-mance degradation in\nreal-world applications when encountering distributional shifts. Consequently,\nthe out-of-distribution (OOD) generalization capability of object detectors has\ngarnered increasing attention from researchers. Despite this growing interest,\nthere remains a lack of a large-scale, comprehensive dataset and evaluation\nbenchmark with fine-grained annotations tailored to assess the OOD\ngeneralization on more intricate tasks like object detection and grounding. To\naddress this gap, we introduce COUNTS, a large-scale OOD dataset with\nobject-level annotations. COUNTS encompasses 14 natural distributional shifts,\nover 222K samples, and more than 1,196K labeled bounding boxes. Leveraging\nCOUNTS, we introduce two novel benchmarks: O(OD)2 and OODG. O(OD)2 is designed\nto comprehensively evaluate the OOD generalization capabilities of object\ndetectors by utilizing controlled distribution shifts between training and\ntesting data. OODG, on the other hand, aims to assess the OOD generalization of\ngrounding abilities in multimodal large language models (MLLMs). Our findings\nreveal that, while large models and extensive pre-training data substantially\nen hance performance in in-distribution (IID) scenarios, significant\nlimitations and opportunities for improvement persist in OOD contexts for both\nobject detectors and MLLMs. In visual grounding tasks, even the advanced GPT-4o\nand Gemini-1.5 only achieve 56.7% and 28.0% accuracy, respectively. We hope\nCOUNTS facilitates advancements in the development and assessment of robust\nobject detectors and MLLMs capable of maintaining high performance under\ndistributional shifts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10081", "pdf": "https://arxiv.org/pdf/2504.10081", "abs": "https://arxiv.org/abs/2504.10081", "authors": ["Yichi Zhang", "Zihao Zeng", "Dongbai Li", "Yao Huang", "Zhijie Deng", "Yinpeng Dong"], "title": "RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been\nrapidly progressing and achieving breakthrough performance on complex reasoning\ntasks such as mathematics and coding. However, the open-source R1 models have\nraised safety concerns in wide applications, such as the tendency to comply\nwith malicious queries, which greatly impacts the utility of these powerful\nmodels in their applications. In this paper, we introduce RealSafe-R1 as\nsafety-aligned versions of DeepSeek-R1 distilled models. To train these models,\nwe construct a dataset of 15k safety-aware reasoning trajectories generated by\nDeepSeek-R1, under explicit instructions for expected refusal behavior. Both\nquantitative experiments and qualitative case studies demonstrate the models'\nimprovements, which are shown in their safety guardrails against both harmful\nqueries and jailbreak attacks. Importantly, unlike prior safety alignment\nefforts that often compromise reasoning performance, our method preserves the\nmodels' reasoning capabilities by maintaining the training data within the\noriginal distribution of generation. Model weights of RealSafe-R1 are\nopen-source at https://huggingface.co/RealSafe.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10090", "pdf": "https://arxiv.org/pdf/2504.10090", "abs": "https://arxiv.org/abs/2504.10090", "authors": ["I-Sheng Fang", "Jun-Cheng Chen"], "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10242", "pdf": "https://arxiv.org/pdf/2504.10242", "abs": "https://arxiv.org/abs/2504.10242", "authors": ["Tianyu Xin", "Jin-Liang Xiao", "Zeyu Xia", "Shan Yin", "Liang-Jian Deng"], "title": "CAT: A Conditional Adaptation Tailor for Efficient and Effective Instance-Specific Pansharpening on Real-World Data", "categories": ["cs.CV"], "comment": null, "summary": "Pansharpening is a crucial remote sensing technique that fuses low-resolution\nmultispectral (LRMS) images with high-resolution panchromatic (PAN) images to\ngenerate high-resolution multispectral (HRMS) imagery. Although deep learning\ntechniques have significantly advanced pansharpening, many existing methods\nsuffer from limited cross-sensor generalization and high computational\noverhead, restricting their real-time applications. To address these\nchallenges, we propose an efficient framework that quickly adapts to a specific\ninput instance, completing both training and inference in a short time. Our\nframework splits the input image into multiple patches, selects a subset for\nunsupervised CAT training, and then performs inference on all patches,\nstitching them into the final output. The CAT module, integrated between the\nfeature extraction and channel transformation stages of a pre-trained network,\ntailors the fused features and fixes the parameters for efficient inference,\ngenerating improved results. Our approach offers two key advantages: (1)\n$\\textit{Improved Generalization Ability}$: by mitigating cross-sensor\ndegradation, our model--although pre-trained on a specific dataset--achieves\nsuperior performance on datasets captured by other sensors; (2)\n$\\textit{Enhanced Computational Efficiency}$: the CAT-enhanced network can\nswiftly adapt to the test sample using the single LRMS-PAN pair input, without\nrequiring extensive large-scale data retraining. Experiments on the real-world\ndata from WorldView-3 and WorldView-2 datasets demonstrate that our method\nachieves state-of-the-art performance on cross-sensor real-world data, while\nachieving both training and inference of $512\\times512$ image within\n$\\textit{0.4 seconds}$ and $4000\\times4000$ image within $\\textit{3 seconds}$\nat the fastest setting on a commonly used RTX 3090 GPU.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10254", "pdf": "https://arxiv.org/pdf/2504.10254", "abs": "https://arxiv.org/abs/2504.10254", "authors": ["Xuqiang Cao", "Linnan Zhao", "Jiaxuan Zhao", "Fang Liu", "Puhua Chen", "Wenping Ma"], "title": "MASSeg : 2nd Technical Report for 4th PVUW MOSE Track", "categories": ["cs.CV", "cs.AI"], "comment": "5 pages,4 figures,Technical report on Complex Video Object\n  Segmentation", "summary": "Complex video object segmentation continues to face significant challenges in\nsmall object recognition, occlusion handling, and dynamic scene modeling. This\nreport presents our solution, which ranked second in the MOSE track of CVPR\n2025 PVUW Challenge. Based on an existing segmentation framework, we propose an\nimproved model named MASSeg for complex video object segmentation, and\nconstruct an enhanced dataset, MOSE+, which includes typical scenarios with\nocclusions, cluttered backgrounds, and small target instances. During training,\nwe incorporate a combination of inter-frame consistent and inconsistent data\naugmentation strategies to improve robustness and generalization. During\ninference, we design a mask output scaling strategy to better adapt to varying\nobject sizes and occlusion levels. As a result, MASSeg achieves a J score of\n0.8250, F score of 0.9007, and a J&F score of 0.8628 on the MOSE test set.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10352", "pdf": "https://arxiv.org/pdf/2504.10352", "abs": "https://arxiv.org/abs/2504.10352", "authors": ["Yifan Yang", "Shujie Liu", "Jinyu Li", "Yuxuan Hu", "Haibin Wu", "Hui Wang", "Jianwei Yu", "Lingwei Meng", "Haiyang Sun", "Yanqing Liu", "Yan Lu", "Kai Yu", "Xie Chen"], "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis", "categories": ["eess.AS", "cs.CL"], "comment": "Submitted to ACM MM 2025", "summary": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma:\nautoregressive (AR) models suffer from slow generation and lack duration\ncontrollability, while non-autoregressive (NAR) models lack temporal modeling\nand typically require complex designs. In this paper, we introduce a novel\npseudo-autoregressive (PAR) codec language modeling approach that unifies AR\nand NAR modeling. Combining explicit temporal modeling from AR with parallel\ngeneration from NAR, PAR generates dynamic-length spans at fixed time steps.\nBuilding on PAR, we propose PALLE, a two-stage TTS system that leverages PAR\nfor initial generation followed by NAR refinement. In the first stage, PAR\nprogressively generates speech tokens along the time dimension, with each step\npredicting all positions in parallel but only retaining the left-most span. In\nthe second stage, low-confidence tokens are iteratively refined in parallel,\nleveraging the global contextual information. Experiments demonstrate that\nPALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on\nlarge-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech\ntest-clean set in terms of speech quality, speaker similarity, and\nintelligibility, while achieving up to ten times faster inference speed. Audio\nsamples are available at https://anonymous-palle.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10275", "pdf": "https://arxiv.org/pdf/2504.10275", "abs": "https://arxiv.org/abs/2504.10275", "authors": ["Harsh Yadav", "Maximilian Schaefer", "Kun Zhao", "Tobias Meisen"], "title": "LMFormer: Lane based Motion Prediction Transformer", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted: Autonomous Driving Workshop, CVPR 2025", "summary": "Motion prediction plays an important role in autonomous driving. This study\npresents LMFormer, a lane-aware transformer network for trajectory prediction\ntasks. In contrast to previous studies, our work provides a simple mechanism to\ndynamically prioritize the lanes and shows that such a mechanism introduces\nexplainability into the learning behavior of the network. Additionally,\nLMFormer uses the lane connection information at intersections, lane merges,\nand lane splits, in order to learn long-range dependency in lane structure.\nMoreover, we also address the issue of refining the predicted trajectories and\npropose an efficient method for iterative refinement through stacked\ntransformer layers. For benchmarking, we evaluate LMFormer on the nuScenes\ndataset and demonstrate that it achieves SOTA performance across multiple\nmetrics. Furthermore, the Deep Scenario dataset is used to not only illustrate\ncross-dataset network performance but also the unification capabilities of\nLMFormer to train on multiple datasets and achieve better performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10288", "pdf": "https://arxiv.org/pdf/2504.10288", "abs": "https://arxiv.org/abs/2504.10288", "authors": ["Mathieu Manni", "Dmitry Karpov", "K. Joost Batenburg", "Sharon Shwartz", "Nicola Viganò"], "title": "Noise2Ghost: Self-supervised deep convolutional reconstruction for ghost imaging", "categories": ["cs.CV", "cs.LG", "physics.data-an"], "comment": null, "summary": "We present a new self-supervised deep-learning-based Ghost Imaging (GI)\nreconstruction method, which provides unparalleled reconstruction performance\nfor noisy acquisitions among unsupervised methods. We present the supporting\nmathematical framework and results from theoretical and real data use cases.\nSelf-supervision removes the need for clean reference data while offering\nstrong noise reduction. This provides the necessary tools for addressing\nsignal-to-noise ratio concerns for GI acquisitions in emerging and cutting-edge\nlow-light GI scenarios. Notable examples include micro- and nano-scale x-ray\nemission imaging, e.g., x-ray fluorescence imaging of dose-sensitive samples.\nTheir applications include in-vivo and in-operando case studies for biological\nsamples and batteries.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10329", "pdf": "https://arxiv.org/pdf/2504.10329", "abs": "https://arxiv.org/abs/2504.10329", "authors": ["Xingyu Lu", "Yuhang Hu", "YiFan Zhang", "Kaiyu Jiang", "Changyi Liu", "Tianke Zhang", "Jinpeng Wang", "Bin Wen", "Chun Yuan", "Fan Yang", "Tingting Gao", "Di Zhang"], "title": "InstructEngine: Instruction-driven Text-to-Image Alignment", "categories": ["cs.CV"], "comment": "8 pages, 7 figures", "summary": "Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) has been\nextensively utilized for preference alignment of text-to-image models. Existing\nmethods face certain limitations in terms of both data and algorithm. For\ntraining data, most approaches rely on manual annotated preference data, either\nby directly fine-tuning the generators or by training reward models to provide\ntraining signals. However, the high annotation cost makes them difficult to\nscale up, the reward model consumes extra computation and cannot guarantee\naccuracy. From an algorithmic perspective, most methods neglect the value of\ntext and only take the image feedback as a comparative signal, which is\ninefficient and sparse. To alleviate these drawbacks, we propose the\nInstructEngine framework. Regarding annotation cost, we first construct a\ntaxonomy for text-to-image generation, then develop an automated data\nconstruction pipeline based on it. Leveraging advanced large multimodal models\nand human-defined rules, we generate 25K text-image preference pairs. Finally,\nwe introduce cross-validation alignment method, which refines data efficiency\nby organizing semantically analogous samples into mutually comparable pairs.\nEvaluations on DrawBench demonstrate that InstructEngine improves SD v1.5 and\nSDXL's performance by 10.53% and 5.30%, outperforming state-of-the-art\nbaselines, with ablation study confirming the benefits of InstructEngine's all\ncomponents. A win rate of over 50% in human reviews also proves that\nInstructEngine better aligns with human preferences.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "reinforcement learning", "preference", "RLAIF", "AI feedback", "alignment"], "score": 7}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "accuracy"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10358", "pdf": "https://arxiv.org/pdf/2504.10358", "abs": "https://arxiv.org/abs/2504.10358", "authors": ["Rui Chen", "Lei Sun", "Jing Tang", "Geng Li", "Xiangxiang Chu"], "title": "FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Recent advances in video generation have posed great challenges in the\nassessment of AI-generated content, particularly with the emergence of\nincreasingly sophisticated models. The various inconsistencies and defects\nobserved in such videos are inherently complex, making overall scoring\nnotoriously difficult. In this paper, we emphasize the critical importance of\nintegrating fine-grained reasoning into video evaluation, and we propose\n$\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation\nframework that first automatically generates $\\textbf{F}$ine-grained\n$\\textbf{E}$ntity-level questions, and then answers those questions by a\n$\\textbf{R}$easoning model with scores, which can be subsequently weighted\nsummed to an overall score for different applications. Specifically, we\nleverage LLMs to derive entity-level questions across five distinct\nperspectives, which (i) often focus on some specific entities of the content,\nthereby making answering or scoring much easier by MLLMs, and (ii) are more\ninterpretable. Then we construct a FingER dataset, consisting of approximately\n3.3k videos and corresponding 60k fine-grained QA annotations, each with\ndetailed reasons. Based on that, we further investigate various training\nprotocols to best incentivize the reasoning capability of MLLMs for correct\nanswer prediction. Extensive experiments demonstrate that a reasoning model\ntrained using Group Relative Policy Optimization (GRPO) with a cold-start\nstrategy achieves the best performance. Notably, our model surpasses existing\nmethods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on\nMonetBench with only 3.3k training videos, which is at most one-tenth of the\ntraining samples utilized by other methods. Our code and dataset will be\nreleased soon.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10462", "pdf": "https://arxiv.org/pdf/2504.10462", "abs": "https://arxiv.org/abs/2504.10462", "authors": ["Weixian Lei", "Jiacong Wang", "Haochen Wang", "Xiangtai Li", "Jun Hao Liew", "Jiashi Feng", "Zilong Huang"], "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces SAIL, a single transformer unified multimodal large\nlanguage model (MLLM) that integrates raw pixel encoding and language decoding\nwithin a singular architecture. Unlike existing modular MLLMs, which rely on a\npre-trained vision transformer (ViT), SAIL eliminates the need for a separate\nvision encoder, presenting a more minimalist architecture design. Instead of\nintroducing novel architectural components, SAIL adapts mix-attention\nmechanisms and multimodal positional encodings to better align with the\ndistinct characteristics of visual and textual modalities. We systematically\ncompare SAIL's properties-including scalability, cross-modal information flow\npatterns, and visual representation capabilities-with those of modular MLLMs.\nBy scaling both training data and model size, SAIL achieves performance\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\nenhances SAIL's scalability and results in significantly different cross-modal\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\nrepresentation capabilities, achieving results on par with ViT-22B in vision\ntasks such as semantic segmentation. Code and models are available at\nhttps://github.com/bytedance/SAIL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10465", "pdf": "https://arxiv.org/pdf/2504.10465", "abs": "https://arxiv.org/abs/2504.10465", "authors": ["Tao Zhang", "Xiangtai Li", "Zilong Huang", "Yanwei Li", "Weixian Lei", "Xueqing Deng", "Shihao Chen", "Shunping Ji", "Jiashi Feng"], "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for\nfine-grained pixel-level understanding tasks. However, all the works rely\nheavily on extra components, such as vision encoder (CLIP), segmentation\nexperts, leading to high system complexity and limiting model scaling. In this\nwork, our goal is to explore a highly simplified MLLM without introducing extra\ncomponents. Our work is motivated by the recent works on Single trAnsformer as\na unified vIsion-Language Model (SAIL) design, where these works jointly learn\nvision tokens and text tokens in transformers. We present Pixel-SAIL, a single\ntransformer for pixel-wise MLLM tasks. In particular, we present three\ntechnical improvements on the plain baseline. First, we design a learnable\nupsampling module to refine visual token features. Secondly, we propose a novel\nvisual prompt injection strategy to enable the single transformer to understand\nvisual prompt inputs and benefit from the early fusion of visual prompt\nembeddings and vision tokens. Thirdly, we introduce a vision expert\ndistillation strategy to efficiently enhance the single transformer's\nfine-grained feature extraction capability. In addition, we have collected a\ncomprehensive pixel understanding benchmark (PerBench), using a manual check.\nIt includes three tasks: detailed object description, visual prompt-based\nquestion answering, and visual-text referring segmentation. Extensive\nexperiments on four referring segmentation benchmarks, one visual prompt\nbenchmark, and our PerBench show that our Pixel-SAIL achieves comparable or\neven better results with a much simpler pipeline. Code and model will be\nreleased at https://github.com/magic-research/Sa2VA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering", "fine-grained"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10466", "pdf": "https://arxiv.org/pdf/2504.10466", "abs": "https://arxiv.org/abs/2504.10466", "authors": ["Xiaoyan Cong", "Jiayi Shen", "Zekun Li", "Rao Fu", "Tao Lu", "Srinath Sridhar"], "title": "Art3D: Training-Free 3D Generation from Flat-Colored Illustration", "categories": ["cs.CV"], "comment": "Technical Report. Course Project of Brown CSCI 1430 Computer Vision.\n  Project Page: https://joy-jy11.github.io/", "summary": "Large-scale pre-trained image-to-3D generative models have exhibited\nremarkable capabilities in diverse shape generations. However, most of them\nstruggle to synthesize plausible 3D assets when the reference image is\nflat-colored like hand drawings due to the lack of 3D illusion, which are often\nthe most user-friendly input modalities in art content creation. To this end,\nwe propose Art3D, a training-free method that can lift flat-colored 2D designs\ninto 3D. By leveraging structural and semantic features with pre- trained 2D\nimage generation models and a VLM-based realism evaluation, Art3D successfully\nenhances the three-dimensional illusion in reference images, thus simplifying\nthe process of generating 3D from 2D, and proves adaptable to a wide range of\npainting styles. To benchmark the generalization performance of existing\nimage-to-3D models on flat-colored images without 3D feeling, we collect a new\ndataset, Flat-2D, with over 100 samples. Experimental results demonstrate the\nperformance and robustness of Art3D, exhibiting superior generalizable capacity\nand promising practical applicability. Our source code and dataset will be\npublicly available on our project page: https://joy-jy11.github.io/ .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10486", "pdf": "https://arxiv.org/pdf/2504.10486", "abs": "https://arxiv.org/abs/2504.10486", "authors": ["Zeren Jiang", "Shaofei Wang", "Siyu Tang"], "title": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting", "categories": ["cs.CV"], "comment": "16 pages, 8 figures, Project pages:\n  https://jzr99.github.io/DNF-Avatar/", "summary": "Creating relightable and animatable human avatars from monocular videos is a\nrising research topic with a range of applications, e.g. virtual reality,\nsports, and video games. Previous works utilize neural fields together with\nphysically based rendering (PBR), to estimate geometry and disentangle\nappearance properties of human avatars. However, one drawback of these methods\nis the slow rendering speed due to the expensive Monte Carlo ray tracing. To\ntackle this problem, we proposed to distill the knowledge from implicit neural\nfields (teacher) to explicit 2D Gaussian splatting (student) representation to\ntake advantage of the fast rasterization property of Gaussian splatting. To\navoid ray-tracing, we employ the split-sum approximation for PBR appearance. We\nalso propose novel part-wise ambient occlusion probes for shadow computation.\nShadow prediction is achieved by querying these probes only once per pixel,\nwhich paves the way for real-time relighting of avatars. These techniques\ncombined give high-quality relighting results with realistic shadow effects.\nOur experiments demonstrate that the proposed student model achieves comparable\nor even better relighting results with our teacher model while being 370 times\nfaster at inference time, achieving a 67 FPS rendering speed.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08937", "pdf": "https://arxiv.org/pdf/2504.08937", "abs": "https://arxiv.org/abs/2504.08937", "authors": ["Minjie Deng", "Yan Wei", "Hao Zhai", "An Wu", "Yuncan Ouyang", "Qianyao Peng"], "title": "Rethinking Few-Shot Fusion: Granular Ball Priors Enable General-Purpose Deep Image Fusion", "categories": ["cs.GR", "cs.CV", "cs.LG", "eess.IV", "stat.ML"], "comment": null, "summary": "In image fusion tasks, due to the lack of real fused images as priors, most\ndeep learning-based fusion methods obtain global weight features from original\nimages in large-scale data pairs to generate images that approximate real fused\nimages. However, unlike previous studies, this paper utilizes Granular Ball\nadaptation to extract features in the brightness space as priors for deep\nnetworks, enabling the fusion network to converge quickly and complete the\nfusion task. This leads to few-shot training for a general image fusion\nnetwork, and based on this, we propose the GBFF fusion method. According to the\ninformation expression division of pixel pairs in the original fused image, we\nclassify pixel pairs with significant performance as the positive domain and\nnon-significant pixel pairs as the boundary domain. We perform split inference\nin the brightness space using Granular Ball adaptation to compute weights for\npixels that express information to varying degrees, generating approximate\nsupervision images that provide priors for the neural network in the structural\nbrightness space. Additionally, the extracted global saliency features also\nadaptively provide priors for setting the loss function weights of each image\nin the network, guiding the network to converge quickly at both global and\npixel levels alongside the supervised images, thereby enhancing the\nexpressiveness of the fused images. Each modality only used 10 pairs of images\nas the training set, completing the fusion task with a limited number of\niterations. Experiments validate the effectiveness of the algorithm and theory,\nand qualitative and quantitative comparisons with SOTA methods show that this\napproach is highly competitive in terms of fusion time and image\nexpressiveness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08974", "pdf": "https://arxiv.org/pdf/2504.08974", "abs": "https://arxiv.org/abs/2504.08974", "authors": ["Pouya Pezeshkpour", "Moin Aminnaseri", "Estevam Hruschka"], "title": "Mixed Signals: Decoding VLMs' Reasoning and Underlying Bias in Vision-Language Conflict", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have demonstrated impressive performance by\neffectively integrating visual and textual information to solve complex tasks.\nHowever, it is not clear how these models reason over the visual and textual\ndata together, nor how the flow of information between modalities is\nstructured. In this paper, we examine how VLMs reason by analyzing their biases\nwhen confronted with scenarios that present conflicting image and text cues, a\ncommon occurrence in real-world applications. To uncover the extent and nature\nof these biases, we build upon existing benchmarks to create five datasets\ncontaining mismatched image-text pairs, covering topics in mathematics,\nscience, and visual descriptions. Our analysis shows that VLMs favor text in\nsimpler queries but shift toward images as query complexity increases. This\nbias correlates with model scale, with the difference between the percentage of\nimage- and text-preferred responses ranging from +56.8% (image favored) to\n-74.4% (text favored), depending on the task and model. In addition, we explore\nthree mitigation strategies: simple prompt modifications, modifications that\nexplicitly instruct models on how to handle conflicting information (akin to\nchain-of-thought prompting), and a task decomposition strategy that analyzes\neach modality separately before combining their results. Our findings indicate\nthat the effectiveness of these strategies in identifying and mitigating bias\nvaries significantly and is closely linked to the model's overall performance\non the task and the specific modality in question.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09088", "pdf": "https://arxiv.org/pdf/2504.09088", "abs": "https://arxiv.org/abs/2504.09088", "authors": ["Yonghao Huang", "Leiting Chen", "Chuan Zhou"], "title": "Multi-Modal Brain Tumor Segmentation via 3D Multi-Scale Self-attention and Cross-attention", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Due to the success of CNN-based and Transformer-based models in various\ncomputer vision tasks, recent works study the applicability of CNN-Transformer\nhybrid architecture models in 3D multi-modality medical segmentation tasks.\nIntroducing Transformer brings long-range dependent information modeling\nability in 3D medical images to hybrid models via the self-attention mechanism.\nHowever, these models usually employ fixed receptive fields of 3D volumetric\nfeatures within each self-attention layer, ignoring the multi-scale volumetric\nlesion features. To address this issue, we propose a CNN-Transformer hybrid 3D\nmedical image segmentation model, named TMA-TransBTS, based on an\nencoder-decoder structure. TMA-TransBTS realizes simultaneous extraction of\nmulti-scale 3D features and modeling of long-distance dependencies by\nmulti-scale division and aggregation of 3D tokens in a self-attention layer.\nFurthermore, TMA-TransBTS proposes a 3D multi-scale cross-attention module to\nestablish a link between the encoder and the decoder for extracting rich volume\nrepresentations by exploiting the mutual attention mechanism of cross-attention\nand multi-scale aggregation of 3D tokens. Extensive experimental results on\nthree public 3D medical segmentation datasets show that TMA-TransBTS achieves\nhigher averaged segmentation results than previous state-of-the-art CNN-based\n3D methods and CNN-Transform hybrid 3D methods for the segmentation of 3D\nmulti-modality brain tumors.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09265", "pdf": "https://arxiv.org/pdf/2504.09265", "abs": "https://arxiv.org/abs/2504.09265", "authors": ["Lei Kang", "Jia Li", "Mi Tian", "Hua Huang"], "title": "Mixture of Group Experts for Learning Invariant Representations", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Sparsely activated Mixture-of-Experts (MoE) models effectively increase the\nnumber of parameters while maintaining consistent computational costs per\ntoken. However, vanilla MoE models often suffer from limited diversity and\nspecialization among experts, constraining their performance and scalability,\nespecially as the number of experts increases. In this paper, we present a\nnovel perspective on vanilla MoE with top-$k$ routing inspired by sparse\nrepresentation. This allows us to bridge established theoretical insights from\nsparse representation into MoE models. Building on this foundation, we propose\na group sparse regularization approach for the input of top-$k$ routing, termed\nMixture of Group Experts (MoGE). MoGE indirectly regularizes experts by\nimposing structural constraints on the routing inputs, while preserving the\noriginal MoE architecture. Furthermore, we organize the routing input into a 2D\ntopographic map, spatially grouping neighboring elements. This structure\nenables MoGE to capture representations invariant to minor transformations,\nthereby significantly enhancing expert diversity and specialization.\nComprehensive evaluations across various Transformer models for image\nclassification and language modeling tasks demonstrate that MoGE substantially\noutperforms its MoE counterpart, with minimal additional memory and computation\noverhead. Our approach provides a simple yet effective solution to scale the\nnumber of experts and reduce redundancy among them. The source code is included\nin the supplementary material and will be publicly released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09352", "pdf": "https://arxiv.org/pdf/2504.09352", "abs": "https://arxiv.org/abs/2504.09352", "authors": ["Iason Chaimalas", "Arnas Vyšniauskas", "Gabriel Brostow"], "title": "Explorer: Robust Collection of Interactable GUI Elements", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "19 pages, 17 figures", "summary": "Automation of existing Graphical User Interfaces (GUIs) is important but hard\nto achieve. Upstream of making the GUI user-accessible or somehow scriptable,\neven the data-collection to understand the original interface poses significant\nchallenges. For example, large quantities of general UI data seem helpful for\ntraining general machine learning (ML) models, but accessibility for each\nperson can hinge on the ML's precision on a specific app. We therefore take the\nperspective that a given user needs confidence, that the relevant UI elements\nare being detected correctly throughout one app or digital environment. We\nmostly assume that the target application is known in advance, so that data\ncollection and ML-training can be personalized for the test-time target domain.\nThe proposed Explorer system focuses on detecting on-screen buttons and\ntext-entry fields, i.e. interactables, where the training process has access to\na live version of the application. The live application can run on almost any\npopular platform except iOS phones, and the collection is especially\nstreamlined for Android phones or for desktop Chrome browsers. Explorer also\nenables the recording of interactive user sessions, and subsequent mapping of\nhow such sessions overlap and sometimes loop back to similar states. We show\nhow having such a map enables a kind of path planning through the GUI, letting\na user issue audio commands to get to their destination. Critically, we are\nreleasing our code for Explorer openly at https://github.com/varnelis/Explorer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09975", "pdf": "https://arxiv.org/pdf/2504.09975", "abs": "https://arxiv.org/abs/2504.09975", "authors": ["Si-Tong Wei", "Rui-Huan Wang", "Chuan-Zhi Zhou", "Baoquan Chen", "Peng-Shuai Wang"], "title": "OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape Generation", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025", "summary": "Autoregressive models have achieved remarkable success across various\ndomains, yet their performance in 3D shape generation lags significantly behind\nthat of diffusion models. In this paper, we introduce OctGPT, a novel\nmultiscale autoregressive model for 3D shape generation that dramatically\nimproves the efficiency and performance of prior 3D autoregressive approaches,\nwhile rivaling or surpassing state-of-the-art diffusion models. Our method\nemploys a serialized octree representation to efficiently capture the\nhierarchical and spatial structures of 3D shapes. Coarse geometry is encoded\nvia octree structures, while fine-grained details are represented by binary\ntokens generated using a vector quantized variational autoencoder (VQVAE),\ntransforming 3D shapes into compact \\emph{multiscale binary sequences} suitable\nfor autoregressive prediction. To address the computational challenges of\nhandling long sequences, we incorporate octree-based transformers enhanced with\n3D rotary positional encodings, scale-specific embeddings, and token-parallel\ngeneration schemes. These innovations reduce training time by 13 folds and\ngeneration time by 69 folds, enabling the efficient training of high-resolution\n3D shapes, e.g.,$1024^3$, on just four NVIDIA 4090 GPUs only within days.\nOctGPT showcases exceptional versatility across various tasks, including text-,\nsketch-, and image-conditioned generation, as well as scene-level synthesis\ninvolving multiple objects. Extensive experiments demonstrate that OctGPT\naccelerates convergence and improves generation quality over prior\nautoregressive methods, offering a new paradigm for high-quality, scalable 3D\ncontent creation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10025", "pdf": "https://arxiv.org/pdf/2504.10025", "abs": "https://arxiv.org/abs/2504.10025", "authors": ["Uyen Phan", "Ozer Can Devecioglu", "Serkan Kiranyaz", "Moncef Gabbouj"], "title": "Progressive Transfer Learning for Multi-Pass Fundus Image Restoration", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "13 pages, 12 figures including appendix", "summary": "Diabetic retinopathy is a leading cause of vision impairment, making its\nearly diagnosis through fundus imaging critical for effective treatment\nplanning. However, the presence of poor quality fundus images caused by factors\nsuch as inadequate illumination, noise, blurring and other motion artifacts\nyields a significant challenge for accurate DR screening. In this study, we\npropose progressive transfer learning for multi pass restoration to iteratively\nenhance the quality of degraded fundus images, ensuring more reliable DR\nscreening. Unlike previous methods that often focus on a single pass\nrestoration, multi pass restoration via PTL can achieve a superior blind\nrestoration performance that can even improve most of the good quality fundus\nimages in the dataset. Initially, a Cycle GAN model is trained to restore low\nquality images, followed by PTL induced restoration passes over the latest\nrestored outputs to improve overall quality in each pass. The proposed method\ncan learn blind restoration without requiring any paired data while surpassing\nits limitations by leveraging progressive learning and fine tuning strategies\nto minimize distortions and preserve critical retinal features. To evaluate\nPTL's effectiveness on multi pass restoration, we conducted experiments on\nDeepDRiD, a large scale fundus imaging dataset specifically curated for\ndiabetic retinopathy detection. Our result demonstrates state of the art\nperformance, showcasing PTL's potential as a superior approach to iterative\nimage quality restoration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10281", "pdf": "https://arxiv.org/pdf/2504.10281", "abs": "https://arxiv.org/abs/2504.10281", "authors": ["Jingyun Yang", "Ruoyan Avery Yin", "Chi Jiang", "Yuepeng Hu", "Xiaokai Zhu", "Xingjian Hu", "Sutharsika Kumar", "Xiao Wang", "Xiaohua Zhai", "Keran Rong", "Yunyue Zhu", "Tianyi Zhang", "Zongyou Yin", "Jing Kong", "Neil Zhenqiang Gong", "Zhichu Ren", "Haozhe Wang"], "title": "Zero-shot Autonomous Microscopy for Scalable and Intelligent Characterization of 2D Materials", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "cs.AI", "cs.CV", "cs.LG"], "comment": "13 pages, 4 figures", "summary": "Characterization of atomic-scale materials traditionally requires human\nexperts with months to years of specialized training. Even for trained human\noperators, accurate and reliable characterization remains challenging when\nexamining newly discovered materials such as two-dimensional (2D) structures.\nThis bottleneck drives demand for fully autonomous experimentation systems\ncapable of comprehending research objectives without requiring large training\ndatasets. In this work, we present ATOMIC (Autonomous Technology for Optical\nMicroscopy & Intelligent Characterization), an end-to-end framework that\nintegrates foundation models to enable fully autonomous, zero-shot\ncharacterization of 2D materials. Our system integrates the vision foundation\nmodel (i.e., Segment Anything Model), large language models (i.e., ChatGPT),\nunsupervised clustering, and topological analysis to automate microscope\ncontrol, sample scanning, image segmentation, and intelligent analysis through\nprompt engineering, eliminating the need for additional training. When\nanalyzing typical MoS2 samples, our approach achieves 99.7% segmentation\naccuracy for single layer identification, which is equivalent to that of human\nexperts. In addition, the integrated model is able to detect grain boundary\nslits that are challenging to identify with human eyes. Furthermore, the system\nretains robust accuracy despite variable conditions including defocus, color\ntemperature fluctuations, and exposure variations. It is applicable to a broad\nspectrum of common 2D materials-including graphene, MoS2, WSe2, SnSe-regardless\nof whether they were fabricated via chemical vapor deposition or mechanical\nexfoliation. This work represents the implementation of foundation models to\nachieve autonomous analysis, establishing a scalable and data-efficient\ncharacterization paradigm that fundamentally transforms the approach to\nnanoscale materials research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08959", "pdf": "https://arxiv.org/pdf/2504.08959", "abs": "https://arxiv.org/abs/2504.08959", "authors": ["Yilin Wang", "Chuan Guo", "Yuxuan Mu", "Muhammad Gohar Javed", "Xinxin Zuo", "Juwei Lu", "Hai Jiang", "Li Cheng"], "title": "MotionDreamer: One-to-Many Motion Synthesis with Localized Generative Masked Transformer", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "ICLR 2025 acceptance", "summary": "Generative masked transformers have demonstrated remarkable success across\nvarious content generation tasks, primarily due to their ability to effectively\nmodel large-scale dataset distributions with high consistency. However, in the\nanimation domain, large datasets are not always available. Applying generative\nmasked modeling to generate diverse instances from a single MoCap reference may\nlead to overfitting, a challenge that remains unexplored. In this work, we\npresent MotionDreamer, a localized masked modeling paradigm designed to learn\ninternal motion patterns from a given motion with arbitrary topology and\nduration. By embedding the given motion into quantized tokens with a novel\ndistribution regularization method, MotionDreamer constructs a robust and\ninformative codebook for local motion patterns. Moreover, a sliding window\nlocal attention is introduced in our masked transformer, enabling the\ngeneration of natural yet diverse animations that closely resemble the\nreference motion patterns. As demonstrated through comprehensive experiments,\nMotionDreamer outperforms the state-of-the-art methods that are typically GAN\nor Diffusion-based in both faithfulness and diversity. Thanks to the\nconsistency and robustness of the quantization-based approach, MotionDreamer\ncan also effectively perform downstream tasks such as temporal motion editing,\n\\textcolor{update}{crowd animation}, and beat-aligned dance generation, all\nusing a single reference motion. Visit our project page:\nhttps://motiondreamer.github.io/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08966", "pdf": "https://arxiv.org/pdf/2504.08966", "abs": "https://arxiv.org/abs/2504.08966", "authors": ["Mohamed Dhouib", "Davide Buscaldi", "Sonia Vanier", "Aymen Shabou"], "title": "PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Visual Language Models require substantial computational resources for\ninference due to the additional input tokens needed to represent visual\ninformation. However, these visual tokens often contain redundant and\nunimportant information, resulting in an unnecessarily high number of tokens.\nTo address this, we introduce PACT, a method that reduces inference time and\nmemory usage by pruning irrelevant tokens and merging visually redundant ones\nat an early layer of the language model. Our approach uses a novel importance\nmetric to identify unimportant tokens without relying on attention scores,\nmaking it compatible with FlashAttention. We also propose a novel clustering\nalgorithm, called Distance Bounded Density Peak Clustering, which efficiently\nclusters visual tokens while constraining the distances between elements within\na cluster by a predefined threshold. We demonstrate the effectiveness of PACT\nthrough extensive experiments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09048", "pdf": "https://arxiv.org/pdf/2504.09048", "abs": "https://arxiv.org/abs/2504.09048", "authors": ["Yongchang Wu", "Zipeng Qi", "Zhenwei Shi", "Zhengxia Zou"], "title": "BlockGaussian: Efficient Large-Scale Scene NovelView Synthesis via Adaptive Block-Based Gaussian Splatting", "categories": ["cs.CV"], "comment": "https://github.com/SunshineWYC/BlockGaussian", "summary": "The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated\nremarkable potential in novel view synthesis tasks. The divide-and-conquer\nparadigm has enabled large-scale scene reconstruction, but significant\nchallenges remain in scene partitioning, optimization, and merging processes.\nThis paper introduces BlockGaussian, a novel framework incorporating a\ncontent-aware scene partition strategy and visibility-aware block optimization\nto achieve efficient and high-quality large-scale scene reconstruction.\nSpecifically, our approach considers the content-complexity variation across\ndifferent regions and balances computational load during scene partitioning,\nenabling efficient scene reconstruction. To tackle the supervision mismatch\nissue during independent block optimization, we introduce auxiliary points\nduring individual block optimization to align the ground-truth supervision,\nwhich enhances the reconstruction quality. Furthermore, we propose a\npseudo-view geometry constraint that effectively mitigates rendering\ndegradation caused by airspace floaters during block merging. Extensive\nexperiments on large-scale scenes demonstrate that our approach achieves\nstate-of-the-art performance in both reconstruction efficiency and rendering\nquality, with a 5x speedup in optimization and an average PSNR improvement of\n1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces\ncomputational requirements, enabling large-scale scene reconstruction on a\nsingle 24GB VRAM device. The project page is available at\nhttps://github.com/SunshineWYC/BlockGaussian", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09106", "pdf": "https://arxiv.org/pdf/2504.09106", "abs": "https://arxiv.org/abs/2504.09106", "authors": ["Yonghao Huang", "Leiting Chen", "Chuan Zhou"], "title": "Multi-modal and Multi-view Fundus Image Fusion for Retinopathy Diagnosis via Multi-scale Cross-attention and Shifted Window Self-attention", "categories": ["cs.CV"], "comment": null, "summary": "The joint interpretation of multi-modal and multi-view fundus images is\ncritical for retinopathy prevention, as different views can show the complete\n3D eyeball field and different modalities can provide complementary lesion\nareas. Compared with single images, the sequence relationships in multi-modal\nand multi-view fundus images contain long-range dependencies in lesion\nfeatures. By modeling the long-range dependencies in these sequences, lesion\nareas can be more comprehensively mined, and modality-specific lesions can be\ndetected. To learn the long-range dependency relationship and fuse\ncomplementary multi-scale lesion features between different fundus modalities,\nwe design a multi-modal fundus image fusion method based on multi-scale\ncross-attention, which solves the static receptive field problem in previous\nmulti-modal medical fusion methods based on attention. To capture multi-view\nrelative positional relationships between different views and fuse\ncomprehensive lesion features between different views, we design a multi-view\nfundus image fusion method based on shifted window self-attention, which also\nsolves the computational complexity of the multi-view fundus fusion method\nbased on self-attention is quadratic to the size and number of multi-view\nfundus images. Finally, we design a multi-task retinopathy diagnosis framework\nto help ophthalmologists reduce workload and improve diagnostic accuracy by\ncombining the proposed two fusion methods. The experimental results of\nretinopathy classification and report generation tasks indicate our method's\npotential to improve the efficiency and reliability of retinopathy diagnosis in\nclinical practice, achieving a classification accuracy of 82.53\\% and a report\ngeneration BlEU-1 of 0.543.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09129", "pdf": "https://arxiv.org/pdf/2504.09129", "abs": "https://arxiv.org/abs/2504.09129", "authors": ["Jizong Peng", "Tze Ho Elden Tse", "Kai Xu", "Wenchao Gao", "Angela Yao"], "title": "A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a powerful reconstruction technique, but it\nneeds to be initialized from accurate camera poses and high-fidelity point\nclouds. Typically, the initialization is taken from Structure-from-Motion (SfM)\nalgorithms; however, SfM is time-consuming and restricts the application of\n3DGS in real-world scenarios and large-scale scene reconstruction. We introduce\na constrained optimization method for simultaneous camera pose estimation and\n3D reconstruction that does not require SfM support. Core to our approach is\ndecomposing a camera pose into a sequence of camera-to-(device-)center and\n(device-)center-to-world optimizations. To facilitate, we propose two\noptimization constraints conditioned to the sensitivity of each parameter group\nand restricts each parameter's search space. In addition, as we learn the scene\ngeometry directly from the noisy point clouds, we propose geometric constraints\nto improve the reconstruction quality. Experiments demonstrate that the\nproposed method significantly outperforms the existing (multi-modal) 3DGS\nbaseline and methods supplemented by COLMAP on both our collected dataset and\ntwo public benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09155", "pdf": "https://arxiv.org/pdf/2504.09155", "abs": "https://arxiv.org/abs/2504.09155", "authors": ["Zhanzhou Feng", "Shiliang Zhang"], "title": "Evolved Hierarchical Masking for Self-Supervised Learning", "categories": ["cs.CV"], "comment": null, "summary": "Existing Masked Image Modeling methods apply fixed mask patterns to guide the\nself-supervised training. As those mask patterns resort to different criteria\nto depict image contents, sticking to a fixed pattern leads to a limited vision\ncues modeling capability.This paper introduces an evolved hierarchical masking\nmethod to pursue general visual cues modeling in self-supervised learning. The\nproposed method leverages the vision model being trained to parse the input\nvisual cues into a hierarchy structure, which is hence adopted to generate\nmasks accordingly. The accuracy of hierarchy is on par with the capability of\nthe model being trained, leading to evolved mask patterns at different training\nstages. Initially, generated masks focus on low-level visual cues to grasp\nbasic textures, then gradually evolve to depict higher-level cues to reinforce\nthe learning of more complicated object semantics and contexts. Our method does\nnot require extra pre-trained models or annotations and ensures training\nefficiency by evolving the training difficulty. We conduct extensive\nexperiments on seven downstream tasks including partial-duplicate image\nretrieval relying on low-level details, as well as image classification and\nsemantic segmentation that require semantic parsing capability. Experimental\nresults demonstrate that it substantially boosts performance across these\ntasks. For instance, it surpasses the recent MAE by 1.1\\% in imageNet-1K\nclassification and 1.4\\% in ADE20K segmentation with the same training epochs.\nWe also align the proposed method with the current research focus on LLMs. The\nproposed approach bridges the gap with large-scale pre-training on semantic\ndemanding tasks and enhances intricate detail perception in tasks requiring\nlow-level feature recognition.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "criteria"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09184", "pdf": "https://arxiv.org/pdf/2504.09184", "abs": "https://arxiv.org/abs/2504.09184", "authors": ["Lennart Finke", "Thomas Dooms", "Mat Allen", "Juan Diego Rodriguez", "Noa Nabeshima", "Dan Braun"], "title": "Parameterized Synthetic Text Generation with SimpleStories", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present SimpleStories, a large synthetic story dataset in simple language,\nconsisting of 2 million stories each in English and Japanese. Our method\nemploys parametrization of prompts with features at multiple levels of\nabstraction, allowing for systematic control over story characteristics to\nensure broad syntactic and semantic diversity. Building on and addressing\nlimitations in the TinyStories dataset, our approach demonstrates that\nsimplicity and variety can be achieved simultaneously in synthetic text\ngeneration at scale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09203", "pdf": "https://arxiv.org/pdf/2504.09203", "abs": "https://arxiv.org/abs/2504.09203", "authors": ["Saikat Dutta", "Akhil Vasim", "Siddhant Gole", "Hamid Rezatofighi", "Biplab Banerjee"], "title": "AerOSeg: Harnessing SAM for Open-Vocabulary Segmentation in Remote Sensing Images", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at EarthVision workshop, CVPR 2025", "summary": "Image segmentation beyond predefined categories is a key challenge in remote\nsensing, where novel and unseen classes often emerge during inference.\nOpen-vocabulary image Segmentation addresses these generalization issues in\ntraditional supervised segmentation models while reducing reliance on extensive\nper-pixel annotations, which are both expensive and labor-intensive to obtain.\nMost Open-Vocabulary Segmentation (OVS) methods are designed for natural images\nbut struggle with remote sensing data due to scale variations, orientation\nchanges, and complex scene compositions. This necessitates the development of\nOVS approaches specifically tailored for remote sensing. In this context, we\npropose AerOSeg, a novel OVS approach for remote sensing data. First, we\ncompute robust image-text correlation features using multiple rotated versions\nof the input image and domain-specific prompts. These features are then refined\nthrough spatial and class refinement blocks. Inspired by the success of the\nSegment Anything Model (SAM) in diverse domains, we leverage SAM features to\nguide the spatial refinement of correlation features. Additionally, we\nintroduce a semantic back-projection module and loss to ensure the seamless\npropagation of SAM's semantic information throughout the segmentation pipeline.\nFinally, we enhance the refined correlation features using a multi-scale\nattention-aware decoder to produce the final segmentation map. We validate our\nSAM-guided Open-Vocabulary Remote Sensing Segmentation model on three benchmark\nremote sensing datasets: iSAID, DLRSD, and OpenEarthMap. Our model outperforms\nstate-of-the-art open-vocabulary segmentation methods, achieving an average\nimprovement of 2.54 h-mIoU.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09215", "pdf": "https://arxiv.org/pdf/2504.09215", "abs": "https://arxiv.org/abs/2504.09215", "authors": ["Zhicheng Zhang", "Hao Tang", "Jinhui Tang"], "title": "Multi-scale Activation, Refinement, and Aggregation: Exploring Diverse Cues for Fine-Grained Bird Recognition", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by AAAI2025", "summary": "Given the critical role of birds in ecosystems, Fine-Grained Bird Recognition\n(FGBR) has gained increasing attention, particularly in distinguishing birds\nwithin similar subcategories. Although Vision Transformer (ViT)-based methods\noften outperform Convolutional Neural Network (CNN)-based methods in FGBR,\nrecent studies reveal that the limited receptive field of plain ViT model\nhinders representational richness and makes them vulnerable to scale variance.\nThus, enhancing the multi-scale capabilities of existing ViT-based models to\novercome this bottleneck in FGBR is a worthwhile pursuit. In this paper, we\npropose a novel framework for FGBR, namely Multi-scale Diverse Cues Modeling\n(MDCM), which explores diverse cues at different scales across various stages\nof a multi-scale Vision Transformer (MS-ViT) in an\n\"Activation-Selection-Aggregation\" paradigm. Specifically, we first propose a\nmulti-scale cue activation module to ensure the discriminative cues learned at\ndifferent stage are mutually different. Subsequently, a multi-scale token\nselection mechanism is proposed to remove redundant noise and highlight\ndiscriminative, scale-specific cues at each stage. Finally, the selected tokens\nfrom each stage are independently utilized for bird recognition, and the\nrecognition results from multiple stages are adaptively fused through a\nmulti-scale dynamic aggregation mechanism for final model decisions. Both\nqualitative and quantitative results demonstrate the effectiveness of our\nproposed MDCM, which outperforms CNN- and ViT-based models on several\nwidely-used FGBR benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09223", "pdf": "https://arxiv.org/pdf/2504.09223", "abs": "https://arxiv.org/abs/2504.09223", "authors": ["Wenjin Ke", "Zhe Li", "Dong Li", "Lu Tian", "Emad Barsoum"], "title": "DL-QAT: Weight-Decomposed Low-Rank Quantization-Aware Training for Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Improving the efficiency of inference in Large Language Models (LLMs) is a\ncritical area of research. Post-training Quantization (PTQ) is a popular\ntechnique, but it often faces challenges at low-bit levels, particularly in\ndownstream tasks. Quantization-aware Training (QAT) can alleviate this problem,\nbut it requires significantly more computational resources. To tackle this, we\nintroduced Weight-Decomposed Low-Rank Quantization-Aware Training (DL-QAT),\nwhich merges the advantages of QAT while training only less than 1% of the\ntotal parameters. Specifically, we introduce a group-specific quantization\nmagnitude to adjust the overall scale of each quantization group. Within each\nquantization group, we use LoRA matrices to update the weight size and\ndirection in the quantization space. We validated the effectiveness of our\nmethod on the LLaMA and LLaMA2 model families. The results show significant\nimprovements over our baseline method across different quantization\ngranularities. For instance, for LLaMA-7B, our approach outperforms the\nprevious state-of-the-art method by 4.2% in MMLU on 3-bit LLaMA-7B model.\nAdditionally, our quantization results on pre-trained models also surpass\nprevious QAT methods, demonstrating the superior performance and efficiency of\nour approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09389", "pdf": "https://arxiv.org/pdf/2504.09389", "abs": "https://arxiv.org/abs/2504.09389", "authors": ["Vishakh Padmakumar", "Chen Yueh-Han", "Jane Pan", "Valerie Chen", "He He"], "title": "Beyond Memorization: Mapping the Originality-Quality Frontier of Language Models", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly used for ideation and\nscientific discovery, it is important to evaluate their ability to generate\nnovel output. Prior work evaluates novelty as the originality with respect to\ntraining data, but original outputs can be low quality. In contrast, non-expert\njudges may favor high-quality but memorized outputs, limiting the reliability\nof human preference as a metric. We propose a new novelty metric for LLM\ngenerations that balances originality and quality -- the harmonic mean of the\nfraction of \\ngrams unseen during training and a task-specific quality score.\nWe evaluate the novelty of generations from two families of open-data models\n(OLMo and Pythia) on three creative tasks: story completion, poetry writing,\nand creative tool use. We find that LLM generated text is less novel than human\nwritten text. To elicit more novel outputs, we experiment with various\ninference-time methods, which reveals a trade-off between originality and\nquality. While these methods can boost novelty, they do so by increasing\noriginality at the expense of quality. In contrast, increasing model size or\napplying post-training reliably shifts the Pareto frontier, highlighting that\nstarting with a stronger base model is a more effective way to improve novelty.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "reliability"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09255", "pdf": "https://arxiv.org/pdf/2504.09255", "abs": "https://arxiv.org/abs/2504.09255", "authors": ["Sijing Wu", "Yunhao Li", "Ziwen Xu", "Yixuan Gao", "Huiyu Duan", "Wei Sun", "Guangtao Zhai"], "title": "FVQ: A Large-Scale Dataset and A LMM-based Method for Face Video Quality Assessment", "categories": ["cs.CV"], "comment": null, "summary": "Face video quality assessment (FVQA) deserves to be explored in addition to\ngeneral video quality assessment (VQA), as face videos are the primary content\non social media platforms and human visual system (HVS) is particularly\nsensitive to human faces. However, FVQA is rarely explored due to the lack of\nlarge-scale FVQA datasets. To fill this gap, we present the first large-scale\nin-the-wild FVQA dataset, FVQ-20K, which contains 20,000 in-the-wild face\nvideos together with corresponding mean opinion score (MOS) annotations. Along\nwith the FVQ-20K dataset, we further propose a specialized FVQA method named\nFVQ-Rater to achieve human-like rating and scoring for face video, which is the\nfirst attempt to explore the potential of large multimodal models (LMMs) for\nthe FVQA task. Concretely, we elaborately extract multi-dimensional features\nincluding spatial features, temporal features, and face-specific features\n(i.e., portrait features and face embeddings) to provide comprehensive visual\ninformation, and take advantage of the LoRA-based instruction tuning technique\nto achieve quality-specific fine-tuning, which shows superior performance on\nboth FVQ-20K and CFVQA datasets. Extensive experiments and comprehensive\nanalysis demonstrate the significant potential of the FVQ-20K dataset and\nFVQ-Rater method in promoting the development of FVQA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "multi-dimensional"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09258", "pdf": "https://arxiv.org/pdf/2504.09258", "abs": "https://arxiv.org/abs/2504.09258", "authors": ["Jianyu Wu", "Hao Yang", "Xinhua Zeng", "Guibing He", "Zhiyu Chen", "Zihui Li", "Xiaochuan Zhang", "Yangyang Ma", "Run Fang", "Yang Liu"], "title": "PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for Pathology Visual-Language Tasks", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The diagnosis of pathological images is often limited by expert availability\nand regional disparities, highlighting the importance of automated diagnosis\nusing Vision-Language Models (VLMs). Traditional multimodal models typically\nemphasize outcomes over the reasoning process, compromising the reliability of\nclinical decisions. To address the weak reasoning abilities and lack of\nsupervised processes in pathological VLMs, we have innovatively proposed\nPathVLM-R1, a visual language model designed specifically for pathological\nimages. We have based our model on Qwen2.5-VL-7B-Instruct and enhanced its\nperformance for pathological tasks through meticulously designed post-training\nstrategies. Firstly, we conduct supervised fine-tuning guided by pathological\ndata to imbue the model with foundational pathological knowledge, forming a new\npathological base model. Subsequently, we introduce Group Relative Policy\nOptimization (GRPO) and propose a dual reward-driven reinforcement learning\noptimization, ensuring strict constraint on logical supervision of the\nreasoning process and accuracy of results via cross-modal process reward and\noutcome accuracy reward. In the pathological image question-answering tasks,\nthe testing results of PathVLM-R1 demonstrate a 14% improvement in accuracy\ncompared to baseline methods, and it demonstrated superior performance compared\nto the Qwen2.5-VL-32B version despite having a significantly smaller parameter\nsize. Furthermore, in out-domain data evaluation involving four medical imaging\nmodalities: Computed Tomography (CT), dermoscopy, fundus photography, and\nOptical Coherence Tomography (OCT) images: PathVLM-R1's transfer performance\nimproved by an average of 17.3% compared to traditional SFT methods. These\nresults clearly indicate that PathVLM-R1 not only enhances accuracy but also\npossesses broad applicability and expansion potential.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09398", "pdf": "https://arxiv.org/pdf/2504.09398", "abs": "https://arxiv.org/abs/2504.09398", "authors": ["Gaurav Kumar", "Murali Mohana Krishna Dandu"], "title": "Composable NLP Workflows for BERT-based Ranking and QA System", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures, 6 tables", "summary": "There has been a lot of progress towards building NLP models that scale to\nmultiple tasks. However, real-world systems contain multiple components and it\nis tedious to handle cross-task interaction with varying levels of text\ngranularity. In this work, we built an end-to-end Ranking and\nQuestion-Answering (QA) system using Forte, a toolkit that makes composable NLP\npipelines. We utilized state-of-the-art deep learning models such as BERT,\nRoBERTa in our pipeline, evaluated the performance on MS-MARCO and Covid-19\ndatasets using metrics such as BLUE, MRR, F1 and compared the results of\nranking and QA systems with their corresponding benchmark results. The modular\nnature of our pipeline and low latency of reranker makes it easy to build\ncomplex NLP applications easily.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09402", "pdf": "https://arxiv.org/pdf/2504.09402", "abs": "https://arxiv.org/abs/2504.09402", "authors": ["Feijiang Han", "Licheng Guo", "Hengtao Cui", "Zhiyuan Lyu"], "title": "Question Tokens Deserve More Attention: Enhancing Large Language Models without Training through Step-by-Step Reading and Question Attention Recalibration", "categories": ["cs.CL", "cs.AI"], "comment": "CIS 5300", "summary": "Large Language Models (LLMs) often struggle with tasks that require a deep\nunderstanding of complex questions, especially when faced with long-range\ndependencies or multi-step reasoning. This work investigates the limitations of\ncurrent LLMs in question comprehension and identifies three insights: (1)\nrepeating question tokens improves comprehension by increasing attention to\nquestion regions, (2) increased backward dependencies negatively affect\nperformance due to unidirectional attentional constraints, and (3)\nrecalibrating attentional mechanisms to prioritize question-relevant regions\nimproves performance.\n  Based on these findings, we first propose a family of prompt-based strategies\n- Step-by-Step Reading (SSR), SSR+, and SSR++ - that guide LLMs to\nincrementally process question tokens and align their reasoning with the input\nstructure. These methods significantly improve performance, with SSR++\nachieving state-of-the-art results on several benchmarks: 96.66% on GSM8K,\n94.61% on ASDiv, and 76.28% on AQuA. Second, we introduce a training-free\nattention recalibration mechanism that dynamically adjusts attention\ndistributions during inference to emphasize question-relevant regions. This\nmethod improves the accuracy of LLaMA 3.1-8B on AQuA by 5.17% without changing\nmodel parameters or input prompts.\n  Taken together, our results highlight the importance of structured prompt\ndesign and attention optimization in improving LLM comprehension, providing\nlightweight yet effective tools for improving performance in various NLP tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09291", "pdf": "https://arxiv.org/pdf/2504.09291", "abs": "https://arxiv.org/abs/2504.09291", "authors": ["Jiaying Qian", "Ziheng Jia", "Zicheng Zhang", "Zeyu Zhang", "Guangtao Zhai", "Xiongkuo Min"], "title": "Towards Explainable Partial-AIGC Image Quality Assessment", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The rapid advancement of AI-driven visual generation technologies has\ncatalyzed significant breakthroughs in image manipulation, particularly in\nachieving photorealistic localized editing effects on natural scene images\n(NSIs). Despite extensive research on image quality assessment (IQA) for\nAI-generated images (AGIs), most studies focus on fully AI-generated outputs\n(e.g., text-to-image generation), leaving the quality assessment of\npartial-AIGC images (PAIs)-images with localized AI-driven edits an almost\nunprecedented field. Motivated by this gap, we construct the first large-scale\nPAI dataset towards explainable partial-AIGC image quality assessment (EPAIQA),\nthe EPAIQA-15K, which includes 15K images with localized AI manipulation in\ndifferent regions and over 300K multi-dimensional human ratings. Based on this,\nwe leverage large multi-modal models (LMMs) and propose a three-stage model\ntraining paradigm. This paradigm progressively trains the LMM for editing\nregion grounding, quantitative quality scoring, and quality explanation.\nFinally, we develop the EPAIQA series models, which possess explainable quality\nfeedback capabilities. Our work represents a pioneering effort in the\nperceptual IQA field for comprehensive PAI quality assessment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "multi-dimensional"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09566", "pdf": "https://arxiv.org/pdf/2504.09566", "abs": "https://arxiv.org/abs/2504.09566", "authors": ["Chenghao Li", "Chaoning Zhang", "Yi Lu", "Jiaquan Zhang", "Qigan Sun", "Xudong Wang", "Jiwei Wei", "Guoqing Wang", "Yang Yang", "Heng Tao Shen"], "title": "Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting enhances the reasoning of large language\nmodels (LLMs) by decomposing problems into sequential steps, mimicking human\nlogic and reducing errors. However, complex tasks with vast solution spaces and\nvague constraints often exceed the capacity of a single reasoning chain.\nInspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic\ngeometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends\nCoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper\nlogical dependencies, enabling more robust and structured problem-solving. MFR\ndecomposes a module into a sequence of free modules with minimal rank,\nproviding a structured analytical approach to complex systems. This method\nintroduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\",\n\"Exactness\" and \"Minimality\", enabling the systematic decomposition of the\noriginal complex problem into logically complete minimal subproblems while\npreserving key problem features and reducing reasoning length. We tested SoT\nacross diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini,\nQwen2.5), achieving inference accuracy that matches or surpasses mainstream\nCoTs standards. Additionally, by aligning the sampling process with algebraic\nconstraints, our approach enhances the scalability of inference time in LLMs,\nensuring both transparent reasoning and high performance. Our code will be\npublicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09426", "pdf": "https://arxiv.org/pdf/2504.09426", "abs": "https://arxiv.org/abs/2504.09426", "authors": ["Shengao Wang", "Arjun Chandra", "Aoming Liu", "Venkatesh Saligrama", "Boqing Gong"], "title": "BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Human infants rapidly develop visual reasoning skills from minimal input,\nsuggesting that developmentally inspired pretraining could significantly\nenhance the efficiency of vision-language models (VLMs). Although recent\nefforts have leveraged infant-inspired datasets like SAYCam, existing\nevaluation benchmarks remain misaligned--they are either too simplistic,\nnarrowly scoped, or tailored for large-scale pretrained models. Additionally,\ntraining exclusively on infant data overlooks the broader, diverse input from\nwhich infants naturally learn. To address these limitations, we propose\nBabyVLM, a novel framework comprising comprehensive in-domain evaluation\nbenchmarks and a synthetic training dataset created via child-directed\ntransformations of existing datasets. We demonstrate that VLMs trained with our\nsynthetic dataset achieve superior performance on BabyVLM tasks compared to\nmodels trained solely on SAYCam or general-purpose data of the SAYCam size.\nBabyVLM thus provides a robust, developmentally aligned evaluation tool and\nillustrates how compact models trained on carefully curated data can generalize\neffectively, opening pathways toward data-efficient vision-language learning\nparadigms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09665", "pdf": "https://arxiv.org/pdf/2504.09665", "abs": "https://arxiv.org/abs/2504.09665", "authors": ["Liqiang Wen", "Guanming Xiong", "Tong Mo", "Bing Li", "Weiping Li", "Wen Zhao"], "title": "CLEAR-KGQA: Clarification-Enhanced Ambiguity Resolution for Knowledge Graph Question Answering", "categories": ["cs.CL"], "comment": "This work has been accepted by the IJCNN 2025 main track", "summary": "This study addresses the challenge of ambiguity in knowledge graph question\nanswering (KGQA). While recent KGQA systems have made significant progress,\nparticularly with the integration of large language models (LLMs), they\ntypically assume user queries are unambiguous, which is an assumption that\nrarely holds in real-world applications. To address these limitations, we\npropose a novel framework that dynamically handles both entity ambiguity (e.g.,\ndistinguishing between entities with similar names) and intent ambiguity (e.g.,\nclarifying different interpretations of user queries) through interactive\nclarification. Our approach employs a Bayesian inference mechanism to quantify\nquery ambiguity and guide LLMs in determining when and how to request\nclarification from users within a multi-turn dialogue framework. We further\ndevelop a two-agent interaction framework where an LLM-based user simulator\nenables iterative refinement of logical forms through simulated user feedback.\nExperimental results on the WebQSP and CWQ dataset demonstrate that our method\nsignificantly improves performance by effectively resolving semantic\nambiguities. Additionally, we contribute a refined dataset of disambiguated\nqueries, derived from interaction histories, to facilitate future research in\nthis direction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue", "question answering"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09687", "pdf": "https://arxiv.org/pdf/2504.09687", "abs": "https://arxiv.org/abs/2504.09687", "authors": ["Salman Faroz"], "title": "Domain-Adaptive Continued Pre-Training of Small Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Continued pre-training of small language models offers a promising path for\ndomain adaptation with limited computational resources. I've investigated this\napproach within educational domains, evaluating it as a resource-efficient\nalternative to training models from scratch. Using a 125M parameter model, I\ndemonstrate significant performance improvements through incremental training\non 400 million tokens, followed by further training to reach 1 billion tokens.\nMy approach includes comprehensive data preprocessing, memory-optimized\ntraining configurations, and benchmark-based evaluation. Results show notable\ngains in knowledge-intensive tasks (MMLU +8.1%) and contextual understanding\n(HellaSwag +7.6%), while revealing educational domain specialization\ntrade-offs. I analyze token efficiency, catastrophic forgetting mitigation\nstrategies, and scaling patterns. My findings suggest that thoughtful\npreprocessing and training methodologies enable meaningful improvements in\nlanguage model capabilities even with constrained computational resources,\nopening pathways for domain-specific adaptation of smaller language models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09696", "pdf": "https://arxiv.org/pdf/2504.09696", "abs": "https://arxiv.org/abs/2504.09696", "authors": ["Jixiao Zhang", "Chunsheng Zuo"], "title": "GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in R1-like reasoning models leveraging Group Relative Policy\nOptimization (GRPO) have significantly improved the performance of language\nmodels on mathematical reasoning tasks. However, current GRPO implementations\nencounter critical challenges, including reward sparsity due to binary accuracy\nmetrics, limited incentives for conciseness, and insufficient focus on complex\nreasoning tasks. To address these issues, we propose GRPO-LEAD, a suite of\nnovel enhancements tailored for mathematical reasoning. Specifically, GRPO-LEAD\nintroduces (1) a length-dependent accuracy reward to encourage concise and\nprecise solutions, (2) an explicit penalty mechanism for incorrect answers to\nsharpen decision boundaries, and (3) a difficulty-aware advantage reweighting\nstrategy that amplifies learning signals for challenging problems. Furthermore,\nwe systematically examine the impact of model scale and supervised fine-tuning\n(SFT) strategies, demonstrating that larger-scale base models and carefully\ncurated datasets significantly enhance reinforcement learning effectiveness.\nExtensive empirical evaluations and ablation studies confirm that GRPO-LEAD\nsubstantially mitigates previous shortcomings, resulting in language models\nthat produce more concise, accurate, and robust reasoning across diverse\nmathematical tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09448", "pdf": "https://arxiv.org/pdf/2504.09448", "abs": "https://arxiv.org/abs/2504.09448", "authors": ["Lin Zhu", "Yifeng Yang", "Zichao Nie", "Yuan Gao", "Jiarui Li", "Qinying Gu", "Xinbing Wang", "Chenghu Zhou", "Nanyang Ye"], "title": "InfoBound: A Provable Information-Bounds Inspired Framework for Both OoD Generalization and OoD Detection", "categories": ["cs.CV", "cs.LG"], "comment": "Under Review", "summary": "In real-world scenarios, distribution shifts give rise to the importance of\ntwo problems: out-of-distribution (OoD) generalization, which focuses on\nmodels' generalization ability against covariate shifts (i.e., the changes of\nenvironments), and OoD detection, which aims to be aware of semantic shifts\n(i.e., test-time unseen classes). Real-world testing environments often involve\na combination of both covariate and semantic shifts. While numerous methods\nhave been proposed to address these critical issues, only a few works tackled\nthem simultaneously. Moreover, prior works often improve one problem but\nsacrifice the other. To overcome these limitations, we delve into boosting OoD\ndetection and OoD generalization from the perspective of information theory,\nwhich can be easily applied to existing models and different tasks. Building\nupon the theoretical bounds for mutual information and conditional entropy, we\nprovide a unified approach, composed of Mutual Information Minimization\n(MI-Min) and Conditional Entropy Maximizing (CE-Max). Extensive experiments and\ncomprehensive evaluations on multi-label image classification and object\ndetection have demonstrated the superiority of our method. It successfully\nmitigates trade-offs between the two challenges compared to competitive\nbaselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09802", "pdf": "https://arxiv.org/pdf/2504.09802", "abs": "https://arxiv.org/abs/2504.09802", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Training Small Reasoning LLMs with Cognitive Preference Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The reasoning capabilities of large language models (LLMs), such as OpenAI's\no1 and DeepSeek-R1, have seen substantial advancements through deep thinking.\nHowever, these enhancements come with significant resource demands,\nunderscoring the need to explore strategies to train effective reasoning LLMs\nwith far fewer parameters. A critical challenge is that smaller models have\ndifferent capacities and cognitive trajectories than their larger counterparts.\nHence, direct distillation of chain-of-thought (CoT) results from large LLMs to\nsmaller ones can be sometimes ineffective and requires a huge amount of\nannotated data. In this paper, we introduce a novel framework called\nCritique-Rethink-Verify (CRV), designed for training smaller yet powerful\nreasoning LLMs. Our CRV framework consists of multiple LLM agents, each\nspecializing in unique abilities: (i) critiquing the CoTs according to the\ncognitive capabilities of smaller models, (ii) rethinking and refining these\nCoTs based on the critiques, and (iii) verifying the correctness of the refined\nresults. We further propose the cognitive preference optimization (CogPO)\nalgorithm to enhance the reasoning abilities of smaller models by aligning\nthoughts of these models with their cognitive capacities. Comprehensive\nevaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV\nand CogPO, which outperforms other training methods by a large margin.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09507", "pdf": "https://arxiv.org/pdf/2504.09507", "abs": "https://arxiv.org/abs/2504.09507", "authors": ["Mengjiao Wang", "Junpei Zhang", "Xu Liu", "Yuting Yang", "Mengru Ma"], "title": "FVOS for MOSE Track of 4th PVUW Challenge: 3rd Place Solution", "categories": ["cs.CV"], "comment": "5 pages, 3 figures", "summary": "Video Object Segmentation (VOS) is one of the most fundamental and\nchallenging tasks in computer vision and has a wide range of applications. Most\nexisting methods rely on spatiotemporal memory networks to extract frame-level\nfeatures and have achieved promising results on commonly used datasets.\nHowever, these methods often struggle in more complex real-world scenarios.\nThis paper addresses this issue, aiming to achieve accurate segmentation of\nvideo objects in challenging scenes. We propose fine-tuning VOS (FVOS),\noptimizing existing methods for specific datasets through tailored training.\nAdditionally, we introduce a morphological post-processing strategy to address\nthe issue of excessively large gaps between adjacent objects in single-model\npredictions. Finally, we apply a voting-based fusion method on multi-scale\nsegmentation results to generate the final output. Our approach achieves J&F\nscores of 76.81% and 83.92% during the validation and testing stages,\nrespectively, securing third place overall in the MOSE Track of the 4th PVUW\nchallenge 2025.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09513", "pdf": "https://arxiv.org/pdf/2504.09513", "abs": "https://arxiv.org/abs/2504.09513", "authors": ["Puyu Han", "Jiaju Kang", "Yuhang Pan", "Erting Pan", "Zeyu Zhang", "Qunchao Jin", "Juntao Jiang", "Zhichen Liu", "Luqi Gong"], "title": "DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale pre-trained diffusion models have produced excellent results in\nthe field of conditional image generation. However, restoration of ancient\nmurals, as an important downstream task in this field, poses significant\nchallenges to diffusion model-based restoration methods due to its large\ndefective area and scarce training samples. Conditional restoration tasks are\nmore concerned with whether the restored part meets the aesthetic standards of\nmural restoration in terms of overall style and seam detail, and such metrics\nfor evaluating heuristic image complements are lacking in current research. We\ntherefore propose DiffuMural, a combined Multi-scale convergence and\nCollaborative Diffusion mechanism with ControlNet and cyclic consistency loss\nto optimise the matching between the generated images and the conditional\ncontrol. DiffuMural demonstrates outstanding capabilities in mural restoration,\nleveraging training data from 23 large-scale Dunhuang murals that exhibit\nconsistent visual aesthetics. The model excels in restoring intricate details,\nachieving a coherent overall appearance, and addressing the unique challenges\nposed by incomplete murals lacking factual grounding. Our evaluation framework\nincorporates four key metrics to quantitatively assess incomplete murals:\nfactual accuracy, textural detail, contextual semantics, and holistic visual\ncoherence. Furthermore, we integrate humanistic value assessments to ensure the\nrestored murals retain their cultural and artistic significance. Extensive\nexperiments validate that our method outperforms state-of-the-art (SOTA)\napproaches in both qualitative and quantitative metrics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09528", "pdf": "https://arxiv.org/pdf/2504.09528", "abs": "https://arxiv.org/abs/2504.09528", "authors": ["Xing Zi", "Tengjun Ni", "Xianjing Fan", "Xian Tao", "Jun Li", "Ali Braytee", "Mukesh Prasad"], "title": "AeroLite: Tag-Guided Lightweight Generation of Aerial Image Captions", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Accurate and automated captioning of aerial imagery is crucial for\napplications like environmental monitoring, urban planning, and disaster\nmanagement. However, this task remains challenging due to complex spatial\nsemantics and domain variability. To address these issues, we introduce\n\\textbf{AeroLite}, a lightweight, tag-guided captioning framework designed to\nequip small-scale language models (1--3B parameters) with robust and\ninterpretable captioning capabilities specifically for remote sensing images.\n\\textbf{AeroLite} leverages GPT-4o to generate a large-scale, semantically rich\npseudo-caption dataset by integrating multiple remote sensing benchmarks,\nincluding DLRSD, iSAID, LoveDA, WHU, and RSSCN7. To explicitly capture key\nsemantic elements such as orientation and land-use types, AeroLite employs\nnatural language processing techniques to extract relevant semantic tags. These\ntags are then learned by a dedicated multi-label CLIP encoder, ensuring precise\nsemantic predictions. To effectively fuse visual and semantic information, we\npropose a novel bridging multilayer perceptron (MLP) architecture, aligning\nsemantic tags with visual embeddings while maintaining minimal computational\noverhead. AeroLite's flexible design also enables seamless integration with\nvarious pretrained large language models. We adopt a two-stage LoRA-based\ntraining approach: the initial stage leverages our pseudo-caption dataset to\ncapture broad remote sensing semantics, followed by fine-tuning on smaller,\ncurated datasets like UCM and Sydney Captions to refine domain-specific\nalignment. Experimental evaluations demonstrate that AeroLite surpasses\nsignificantly larger models (e.g., 13B parameters) in standard captioning\nmetrics, including BLEU and METEOR, while maintaining substantially lower\ncomputational costs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09555", "pdf": "https://arxiv.org/pdf/2504.09555", "abs": "https://arxiv.org/abs/2504.09555", "authors": ["Jinhao Li", "Zijian Chen", "Runze Dong", "Tingzhu Chen", "Changbo Wang", "Guangtao Zhai"], "title": "Mitigating Long-tail Distribution in Oracle Bone Inscriptions: Dataset, Model, and Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "The oracle bone inscription (OBI) recognition plays a significant role in\nunderstanding the history and culture of ancient China. However, the existing\nOBI datasets suffer from a long-tail distribution problem, leading to biased\nperformance of OBI recognition models across majority and minority classes.\nWith recent advancements in generative models, OBI synthesis-based data\naugmentation has become a promising avenue to expand the sample size of\nminority classes. Unfortunately, current OBI datasets lack large-scale\nstructure-aligned image pairs for generative model training. To address these\nproblems, we first present the Oracle-P15K, a structure-aligned OBI dataset for\nOBI generation and denoising, consisting of 14,542 images infused with domain\nknowledge from OBI experts. Second, we propose a diffusion model-based pseudo\nOBI generator, called OBIDiff, to achieve realistic and controllable OBI\ngeneration. Given a clean glyph image and a target rubbing-style image, it can\neffectively transfer the noise style of the original rubbing to the glyph\nimage. Extensive experiments on OBI downstream tasks and user preference\nstudies show the effectiveness of the proposed Oracle-P15K dataset and\ndemonstrate that OBIDiff can accurately preserve inherent glyph structures\nwhile transferring authentic rubbing styles effectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10157", "pdf": "https://arxiv.org/pdf/2504.10157", "abs": "https://arxiv.org/abs/2504.10157", "authors": ["Xinnong Zhang", "Jiayu Lin", "Xinyi Mou", "Shiyue Yang", "Xiawei Liu", "Libo Sun", "Hanjia Lyu", "Yihang Yang", "Weihong Qi", "Yue Chen", "Guanying Li", "Ling Yan", "Yao Hu", "Siming Chen", "Yu Wang", "Jingxuan Huang", "Jiebo Luo", "Shiping Tang", "Libo Wu", "Baohua Zhou", "Zhongyu Wei"], "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users", "categories": ["cs.CL", "cs.CY"], "comment": "work in progress", "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09608", "pdf": "https://arxiv.org/pdf/2504.09608", "abs": "https://arxiv.org/abs/2504.09608", "authors": ["Xingke Song", "Xiaoying Yang", "Chenglin Yao", "Jianfeng Ren", "Ruibin Bai", "Xin Chen", "Xudong Jiang"], "title": "ERL-MPP: Evolutionary Reinforcement Learning with Multi-head Puzzle Perception for Solving Large-scale Jigsaw Puzzles of Eroded Gaps", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Solving jigsaw puzzles has been extensively studied. While most existing\nmodels focus on solving either small-scale puzzles or puzzles with no gap\nbetween fragments, solving large-scale puzzles with gaps presents distinctive\nchallenges in both image understanding and combinatorial optimization. To\ntackle these challenges, we propose a framework of Evolutionary Reinforcement\nLearning with Multi-head Puzzle Perception (ERL-MPP) to derive a better set of\nswapping actions for solving the puzzles. Specifically, to tackle the\nchallenges of perceiving the puzzle with gaps, a Multi-head Puzzle Perception\nNetwork (MPPN) with a shared encoder is designed, where multiple puzzlet heads\ncomprehensively perceive the local assembly status, and a discriminator head\nprovides a global assessment of the puzzle. To explore the large swapping\naction space efficiently, an Evolutionary Reinforcement Learning (EvoRL) agent\nis designed, where an actor recommends a set of suitable swapping actions from\na large action space based on the perceived puzzle status, a critic updates the\nactor using the estimated rewards and the puzzle status, and an evaluator\ncoupled with evolutionary strategies evolves the actions aligning with the\nhistorical assembly experience. The proposed ERL-MPP is comprehensively\nevaluated on the JPLEG-5 dataset with large gaps and the MIT dataset with\nlarge-scale puzzles. It significantly outperforms all state-of-the-art models\non both datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10160", "pdf": "https://arxiv.org/pdf/2504.10160", "abs": "https://arxiv.org/abs/2504.10160", "authors": ["Zhaopeng Feng", "Shaosheng Cao", "Jiahan Ren", "Jiayuan Su", "Ruizhe Chen", "Yan Zhang", "Zhe Xu", "Yao Hu", "Jian Wu", "Zuozhu Liu"], "title": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress. Our code is available at\n  https://github.com/fzp0424/MT-R1-Zero", "summary": "Large-scale reinforcement learning (RL) methods have proven highly effective\nin enhancing the reasoning abilities of large language models (LLMs),\nparticularly for tasks with verifiable solutions such as mathematics and\ncoding. However, applying this idea to machine translation (MT), where outputs\nare flexibly formatted and difficult to automatically evaluate with explicit\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\nguide LLMs towards improved translation quality via emergent reasoning. On the\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\ngeneralization capabilities on out-of-distribution MT tasks, robustly\nsupporting multilingual and low-resource settings. Extensive analysis of model\nbehavior across different initializations and reward metrics offers pioneering\ninsight into the critical role of reward design, LLM adaptability, training\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\nOur code is available at https://github.com/fzp0424/MT-R1-Zero.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09644", "pdf": "https://arxiv.org/pdf/2504.09644", "abs": "https://arxiv.org/abs/2504.09644", "authors": ["Kaiyu Li", "Zepeng Xin", "Li Pang", "Chao Pang", "Yupeng Deng", "Jing Yao", "Guisong Xia", "Deyu Meng", "Zhi Wang", "Xiangyong Cao"], "title": "SegEarth-R1: Geospatial Pixel Reasoning via Large Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing has become critical for understanding environmental dynamics,\nurban planning, and disaster management. However, traditional remote sensing\nworkflows often rely on explicit segmentation or detection methods, which\nstruggle to handle complex, implicit queries that require reasoning over\nspatial context, domain knowledge, and implicit user intent. Motivated by this,\nwe introduce a new task, \\ie, geospatial pixel reasoning, which allows implicit\nquerying and reasoning and generates the mask of the target region. To advance\nthis task, we construct and release the first large-scale benchmark dataset\ncalled EarthReason, which comprises 5,434 manually annotated image masks with\nover 30,000 implicit question-answer pairs. Moreover, we propose SegEarth-R1, a\nsimple yet effective language-guided segmentation baseline that integrates a\nhierarchical visual encoder, a large language model (LLM) for instruction\nparsing, and a tailored mask generator for spatial correlation. The design of\nSegEarth-R1 incorporates domain-specific adaptations, including aggressive\nvisual token compression to handle ultra-high-resolution remote sensing images,\na description projection module to fuse language and multi-scale features, and\na streamlined mask prediction pipeline that directly queries description\nembeddings. Extensive experiments demonstrate that SegEarth-R1 achieves\nstate-of-the-art performance on both reasoning and referring segmentation\ntasks, significantly outperforming traditional and LLM-based segmentation\nmethods. Our data and code will be released at\nhttps://github.com/earth-insights/SegEarth-R1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "correlation"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10187", "pdf": "https://arxiv.org/pdf/2504.10187", "abs": "https://arxiv.org/abs/2504.10187", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "title": "Deep Reasoning Translation via Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown\npromising performance in various complex tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation and taking cultural differences into account.\nThis task is still under-explored in deep reasoning LLMs. In this paper, we\nintroduce DeepTrans, a deep reasoning translation model that learns free\ntranslation via reinforcement learning. Specifically, we carefully build a\nreward model with pre-defined scoring criteria on both the translation results\nand the thought process. Given the source sentences, the reward model teaches\nthe deep translation model how to think and free-translate them during\nreinforcement learning. In this way, training DeepTrans does not need any\nlabeled translations, avoiding the human-intensive annotation or\nresource-intensive data synthesis. Experimental results show the effectiveness\nof DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance\nby 16.3% in literature translation, and outperforms strong deep reasoning\nbaselines as well as baselines that are fine-tuned with synthesized data.\nMoreover, we summarize the failures and interesting findings during our RL\nexploration. We hope this work could inspire other researchers in free\ntranslation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "criteria"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09666", "pdf": "https://arxiv.org/pdf/2504.09666", "abs": "https://arxiv.org/abs/2504.09666", "authors": ["Yao Yuan", "Pan Gao", "Qun Dai", "Jie Qin", "Wei Xiang"], "title": "Uncertainty Guided Refinement for Fine-Grained Salient Object Detection", "categories": ["cs.CV"], "comment": "IEEE Transactions on Image Processing 2025", "summary": "Recently, salient object detection (SOD) methods have achieved impressive\nperformance. However, salient regions predicted by existing methods usually\ncontain unsaturated regions and shadows, which limits the model for reliable\nfine-grained predictions. To address this, we introduce the uncertainty\nguidance learning approach to SOD, intended to enhance the model's perception\nof uncertain regions. Specifically, we design a novel Uncertainty Guided\nRefinement Attention Network (UGRAN), which incorporates three important\ncomponents, i.e., the Multilevel Interaction Attention (MIA) module, the Scale\nSpatial-Consistent Attention (SSCA) module, and the Uncertainty Refinement\nAttention (URA) module. Unlike conventional methods dedicated to enhancing\nfeatures, the proposed MIA facilitates the interaction and perception of\nmultilevel features, leveraging the complementary characteristics among\nmultilevel features. Then, through the proposed SSCA, the salient information\nacross diverse scales within the aggregated features can be integrated more\ncomprehensively and integrally. In the subsequent steps, we utilize the\nuncertainty map generated from the saliency prediction map to enhance the\nmodel's perception capability of uncertain regions, generating a\nhighly-saturated fine-grained saliency prediction map. Additionally, we devise\nan adaptive dynamic partition (ADP) mechanism to minimize the computational\noverhead of the URA module and improve the utilization of uncertainty guidance.\nExperiments on seven benchmark datasets demonstrate the superiority of the\nproposed UGRAN over the state-of-the-art methodologies. Codes will be released\nat https://github.com/I2-Multimedia-Lab/UGRAN.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09766", "pdf": "https://arxiv.org/pdf/2504.09766", "abs": "https://arxiv.org/abs/2504.09766", "authors": ["Diego Marcondes"], "title": "On the representation of stack operators by mathematical morphology", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces the class of grey-scale image stack operators as those\nthat (a) map binary-images into binary-images and (b) commute in average with\ncross-sectioning. We show that stack operators are 1-Lipchitz extensions of set\noperators which can be represented by applying a characteristic set operator to\nthe cross-sections of the image and summing. In particular, they are a\ngeneralisation of stack filters, for which the characteristic set operators are\nincreasing. Our main result is that stack operators inherit lattice properties\nof the characteristic set operators. We focus on the case of\ntranslation-invariant and locally defined stack operators and show the main\nresult by deducing the characteristic function, kernel, and basis\nrepresentation of stack operators. The results of this paper have implications\non the design of image operators, since imply that to solve some grey-scale\nimage processing problems it is enough to design an operator for performing the\ndesired transformation on binary images, and then considering its extension\ngiven by a stack operator. We leave many topics for future research regarding\nthe machine learning of stack operators and the characterisation of the image\nprocessing problems that can be solved by them.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10391", "pdf": "https://arxiv.org/pdf/2504.10391", "abs": "https://arxiv.org/abs/2504.10391", "authors": ["Varun Vasudevan", "Faezeh Akhavizadegan", "Abhinav Prakash", "Yokila Arora", "Jason Cho", "Tanya Mendiratta", "Sushant Kumar", "Kannan Achan"], "title": "LLM-driven Constrained Copy Generation through Iterative Refinement", "categories": ["cs.CL"], "comment": "10 pages, 2 figures, 7 Tables", "summary": "Crafting a marketing message (copy), or copywriting is a challenging\ngeneration task, as the copy must adhere to various constraints. Copy creation\nis inherently iterative for humans, starting with an initial draft followed by\nsuccessive refinements. However, manual copy creation is time-consuming and\nexpensive, resulting in only a few copies for each use case. This limitation\nrestricts our ability to personalize content to customers. Contrary to the\nmanual approach, LLMs can generate copies quickly, but the generated content\ndoes not consistently meet all the constraints on the first attempt (similar to\nhumans). While recent studies have shown promise in improving constrained\ngeneration through iterative refinement, they have primarily addressed tasks\nwith only a few simple constraints. Consequently, the effectiveness of\niterative refinement for tasks such as copy generation, which involves many\nintricate constraints, remains unclear. To address this gap, we propose an\nLLM-based end-to-end framework for scalable copy generation using iterative\nrefinement. To the best of our knowledge, this is the first study to address\nmultiple challenging constraints simultaneously in copy generation. Examples of\nthese constraints include length, topics, keywords, preferred lexical ordering,\nand tone of voice. We demonstrate the performance of our framework by creating\ncopies for e-commerce banners for three different use cases of varying\ncomplexity. Our results show that iterative refinement increases the copy\nsuccess rate by $16.25-35.91$% across use cases. Furthermore, the copies\ngenerated using our approach outperformed manually created content in multiple\npilot studies using a multi-armed bandit framework. The winning copy improved\nthe click-through rate by $38.5-45.21$%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09876", "pdf": "https://arxiv.org/pdf/2504.09876", "abs": "https://arxiv.org/abs/2504.09876", "authors": ["Tran Quoc Khanh Le", "Nguyen Lan Vi Vu", "Ha-Hieu Pham", "Xuan-Loc Huynh", "Tien-Huy Nguyen", "Minh Huu Nhat Le", "Quan Nguyen", "Hien D. Nguyen"], "title": "HDC: Hierarchical Distillation for Multi-level Noisy Consistency in Semi-Supervised Fetal Ultrasound Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Transvaginal ultrasound is a critical imaging modality for evaluating\ncervical anatomy and detecting physiological changes. However, accurate\nsegmentation of cervical structures remains challenging due to low contrast,\nshadow artifacts, and fuzzy boundaries. While convolutional neural networks\n(CNNs) have shown promising results in medical image segmentation, their\nperformance is often limited by the need for large-scale annotated datasets -\nan impractical requirement in clinical ultrasound imaging. Semi-supervised\nlearning (SSL) offers a compelling solution by leveraging unlabeled data, but\nexisting teacher-student frameworks often suffer from confirmation bias and\nhigh computational costs. We propose HDC, a novel semi-supervised segmentation\nframework that integrates Hierarchical Distillation and Consistency learning\nwithin a multi-level noise mean-teacher framework. Unlike conventional\napproaches that rely solely on pseudo-labeling, we introduce a hierarchical\ndistillation mechanism that guides feature-level learning via two novel\nobjectives: (1) Correlation Guidance Loss to align feature representations\nbetween the teacher and main student branch, and (2) Mutual Information Loss to\nstabilize representations between the main and noisy student branches. Our\nframework reduces model complexity while improving generalization. Extensive\nexperiments on two fetal ultrasound datasets, FUGC and PSFH, demonstrate that\nour method achieves competitive performance with significantly lower\ncomputational overhead than existing multi-teacher models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "consistency"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08744", "pdf": "https://arxiv.org/pdf/2504.08744", "abs": "https://arxiv.org/abs/2504.08744", "authors": ["Esmail Gumaan"], "title": "ExpertRAG: Efficient RAG with Mixture of Experts -- Optimizing Context Retrieval for Adaptive LLM Responses", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "30 pages, 4 figures", "summary": "ExpertRAG is a novel theoretical framework that integrates Mixture-of-Experts\n(MoE) architectures with Retrieval Augmented Generation (RAG) to advance the\nefficiency and accuracy of knowledge-intensive language modeling. We propose a\ndynamic retrieval gating mechanism coupled with expert routing, enabling the\nmodel to selectively consult an external knowledge store or rely on specialized\ninternal experts based on the query's needs. The paper lays out the theoretical\nfoundations of ExpertRAG, including a probabilistic formulation that treats\nretrieval and expert selection as latent decisions, and mathematical\njustifications for its efficiency in both computation and knowledge\nutilization. We derive formulae to quantify the expected computational cost\nsavings from selective retrieval and the capacity gains from sparse expert\nutilization. A comparative analysis positions ExpertRAG against standard RAG\n(with always-on retrieval) and pure MoE models (e.g., Switch Transformer,\nMixtral) to highlight its unique balance between parametric knowledge and\nnon-parametric retrieval. We also outline an experimental validation strategy,\nproposing benchmarks and evaluation protocols to test ExpertRAG's performance\non factual recall, generalization, and inference efficiency. The proposed\nframework, although presented theoretically, is supported by insights from\nprior work in RAG and MoE, and is poised to provide more factual, efficient,\nand adaptive generation by leveraging the best of both paradigms. In summary,\nExpertRAG contributes a new perspective on scaling and augmenting language\nmodels, backed by a thorough analysis and a roadmap for empirical validation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09897", "pdf": "https://arxiv.org/pdf/2504.09897", "abs": "https://arxiv.org/abs/2504.09897", "authors": ["Jaewoo Lee", "Keyang Xuan", "Chanakya Ekbote", "Sandeep Polisetty", "Yi R.", "Fung", "Paul Pu Liang"], "title": "TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language Models", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable versatility in\nunderstanding diverse multimodal data and tasks. However, these capabilities\ncome with an increased model scale. While post-training pruning reduces model\nsize in unimodal models, its application to MLLMs often yields limited success.\nOur analysis discovers that conventional methods fail to account for the unique\ntoken attributes across layers and modalities inherent to MLLMs. Inspired by\nthis observation, we propose TAMP, a simple yet effective pruning framework\ntailored for MLLMs, featuring two key components: (1) Diversity-Aware Sparsity,\nwhich adjusts sparsity ratio per layer based on diversities among multimodal\noutput tokens, preserving more parameters in high-diversity layers; and (2)\nAdaptive Multimodal Input Activation, which identifies representative\nmultimodal input tokens using attention scores to guide unstructured weight\npruning. We validate our method on two state-of-the-art MLLMs: LLaVA-NeXT,\ndesigned for vision-language tasks, and VideoLLaMA2, capable of processing\naudio, visual, and language modalities. Empirical experiments across various\nmultimodal evaluation benchmarks demonstrate that each component of our\napproach substantially outperforms existing pruning techniques.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09899", "pdf": "https://arxiv.org/pdf/2504.09899", "abs": "https://arxiv.org/abs/2504.09899", "authors": ["Ziwang Xu", "Lanqing Guo", "Satoshi Tsutsui", "Shuyan Zhang", "Alex C. Kot", "Bihan Wen"], "title": "Digital Staining with Knowledge Distillation: A Unified Framework for Unpaired and Paired-But-Misaligned Data", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to IEEE Transactions on Medical Imaging", "summary": "Staining is essential in cell imaging and medical diagnostics but poses\nsignificant challenges, including high cost, time consumption, labor intensity,\nand irreversible tissue alterations. Recent advances in deep learning have\nenabled digital staining through supervised model training. However, collecting\nlarge-scale, perfectly aligned pairs of stained and unstained images remains\ndifficult. In this work, we propose a novel unsupervised deep learning\nframework for digital cell staining that reduces the need for extensive paired\ndata using knowledge distillation. We explore two training schemes: (1)\nunpaired and (2) paired-but-misaligned settings. For the unpaired case, we\nintroduce a two-stage pipeline, comprising light enhancement followed by\ncolorization, as a teacher model. Subsequently, we obtain a student staining\ngenerator through knowledge distillation with hybrid non-reference losses. To\nleverage the pixel-wise information between adjacent sections, we further\nextend to the paired-but-misaligned setting, adding the Learning to Align\nmodule to utilize pixel-level information. Experiment results on our dataset\ndemonstrate that our proposed unsupervised deep staining method can generate\nstained images with more accurate positions and shapes of the cell targets in\nboth settings. Compared with competing methods, our method achieves improved\nresults both qualitatively and quantitatively (e.g., NIQE and PSNR).We applied\nour digital staining method to the White Blood Cell (WBC) dataset,\ninvestigating its potential for medical applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08777", "pdf": "https://arxiv.org/pdf/2504.08777", "abs": "https://arxiv.org/abs/2504.08777", "authors": ["Teo Susnjak", "Cole Palffy", "Tatiana Zimina", "Nazgul Altynbekova", "Kunal Garg", "Leona Gilbert"], "title": "The Lyme Disease Controversy: An AI-Driven Discourse Analysis of a Quarter Century of Academic Debate and Divides", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "The scientific discourse surrounding Chronic Lyme Disease (CLD) and\nPost-Treatment Lyme Disease Syndrome (PTLDS) has evolved over the past\ntwenty-five years into a complex and polarised debate, shaped by shifting\nresearch priorities, institutional influences, and competing explanatory\nmodels. This study presents the first large-scale, systematic examination of\nthis discourse using an innovative hybrid AI-driven methodology, combining\nlarge language models with structured human validation to analyse thousands of\nscholarly abstracts spanning 25 years. By integrating Large Language Models\n(LLMs) with expert oversight, we developed a quantitative framework for\ntracking epistemic shifts in contested medical fields, with applications to\nother content analysis domains. Our analysis revealed a progressive transition\nfrom infection-based models of Lyme disease to immune-mediated explanations for\npersistent symptoms. This study offers new empirical insights into the\nstructural and epistemic forces shaping Lyme disease research, providing a\nscalable and replicable methodology for analysing discourse, while underscoring\nthe value of AI-assisted methodologies in social science and medical research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08804", "pdf": "https://arxiv.org/pdf/2504.08804", "abs": "https://arxiv.org/abs/2504.08804", "authors": ["Pooya Razavi", "Sonya J. Powers"], "title": "Estimating Item Difficulty Using Large Language Models and Tree-Based Machine Learning Algorithms", "categories": ["cs.CY", "cs.CL", "cs.LG"], "comment": null, "summary": "Estimating item difficulty through field-testing is often resource-intensive\nand time-consuming. As such, there is strong motivation to develop methods that\ncan predict item difficulty at scale using only the item content. Large\nLanguage Models (LLMs) represent a new frontier for this goal. The present\nresearch examines the feasibility of using an LLM to predict item difficulty\nfor K-5 mathematics and reading assessment items (N = 5170). Two estimation\napproaches were implemented: (a) a direct estimation method that prompted the\nLLM to assign a single difficulty rating to each item, and (b) a feature-based\nstrategy where the LLM extracted multiple cognitive and linguistic features,\nwhich were then used in ensemble tree-based models (random forests and gradient\nboosting) to predict difficulty. Overall, direct LLM estimates showed moderate\nto strong correlations with true item difficulties. However, their accuracy\nvaried by grade level, often performing worse for early grades. In contrast,\nthe feature-based method yielded stronger predictive accuracy, with\ncorrelations as high as r = 0.87 and lower error estimates compared to both\ndirect LLM predictions and baseline regressors. These findings highlight the\npromise of LLMs in streamlining item development and reducing reliance on\nextensive field testing and underscore the importance of structured feature\nextraction. We provide a seven-step workflow for testing professionals who\nwould want to implement a similar item difficulty estimation approach with\ntheir item pool.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08949", "pdf": "https://arxiv.org/pdf/2504.08949", "abs": "https://arxiv.org/abs/2504.08949", "authors": ["Haokai Ma", "Yunshan Ma", "Ruobing Xie", "Lei Meng", "Jialie Shen", "Xingwu Sun", "Zhanhui Kang", "Tat-Seng Chua"], "title": "Large Language Model Empowered Recommendation Meets All-domain Continual Pre-Training", "categories": ["cs.IR", "cs.CL"], "comment": "In submission", "summary": "Recent research efforts have investigated how to integrate Large Language\nModels (LLMs) into recommendation, capitalizing on their semantic comprehension\nand open-world knowledge for user behavior understanding. These approaches\npredominantly employ supervised fine-tuning on single-domain user interactions\nto adapt LLMs for specific recommendation tasks. However, they typically\nencounter dual challenges: the mismatch between general language\nrepresentations and domain-specific preference patterns, as well as the limited\nadaptability to multi-domain recommendation scenarios. To bridge these gaps, we\nintroduce CPRec -- an All-domain Continual Pre-Training framework for\nRecommendation -- designed to holistically align LLMs with universal user\nbehaviors through the continual pre-training paradigm. Specifically, we first\ndesign a unified prompt template and organize users' multi-domain behaviors\ninto domain-specific behavioral sequences and all-domain mixed behavioral\nsequences that emulate real-world user decision logic. To optimize behavioral\nknowledge infusion, we devise a Warmup-Stable-Annealing learning rate schedule\ntailored for the continual pre-training paradigm in recommendation to\nprogressively enhance the LLM's capability in knowledge adaptation from\nopen-world knowledge to universal recommendation tasks. To evaluate the\neffectiveness of our CPRec, we implement it on a large-scale dataset covering\nseven domains and conduct extensive experiments on five real-world datasets\nfrom two distinct platforms. Experimental results confirm that our continual\npre-training paradigm significantly mitigates the semantic-behavioral\ndiscrepancy and achieves state-of-the-art performance in all recommendation\nscenarios. The source code will be released upon acceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09081", "pdf": "https://arxiv.org/pdf/2504.09081", "abs": "https://arxiv.org/abs/2504.09081", "authors": ["Prabhat Pandey", "Rupak Vignesh Swaminathan", "K V Vijay Girish", "Arunasish Sen", "Jian Xie", "Grant P. Strimel", "Andreas Schwarz"], "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10004", "pdf": "https://arxiv.org/pdf/2504.10004", "abs": "https://arxiv.org/abs/2504.10004", "authors": ["Matías Piqueras", "Alexandra Segerberg", "Matteo Magnani", "Måns Magnusson", "Nataša Sladoje"], "title": "An Image is Worth $K$ Topics: A Visual Structural Topic Model with Pretrained Image Embeddings", "categories": ["cs.CV", "cs.CY", "stat.AP", "stat.ME"], "comment": null, "summary": "Political scientists are increasingly interested in analyzing visual content\nat scale. However, the existing computational toolbox is still in need of\nmethods and models attuned to the specific challenges and goals of social and\npolitical inquiry. In this article, we introduce a visual Structural Topic\nModel (vSTM) that combines pretrained image embeddings with a structural topic\nmodel. This has important advantages compared to existing approaches. First,\npretrained embeddings allow the model to capture the semantic complexity of\nimages relevant to political contexts. Second, the structural topic model\nprovides the ability to analyze how topics and covariates are related, while\nmaintaining a nuanced representation of images as a mixture of multiple topics.\nIn our empirical application, we show that the vSTM is able to identify topics\nthat are interpretable, coherent, and substantively relevant to the study of\nonline political communication.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09265", "pdf": "https://arxiv.org/pdf/2504.09265", "abs": "https://arxiv.org/abs/2504.09265", "authors": ["Lei Kang", "Jia Li", "Mi Tian", "Hua Huang"], "title": "Mixture of Group Experts for Learning Invariant Representations", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Sparsely activated Mixture-of-Experts (MoE) models effectively increase the\nnumber of parameters while maintaining consistent computational costs per\ntoken. However, vanilla MoE models often suffer from limited diversity and\nspecialization among experts, constraining their performance and scalability,\nespecially as the number of experts increases. In this paper, we present a\nnovel perspective on vanilla MoE with top-$k$ routing inspired by sparse\nrepresentation. This allows us to bridge established theoretical insights from\nsparse representation into MoE models. Building on this foundation, we propose\na group sparse regularization approach for the input of top-$k$ routing, termed\nMixture of Group Experts (MoGE). MoGE indirectly regularizes experts by\nimposing structural constraints on the routing inputs, while preserving the\noriginal MoE architecture. Furthermore, we organize the routing input into a 2D\ntopographic map, spatially grouping neighboring elements. This structure\nenables MoGE to capture representations invariant to minor transformations,\nthereby significantly enhancing expert diversity and specialization.\nComprehensive evaluations across various Transformer models for image\nclassification and language modeling tasks demonstrate that MoGE substantially\noutperforms its MoE counterpart, with minimal additional memory and computation\noverhead. Our approach provides a simple yet effective solution to scale the\nnumber of experts and reduce redundancy among them. The source code is included\nin the supplementary material and will be publicly released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10018", "pdf": "https://arxiv.org/pdf/2504.10018", "abs": "https://arxiv.org/abs/2504.10018", "authors": ["Xiao Wang", "Haiyang Wang", "Shiao Wang", "Qiang Chen", "Jiandong Jin", "Haoyu Song", "Bo Jiang", "Chenglong Li"], "title": "RGB-Event based Pedestrian Attribute Recognition: A Benchmark Dataset and An Asymmetric RWKV Fusion Framework", "categories": ["cs.CV", "cs.AI"], "comment": "The First Benchmark Dataset for RGB-Event Multimodal Pedestrian\n  Attribute Recognition Task", "summary": "Existing pedestrian attribute recognition methods are generally developed\nbased on RGB frame cameras. However, these approaches are constrained by the\nlimitations of RGB cameras, such as sensitivity to lighting conditions and\nmotion blur, which hinder their performance. Furthermore, current attribute\nrecognition primarily focuses on analyzing pedestrians' external appearance and\nclothing, lacking an exploration of emotional dimensions. In this paper, we\nrevisit these issues and propose a novel multi-modal RGB-Event attribute\nrecognition task by drawing inspiration from the advantages of event cameras in\nlow-light, high-speed, and low-power consumption. Specifically, we introduce\nthe first large-scale multi-modal pedestrian attribute recognition dataset,\ntermed EventPAR, comprising 100K paired RGB-Event samples that cover 50\nattributes related to both appearance and six human emotions, diverse scenes,\nand various seasons. By retraining and evaluating mainstream PAR models on this\ndataset, we establish a comprehensive benchmark and provide a solid foundation\nfor future research in terms of data and algorithmic baselines. In addition, we\npropose a novel RWKV-based multi-modal pedestrian attribute recognition\nframework, featuring an RWKV visual encoder and an asymmetric RWKV fusion\nmodule. Extensive experiments are conducted on our proposed dataset as well as\ntwo simulated datasets (MARS-Attribute and DukeMTMC-VID-Attribute), achieving\nstate-of-the-art results. The source code and dataset will be released on\nhttps://github.com/Event-AHU/OpenPAR", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09426", "pdf": "https://arxiv.org/pdf/2504.09426", "abs": "https://arxiv.org/abs/2504.09426", "authors": ["Shengao Wang", "Arjun Chandra", "Aoming Liu", "Venkatesh Saligrama", "Boqing Gong"], "title": "BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Human infants rapidly develop visual reasoning skills from minimal input,\nsuggesting that developmentally inspired pretraining could significantly\nenhance the efficiency of vision-language models (VLMs). Although recent\nefforts have leveraged infant-inspired datasets like SAYCam, existing\nevaluation benchmarks remain misaligned--they are either too simplistic,\nnarrowly scoped, or tailored for large-scale pretrained models. Additionally,\ntraining exclusively on infant data overlooks the broader, diverse input from\nwhich infants naturally learn. To address these limitations, we propose\nBabyVLM, a novel framework comprising comprehensive in-domain evaluation\nbenchmarks and a synthetic training dataset created via child-directed\ntransformations of existing datasets. We demonstrate that VLMs trained with our\nsynthetic dataset achieve superior performance on BabyVLM tasks compared to\nmodels trained solely on SAYCam or general-purpose data of the SAYCam size.\nBabyVLM thus provides a robust, developmentally aligned evaluation tool and\nillustrates how compact models trained on carefully curated data can generalize\neffectively, opening pathways toward data-efficient vision-language learning\nparadigms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10070", "pdf": "https://arxiv.org/pdf/2504.10070", "abs": "https://arxiv.org/abs/2504.10070", "authors": ["Kiana Hoshanfar", "Alireza Hosseini", "Ahmad Kalhor", "Babak Nadjar Araabi"], "title": "DTFSal: Audio-Visual Dynamic Token Fusion for Video Saliency Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Audio-visual saliency prediction aims to mimic human visual attention by\nidentifying salient regions in videos through the integration of both visual\nand auditory information. Although visual-only approaches have significantly\nadvanced, effectively incorporating auditory cues remains challenging due to\ncomplex spatio-temporal interactions and high computational demands. To address\nthese challenges, we propose Dynamic Token Fusion Saliency (DFTSal), a novel\naudio-visual saliency prediction framework designed to balance accuracy with\ncomputational efficiency. Our approach features a multi-scale visual encoder\nequipped with two novel modules: the Learnable Token Enhancement Block (LTEB),\nwhich adaptively weights tokens to emphasize crucial saliency cues, and the\nDynamic Learnable Token Fusion Block (DLTFB), which employs a shifting\noperation to reorganize and merge features, effectively capturing long-range\ndependencies and detailed spatial information. In parallel, an audio branch\nprocesses raw audio signals to extract meaningful auditory features. Both\nvisual and audio features are integrated using our Adaptive Multimodal Fusion\nBlock (AMFB), which employs local, global, and adaptive fusion streams for\nprecise cross-modal fusion. The resulting fused features are processed by a\nhierarchical multi-decoder structure, producing accurate saliency maps.\nExtensive evaluations on six audio-visual benchmarks demonstrate that DFTSal\nachieves SOTA performance while maintaining computational efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09723", "pdf": "https://arxiv.org/pdf/2504.09723", "abs": "https://arxiv.org/abs/2504.09723", "authors": ["Dakuo Wang", "Ting-Yao Hsu", "Yuxuan Lu", "Limeng Cui", "Yaochen Xie", "William Headean", "Bingsheng Yao", "Akash Veeragouni", "Jiapeng Liu", "Sreyashi Nag", "Jessie Wang"], "title": "AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "A/B testing experiment is a widely adopted method for evaluating UI/UX design\ndecisions in modern web applications. Yet, traditional A/B testing remains\nconstrained by its dependence on the large-scale and live traffic of human\nparticipants, and the long time of waiting for the testing result. Through\nformative interviews with six experienced industry practitioners, we identified\ncritical bottlenecks in current A/B testing workflows. In response, we present\nAgentA/B, a novel system that leverages Large Language Model-based autonomous\nagents (LLM Agents) to automatically simulate user interaction behaviors with\nreal webpages. AgentA/B enables scalable deployment of LLM agents with diverse\npersonas, each capable of navigating the dynamic webpage and interactively\nexecuting multi-step interactions like search, clicking, filtering, and\npurchasing. In a demonstrative controlled experiment, we employ AgentA/B to\nsimulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and\ncompare agent behaviors with real human shopping behaviors at a scale. Our\nfindings suggest AgentA/B can emulate human-like behavior patterns.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09737", "pdf": "https://arxiv.org/pdf/2504.09737", "abs": "https://arxiv.org/abs/2504.09737", "authors": ["Nitya Thakkar", "Mert Yuksekgonul", "Jake Silberg", "Animesh Garg", "Nanyun Peng", "Fei Sha", "Rose Yu", "Carl Vondrick", "James Zou"], "title": "Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "30 pages, 7 figures", "summary": "Peer review at AI conferences is stressed by rapidly rising submission\nvolumes, leading to deteriorating review quality and increased author\ndissatisfaction. To address these issues, we developed Review Feedback Agent, a\nsystem leveraging multiple large language models (LLMs) to improve review\nclarity and actionability by providing automated feedback on vague comments,\ncontent misunderstandings, and unprofessional remarks to reviewers. Implemented\nat ICLR 2025 as a large randomized control study, our system provided optional\nfeedback to more than 20,000 randomly selected reviews. To ensure high-quality\nfeedback for reviewers at this scale, we also developed a suite of automated\nreliability tests powered by LLMs that acted as guardrails to ensure feedback\nquality, with feedback only being sent to reviewers if it passed all the tests.\nThe results show that 27% of reviewers who received feedback updated their\nreviews, and over 12,000 feedback suggestions from the agent were incorporated\nby those reviewers. This suggests that many reviewers found the AI-generated\nfeedback sufficiently helpful to merit updating their reviews. Incorporating AI\nfeedback led to significantly longer reviews (an average increase of 80 words\namong those who updated after receiving feedback) and more informative reviews,\nas evaluated by blinded researchers. Moreover, reviewers who were selected to\nreceive AI feedback were also more engaged during paper rebuttals, as seen in\nlonger author-reviewer discussions. This work demonstrates that carefully\ndesigned LLM-generated review feedback can enhance peer review quality by\nmaking reviews more specific and actionable while increasing engagement between\nreviewers and authors. The Review Feedback Agent is publicly available at\nhttps://github.com/zou-group/review_feedback_agent.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["AI feedback"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10090", "pdf": "https://arxiv.org/pdf/2504.10090", "abs": "https://arxiv.org/abs/2504.10090", "authors": ["I-Sheng Fang", "Jun-Cheng Chen"], "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09858", "pdf": "https://arxiv.org/pdf/2504.09858", "abs": "https://arxiv.org/abs/2504.09858", "authors": ["Wenjie Ma", "Jingxuan He", "Charlie Snell", "Tyler Griggs", "Sewon Min", "Matei Zaharia"], "title": "Reasoning Models Can Be Effective Without Thinking", "categories": ["cs.AI", "cs.CL"], "comment": "33 pages, 7 main figures, 2 tables", "summary": "Recent LLMs have significantly improved reasoning capabilities, primarily by\nincluding an explicit, lengthy Thinking process as part of generation. In this\npaper, we question whether this explicit thinking is necessary. Using the\nstate-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking\nprocess via simple prompting, denoted as NoThinking, can be surprisingly\neffective. When controlling for the number of tokens, NoThinking outperforms\nThinking across a diverse set of seven challenging reasoning\ndatasets--including mathematical problem solving, formal theorem proving, and\ncoding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with\n700 tokens. Notably, the performance of NoThinking becomes more competitive\nwith pass@k as k increases. Building on this observation, we demonstrate that a\nparallel scaling approach that uses NoThinking to generate N outputs\nindependently and aggregates them is highly effective. For aggregation, we use\ntask-specific verifiers when available, or we apply simple best-of-N strategies\nsuch as confidence-based selection. Our method outperforms a range of baselines\nwith similar latency using Thinking, and is comparable to Thinking with\nsignificantly longer latency (up to 9x). Together, our research encourages a\nreconsideration of the necessity of lengthy thinking processes, while also\nestablishing a competitive reference for achieving strong reasoning performance\nin low-budget settings or at low latency using parallel scaling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09946", "pdf": "https://arxiv.org/pdf/2504.09946", "abs": "https://arxiv.org/abs/2504.09946", "authors": ["Qian Wang", "Zhanzhi Lou", "Zhenheng Tang", "Nuo Chen", "Xuandong Zhao", "Wenxuan Zhang", "Dawn Song", "Bingsheng He"], "title": "Assessing Judging Bias in Large Reasoning Models: An Empirical Study", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have\ndemonstrated remarkable reasoning capabilities, raising important questions\nabout their biases in LLM-as-a-judge settings. We present a comprehensive\nbenchmark comparing judging biases between LLMs and LRMs across both subjective\npreference-alignment datasets and objective fact-based datasets. Through\ninvestigation of bandwagon, authority, position, and distraction biases, we\nuncover four key findings: (1) despite their advanced reasoning capabilities,\nLRMs remain susceptible to the above biases; (2) LRMs demonstrate better\nrobustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit\nnotable position bias, preferring options in later positions; and (4) we\nidentify a novel \"superficial reflection bias\" where phrases mimicking\nreasoning (e.g., \"wait, let me think...\") significantly influence model\njudgments. To address these biases, we design and evaluate three mitigation\nstrategies: specialized system prompts that reduce judging biases by up to 19\\%\nin preference alignment datasets and 14\\% in fact-related datasets, in-context\nlearning that provides up to 27\\% improvement on preference tasks but shows\ninconsistent results on factual tasks, and a self-reflection mechanism that\nreduces biases by up to 10\\% in preference datasets and 16\\% in fact-related\ndatasets, with self-reflection proving particularly effective for LRMs. Our\nwork provides crucial insights for developing more reliable LLM-as-a-Judge\nframeworks, especially as LRMs become increasingly deployed as automated\njudges.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10158", "pdf": "https://arxiv.org/pdf/2504.10158", "abs": "https://arxiv.org/abs/2504.10158", "authors": ["Jiansheng Li", "Xingxuan Zhang", "Hao Zou", "Yige Guo", "Renzhe Xu", "Yilong Liu", "Chuzhao Zhu", "Yue He", "Peng Cui"], "title": "COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current object detectors often suffer significant perfor-mance degradation in\nreal-world applications when encountering distributional shifts. Consequently,\nthe out-of-distribution (OOD) generalization capability of object detectors has\ngarnered increasing attention from researchers. Despite this growing interest,\nthere remains a lack of a large-scale, comprehensive dataset and evaluation\nbenchmark with fine-grained annotations tailored to assess the OOD\ngeneralization on more intricate tasks like object detection and grounding. To\naddress this gap, we introduce COUNTS, a large-scale OOD dataset with\nobject-level annotations. COUNTS encompasses 14 natural distributional shifts,\nover 222K samples, and more than 1,196K labeled bounding boxes. Leveraging\nCOUNTS, we introduce two novel benchmarks: O(OD)2 and OODG. O(OD)2 is designed\nto comprehensively evaluate the OOD generalization capabilities of object\ndetectors by utilizing controlled distribution shifts between training and\ntesting data. OODG, on the other hand, aims to assess the OOD generalization of\ngrounding abilities in multimodal large language models (MLLMs). Our findings\nreveal that, while large models and extensive pre-training data substantially\nen hance performance in in-distribution (IID) scenarios, significant\nlimitations and opportunities for improvement persist in OOD contexts for both\nobject detectors and MLLMs. In visual grounding tasks, even the advanced GPT-4o\nand Gemini-1.5 only achieve 56.7% and 28.0% accuracy, respectively. We hope\nCOUNTS facilitates advancements in the development and assessment of robust\nobject detectors and MLLMs capable of maintaining high performance under\ndistributional shifts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10081", "pdf": "https://arxiv.org/pdf/2504.10081", "abs": "https://arxiv.org/abs/2504.10081", "authors": ["Yichi Zhang", "Zihao Zeng", "Dongbai Li", "Yao Huang", "Zhijie Deng", "Yinpeng Dong"], "title": "RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been\nrapidly progressing and achieving breakthrough performance on complex reasoning\ntasks such as mathematics and coding. However, the open-source R1 models have\nraised safety concerns in wide applications, such as the tendency to comply\nwith malicious queries, which greatly impacts the utility of these powerful\nmodels in their applications. In this paper, we introduce RealSafe-R1 as\nsafety-aligned versions of DeepSeek-R1 distilled models. To train these models,\nwe construct a dataset of 15k safety-aware reasoning trajectories generated by\nDeepSeek-R1, under explicit instructions for expected refusal behavior. Both\nquantitative experiments and qualitative case studies demonstrate the models'\nimprovements, which are shown in their safety guardrails against both harmful\nqueries and jailbreak attacks. Importantly, unlike prior safety alignment\nefforts that often compromise reasoning performance, our method preserves the\nmodels' reasoning capabilities by maintaining the training data within the\noriginal distribution of generation. Model weights of RealSafe-R1 are\nopen-source at https://huggingface.co/RealSafe.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10090", "pdf": "https://arxiv.org/pdf/2504.10090", "abs": "https://arxiv.org/abs/2504.10090", "authors": ["I-Sheng Fang", "Jun-Cheng Chen"], "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10242", "pdf": "https://arxiv.org/pdf/2504.10242", "abs": "https://arxiv.org/abs/2504.10242", "authors": ["Tianyu Xin", "Jin-Liang Xiao", "Zeyu Xia", "Shan Yin", "Liang-Jian Deng"], "title": "CAT: A Conditional Adaptation Tailor for Efficient and Effective Instance-Specific Pansharpening on Real-World Data", "categories": ["cs.CV"], "comment": null, "summary": "Pansharpening is a crucial remote sensing technique that fuses low-resolution\nmultispectral (LRMS) images with high-resolution panchromatic (PAN) images to\ngenerate high-resolution multispectral (HRMS) imagery. Although deep learning\ntechniques have significantly advanced pansharpening, many existing methods\nsuffer from limited cross-sensor generalization and high computational\noverhead, restricting their real-time applications. To address these\nchallenges, we propose an efficient framework that quickly adapts to a specific\ninput instance, completing both training and inference in a short time. Our\nframework splits the input image into multiple patches, selects a subset for\nunsupervised CAT training, and then performs inference on all patches,\nstitching them into the final output. The CAT module, integrated between the\nfeature extraction and channel transformation stages of a pre-trained network,\ntailors the fused features and fixes the parameters for efficient inference,\ngenerating improved results. Our approach offers two key advantages: (1)\n$\\textit{Improved Generalization Ability}$: by mitigating cross-sensor\ndegradation, our model--although pre-trained on a specific dataset--achieves\nsuperior performance on datasets captured by other sensors; (2)\n$\\textit{Enhanced Computational Efficiency}$: the CAT-enhanced network can\nswiftly adapt to the test sample using the single LRMS-PAN pair input, without\nrequiring extensive large-scale data retraining. Experiments on the real-world\ndata from WorldView-3 and WorldView-2 datasets demonstrate that our method\nachieves state-of-the-art performance on cross-sensor real-world data, while\nachieving both training and inference of $512\\times512$ image within\n$\\textit{0.4 seconds}$ and $4000\\times4000$ image within $\\textit{3 seconds}$\nat the fastest setting on a commonly used RTX 3090 GPU.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10254", "pdf": "https://arxiv.org/pdf/2504.10254", "abs": "https://arxiv.org/abs/2504.10254", "authors": ["Xuqiang Cao", "Linnan Zhao", "Jiaxuan Zhao", "Fang Liu", "Puhua Chen", "Wenping Ma"], "title": "MASSeg : 2nd Technical Report for 4th PVUW MOSE Track", "categories": ["cs.CV", "cs.AI"], "comment": "5 pages,4 figures,Technical report on Complex Video Object\n  Segmentation", "summary": "Complex video object segmentation continues to face significant challenges in\nsmall object recognition, occlusion handling, and dynamic scene modeling. This\nreport presents our solution, which ranked second in the MOSE track of CVPR\n2025 PVUW Challenge. Based on an existing segmentation framework, we propose an\nimproved model named MASSeg for complex video object segmentation, and\nconstruct an enhanced dataset, MOSE+, which includes typical scenarios with\nocclusions, cluttered backgrounds, and small target instances. During training,\nwe incorporate a combination of inter-frame consistent and inconsistent data\naugmentation strategies to improve robustness and generalization. During\ninference, we design a mask output scaling strategy to better adapt to varying\nobject sizes and occlusion levels. As a result, MASSeg achieves a J score of\n0.8250, F score of 0.9007, and a J&F score of 0.8628 on the MOSE test set.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10352", "pdf": "https://arxiv.org/pdf/2504.10352", "abs": "https://arxiv.org/abs/2504.10352", "authors": ["Yifan Yang", "Shujie Liu", "Jinyu Li", "Yuxuan Hu", "Haibin Wu", "Hui Wang", "Jianwei Yu", "Lingwei Meng", "Haiyang Sun", "Yanqing Liu", "Yan Lu", "Kai Yu", "Xie Chen"], "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis", "categories": ["eess.AS", "cs.CL"], "comment": "Submitted to ACM MM 2025", "summary": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma:\nautoregressive (AR) models suffer from slow generation and lack duration\ncontrollability, while non-autoregressive (NAR) models lack temporal modeling\nand typically require complex designs. In this paper, we introduce a novel\npseudo-autoregressive (PAR) codec language modeling approach that unifies AR\nand NAR modeling. Combining explicit temporal modeling from AR with parallel\ngeneration from NAR, PAR generates dynamic-length spans at fixed time steps.\nBuilding on PAR, we propose PALLE, a two-stage TTS system that leverages PAR\nfor initial generation followed by NAR refinement. In the first stage, PAR\nprogressively generates speech tokens along the time dimension, with each step\npredicting all positions in parallel but only retaining the left-most span. In\nthe second stage, low-confidence tokens are iteratively refined in parallel,\nleveraging the global contextual information. Experiments demonstrate that\nPALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on\nlarge-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech\ntest-clean set in terms of speech quality, speaker similarity, and\nintelligibility, while achieving up to ten times faster inference speed. Audio\nsamples are available at https://anonymous-palle.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10275", "pdf": "https://arxiv.org/pdf/2504.10275", "abs": "https://arxiv.org/abs/2504.10275", "authors": ["Harsh Yadav", "Maximilian Schaefer", "Kun Zhao", "Tobias Meisen"], "title": "LMFormer: Lane based Motion Prediction Transformer", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted: Autonomous Driving Workshop, CVPR 2025", "summary": "Motion prediction plays an important role in autonomous driving. This study\npresents LMFormer, a lane-aware transformer network for trajectory prediction\ntasks. In contrast to previous studies, our work provides a simple mechanism to\ndynamically prioritize the lanes and shows that such a mechanism introduces\nexplainability into the learning behavior of the network. Additionally,\nLMFormer uses the lane connection information at intersections, lane merges,\nand lane splits, in order to learn long-range dependency in lane structure.\nMoreover, we also address the issue of refining the predicted trajectories and\npropose an efficient method for iterative refinement through stacked\ntransformer layers. For benchmarking, we evaluate LMFormer on the nuScenes\ndataset and demonstrate that it achieves SOTA performance across multiple\nmetrics. Furthermore, the Deep Scenario dataset is used to not only illustrate\ncross-dataset network performance but also the unification capabilities of\nLMFormer to train on multiple datasets and achieve better performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10288", "pdf": "https://arxiv.org/pdf/2504.10288", "abs": "https://arxiv.org/abs/2504.10288", "authors": ["Mathieu Manni", "Dmitry Karpov", "K. Joost Batenburg", "Sharon Shwartz", "Nicola Viganò"], "title": "Noise2Ghost: Self-supervised deep convolutional reconstruction for ghost imaging", "categories": ["cs.CV", "cs.LG", "physics.data-an"], "comment": null, "summary": "We present a new self-supervised deep-learning-based Ghost Imaging (GI)\nreconstruction method, which provides unparalleled reconstruction performance\nfor noisy acquisitions among unsupervised methods. We present the supporting\nmathematical framework and results from theoretical and real data use cases.\nSelf-supervision removes the need for clean reference data while offering\nstrong noise reduction. This provides the necessary tools for addressing\nsignal-to-noise ratio concerns for GI acquisitions in emerging and cutting-edge\nlow-light GI scenarios. Notable examples include micro- and nano-scale x-ray\nemission imaging, e.g., x-ray fluorescence imaging of dose-sensitive samples.\nTheir applications include in-vivo and in-operando case studies for biological\nsamples and batteries.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10329", "pdf": "https://arxiv.org/pdf/2504.10329", "abs": "https://arxiv.org/abs/2504.10329", "authors": ["Xingyu Lu", "Yuhang Hu", "YiFan Zhang", "Kaiyu Jiang", "Changyi Liu", "Tianke Zhang", "Jinpeng Wang", "Bin Wen", "Chun Yuan", "Fan Yang", "Tingting Gao", "Di Zhang"], "title": "InstructEngine: Instruction-driven Text-to-Image Alignment", "categories": ["cs.CV"], "comment": "8 pages, 7 figures", "summary": "Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) has been\nextensively utilized for preference alignment of text-to-image models. Existing\nmethods face certain limitations in terms of both data and algorithm. For\ntraining data, most approaches rely on manual annotated preference data, either\nby directly fine-tuning the generators or by training reward models to provide\ntraining signals. However, the high annotation cost makes them difficult to\nscale up, the reward model consumes extra computation and cannot guarantee\naccuracy. From an algorithmic perspective, most methods neglect the value of\ntext and only take the image feedback as a comparative signal, which is\ninefficient and sparse. To alleviate these drawbacks, we propose the\nInstructEngine framework. Regarding annotation cost, we first construct a\ntaxonomy for text-to-image generation, then develop an automated data\nconstruction pipeline based on it. Leveraging advanced large multimodal models\nand human-defined rules, we generate 25K text-image preference pairs. Finally,\nwe introduce cross-validation alignment method, which refines data efficiency\nby organizing semantically analogous samples into mutually comparable pairs.\nEvaluations on DrawBench demonstrate that InstructEngine improves SD v1.5 and\nSDXL's performance by 10.53% and 5.30%, outperforming state-of-the-art\nbaselines, with ablation study confirming the benefits of InstructEngine's all\ncomponents. A win rate of over 50% in human reviews also proves that\nInstructEngine better aligns with human preferences.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "reinforcement learning", "preference", "RLAIF", "AI feedback", "alignment"], "score": 7}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "accuracy"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10358", "pdf": "https://arxiv.org/pdf/2504.10358", "abs": "https://arxiv.org/abs/2504.10358", "authors": ["Rui Chen", "Lei Sun", "Jing Tang", "Geng Li", "Xiangxiang Chu"], "title": "FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Recent advances in video generation have posed great challenges in the\nassessment of AI-generated content, particularly with the emergence of\nincreasingly sophisticated models. The various inconsistencies and defects\nobserved in such videos are inherently complex, making overall scoring\nnotoriously difficult. In this paper, we emphasize the critical importance of\nintegrating fine-grained reasoning into video evaluation, and we propose\n$\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation\nframework that first automatically generates $\\textbf{F}$ine-grained\n$\\textbf{E}$ntity-level questions, and then answers those questions by a\n$\\textbf{R}$easoning model with scores, which can be subsequently weighted\nsummed to an overall score for different applications. Specifically, we\nleverage LLMs to derive entity-level questions across five distinct\nperspectives, which (i) often focus on some specific entities of the content,\nthereby making answering or scoring much easier by MLLMs, and (ii) are more\ninterpretable. Then we construct a FingER dataset, consisting of approximately\n3.3k videos and corresponding 60k fine-grained QA annotations, each with\ndetailed reasons. Based on that, we further investigate various training\nprotocols to best incentivize the reasoning capability of MLLMs for correct\nanswer prediction. Extensive experiments demonstrate that a reasoning model\ntrained using Group Relative Policy Optimization (GRPO) with a cold-start\nstrategy achieves the best performance. Notably, our model surpasses existing\nmethods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on\nMonetBench with only 3.3k training videos, which is at most one-tenth of the\ntraining samples utilized by other methods. Our code and dataset will be\nreleased soon.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10462", "pdf": "https://arxiv.org/pdf/2504.10462", "abs": "https://arxiv.org/abs/2504.10462", "authors": ["Weixian Lei", "Jiacong Wang", "Haochen Wang", "Xiangtai Li", "Jun Hao Liew", "Jiashi Feng", "Zilong Huang"], "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces SAIL, a single transformer unified multimodal large\nlanguage model (MLLM) that integrates raw pixel encoding and language decoding\nwithin a singular architecture. Unlike existing modular MLLMs, which rely on a\npre-trained vision transformer (ViT), SAIL eliminates the need for a separate\nvision encoder, presenting a more minimalist architecture design. Instead of\nintroducing novel architectural components, SAIL adapts mix-attention\nmechanisms and multimodal positional encodings to better align with the\ndistinct characteristics of visual and textual modalities. We systematically\ncompare SAIL's properties-including scalability, cross-modal information flow\npatterns, and visual representation capabilities-with those of modular MLLMs.\nBy scaling both training data and model size, SAIL achieves performance\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\nenhances SAIL's scalability and results in significantly different cross-modal\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\nrepresentation capabilities, achieving results on par with ViT-22B in vision\ntasks such as semantic segmentation. Code and models are available at\nhttps://github.com/bytedance/SAIL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10465", "pdf": "https://arxiv.org/pdf/2504.10465", "abs": "https://arxiv.org/abs/2504.10465", "authors": ["Tao Zhang", "Xiangtai Li", "Zilong Huang", "Yanwei Li", "Weixian Lei", "Xueqing Deng", "Shihao Chen", "Shunping Ji", "Jiashi Feng"], "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for\nfine-grained pixel-level understanding tasks. However, all the works rely\nheavily on extra components, such as vision encoder (CLIP), segmentation\nexperts, leading to high system complexity and limiting model scaling. In this\nwork, our goal is to explore a highly simplified MLLM without introducing extra\ncomponents. Our work is motivated by the recent works on Single trAnsformer as\na unified vIsion-Language Model (SAIL) design, where these works jointly learn\nvision tokens and text tokens in transformers. We present Pixel-SAIL, a single\ntransformer for pixel-wise MLLM tasks. In particular, we present three\ntechnical improvements on the plain baseline. First, we design a learnable\nupsampling module to refine visual token features. Secondly, we propose a novel\nvisual prompt injection strategy to enable the single transformer to understand\nvisual prompt inputs and benefit from the early fusion of visual prompt\nembeddings and vision tokens. Thirdly, we introduce a vision expert\ndistillation strategy to efficiently enhance the single transformer's\nfine-grained feature extraction capability. In addition, we have collected a\ncomprehensive pixel understanding benchmark (PerBench), using a manual check.\nIt includes three tasks: detailed object description, visual prompt-based\nquestion answering, and visual-text referring segmentation. Extensive\nexperiments on four referring segmentation benchmarks, one visual prompt\nbenchmark, and our PerBench show that our Pixel-SAIL achieves comparable or\neven better results with a much simpler pipeline. Code and model will be\nreleased at https://github.com/magic-research/Sa2VA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering", "fine-grained"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10466", "pdf": "https://arxiv.org/pdf/2504.10466", "abs": "https://arxiv.org/abs/2504.10466", "authors": ["Xiaoyan Cong", "Jiayi Shen", "Zekun Li", "Rao Fu", "Tao Lu", "Srinath Sridhar"], "title": "Art3D: Training-Free 3D Generation from Flat-Colored Illustration", "categories": ["cs.CV"], "comment": "Technical Report. Course Project of Brown CSCI 1430 Computer Vision.\n  Project Page: https://joy-jy11.github.io/", "summary": "Large-scale pre-trained image-to-3D generative models have exhibited\nremarkable capabilities in diverse shape generations. However, most of them\nstruggle to synthesize plausible 3D assets when the reference image is\nflat-colored like hand drawings due to the lack of 3D illusion, which are often\nthe most user-friendly input modalities in art content creation. To this end,\nwe propose Art3D, a training-free method that can lift flat-colored 2D designs\ninto 3D. By leveraging structural and semantic features with pre- trained 2D\nimage generation models and a VLM-based realism evaluation, Art3D successfully\nenhances the three-dimensional illusion in reference images, thus simplifying\nthe process of generating 3D from 2D, and proves adaptable to a wide range of\npainting styles. To benchmark the generalization performance of existing\nimage-to-3D models on flat-colored images without 3D feeling, we collect a new\ndataset, Flat-2D, with over 100 samples. Experimental results demonstrate the\nperformance and robustness of Art3D, exhibiting superior generalizable capacity\nand promising practical applicability. Our source code and dataset will be\npublicly available on our project page: https://joy-jy11.github.io/ .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10486", "pdf": "https://arxiv.org/pdf/2504.10486", "abs": "https://arxiv.org/abs/2504.10486", "authors": ["Zeren Jiang", "Shaofei Wang", "Siyu Tang"], "title": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting", "categories": ["cs.CV"], "comment": "16 pages, 8 figures, Project pages:\n  https://jzr99.github.io/DNF-Avatar/", "summary": "Creating relightable and animatable human avatars from monocular videos is a\nrising research topic with a range of applications, e.g. virtual reality,\nsports, and video games. Previous works utilize neural fields together with\nphysically based rendering (PBR), to estimate geometry and disentangle\nappearance properties of human avatars. However, one drawback of these methods\nis the slow rendering speed due to the expensive Monte Carlo ray tracing. To\ntackle this problem, we proposed to distill the knowledge from implicit neural\nfields (teacher) to explicit 2D Gaussian splatting (student) representation to\ntake advantage of the fast rasterization property of Gaussian splatting. To\navoid ray-tracing, we employ the split-sum approximation for PBR appearance. We\nalso propose novel part-wise ambient occlusion probes for shadow computation.\nShadow prediction is achieved by querying these probes only once per pixel,\nwhich paves the way for real-time relighting of avatars. These techniques\ncombined give high-quality relighting results with realistic shadow effects.\nOur experiments demonstrate that the proposed student model achieves comparable\nor even better relighting results with our teacher model while being 370 times\nfaster at inference time, achieving a 67 FPS rendering speed.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08937", "pdf": "https://arxiv.org/pdf/2504.08937", "abs": "https://arxiv.org/abs/2504.08937", "authors": ["Minjie Deng", "Yan Wei", "Hao Zhai", "An Wu", "Yuncan Ouyang", "Qianyao Peng"], "title": "Rethinking Few-Shot Fusion: Granular Ball Priors Enable General-Purpose Deep Image Fusion", "categories": ["cs.GR", "cs.CV", "cs.LG", "eess.IV", "stat.ML"], "comment": null, "summary": "In image fusion tasks, due to the lack of real fused images as priors, most\ndeep learning-based fusion methods obtain global weight features from original\nimages in large-scale data pairs to generate images that approximate real fused\nimages. However, unlike previous studies, this paper utilizes Granular Ball\nadaptation to extract features in the brightness space as priors for deep\nnetworks, enabling the fusion network to converge quickly and complete the\nfusion task. This leads to few-shot training for a general image fusion\nnetwork, and based on this, we propose the GBFF fusion method. According to the\ninformation expression division of pixel pairs in the original fused image, we\nclassify pixel pairs with significant performance as the positive domain and\nnon-significant pixel pairs as the boundary domain. We perform split inference\nin the brightness space using Granular Ball adaptation to compute weights for\npixels that express information to varying degrees, generating approximate\nsupervision images that provide priors for the neural network in the structural\nbrightness space. Additionally, the extracted global saliency features also\nadaptively provide priors for setting the loss function weights of each image\nin the network, guiding the network to converge quickly at both global and\npixel levels alongside the supervised images, thereby enhancing the\nexpressiveness of the fused images. Each modality only used 10 pairs of images\nas the training set, completing the fusion task with a limited number of\niterations. Experiments validate the effectiveness of the algorithm and theory,\nand qualitative and quantitative comparisons with SOTA methods show that this\napproach is highly competitive in terms of fusion time and image\nexpressiveness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08974", "pdf": "https://arxiv.org/pdf/2504.08974", "abs": "https://arxiv.org/abs/2504.08974", "authors": ["Pouya Pezeshkpour", "Moin Aminnaseri", "Estevam Hruschka"], "title": "Mixed Signals: Decoding VLMs' Reasoning and Underlying Bias in Vision-Language Conflict", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have demonstrated impressive performance by\neffectively integrating visual and textual information to solve complex tasks.\nHowever, it is not clear how these models reason over the visual and textual\ndata together, nor how the flow of information between modalities is\nstructured. In this paper, we examine how VLMs reason by analyzing their biases\nwhen confronted with scenarios that present conflicting image and text cues, a\ncommon occurrence in real-world applications. To uncover the extent and nature\nof these biases, we build upon existing benchmarks to create five datasets\ncontaining mismatched image-text pairs, covering topics in mathematics,\nscience, and visual descriptions. Our analysis shows that VLMs favor text in\nsimpler queries but shift toward images as query complexity increases. This\nbias correlates with model scale, with the difference between the percentage of\nimage- and text-preferred responses ranging from +56.8% (image favored) to\n-74.4% (text favored), depending on the task and model. In addition, we explore\nthree mitigation strategies: simple prompt modifications, modifications that\nexplicitly instruct models on how to handle conflicting information (akin to\nchain-of-thought prompting), and a task decomposition strategy that analyzes\neach modality separately before combining their results. Our findings indicate\nthat the effectiveness of these strategies in identifying and mitigating bias\nvaries significantly and is closely linked to the model's overall performance\non the task and the specific modality in question.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09088", "pdf": "https://arxiv.org/pdf/2504.09088", "abs": "https://arxiv.org/abs/2504.09088", "authors": ["Yonghao Huang", "Leiting Chen", "Chuan Zhou"], "title": "Multi-Modal Brain Tumor Segmentation via 3D Multi-Scale Self-attention and Cross-attention", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Due to the success of CNN-based and Transformer-based models in various\ncomputer vision tasks, recent works study the applicability of CNN-Transformer\nhybrid architecture models in 3D multi-modality medical segmentation tasks.\nIntroducing Transformer brings long-range dependent information modeling\nability in 3D medical images to hybrid models via the self-attention mechanism.\nHowever, these models usually employ fixed receptive fields of 3D volumetric\nfeatures within each self-attention layer, ignoring the multi-scale volumetric\nlesion features. To address this issue, we propose a CNN-Transformer hybrid 3D\nmedical image segmentation model, named TMA-TransBTS, based on an\nencoder-decoder structure. TMA-TransBTS realizes simultaneous extraction of\nmulti-scale 3D features and modeling of long-distance dependencies by\nmulti-scale division and aggregation of 3D tokens in a self-attention layer.\nFurthermore, TMA-TransBTS proposes a 3D multi-scale cross-attention module to\nestablish a link between the encoder and the decoder for extracting rich volume\nrepresentations by exploiting the mutual attention mechanism of cross-attention\nand multi-scale aggregation of 3D tokens. Extensive experimental results on\nthree public 3D medical segmentation datasets show that TMA-TransBTS achieves\nhigher averaged segmentation results than previous state-of-the-art CNN-based\n3D methods and CNN-Transform hybrid 3D methods for the segmentation of 3D\nmulti-modality brain tumors.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09265", "pdf": "https://arxiv.org/pdf/2504.09265", "abs": "https://arxiv.org/abs/2504.09265", "authors": ["Lei Kang", "Jia Li", "Mi Tian", "Hua Huang"], "title": "Mixture of Group Experts for Learning Invariant Representations", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Sparsely activated Mixture-of-Experts (MoE) models effectively increase the\nnumber of parameters while maintaining consistent computational costs per\ntoken. However, vanilla MoE models often suffer from limited diversity and\nspecialization among experts, constraining their performance and scalability,\nespecially as the number of experts increases. In this paper, we present a\nnovel perspective on vanilla MoE with top-$k$ routing inspired by sparse\nrepresentation. This allows us to bridge established theoretical insights from\nsparse representation into MoE models. Building on this foundation, we propose\na group sparse regularization approach for the input of top-$k$ routing, termed\nMixture of Group Experts (MoGE). MoGE indirectly regularizes experts by\nimposing structural constraints on the routing inputs, while preserving the\noriginal MoE architecture. Furthermore, we organize the routing input into a 2D\ntopographic map, spatially grouping neighboring elements. This structure\nenables MoGE to capture representations invariant to minor transformations,\nthereby significantly enhancing expert diversity and specialization.\nComprehensive evaluations across various Transformer models for image\nclassification and language modeling tasks demonstrate that MoGE substantially\noutperforms its MoE counterpart, with minimal additional memory and computation\noverhead. Our approach provides a simple yet effective solution to scale the\nnumber of experts and reduce redundancy among them. The source code is included\nin the supplementary material and will be publicly released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09352", "pdf": "https://arxiv.org/pdf/2504.09352", "abs": "https://arxiv.org/abs/2504.09352", "authors": ["Iason Chaimalas", "Arnas Vyšniauskas", "Gabriel Brostow"], "title": "Explorer: Robust Collection of Interactable GUI Elements", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "19 pages, 17 figures", "summary": "Automation of existing Graphical User Interfaces (GUIs) is important but hard\nto achieve. Upstream of making the GUI user-accessible or somehow scriptable,\neven the data-collection to understand the original interface poses significant\nchallenges. For example, large quantities of general UI data seem helpful for\ntraining general machine learning (ML) models, but accessibility for each\nperson can hinge on the ML's precision on a specific app. We therefore take the\nperspective that a given user needs confidence, that the relevant UI elements\nare being detected correctly throughout one app or digital environment. We\nmostly assume that the target application is known in advance, so that data\ncollection and ML-training can be personalized for the test-time target domain.\nThe proposed Explorer system focuses on detecting on-screen buttons and\ntext-entry fields, i.e. interactables, where the training process has access to\na live version of the application. The live application can run on almost any\npopular platform except iOS phones, and the collection is especially\nstreamlined for Android phones or for desktop Chrome browsers. Explorer also\nenables the recording of interactive user sessions, and subsequent mapping of\nhow such sessions overlap and sometimes loop back to similar states. We show\nhow having such a map enables a kind of path planning through the GUI, letting\na user issue audio commands to get to their destination. Critically, we are\nreleasing our code for Explorer openly at https://github.com/varnelis/Explorer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09975", "pdf": "https://arxiv.org/pdf/2504.09975", "abs": "https://arxiv.org/abs/2504.09975", "authors": ["Si-Tong Wei", "Rui-Huan Wang", "Chuan-Zhi Zhou", "Baoquan Chen", "Peng-Shuai Wang"], "title": "OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape Generation", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025", "summary": "Autoregressive models have achieved remarkable success across various\ndomains, yet their performance in 3D shape generation lags significantly behind\nthat of diffusion models. In this paper, we introduce OctGPT, a novel\nmultiscale autoregressive model for 3D shape generation that dramatically\nimproves the efficiency and performance of prior 3D autoregressive approaches,\nwhile rivaling or surpassing state-of-the-art diffusion models. Our method\nemploys a serialized octree representation to efficiently capture the\nhierarchical and spatial structures of 3D shapes. Coarse geometry is encoded\nvia octree structures, while fine-grained details are represented by binary\ntokens generated using a vector quantized variational autoencoder (VQVAE),\ntransforming 3D shapes into compact \\emph{multiscale binary sequences} suitable\nfor autoregressive prediction. To address the computational challenges of\nhandling long sequences, we incorporate octree-based transformers enhanced with\n3D rotary positional encodings, scale-specific embeddings, and token-parallel\ngeneration schemes. These innovations reduce training time by 13 folds and\ngeneration time by 69 folds, enabling the efficient training of high-resolution\n3D shapes, e.g.,$1024^3$, on just four NVIDIA 4090 GPUs only within days.\nOctGPT showcases exceptional versatility across various tasks, including text-,\nsketch-, and image-conditioned generation, as well as scene-level synthesis\ninvolving multiple objects. Extensive experiments demonstrate that OctGPT\naccelerates convergence and improves generation quality over prior\nautoregressive methods, offering a new paradigm for high-quality, scalable 3D\ncontent creation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10025", "pdf": "https://arxiv.org/pdf/2504.10025", "abs": "https://arxiv.org/abs/2504.10025", "authors": ["Uyen Phan", "Ozer Can Devecioglu", "Serkan Kiranyaz", "Moncef Gabbouj"], "title": "Progressive Transfer Learning for Multi-Pass Fundus Image Restoration", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "13 pages, 12 figures including appendix", "summary": "Diabetic retinopathy is a leading cause of vision impairment, making its\nearly diagnosis through fundus imaging critical for effective treatment\nplanning. However, the presence of poor quality fundus images caused by factors\nsuch as inadequate illumination, noise, blurring and other motion artifacts\nyields a significant challenge for accurate DR screening. In this study, we\npropose progressive transfer learning for multi pass restoration to iteratively\nenhance the quality of degraded fundus images, ensuring more reliable DR\nscreening. Unlike previous methods that often focus on a single pass\nrestoration, multi pass restoration via PTL can achieve a superior blind\nrestoration performance that can even improve most of the good quality fundus\nimages in the dataset. Initially, a Cycle GAN model is trained to restore low\nquality images, followed by PTL induced restoration passes over the latest\nrestored outputs to improve overall quality in each pass. The proposed method\ncan learn blind restoration without requiring any paired data while surpassing\nits limitations by leveraging progressive learning and fine tuning strategies\nto minimize distortions and preserve critical retinal features. To evaluate\nPTL's effectiveness on multi pass restoration, we conducted experiments on\nDeepDRiD, a large scale fundus imaging dataset specifically curated for\ndiabetic retinopathy detection. Our result demonstrates state of the art\nperformance, showcasing PTL's potential as a superior approach to iterative\nimage quality restoration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10281", "pdf": "https://arxiv.org/pdf/2504.10281", "abs": "https://arxiv.org/abs/2504.10281", "authors": ["Jingyun Yang", "Ruoyan Avery Yin", "Chi Jiang", "Yuepeng Hu", "Xiaokai Zhu", "Xingjian Hu", "Sutharsika Kumar", "Xiao Wang", "Xiaohua Zhai", "Keran Rong", "Yunyue Zhu", "Tianyi Zhang", "Zongyou Yin", "Jing Kong", "Neil Zhenqiang Gong", "Zhichu Ren", "Haozhe Wang"], "title": "Zero-shot Autonomous Microscopy for Scalable and Intelligent Characterization of 2D Materials", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "cs.AI", "cs.CV", "cs.LG"], "comment": "13 pages, 4 figures", "summary": "Characterization of atomic-scale materials traditionally requires human\nexperts with months to years of specialized training. Even for trained human\noperators, accurate and reliable characterization remains challenging when\nexamining newly discovered materials such as two-dimensional (2D) structures.\nThis bottleneck drives demand for fully autonomous experimentation systems\ncapable of comprehending research objectives without requiring large training\ndatasets. In this work, we present ATOMIC (Autonomous Technology for Optical\nMicroscopy & Intelligent Characterization), an end-to-end framework that\nintegrates foundation models to enable fully autonomous, zero-shot\ncharacterization of 2D materials. Our system integrates the vision foundation\nmodel (i.e., Segment Anything Model), large language models (i.e., ChatGPT),\nunsupervised clustering, and topological analysis to automate microscope\ncontrol, sample scanning, image segmentation, and intelligent analysis through\nprompt engineering, eliminating the need for additional training. When\nanalyzing typical MoS2 samples, our approach achieves 99.7% segmentation\naccuracy for single layer identification, which is equivalent to that of human\nexperts. In addition, the integrated model is able to detect grain boundary\nslits that are challenging to identify with human eyes. Furthermore, the system\nretains robust accuracy despite variable conditions including defocus, color\ntemperature fluctuations, and exposure variations. It is applicable to a broad\nspectrum of common 2D materials-including graphene, MoS2, WSe2, SnSe-regardless\nof whether they were fabricated via chemical vapor deposition or mechanical\nexfoliation. This work represents the implementation of foundation models to\nachieve autonomous analysis, establishing a scalable and data-efficient\ncharacterization paradigm that fundamentally transforms the approach to\nnanoscale materials research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
