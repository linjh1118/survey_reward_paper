{"id": "2504.10615", "pdf": "https://arxiv.org/pdf/2504.10615", "abs": "https://arxiv.org/abs/2504.10615", "authors": ["Thilo Hagendorff", "Sarah Fabi"], "title": "Beyond Chains of Thought: Benchmarking Latent-Space Reasoning Abilities in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) can perform reasoning computations both\ninternally within their latent space and externally by generating explicit\ntoken sequences like chains of thought. Significant progress in enhancing\nreasoning abilities has been made by scaling test-time compute. However,\nunderstanding and quantifying model-internal reasoning abilities - the\ninferential \"leaps\" models make between individual token predictions - remains\ncrucial. This study introduces a benchmark (n = 4,000 items) designed to\nquantify model-internal reasoning in different domains. We achieve this by\nhaving LLMs indicate the correct solution to reasoning problems not through\ndescriptive text, but by selecting a specific language of their initial\nresponse token that is different from English, the benchmark language. This not\nonly requires models to reason beyond their context window, but also to\noverrise their default tendency to respond in the same language as the prompt,\nthereby posing an additional cognitive strain. We evaluate a set of 18 LLMs,\nshowing significant performance variations, with GPT-4.5 achieving the highest\naccuracy (74.7%), outperforming models like Grok-2 (67.2%), and Llama 3.1 405B\n(65.6%). Control experiments and difficulty scaling analyses suggest that while\nLLMs engage in internal reasoning, we cannot rule out heuristic exploitations\nunder certain conditions, marking an area for future investigation. Our\nexperiments demonstrate that LLMs can \"think\" via latent-space computations,\nrevealing model-internal inference strategies that need further understanding,\nespecially regarding safety-related concerns such as covert planning,\ngoal-seeking, or deception emerging without explicit token traces.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "test-time compute"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety", "accuracy"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11393", "pdf": "https://arxiv.org/pdf/2504.11393", "abs": "https://arxiv.org/abs/2504.11393", "authors": ["Ian Magnusson", "Nguyen Tai", "Ben Bogin", "David Heineman", "Jena D. Hwang", "Luca Soldaini", "Akshita Bhagia", "Jiacheng Liu", "Dirk Groeneveld", "Oyvind Tafjord", "Noah A. Smith", "Pang Wei Koh", "Jesse Dodge"], "title": "DataDecide: How to Predict Best Pretraining Data with Small Experiments", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Because large language models are expensive to pretrain on different\ndatasets, using smaller-scale experiments to decide on data is crucial for\nreducing costs. Which benchmarks and methods of making decisions from observed\nperformance at small scale most accurately predict the datasets that yield the\nbest large models? To empower open exploration of this question, we release\nmodels, data, and evaluations in DataDecide -- the most extensive open suite of\nmodels over differences in data and scale. We conduct controlled pretraining\nexperiments across 25 corpora with differing sources, deduplication, and\nfiltering up to 100B tokens, model sizes up to 1B parameters, and 3 random\nseeds. We find that the ranking of models at a single, small size (e.g., 150M\nparameters) is a strong baseline for predicting best models at our larger\ntarget scale (1B) (~80% of com parisons correct). No scaling law methods among\n8 baselines exceed the compute-decision frontier of single-scale predictions,\nbut DataDecide can measure improvement in future scaling laws. We also identify\nthat using continuous likelihood metrics as proxies in small experiments makes\nbenchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable\nat the target 1B scale with just 0.01% of the compute.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale", "scaling law"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10514", "pdf": "https://arxiv.org/pdf/2504.10514", "abs": "https://arxiv.org/abs/2504.10514", "authors": ["Yijun Liang", "Ming Li", "Chenrui Fan", "Ziyue Li", "Dang Nguyen", "Kwesi Cobbina", "Shweta Bhardwaj", "Jiuhai Chen", "Fuxiao Liu", "Tianyi Zhou"], "title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "33 pages, including references and appendix. Code is available at\n  https://github.com/tianyi-lab/ColorBench", "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scaling law"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11186", "pdf": "https://arxiv.org/pdf/2504.11186", "abs": "https://arxiv.org/abs/2504.11186", "authors": ["Minjie Zou", "Sahana Srinivasan", "Thaddaeus Wai Soon Lo", "Ke Zou", "Gabriel Dawei Yang", "Xuguang Ai", "Hyunjae Kim", "Maxwell Singer", "Fares Antaki", "Kelvin Li", "Robert Chang", "Marcus Tan", "David Ziyou Chen", "Dianbo Liu", "Qingyu Chen", "Yih Chung Tham"], "title": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items", "categories": ["cs.CL", "cs.AI"], "comment": "83 pages, 6 figures, 3 tables, 9 supplementary figures, 7\n  supplementary tables", "summary": "Recent advances in reasoning-focused large language models (LLMs) mark a\nshift from general LLMs toward models designed for complex decision-making, a\ncrucial aspect in medicine. However, their performance in specialized domains\nlike ophthalmology remains underexplored. This study comprehensively evaluated\nand compared the accuracy and reasoning capabilities of four newly developed\nreasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0\nFlash-Thinking. Each model was assessed using 5,888 multiple-choice\nophthalmology exam questions from the MedMCQA dataset in zero-shot setting.\nQuantitative evaluation included accuracy, Macro-F1, and five text-generation\nmetrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed\nagainst ground-truth reasonings. Average inference time was recorded for a\nsubset of 100 randomly selected questions. Additionally, two board-certified\nophthalmologists qualitatively assessed clarity, completeness, and reasoning\nstructure of responses to differential diagnosis questions.O1 (0.902) and\nDeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in\nMacro-F1 (0.900). The performance of models across the text-generation metrics\nvaried: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1\nand o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0\nFlash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and\no1 (0.176) led AlignScore. Inference time across the models varied, with\nDeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest\n(6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0\nFlash-Thinking tended to provide detailed and comprehensive intermediate\nreasoning, whereas o1 and o3-mini displayed concise and summarized\njustifications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "o1"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10514", "pdf": "https://arxiv.org/pdf/2504.10514", "abs": "https://arxiv.org/abs/2504.10514", "authors": ["Yijun Liang", "Ming Li", "Chenrui Fan", "Ziyue Li", "Dang Nguyen", "Kwesi Cobbina", "Shweta Bhardwaj", "Jiuhai Chen", "Fuxiao Liu", "Tianyi Zhou"], "title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "33 pages, including references and appendix. Code is available at\n  https://github.com/tianyi-lab/ColorBench", "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scaling law"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10816", "pdf": "https://arxiv.org/pdf/2504.10816", "abs": "https://arxiv.org/abs/2504.10816", "authors": ["Zhichao Xu", "Aosong Feng", "Yijun Tian", "Haibo Ding", "Lin Leee Cheong"], "title": "CSPLADE: Learned Sparse Retrieval with Causal Language Models", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "In recent years, dense retrieval has been the focus of information retrieval\n(IR) research. While effective, dense retrieval produces uninterpretable dense\nvectors, and suffers from the drawback of large index size. Learned sparse\nretrieval (LSR) has emerged as promising alternative, achieving competitive\nretrieval performance while also being able to leverage the classical inverted\nindex data structure for efficient retrieval. However, limited works have\nexplored scaling LSR beyond BERT scale. In this work, we identify two\nchallenges in training large language models (LLM) for LSR: (1) training\ninstability during the early stage of contrastive training; (2) suboptimal\nperformance due to pre-trained LLM's unidirectional attention. To address these\nchallenges, we propose two corresponding techniques: (1) a lightweight\nadaptation training phase to eliminate training instability; (2) two model\nvariants to enable bidirectional information. With these techniques, we are\nable to train LSR models with 8B scale LLM, and achieve competitive retrieval\nperformance with reduced index size. Furthermore, we are among the first to\nanalyze the performance-efficiency tradeoff of LLM-based LSR model through the\nlens of model quantization. Our findings provide insights into adapting LLMs\nfor efficient retrieval modeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10893", "pdf": "https://arxiv.org/pdf/2504.10893", "abs": "https://arxiv.org/abs/2504.10893", "authors": ["Yize Zhang", "Tianshu Wang", "Sirui Chen", "Kun Wang", "Xingyu Zeng", "Hongyu Lin", "Xianpei Han", "Le Sun", "Chaochao Lu"], "title": "ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search", "categories": ["cs.AI", "cs.CL"], "comment": "Project homepage: https://opencausalab.github.io/ARise", "summary": "Large language models (LLMs) have demonstrated impressive capabilities and\nare receiving increasing attention to enhance their reasoning through scaling\ntest--time compute. However, their application in open--ended,\nknowledge--intensive, complex reasoning scenarios is still limited.\nReasoning--oriented methods struggle to generalize to open--ended scenarios due\nto implicit assumptions of complete world knowledge. Meanwhile,\nknowledge--augmented reasoning (KAR) methods fail to address two core\nchallenges: 1) error propagation, where errors in early steps cascade through\nthe chain, and 2) verification bottleneck, where the explore--exploit tradeoff\narises in multi--branch decision processes. To overcome these limitations, we\nintroduce ARise, a novel framework that integrates risk assessment of\nintermediate reasoning states with dynamic retrieval--augmented generation\n(RAG) within a Monte Carlo tree search paradigm. This approach enables\neffective construction and optimization of reasoning plans across multiple\nmaintained hypothesis branches. Experimental results show that ARise\nsignificantly outperforms the state--of--the--art KAR methods by up to 23.10%,\nand the latest RAG-equipped large reasoning models by up to 25.37%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "monte carlo tree search"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11239", "pdf": "https://arxiv.org/pdf/2504.11239", "abs": "https://arxiv.org/abs/2504.11239", "authors": ["Chang Yang", "Ruiyu Wang", "Junzhe Jiang", "Qi Jiang", "Qinggang Zhang", "Yanchen Deng", "Shuxin Li", "Shuyue Hu", "Bo Li", "Florian T. Pokorny", "Xiao Huang", "Xinrun Wang"], "title": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs", "categories": ["cs.AI", "cs.CL"], "comment": "Preliminary work, 10 pages for main text", "summary": "Reasoning is the fundamental capability of large language models (LLMs). Due\nto the rapid progress of LLMs, there are two main issues of current benchmarks:\ni) these benchmarks can be crushed in a short time (less than 1 year), and ii)\nthese benchmarks may be easily hacked. To handle these issues, we propose the\never-scalingness for building the benchmarks which are uncrushable, unhackable,\nauto-verifiable and general. This paper presents Nondeterministic\nPolynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark\nfor LLMs. Specifically, the NPPC has three main modules: i) npgym, which\nprovides a unified interface of 25 well-known NP-complete problems and can\ngenerate any number of instances with any levels of complexities, ii) npsolver:\nwhich provides a unified interface to evaluate the problem instances with both\nonline and offline models via APIs and local deployments, respectively, and\niii) npeval: which provides the comprehensive and ready-to-use tools to analyze\nthe performances of LLMs over different problems, the number of tokens, the aha\nmoments, the reasoning errors and the solution errors. Extensive experiments\nover widely-used LLMs demonstrate: i) NPPC can successfully decrease the\nperformances of advanced LLMs' performances to below 10%, demonstrating that\nNPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the\nmost powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and\no1/o3-mini in most NP-complete problems considered, and iii) the numbers of\ntokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and\nDeepSeek-R1, are observed first to increase and then decrease when the problem\ninstances become more and more difficult. We believe that NPPC is the first\never-scaling reasoning benchmark, serving as the uncrushable and unhackable\ntestbed for LLMs toward artificial general intelligence (AGI).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "o1"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "testbed"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11364", "pdf": "https://arxiv.org/pdf/2504.11364", "abs": "https://arxiv.org/abs/2504.11364", "authors": ["Tianwei Ni", "Allen Nie", "Sapana Chaudhary", "Yao Liu", "Huzefa Rangwala", "Rasool Fakoor"], "title": "Teaching Large Language Models to Reason through Learning and Forgetting", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it using both successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. While fine-tuning the model with these data might seem\nstraightforward, we identify a critical issue: the model's search capability\ntends to degrade rapidly if fine-tuning is performed naively. We show that this\ndegradation can be substantially mitigated by employing a smaller learning\nrate. Extensive experiments on the challenging Game-of-24 and Countdown\nmathematical reasoning benchmarks show that our approach not only outperforms\nboth standard fine-tuning and inference-time search baselines but also\nsignificantly reduces inference time by 180$\\times$.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "inference time"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10646", "pdf": "https://arxiv.org/pdf/2504.10646", "abs": "https://arxiv.org/abs/2504.10646", "authors": ["Saif Punjwani", "Larry Heck"], "title": "Weight-of-Thought Reasoning: Exploring Neural Network Weights for Enhanced LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities when prompted with strategies such as Chain-of-Thought (CoT).\nHowever, these approaches focus on token-level output without considering\ninternal weight dynamics. We introduce Weight-of-Thought (WoT) reasoning, a\nnovel approach that examines neural network weights before inference to\nidentify reasoning pathways. Unlike existing methods, WoT explores the weight\nspace through graph-based message passing, multi-step reasoning processes, and\nattention mechanisms. Our implementation creates an interconnected graph of\nreasoning nodes. Experiments on diverse reasoning tasks (syllogistic,\nmathematical, algebraic, combinatorial, and geometric) demonstrate that WoT\nachieves superior performance compared to traditional methods, particularly for\ncomplex problems. This approach leads to both improved performance and greater\ninterpretability of the reasoning process, offering a promising direction for\nenhancing LLM reasoning capabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10558", "pdf": "https://arxiv.org/pdf/2504.10558", "abs": "https://arxiv.org/abs/2504.10558", "authors": ["Hu Gao", "Depeng Dang"], "title": "Enhancing Image Restoration through Learning Context-Rich and Detail-Accurate Features", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2407.08950", "summary": "Image restoration involves recovering high-quality images from their\ncorrupted versions, requiring a nuanced balance between spatial details and\ncontextual information. While certain methods address this balance, they\npredominantly emphasize spatial aspects, neglecting frequency variation\ncomprehension. In this paper, we present a multi-scale design that optimally\nbalances these competing objectives, seamlessly integrating spatial and\nfrequency domain knowledge to selectively recover the most informative\ninformation. Specifically, we develop a hybrid scale frequency selection block\n(HSFSBlock), which not only captures multi-scale information from the spatial\ndomain, but also selects the most informative components for image restoration\nin the frequency domain. Furthermore, to mitigate the inherent noise introduced\nby skip connections employing only addition or concatenation, we introduce a\nskip connection attention mechanism (SCAM) to selectively determines the\ninformation that should propagate through skip connections. The resulting\ntightly interlinked architecture, named as LCDNet. Extensive experiments\nconducted across diverse image restoration tasks showcase that our model\nattains performance levels that are either superior or comparable to those of\nstate-of-the-art algorithms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10567", "pdf": "https://arxiv.org/pdf/2504.10567", "abs": "https://arxiv.org/abs/2504.10567", "authors": ["Yushu Wu", "Yanyu Li", "Ivan Skorokhodov", "Anil Kag", "Willi Menapace", "Sharath Girish", "Aliaksandr Siarohin", "Yanzhi Wang", "Sergey Tulyakov"], "title": "H3AE: High Compression, High Speed, and High Quality AutoEncoder for Video Diffusion Models", "categories": ["cs.CV", "eess.IV"], "comment": "8 pages, 4 figures, 6 tables", "summary": "Autoencoder (AE) is the key to the success of latent diffusion models for\nimage and video generation, reducing the denoising resolution and improving\nefficiency. However, the power of AE has long been underexplored in terms of\nnetwork design, compression ratio, and training strategy. In this work, we\nsystematically examine the architecture design choices and optimize the\ncomputation distribution to obtain a series of efficient and high-compression\nvideo AEs that can decode in real time on mobile devices. We also unify the\ndesign of plain Autoencoder and image-conditioned I2V VAE, achieving\nmultifunctionality in a single network. In addition, we find that the widely\nadopted discriminative losses, i.e., GAN, LPIPS, and DWT losses, provide no\nsignificant improvements when training AEs at scale. We propose a novel latent\nconsistency loss that does not require complicated discriminator design or\nhyperparameter tuning, but provides stable improvements in reconstruction\nquality. Our AE achieves an ultra-high compression ratio and real-time decoding\nspeed on mobile while outperforming prior art in terms of reconstruction\nmetrics by a large margin. We finally validate our AE by training a DiT on its\nlatent space and demonstrate fast, high-quality text-to-video generation\ncapability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10797", "pdf": "https://arxiv.org/pdf/2504.10797", "abs": "https://arxiv.org/abs/2504.10797", "authors": ["Annabella Sakunkoo", "Jonathan Sakunkoo"], "title": "Name of Thrones: Evaluating How LLMs Rank Student Names, Race, and Gender in Status Hierarchies", "categories": ["cs.CL", "cs.AI", "cs.HC", "H.5; J.4"], "comment": null, "summary": "Across cultures, names tell a lot about their bearers as they carry deep\npersonal and cultural significance. Names also serve as powerful signals of\ngender, race, and status in the social hierarchy - a pecking order in which\nindividual positions shape others' expectations on their perceived competence\nand worth. With the widespread adoption of LLMs and as names are often an input\nfor LLMs, it is crucial to evaluate whether LLMs may sort people into status\npositions based on first and last names and, if so, whether it is in an unfair,\nbiased fashion. While prior work has primarily investigated biases in first\nnames, little attention has been paid to last names and even less to the\ncombined effects of first and last names. In this study, we conduct a\nlarge-scale analysis of name variations across 5 ethnicities to examine how AI\nexhibits name biases. Our study investigates three key characteristics of\ninequality and finds that LLMs reflect and reinforce status hierarchies based\non names that signal gender and ethnicity as they encode differential\nexpectations of competence, leadership, and economic potential. Contrary to the\ncommon assumption that AI tends to favor Whites, we show that East and, in some\ncontexts, South Asian names receive higher rankings. We also disaggregate\nAsians, a population projected to be the largest immigrant group in the U.S. by\n2055. Our results challenge the monolithic Asian model minority assumption,\nillustrating a more complex and stratified model of bias. Gender moderates\nbiases, with girls facing unfair disadvantages in certain racial groups.\nAdditionally, spanning cultural categories by adopting Western first names\nimproves AI-perceived status for East and Southeast Asian students,\nparticularly for girls. Our findings underscore the importance of\nintersectional and more nuanced understandings of race, gender, and mixed\nidentities in the evaluation of LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10716", "pdf": "https://arxiv.org/pdf/2504.10716", "abs": "https://arxiv.org/abs/2504.10716", "authors": ["Stathis Galanakis", "Alexandros Lattas", "Stylianos Moschoglou", "Bernhard Kainz", "Stefanos Zafeiriou"], "title": "SpinMeRound: Consistent Multi-View Identity Generation Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent progress in diffusion models, generating realistic head\nportraits from novel viewpoints remains a significant challenge. Most current\napproaches are constrained to limited angular ranges, predominantly focusing on\nfrontal or near-frontal views. Moreover, although the recent emerging\nlarge-scale diffusion models have been proven robust in handling 3D scenes,\nthey underperform on facial data, given their complex structure and the uncanny\nvalley pitfalls. In this paper, we propose SpinMeRound, a diffusion-based\napproach designed to generate consistent and accurate head portraits from novel\nviewpoints. By leveraging a number of input views alongside an identity\nembedding, our method effectively synthesizes diverse viewpoints of a subject\nwhilst robustly maintaining its unique identity features. Through\nexperimentation, we showcase our model's generation capabilities in 360 head\nsynthesis, while beating current state-of-the-art multiview diffusion models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10906", "pdf": "https://arxiv.org/pdf/2504.10906", "abs": "https://arxiv.org/abs/2504.10906", "authors": ["Changjiang Gao", "Hankun Lin", "Shujian Huang", "Xin Huang", "Xue Han", "Junlan Feng", "Chao Deng", "Jiajun Chen"], "title": "Understanding LLMs' Cross-Lingual Context Retrieval: How Good It Is And Where It Comes From", "categories": ["cs.CL"], "comment": null, "summary": "The ability of cross-lingual context retrieval is a fundamental aspect of\ncross-lingual alignment of large language models (LLMs), where the model\nextracts context information in one language based on requests in another\nlanguage. Despite its importance in real-life applications, this ability has\nnot been adequately investigated for state-of-the-art models. In this paper, we\nevaluate the cross-lingual context retrieval ability of over 40 LLMs across 12\nlanguages to understand the source of this ability, using cross-lingual machine\nreading comprehension (xMRC) as a representative scenario. Our results show\nthat several small, post-trained open LLMs show strong cross-lingual context\nretrieval ability, comparable to closed-source LLMs such as GPT-4o, and their\nestimated oracle performances greatly improve after post-training. Our\ninterpretability analysis shows that the cross-lingual context retrieval\nprocess can be divided into two main phases: question encoding and answer\nretrieval, which are formed in pre-training and post-training, respectively.\nThe phasing stability correlates with xMRC performance, and the xMRC bottleneck\nlies at the last model layers in the second phase, where the effect of\npost-training can be evidently observed. Our results also indicate that\nlarger-scale pretraining cannot improve the xMRC performance. Instead, larger\nLLMs need further multilingual post-training to fully unlock their\ncross-lingual context retrieval potential. Our code and is available at\nhttps://github.com/NJUNLP/Cross-Lingual-Context-Retrieval", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10982", "pdf": "https://arxiv.org/pdf/2504.10982", "abs": "https://arxiv.org/abs/2504.10982", "authors": ["Yingjian Chen", "Feiyang Li", "Xingyu Song", "Tianxiao Li", "Issey Sudeka", "Irene Li"], "title": "Exploring the Role of KG-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10765", "pdf": "https://arxiv.org/pdf/2504.10765", "abs": "https://arxiv.org/abs/2504.10765", "authors": ["Jeremy Klotz", "Shree K. Nayar"], "title": "Minimal Sensing for Orienting a Solar Panel", "categories": ["cs.CV"], "comment": "12 pages, 9 figures", "summary": "A solar panel harvests the most energy when pointing in the direction that\nmaximizes the total illumination (irradiance) falling on it. Given an arbitrary\norientation of a panel and an arbitrary environmental illumination, we address\nthe problem of finding the direction of maximum total irradiance. We develop a\nminimal sensing approach where measurements from just four photodetectors are\nused to iteratively vary the tilt of the panel to maximize the irradiance. Many\nenvironments produce irradiance functions with multiple local maxima. As a\nresult, simply measuring the gradient of the irradiance function and applying\ngradient ascent will not work. We show that a larger, optimized tilt between\nthe detectors and the panel is equivalent to blurring the irradiance function.\nThis has the effect of eliminating local maxima and turning the irradiance\nfunction into a unimodal one, whose maximum can be found using gradient ascent.\nWe show that there is a close relationship between our approach and scale space\ntheory. We have collected a large dataset of high-dynamic range lighting\nenvironments in New York City, called \\textit{UrbanSky}. We used this dataset\nto conduct simulations to verify the robustness of our approach. Finally, we\nhave built a portable solar panel with four compact detectors and an actuator\nto conduct experiments in various real-world settings: direct sunlight, cloudy\nsky, urban settings with occlusions and shadows, and complex indoor lighting.\nIn all cases, we show significant improvements in harvested energy compared to\nstandard approaches for controlling the orientation of a solar panel.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10804", "pdf": "https://arxiv.org/pdf/2504.10804", "abs": "https://arxiv.org/abs/2504.10804", "authors": ["Jiani Liu", "Zhiyuan Wang", "Zeliang Zhang", "Chao Huang", "Susan Liang", "Yunlong Tang", "Chenliang Xu"], "title": "The Sword of Damocles in ViTs: Computational Redundancy Amplifies Adversarial Transferability", "categories": ["cs.CV"], "comment": "Work in progress. 10 pages. 4 figures", "summary": "Vision Transformers (ViTs) have demonstrated impressive performance across a\nrange of applications, including many safety-critical tasks. However, their\nunique architectural properties raise new challenges and opportunities in\nadversarial robustness. In particular, we observe that adversarial examples\ncrafted on ViTs exhibit higher transferability compared to those crafted on\nCNNs, suggesting that ViTs contain structural characteristics favorable for\ntransferable attacks. In this work, we investigate the role of computational\nredundancy in ViTs and its impact on adversarial transferability. Unlike prior\nstudies that aim to reduce computation for efficiency, we propose to exploit\nthis redundancy to improve the quality and transferability of adversarial\nexamples. Through a detailed analysis, we identify two forms of redundancy,\nincluding the data-level and model-level, that can be harnessed to amplify\nattack effectiveness. Building on this insight, we design a suite of\ntechniques, including attention sparsity manipulation, attention head\npermutation, clean token regularization, ghost MoE diversification, and\ntest-time adversarial training. Extensive experiments on the ImageNet-1k\ndataset validate the effectiveness of our approach, showing that our methods\nsignificantly outperform existing baselines in both transferability and\ngenerality across diverse model architectures.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10810", "pdf": "https://arxiv.org/pdf/2504.10810", "abs": "https://arxiv.org/abs/2504.10810", "authors": ["Anmol Singhal Navya Singhal"], "title": "PatrolVision: Automated License Plate Recognition in the wild", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted in IEEE Southeast Con 2025. To be published in IEEEXplore", "summary": "Adoption of AI driven techniques in public services remains low due to\nchallenges related to accuracy and speed of information at population scale.\nComputer vision techniques for traffic monitoring have not gained much\npopularity despite their relative strength in areas such as autonomous driving.\nDespite large number of academic methods for Automatic License Plate\nRecognition (ALPR) systems, very few provide an end to end solution for\npatrolling in the city. This paper presents a novel prototype for a low power\nGPU based patrolling system to be deployed in an urban environment on\nsurveillance vehicles for automated vehicle detection, recognition and\ntracking. In this work, we propose a complete ALPR system for Singapore license\nplates having both single and double line creating our own YOLO based network.\nWe focus on unconstrained capture scenarios as would be the case in real world\napplication, where the license plate (LP) might be considerably distorted due\nto oblique views. In this work, we first detect the license plate from the full\nimage using RFB-Net and rectify multiple distorted license plates in a single\nimage. After that, the detected license plate image is fed to our network for\ncharacter recognition. We evaluate the performance of our proposed system on a\nnewly built dataset covering more than 16,000 images. The system was able to\ncorrectly detect license plates with 86\\% precision and recognize characters of\na license plate in 67\\% of the test set, and 89\\% accuracy with one incorrect\ncharacter (partial match). We also test latency of our system and achieve 64FPS\non Tesla P4 GPU", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10822", "pdf": "https://arxiv.org/pdf/2504.10822", "abs": "https://arxiv.org/abs/2504.10822", "authors": ["Janna Bruner", "Amit Moryossef", "Lior Wolf"], "title": "IlluSign: Illustrating Sign Language Videos by Leveraging the Attention Mechanism", "categories": ["cs.CV"], "comment": null, "summary": "Sign languages are dynamic visual languages that involve hand gestures, in\ncombination with non manual elements such as facial expressions. While video\nrecordings of sign language are commonly used for education and documentation,\nthe dynamic nature of signs can make it challenging to study them in detail,\nespecially for new learners and educators. This work aims to convert sign\nlanguage video footage into static illustrations, which serve as an additional\neducational resource to complement video content. This process is usually done\nby an artist, and is therefore quite costly. We propose a method that\nillustrates sign language videos by leveraging generative models' ability to\nunderstand both the semantic and geometric aspects of images. Our approach\nfocuses on transferring a sketch like illustration style to video footage of\nsign language, combining the start and end frames of a sign into a single\nillustration, and using arrows to highlight the hand's direction and motion.\nWhile many style transfer methods address domain adaptation at varying levels\nof abstraction, applying a sketch like style to sign languages, especially for\nhand gestures and facial expressions, poses a significant challenge. To tackle\nthis, we intervene in the denoising process of a diffusion model, injecting\nstyle as keys and values into high resolution attention layers, and fusing\ngeometric information from the image and edges as queries. For the final\nillustration, we use the attention mechanism to combine the attention weights\nfrom both the start and end illustrations, resulting in a soft combination. Our\nmethod offers a cost effective solution for generating sign language\nillustrations at inference time, addressing the lack of such resources in\neducational materials.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10829", "pdf": "https://arxiv.org/pdf/2504.10829", "abs": "https://arxiv.org/abs/2504.10829", "authors": ["Hengyu Shi", "Junhao Su", "Huansheng Ning", "Xiaoming Wei", "Jialin Gao"], "title": "LayoutCoT: Unleashing the Deep Reasoning Potential of Large Language Models for Layout Generation", "categories": ["cs.CV"], "comment": null, "summary": "Conditional layout generation aims to automatically generate visually\nappealing and semantically coherent layouts from user-defined constraints.\nWhile recent methods based on generative models have shown promising results,\nthey typically require substantial amounts of training data or extensive\nfine-tuning, limiting their versatility and practical applicability.\nAlternatively, some training-free approaches leveraging in-context learning\nwith Large Language Models (LLMs) have emerged, but they often suffer from\nlimited reasoning capabilities and overly simplistic ranking mechanisms, which\nrestrict their ability to generate consistently high-quality layouts. To this\nend, we propose LayoutCoT, a novel approach that leverages the reasoning\ncapabilities of LLMs through a combination of Retrieval-Augmented Generation\n(RAG) and Chain-of-Thought (CoT) techniques. Specifically, LayoutCoT transforms\nlayout representations into a standardized serialized format suitable for\nprocessing by LLMs. A Layout-aware RAG is used to facilitate effective\nretrieval and generate a coarse layout by LLMs. This preliminary layout,\ntogether with the selected exemplars, is then fed into a specially designed CoT\nreasoning module for iterative refinement, significantly enhancing both\nsemantic coherence and visual quality. We conduct extensive experiments on five\npublic datasets spanning three conditional layout generation tasks.\nExperimental results demonstrate that LayoutCoT achieves state-of-the-art\nperformance without requiring training or fine-tuning. Notably, our CoT\nreasoning module enables standard LLMs, even those without explicit deep\nreasoning abilities, to outperform specialized deep-reasoning models such as\ndeepseek-R1, highlighting the potential of our approach in unleashing the deep\nreasoning capabilities of LLMs for layout generation tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10834", "pdf": "https://arxiv.org/pdf/2504.10834", "abs": "https://arxiv.org/abs/2504.10834", "authors": ["Sihang Chen", "Lijun Yun", "Ze Liu", "JianFeng Zhu", "Jie Chen", "Hui Wang", "Yueping Nie"], "title": "LightFormer: A lightweight and efficient decoder for remote sensing image segmentation", "categories": ["cs.CV"], "comment": "26 pages, 69 figures", "summary": "Deep learning techniques have achieved remarkable success in the semantic\nsegmentation of remote sensing images and in land-use change detection.\nNevertheless, their real-time deployment on edge platforms remains constrained\nby decoder complexity. Herein, we introduce LightFormer, a lightweight decoder\nfor time-critical tasks that involve unstructured targets, such as disaster\nassessment, unmanned aerial vehicle search-and-rescue, and cultural heritage\nmonitoring. LightFormer employs a feature-fusion and refinement module built on\nchannel processing and a learnable gating mechanism to aggregate multi-scale,\nmulti-range information efficiently, which drastically curtails model\ncomplexity. Furthermore, we propose a spatial information selection module\n(SISM) that integrates long-range attention with a detail preservation branch\nto capture spatial dependencies across multiple scales, thereby substantially\nimproving the recognition of unstructured targets in complex scenes. On the\nISPRS Vaihingen benchmark, LightFormer attains 99.9% of GLFFNet's mIoU (83.9%\nvs. 84.0%) while requiring only 14.7% of its FLOPs and 15.9% of its parameters,\nthus achieving an excellent accuracy-efficiency trade-off. Consistent results\non LoveDA, ISPRS Potsdam, RescueNet, and FloodNet further demonstrate its\nrobustness and superior perception of unstructured objects. These findings\nhighlight LightFormer as a practical solution for remote sensing applications\nwhere both computational economy and high-precision segmentation are\nimperative.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10871", "pdf": "https://arxiv.org/pdf/2504.10871", "abs": "https://arxiv.org/abs/2504.10871", "authors": ["Tianpei Zhang", "Jufeng Zhao", "Yiming Zhu", "Guangmang Cui", "Yuxin Jing", "Yuhan Lyu"], "title": "DAAF:Degradation-Aware Adaptive Fusion Framework for Robust Infrared and Visible Images Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Existing infrared and visible image fusion(IVIF) algorithms often prioritize\nhigh-quality images, neglecting image degradation such as low light and noise,\nwhich limits the practical potential. This paper propose Degradation-Aware\nAdaptive image Fusion (DAAF), which achieves unified modeling of adaptive\ndegradation optimization and image fusion. Specifically, DAAF comprises an\nauxiliary Adaptive Degradation Optimization Network (ADON) and a Feature\nInteractive Local-Global Fusion (FILGF) Network. Firstly, ADON includes\ninfrared and visible-light branches. Within the infrared branch,\nfrequency-domain feature decomposition and extraction are employed to isolate\nGaussian and stripe noise. In the visible-light branch, Retinex decomposition\nis applied to extract illumination and reflectance components, enabling\ncomplementary enhancement of detail and illumination distribution.\nSubsequently, FILGF performs interactive multi-scale local-global feature\nfusion. Local feature fusion consists of intra-inter model feature complement,\nwhile global feature fusion is achieved through a interactive cross-model\nattention. Extensive experiments have shown that DAAF outperforms current IVIF\nalgorithms in normal and complex degradation scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11431", "pdf": "https://arxiv.org/pdf/2504.11431", "abs": "https://arxiv.org/abs/2504.11431", "authors": ["Maria Teleki", "Xiangjue Dong", "Haoran Liu", "James Caverlee"], "title": "Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "comment": "To appear in ICWSM 2025", "summary": "Masculine defaults are widely recognized as a significant type of gender\nbias, but they are often unseen as they are under-researched. Masculine\ndefaults involve three key parts: (i) the cultural context, (ii) the masculine\ncharacteristics or behaviors, and (iii) the reward for, or simply acceptance\nof, those masculine characteristics or behaviors. In this work, we study\ndiscourse-based masculine defaults, and propose a twofold framework for (i) the\nlarge-scale discovery and analysis of gendered discourse words in spoken\ncontent via our Gendered Discourse Correlation Framework (GDCF); and (ii) the\nmeasurement of the gender bias associated with these gendered discourse words\nin LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus\nour study on podcasts, a popular and growing form of social media, analyzing\n15,117 podcast episodes. We analyze correlations between gender and discourse\nwords -- discovered via LDA and BERTopic -- to automatically form gendered\ndiscourse word lists. We then study the prevalence of these gendered discourse\nwords in domain-specific contexts, and find that gendered discourse-based\nmasculine defaults exist in the domains of business, technology/politics, and\nvideo games. Next, we study the representation of these gendered discourse\nwords from a state-of-the-art LLM embedding model from OpenAI, and find that\nthe masculine discourse words have a more stable and robust representation than\nthe feminine discourse words, which may result in better system performance on\ndownstream tasks for men. Hence, men are rewarded for their discourse patterns\nwith better system performance by one of the state-of-the-art language models\n-- and this embedding disparity is a representational harm and a masculine\ndefault.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10877", "pdf": "https://arxiv.org/pdf/2504.10877", "abs": "https://arxiv.org/abs/2504.10877", "authors": ["Soheil Gharatappeh", "Salimeh Sekeh", "Vikas Dhiman"], "title": "Weather-Aware Object Detection Transformer for Domain Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "RT-DETRs have shown strong performance across various computer vision tasks\nbut are known to degrade under challenging weather conditions such as fog. In\nthis work, we investigate three novel approaches to enhance RT-DETR robustness\nin foggy environments: (1) Domain Adaptation via Perceptual Loss, which\ndistills domain-invariant features from a teacher network to a student using\nperceptual supervision; (2) Weather Adaptive Attention, which augments the\nattention mechanism with fog-sensitive scaling by introducing an auxiliary\nfoggy image stream; and (3) Weather Fusion Encoder, which integrates a\ndual-stream encoder architecture that fuses clear and foggy image features via\nmulti-head self and cross-attention. Despite the architectural innovations,\nnone of the proposed methods consistently outperform the baseline RT-DETR. We\nanalyze the limitations and potential causes, offering insights for future\nresearch in weather-aware object detection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11456", "pdf": "https://arxiv.org/pdf/2504.11456", "abs": "https://arxiv.org/abs/2504.11456", "authors": ["Zhiwei He", "Tian Liang", "Jiahao Xu", "Qiuzhi Liu", "Xingyu Chen", "Yue Wang", "Linfeng Song", "Dian Yu", "Zhenwen Liang", "Wenxuan Wang", "Zhuosheng Zhang", "Rui Wang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "WIP", "summary": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "mathematical reasoning"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10888", "pdf": "https://arxiv.org/pdf/2504.10888", "abs": "https://arxiv.org/abs/2504.10888", "authors": ["Jiahuan Long", "Wen Yao", "Tingsong Jiang", "Chao Ma"], "title": "CDUPatch: Color-Driven Universal Adversarial Patch Attack for Dual-Modal Visible-Infrared Detectors", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adversarial patches are widely used to evaluate the robustness of object\ndetection systems in real-world scenarios. These patches were initially\ndesigned to deceive single-modal detectors (e.g., visible or infrared) and have\nrecently been extended to target visible-infrared dual-modal detectors.\nHowever, existing dual-modal adversarial patch attacks have limited attack\neffectiveness across diverse physical scenarios. To address this, we propose\nCDUPatch, a universal cross-modal patch attack against visible-infrared object\ndetectors across scales, views, and scenarios. Specifically, we observe that\ncolor variations lead to different levels of thermal absorption, resulting in\ntemperature differences in infrared imaging. Leveraging this property, we\npropose an RGB-to-infrared adapter that maps RGB patches to infrared patches,\nenabling unified optimization of cross-modal patches. By learning an optimal\ncolor distribution on the adversarial patch, we can manipulate its thermal\nresponse and generate an adversarial infrared texture. Additionally, we\nintroduce a multi-scale clipping strategy and construct a new visible-infrared\ndataset, MSDrone, which contains aerial vehicle images in varying scales and\nperspectives. These data augmentation strategies enhance the robustness of our\npatch in real-world conditions. Experiments on four benchmark datasets (e.g.,\nDroneVehicle, LLVIP, VisDrone, MSDrone) show that our method outperforms\nexisting patch attacks in the digital domain. Extensive physical tests further\nconfirm strong transferability across scales, views, and scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10512", "pdf": "https://arxiv.org/pdf/2504.10512", "abs": "https://arxiv.org/abs/2504.10512", "authors": ["Minh-Anh Nguyen", "Dung D. Le"], "title": "JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Language representation learning has emerged as a promising approach for\nsequential recommendation, thanks to its ability to learn generalizable\nrepresentations. However, despite its advantages, this approach still struggles\nwith data sparsity and a limited understanding of common-sense user\npreferences. To address these limitations, we propose $\\textbf{JEPA4Rec}$, a\nframework that combines $\\textbf{J}$oint $\\textbf{E}$mbedding\n$\\textbf{P}$redictive $\\textbf{A}$rchitecture with language modeling of item\ntextual descriptions. JEPA4Rec captures semantically rich and transferable\nrepresentations, improving recommendation performance and reducing reliance on\nlarge-scale pre-training data. Specifically, JEPA4Rec represents items as text\nsentences by flattening descriptive information such as $\\textit{title,\ncategory}$, and other attributes. To encode these sentences, we employ a\nbidirectional Transformer encoder with modified embedding layers tailored for\ncapturing item information in recommendation datasets. We apply masking to text\nsentences and use them to predict the representations of the unmasked\nsentences, helping the model learn generalizable item embeddings. To further\nimprove recommendation performance and language understanding, we employ a\ntwo-stage training strategy incorporating self-supervised learning losses.\nExperiments on six real-world datasets demonstrate that JEPA4Rec consistently\noutperforms state-of-the-art methods, particularly in cross-domain,\ncross-platform, and low-resource scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10905", "pdf": "https://arxiv.org/pdf/2504.10905", "abs": "https://arxiv.org/abs/2504.10905", "authors": ["Yukang Lin", "Yan Hong", "Zunnan Xu", "Xindi Li", "Chao Xu", "Chuanbiao Song", "Ronghui Li", "Haoxing Chen", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Jianfu Zhang", "Xiu Li"], "title": "InterAnimate: Taming Region-aware Diffusion Model for Realistic Human Interaction Animation", "categories": ["cs.CV", "cs.HC"], "comment": "under preview", "summary": "Recent video generation research has focused heavily on isolated actions,\nleaving interactive motions-such as hand-face interactions-largely unexamined.\nThese interactions are essential for emerging biometric authentication systems,\nwhich rely on interactive motion-based anti-spoofing approaches. From a\nsecurity perspective, there is a growing need for large-scale, high-quality\ninteractive videos to train and strengthen authentication models. In this work,\nwe introduce a novel paradigm for animating realistic hand-face interactions.\nOur approach simultaneously learns spatio-temporal contact dynamics and\nbiomechanically plausible deformation effects, enabling natural interactions\nwhere hand movements induce anatomically accurate facial deformations while\nmaintaining collision-free contact. To facilitate this research, we present\nInterHF, a large-scale hand-face interaction dataset featuring 18 interaction\npatterns and 90,000 annotated videos. Additionally, we propose InterAnimate, a\nregion-aware diffusion model designed specifically for interaction animation.\nInterAnimate leverages learnable spatial and temporal latents to effectively\ncapture dynamic interaction priors and integrates a region-aware interaction\nmechanism that injects these priors into the denoising process. To the best of\nour knowledge, this work represents the first large-scale effort to\nsystematically study human hand-face interactions. Qualitative and quantitative\nresults show InterAnimate produces highly realistic animations, setting a new\nbenchmark. Code and data will be made public to advance research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10519", "pdf": "https://arxiv.org/pdf/2504.10519", "abs": "https://arxiv.org/abs/2504.10519", "authors": ["Yuhang Yao", "Haixin Wang", "Yibo Chen", "Jiawen Wang", "Min Chang Jordan Ren", "Bosheng Ding", "Salman Avestimehr", "Chaoyang He"], "title": "Toward Super Agent System with Hybrid AI Routers", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "AI Agents powered by Large Language Models are transforming the world through\nenormous applications. A super agent has the potential to fulfill diverse user\nneeds, such as summarization, coding, and research, by accurately understanding\nuser intent and leveraging the appropriate tools to solve tasks. However, to\nmake such an agent viable for real-world deployment and accessible at scale,\nsignificant optimizations are required to ensure high efficiency and low cost.\nThis paper presents a design of the Super Agent System. Upon receiving a user\nprompt, the system first detects the intent of the user, then routes the\nrequest to specialized task agents with the necessary tools or automatically\ngenerates agentic workflows. In practice, most applications directly serve as\nAI assistants on edge devices such as phones and robots. As different language\nmodels vary in capability and cloud-based models often entail high\ncomputational costs, latency, and privacy concerns, we then explore the hybrid\nmode where the router dynamically selects between local and cloud models based\non task complexity. Finally, we introduce the blueprint of an on-device super\nagent enhanced with cloud. With advances in multi-modality models and edge\nhardware, we envision that most computations can be handled locally, with cloud\ncollaboration only as needed. Such architecture paves the way for super agents\nto be seamlessly integrated into everyday life in the near future.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10920", "pdf": "https://arxiv.org/pdf/2504.10920", "abs": "https://arxiv.org/abs/2504.10920", "authors": ["Peipei Song", "Long Zhang", "Long Lan", "Weidong Chen", "Dan Guo", "Xun Yang", "Meng Wang"], "title": "Towards Efficient Partially Relevant Video Retrieval with Active Moment Discovering", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM) on January 19,\n  2025. The code is available at https://github.com/songpipi/AMDNet", "summary": "Partially relevant video retrieval (PRVR) is a practical yet challenging task\nin text-to-video retrieval, where videos are untrimmed and contain much\nbackground content. The pursuit here is of both effective and efficient\nsolutions to capture the partial correspondence between text queries and\nuntrimmed videos. Existing PRVR methods, which typically focus on modeling\nmulti-scale clip representations, however, suffer from content independence and\ninformation redundancy, impairing retrieval performance. To overcome these\nlimitations, we propose a simple yet effective approach with active moment\ndiscovering (AMDNet). We are committed to discovering video moments that are\nsemantically consistent with their queries. By using learnable span anchors to\ncapture distinct moments and applying masked multi-moment attention to\nemphasize salient moments while suppressing redundant backgrounds, we achieve\nmore compact and informative video representations. To further enhance moment\nmodeling, we introduce a moment diversity loss to encourage different moments\nof distinct regions and a moment relevance loss to promote semantically\nquery-relevant moments, which cooperate with a partially relevant retrieval\nloss for end-to-end optimization. Extensive experiments on two large-scale\nvideo datasets (\\ie, TVR and ActivityNet Captions) demonstrate the superiority\nand efficiency of our AMDNet. In particular, AMDNet is about 15.5 times smaller\n(\\#parameters) while 6.0 points higher (SumR) than the up-to-date method\nGMMFormer on TVR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10967", "pdf": "https://arxiv.org/pdf/2504.10967", "abs": "https://arxiv.org/abs/2504.10967", "authors": ["Yubin Gu", "Yuan Meng", "Kaihang Zheng", "Xiaoshuai Sun", "Jiayi Ji", "Weijian Ruan", "Liujuan Cao", "Rongrong Ji"], "title": "An Efficient and Mixed Heterogeneous Model for Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Image restoration~(IR), as a fundamental multimedia data processing task, has\na significant impact on downstream visual applications. In recent years,\nresearchers have focused on developing general-purpose IR models capable of\nhandling diverse degradation types, thereby reducing the cost and complexity of\nmodel development. Current mainstream approaches are based on three\narchitectural paradigms: CNNs, Transformers, and Mambas. CNNs excel in\nefficient inference, whereas Transformers and Mamba excel at capturing\nlong-range dependencies and modeling global contexts. While each architecture\nhas demonstrated success in specialized, single-task settings, limited efforts\nhave been made to effectively integrate heterogeneous architectures to jointly\naddress diverse IR challenges. To bridge this gap, we propose RestorMixer, an\nefficient and general-purpose IR model based on mixed-architecture fusion.\nRestorMixer adopts a three-stage encoder-decoder structure, where each stage is\ntailored to the resolution and feature characteristics of the input. In the\ninitial high-resolution stage, CNN-based blocks are employed to rapidly extract\nshallow local features. In the subsequent stages, we integrate a refined\nmulti-directional scanning Mamba module with a multi-scale window-based\nself-attention mechanism. This hierarchical and adaptive design enables the\nmodel to leverage the strengths of CNNs in local feature extraction, Mamba in\nglobal context modeling, and attention mechanisms in dynamic feature\nrefinement. Extensive experimental results demonstrate that RestorMixer\nachieves leading performance across multiple IR tasks while maintaining high\ninference efficiency. The official code can be accessed at\nhttps://github.com/ClimBin/RestorMixer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10979", "pdf": "https://arxiv.org/pdf/2504.10979", "abs": "https://arxiv.org/abs/2504.10979", "authors": ["Pancheng Zhao", "Deng-Ping Fan", "Shupeng Cheng", "Salman Khan", "Fahad Shahbaz Khan", "David Clifton", "Peng Xu", "Jufeng Yang"], "title": "Deep Learning in Concealed Dense Prediction", "categories": ["cs.CV"], "comment": "Technique Report", "summary": "Deep learning is developing rapidly and handling common computer vision tasks\nwell. It is time to pay attention to more complex vision tasks, as model size,\nknowledge, and reasoning capabilities continue to improve. In this paper, we\nintroduce and review a family of complex tasks, termed Concealed Dense\nPrediction (CDP), which has great value in agriculture, industry, etc. CDP's\nintrinsic trait is that the targets are concealed in their surroundings, thus\nfully perceiving them requires fine-grained representations, prior knowledge,\nauxiliary reasoning, etc. The contributions of this review are three-fold: (i)\nWe introduce the scope, characteristics, and challenges specific to CDP tasks\nand emphasize their essential differences from generic vision tasks. (ii) We\ndevelop a taxonomy based on concealment counteracting to summarize deep\nlearning efforts in CDP through experiments on three tasks. We compare 25\nstate-of-the-art methods across 12 widely used concealed datasets. (iii) We\ndiscuss the potential applications of CDP in the large model era and summarize\n6 potential research directions. We offer perspectives for the future\ndevelopment of CDP by constructing a large-scale multimodal instruction\nfine-tuning dataset, CvpINST, and a concealed visual perception agent,\nCvpAgent.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10985", "pdf": "https://arxiv.org/pdf/2504.10985", "abs": "https://arxiv.org/abs/2504.10985", "authors": ["Minghui Lin", "Shu Wang", "Xiang Wang", "Jianhua Tang", "Longbin Fu", "Zhengrong Zuo", "Nong Sang"], "title": "DMPT: Decoupled Modality-aware Prompt Tuning for Multi-modal Object Re-identification", "categories": ["cs.CV"], "comment": "2025 IEEE/CVF Winter Conference on Applications of Computer Vision\n  (WACV)", "summary": "Current multi-modal object re-identification approaches based on large-scale\npre-trained backbones (i.e., ViT) have displayed remarkable progress and\nachieved excellent performance. However, these methods usually adopt the\nstandard full fine-tuning paradigm, which requires the optimization of\nconsiderable backbone parameters, causing extensive computational and storage\nrequirements. In this work, we propose an efficient prompt-tuning framework\ntailored for multi-modal object re-identification, dubbed DMPT, which freezes\nthe main backbone and only optimizes several newly added decoupled\nmodality-aware parameters. Specifically, we explicitly decouple the visual\nprompts into modality-specific prompts which leverage prior modality knowledge\nfrom a powerful text encoder and modality-independent semantic prompts which\nextract semantic information from multi-modal inputs, such as visible,\nnear-infrared, and thermal-infrared. Built upon the extracted features, we\nfurther design a Prompt Inverse Bind (PromptIBind) strategy that employs bind\nprompts as a medium to connect the semantic prompt tokens of different\nmodalities and facilitates the exchange of complementary multi-modal\ninformation, boosting final re-identification results. Experimental results on\nmultiple common benchmarks demonstrate that our DMPT can achieve competitive\nresults to existing state-of-the-art methods while requiring only 6.5%\nfine-tuning of the backbone parameters.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11257", "pdf": "https://arxiv.org/pdf/2504.11257", "abs": "https://arxiv.org/abs/2504.11257", "authors": ["Xinyi Liu", "Xiaoyi Zhang", "Ziyun Zhang", "Yan Lu"], "title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation.In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11015", "pdf": "https://arxiv.org/pdf/2504.11015", "abs": "https://arxiv.org/abs/2504.11015", "authors": ["Chenyang Zhu", "Xing Zhang", "Yuyang Sun", "Ching-Chun Chang", "Isao Echizen"], "title": "AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and Localization in Diffusion Era", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in image generation, particularly diffusion models, have\nsignificantly lowered the barrier for creating sophisticated forgeries, making\nimage manipulation detection and localization (IMDL) increasingly challenging.\nWhile prior work in IMDL has focused largely on natural images, the anime\ndomain remains underexplored-despite its growing vulnerability to AI-generated\nforgeries. Misrepresentations of AI-generated images as hand-drawn artwork,\ncopyright violations, and inappropriate content modifications pose serious\nthreats to the anime community and industry. To address this gap, we propose\nAnimeDL-2M, the first large-scale benchmark for anime IMDL with comprehensive\nannotations. It comprises over two million images including real, partially\nmanipulated, and fully AI-generated samples. Experiments indicate that models\ntrained on existing IMDL datasets of natural images perform poorly when applied\nto anime images, highlighting a clear domain gap between anime and natural\nimages. To better handle IMDL tasks in anime domain, we further propose\nAniXplore, a novel model tailored to the visual characteristics of anime\nimagery. Extensive evaluations demonstrate that AniXplore achieves superior\nperformance compared to existing methods. Dataset and code can be found in\nhttps://flytweety.github.io/AnimeDL2M/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11019", "pdf": "https://arxiv.org/pdf/2504.11019", "abs": "https://arxiv.org/abs/2504.11019", "authors": ["Hyejin Lee", "Seokjun Hong", "Jeonghoon Song", "Haechan Cho", "Zhixiong Jin", "Byeonghun Kim", "Joobin Jin", "Jaegyun Im", "Byeongjoon Noh", "Hwasoo Yeo"], "title": "DRIFT open dataset: A drone-derived intelligence for traffic analysis in urban environmen", "categories": ["cs.CV", "I.2.10; I.4.8; H.2.8; J.7"], "comment": "30 pages, 15 figures", "summary": "Reliable traffic data are essential for understanding urban mobility and\ndeveloping effective traffic management strategies. This study introduces the\nDRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale\nurban traffic dataset collected systematically from synchronized drone videos\nat approximately 250 meters altitude, covering nine interconnected\nintersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle\ntrajectories that include directional information, processed through video\nsynchronization and orthomap alignment, resulting in a comprehensive dataset of\n81,699 vehicle trajectories. Through our DRIFT dataset, researchers can\nsimultaneously analyze traffic at multiple scales - from individual vehicle\nmaneuvers like lane-changes and safety metrics such as time-to-collision to\naggregate network flow dynamics across interconnected urban intersections. The\nDRIFT dataset is structured to enable immediate use without additional\npreprocessing, complemented by open-source models for object detection and\ntrajectory extraction, as well as associated analytical tools. DRIFT is\nexpected to significantly contribute to academic research and practical\napplications, such as traffic flow analysis and simulation studies. The dataset\nand related resources are publicly accessible at\nhttps://github.com/AIxMobility/The-DRIFT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11080", "pdf": "https://arxiv.org/pdf/2504.11080", "abs": "https://arxiv.org/abs/2504.11080", "authors": ["Elman Ghazaei", "Erchan Aptoula"], "title": "Change State Space Models for Remote Sensing Change Detection", "categories": ["cs.CV"], "comment": null, "summary": "Despite their frequent use for change detection, both ConvNets and Vision\ntransformers (ViT) exhibit well-known limitations, namely the former struggle\nto model long-range dependencies while the latter are computationally\ninefficient, rendering them challenging to train on large-scale datasets.\nVision Mamba, an architecture based on State Space Models has emerged as an\nalternative addressing the aforementioned deficiencies and has been already\napplied to remote sensing change detection, though mostly as a feature\nextracting backbone. In this article the Change State Space Model is\nintroduced, that has been specifically designed for change detection by\nfocusing on the relevant changes between bi-temporal images, effectively\nfiltering out irrelevant information. By concentrating solely on the changed\nfeatures, the number of network parameters is reduced, enhancing significantly\ncomputational efficiency while maintaining high detection performance and\nrobustness against input degradation. The proposed model has been evaluated via\nthree benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based\ncounterparts at a fraction of their computational complexity. The\nimplementation will be made available at https://github.com/Elman295/CSSM upon\nacceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11134", "pdf": "https://arxiv.org/pdf/2504.11134", "abs": "https://arxiv.org/abs/2504.11134", "authors": ["Gustav Hanning", "Gabrielle Flood", "Viktor Larsson"], "title": "Visual Re-Ranking with Non-Visual Side Information", "categories": ["cs.CV"], "comment": "Accepted at Scandinavian Conference on Image Analysis (SCIA) 2025", "summary": "The standard approach for visual place recognition is to use global image\ndescriptors to retrieve the most similar database images for a given query\nimage. The results can then be further improved with re-ranking methods that\nre-order the top scoring images. However, existing methods focus on re-ranking\nbased on the same image descriptors that were used for the initial retrieval,\nwhich we argue provides limited additional signal.\n  In this work we propose Generalized Contextual Similarity Aggregation (GCSA),\nwhich is a graph neural network-based re-ranking method that, in addition to\nthe visual descriptors, can leverage other types of available side information.\nThis can for example be other sensor data (such as signal strength of nearby\nWiFi or BlueTooth endpoints) or geometric properties such as camera poses for\ndatabase images. In many applications this information is already present or\ncan be acquired with low effort. Our architecture leverages the concept of\naffinity vectors to allow for a shared encoding of the heterogeneous\nmulti-modal input. Two large-scale datasets, covering both outdoor and indoor\nlocalization scenarios, are utilized for training and evaluation. In\nexperiments we show significant improvement not only on image retrieval\nmetrics, but also for the downstream visual localization task.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11160", "pdf": "https://arxiv.org/pdf/2504.11160", "abs": "https://arxiv.org/abs/2504.11160", "authors": ["Haohan Chen", "Hongjia Liu", "Shiyong Lan", "Wenwu Wang", "Yixin Qiao", "Yao Li", "Guonan Deng"], "title": "DMAGaze: Gaze Estimation Based on Feature Disentanglement and Multi-Scale Attention", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Gaze estimation, which predicts gaze direction, commonly faces the challenge\nof interference from complex gaze-irrelevant information in face images. In\nthis work, we propose DMAGaze, a novel gaze estimation framework that exploits\ninformation from facial images in three aspects: gaze-relevant global features\n(disentangled from facial image), local eye features (extracted from cropped\neye patch), and head pose estimation features, to improve overall performance.\nFirstly, we design a new continuous mask-based Disentangler to accurately\ndisentangle gaze-relevant and gaze-irrelevant information in facial images by\nachieving the dual-branch disentanglement goal through separately\nreconstructing the eye and non-eye regions. Furthermore, we introduce a new\ncascaded attention module named Multi-Scale Global Local Attention Module\n(MS-GLAM). Through a customized cascaded attention structure, it effectively\nfocuses on global and local information at multiple scales, further enhancing\nthe information from the Disentangler. Finally, the global gaze-relevant\nfeatures disentangled by the upper face branch, combined with head pose and\nlocal eye features, are passed through the detection head for high-precision\ngaze estimation. Our proposed DMAGaze has been extensively validated on two\nmainstream public datasets, achieving state-of-the-art performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11165", "pdf": "https://arxiv.org/pdf/2504.11165", "abs": "https://arxiv.org/abs/2504.11165", "authors": ["Linlin Xiao", "Zhang Tiancong", "Yutong Jia", "Xinyu Nie", "Mengyao Wang", "Xiaohang Shao"], "title": "YOLO-RS: Remote Sensing Enhanced Crop Detection Methods", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid development of remote sensing technology, crop classification\nand health detection based on deep learning have gradually become a research\nhotspot. However, the existing target detection methods show poor performance\nwhen dealing with small targets in remote sensing images, especially in the\ncase of complex background and image mixing, which is difficult to meet the\npractical application requirementsite. To address this problem, a novel target\ndetection model YOLO-RS is proposed in this paper. The model is based on the\nlatest Yolov11 which significantly enhances the detection of small targets by\nintroducing the Context Anchor Attention (CAA) mechanism and an efficient\nmulti-field multi-scale feature fusion network. YOLO-RS adopts a bidirectional\nfeature fusion strategy in the feature fusion process, which effectively\nenhances the model's performance in the detection of small targets. Small\ntarget detection. Meanwhile, the ACmix module at the end of the model backbone\nnetwork solves the category imbalance problem by adaptively adjusting the\ncontrast and sample mixing, thus enhancing the detection accuracy in complex\nscenes. In the experiments on the PDT remote sensing crop health detection\ndataset and the CWC crop classification dataset, YOLO-RS improves both the\nrecall and the mean average precision (mAP) by about 2-3\\% or so compared with\nthe existing state-of-the-art methods, while the F1-score is also significantly\nimproved. Moreover, the computational complexity of the model only increases by\nabout 5.2 GFLOPs, indicating its significant advantages in both performance and\nefficiency. The experimental results validate the effectiveness and application\npotential of YOLO-RS in the task of detecting small targets in remote sensing\nimages.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11171", "pdf": "https://arxiv.org/pdf/2504.11171", "abs": "https://arxiv.org/abs/2504.11171", "authors": ["Johannes Jakubik", "Felix Yang", "Benedikt Blumenstiel", "Erik Scheurer", "Rocco Sedona", "Stefano Maurogiovanni", "Jente Bosmans", "Nikolaos Dionelis", "Valerio Marsocci", "Niklas Kopp", "Rahul Ramachandran", "Paolo Fraccaro", "Thomas Brunschwiler", "Gabriele Cavallaro", "Juan Bernabe-Moreno", "Nicolas Longp"], "title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code is open-sourced under a permissive license.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11172", "pdf": "https://arxiv.org/pdf/2504.11172", "abs": "https://arxiv.org/abs/2504.11172", "authors": ["Benedikt Blumenstiel", "Paolo Fraccaro", "Valerio Marsocci", "Johannes Jakubik", "Stefano Maurogiovanni", "Mikolaj Czerkawski", "Rocco Sedona", "Gabriele Cavallaro", "Thomas Brunschwiler", "Juan Bernabe-Moreno", "Nicolas Longp"], "title": "TerraMesh: A Planetary Mosaic of Multimodal Earth Observation Data", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale foundation models in Earth Observation can learn versatile,\nlabel-efficient representations by leveraging massive amounts of unlabeled\ndata. However, existing public datasets are often limited in scale, geographic\ncoverage, or sensor variety. We introduce TerraMesh, a new globally diverse,\nmultimodal dataset combining optical, synthetic aperture radar, elevation, and\nland-cover modalities in an Analysis-Ready Data format. TerraMesh includes over\n9 million samples with eight spatiotemporal aligned modalities, enabling\nlarge-scale pre-training and fostering robust cross-modal correlation learning.\nWe provide detailed data processing steps, comprehensive statistics, and\nempirical evidence demonstrating improved model performance when pre-trained on\nTerraMesh. The dataset will be made publicly available with a permissive\nlicense.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11218", "pdf": "https://arxiv.org/pdf/2504.11218", "abs": "https://arxiv.org/abs/2504.11218", "authors": ["Zeming wei", "Junyi Lin", "Yang Liu", "Weixing Chen", "Jingzhou Luo", "Guanbin Li", "Liang Lin"], "title": "3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians", "categories": ["cs.CV"], "comment": "The first large-scale 3D Gaussians Affordance Reasoning Benchmark", "summary": "3D affordance reasoning is essential in associating human instructions with\nthe functional regions of 3D objects, facilitating precise, task-oriented\nmanipulations in embodied AI. However, current methods, which predominantly\ndepend on sparse 3D point clouds, exhibit limited generalizability and\nrobustness due to their sensitivity to coordinate variations and the inherent\nsparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers\nhigh-fidelity, real-time rendering with minimal computational overhead by\nrepresenting scenes as dense, continuous distributions. This positions 3DGS as\na highly effective approach for capturing fine-grained affordance details and\nimproving recognition accuracy. Nevertheless, its full potential remains\nlargely untapped due to the absence of large-scale, 3DGS-specific affordance\ndatasets. To overcome these limitations, we present 3DAffordSplat, the first\nlarge-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning.\nThis dataset includes 23,677 Gaussian instances, 8,354 point cloud instances,\nand 6,631 manually annotated affordance labels, encompassing 21 object\ncategories and 18 affordance types. Building upon this dataset, we introduce\nAffordSplatNet, a novel model specifically designed for affordance reasoning\nusing 3DGS representations. AffordSplatNet features an innovative cross-modal\nstructure alignment module that exploits structural consistency priors to align\n3D point cloud and 3DGS representations, resulting in enhanced affordance\nrecognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat\ndataset significantly advances affordance learning within the 3DGS domain,\nwhile AffordSplatNet consistently outperforms existing methods across both seen\nand unseen settings, highlighting its robust generalization capabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11289", "pdf": "https://arxiv.org/pdf/2504.11289", "abs": "https://arxiv.org/abs/2504.11289", "authors": ["Xiang Wang", "Shiwei Zhang", "Longxiang Tang", "Yingya Zhang", "Changxin Gao", "Yuehuan Wang", "Nong Sang"], "title": "UniAnimate-DiT: Human Image Animation with Large-Scale Video Diffusion Transformer", "categories": ["cs.CV"], "comment": "The training and inference code (based on Wan2.1) is available at\n  https://github.com/ali-vilab/UniAnimate-DiT", "summary": "This report presents UniAnimate-DiT, an advanced project that leverages the\ncutting-edge and powerful capabilities of the open-source Wan2.1 model for\nconsistent human image animation. Specifically, to preserve the robust\ngenerative capabilities of the original Wan2.1 model, we implement Low-Rank\nAdaptation (LoRA) technique to fine-tune a minimal set of parameters,\nsignificantly reducing training memory overhead. A lightweight pose encoder\nconsisting of multiple stacked 3D convolutional layers is designed to encode\nmotion information of driving poses. Furthermore, we adopt a simple\nconcatenation operation to integrate the reference appearance into the model\nand incorporate the pose information of the reference image for enhanced pose\nalignment. Experimental results show that our approach achieves visually\nappearing and temporally consistent high-fidelity animations. Trained on 480p\n(832x480) videos, UniAnimate-DiT demonstrates strong generalization\ncapabilities to seamlessly upscale to 720P (1280x720) during inference. The\ntraining and inference code is publicly available at\nhttps://github.com/ali-vilab/UniAnimate-DiT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11305", "pdf": "https://arxiv.org/pdf/2504.11305", "abs": "https://arxiv.org/abs/2504.11305", "authors": ["Jincheng Kang", "Yi Cen", "Yigang Cen", "Ke Wang", "Yuhan Liu"], "title": "CFIS-YOLO: A Lightweight Multi-Scale Fusion Network for Edge-Deployable Wood Defect Detection", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 11 figures", "summary": "Wood defect detection is critical for ensuring quality control in the wood\nprocessing industry. However, current industrial applications face two major\nchallenges: traditional methods are costly, subjective, and labor-intensive,\nwhile mainstream deep learning models often struggle to balance detection\naccuracy and computational efficiency for edge deployment. To address these\nissues, this study proposes CFIS-YOLO, a lightweight object detection model\noptimized for edge devices. The model introduces an enhanced C2f structure, a\ndynamic feature recombination module, and a novel loss function that\nincorporates auxiliary bounding boxes and angular constraints. These\ninnovations improve multi-scale feature fusion and small object localization\nwhile significantly reducing computational overhead. Evaluated on a public wood\ndefect dataset, CFIS-YOLO achieves a mean Average Precision (mAP@0.5) of\n77.5\\%, outperforming the baseline YOLOv10s by 4 percentage points. On SOPHON\nBM1684X edge devices, CFIS-YOLO delivers 135 FPS, reduces power consumption to\n17.3\\% of the original implementation, and incurs only a 0.5 percentage point\ndrop in mAP. These results demonstrate that CFIS-YOLO is a practical and\neffective solution for real-world wood defect detection in resource-constrained\nenvironments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11346", "pdf": "https://arxiv.org/pdf/2504.11346", "abs": "https://arxiv.org/abs/2504.11346", "authors": ["Yu Gao", "Lixue Gong", "Qiushan Guo", "Xiaoxia Hou", "Zhichao Lai", "Fanshi Li", "Liang Li", "Xiaochen Lian", "Chao Liao", "Liyang Liu", "Wei Liu", "Yichun Shi", "Shiqi Sun", "Yu Tian", "Zhi Tian", "Peng Wang", "Rui Wang", "Xuanda Wang", "Xun Wang", "Ye Wang", "Guofeng Wu", "Jie Wu", "Xin Xia", "Xuefeng Xiao", "Zhonghua Zhai", "Xinyu Zhang", "Qi Zhang", "Yuwei Zhang", "Shijia Zhao", "Jianchao Yang", "Weilin Huang"], "title": "Seedream 3.0 Technical Report", "categories": ["cs.CV"], "comment": "Seedream 3.0 Technical Report", "summary": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11347", "pdf": "https://arxiv.org/pdf/2504.11347", "abs": "https://arxiv.org/abs/2504.11347", "authors": ["Soyoung Yoo", "Namwoo Kang"], "title": "DeepWheel: Generating a 3D Synthetic Wheel Dataset for Design and Performance Evaluation", "categories": ["cs.CV", "physics.app-ph", "68T07"], "comment": "28 pages, 18 figures. Not yet submitted to a journal or conference", "summary": "Data-driven design is emerging as a powerful strategy to accelerate\nengineering innovation. However, its application to vehicle wheel design\nremains limited due to the lack of large-scale, high-quality datasets that\ninclude 3D geometry and physical performance metrics. To address this gap, this\nstudy proposes a synthetic design-performance dataset generation framework\nusing generative AI. The proposed framework first generates 2D rendered images\nusing Stable Diffusion, and then reconstructs the 3D geometry through 2.5D\ndepth estimation. Structural simulations are subsequently performed to extract\nengineering performance data. To further expand the design and performance\nspace, topology optimization is applied, enabling the generation of a more\ndiverse set of wheel designs. The final dataset, named DeepWheel, consists of\nover 6,000 photo-realistic images and 900 structurally analyzed 3D models. This\nmulti-modal dataset serves as a valuable resource for surrogate model training,\ndata-driven inverse design, and design space exploration. The proposed\nmethodology is also applicable to other complex design domains. The dataset is\nreleased under the Creative Commons Attribution-NonCommercial 4.0\nInternational(CC BY-NC 4.0) and is available on the\nhttps://www.smartdesignlab.org/datasets", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11368", "pdf": "https://arxiv.org/pdf/2504.11368", "abs": "https://arxiv.org/abs/2504.11368", "authors": ["Jingkun Chen", "Haoran Duan", "Xiao Zhang", "Boyan Gao", "Tao Tan", "Vicente Grau", "Jungong Han"], "title": "From Gaze to Insight: Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation", "categories": ["cs.CV", "68T45", "I.2.10; I.4.8"], "comment": "10 pages, 5 figures", "summary": "Medical image segmentation remains challenging due to the high cost of\npixel-level annotations for training. In the context of weak supervision,\nclinician gaze data captures regions of diagnostic interest; however, its\nsparsity limits its use for segmentation. In contrast, vision-language models\n(VLMs) provide semantic context through textual descriptions but lack the\nexplanation precision required. Recognizing that neither source alone suffices,\nwe propose a teacher-student framework that integrates both gaze and language\nsupervision, leveraging their complementary strengths. Our key insight is that\ngaze data indicates where clinicians focus during diagnosis, while VLMs explain\nwhy those regions are significant. To implement this, the teacher model first\nlearns from gaze points enhanced by VLM-generated descriptions of lesion\nmorphology, establishing a foundation for guiding the student model. The\nteacher then directs the student through three strategies: (1) Multi-scale\nfeature alignment to fuse visual cues with textual semantics; (2)\nConfidence-weighted consistency constraints to focus on reliable predictions;\n(3) Adaptive masking to limit error propagation in uncertain areas. Experiments\non the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves\nDice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over\ngaze baselines without increasing the annotation burden. By preserving\ncorrelations among predictions, gaze data, and lesion descriptions, our\nframework also maintains clinical interpretability. This work illustrates how\nintegrating human visual attention with AI-generated semantic context can\neffectively overcome the limitations of individual weak supervision signals,\nthereby advancing the development of deployable, annotation-efficient medical\nAI systems. Code is available at: https://github.com/jingkunchen/FGI.git.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11451", "pdf": "https://arxiv.org/pdf/2504.11451", "abs": "https://arxiv.org/abs/2504.11451", "authors": ["Minghua Liu", "Mikaela Angelina Uy", "Donglai Xiang", "Hao Su", "Sanja Fidler", "Nicholas Sharp", "Jun Gao"], "title": "PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond", "categories": ["cs.CV"], "comment": "https://research.nvidia.com/labs/toronto-ai/partfield-release/", "summary": "We propose PartField, a feedforward approach for learning part-based 3D\nfeatures, which captures the general concept of parts and their hierarchy\nwithout relying on predefined templates or text-based names, and can be applied\nto open-world 3D shapes across various modalities. PartField requires only a 3D\nfeedforward pass at inference time, significantly improving runtime and\nrobustness compared to prior approaches. Our model is trained by distilling 2D\nand 3D part proposals from a mix of labeled datasets and image segmentations on\nlarge unsupervised datasets, via a contrastive learning formulation. It\nproduces a continuous feature field which can be clustered to yield a\nhierarchical part decomposition. Comparisons show that PartField is up to 20%\nmore accurate and often orders of magnitude faster than other recent\nclass-agnostic part-segmentation methods. Beyond single-shape part\ndecomposition, consistency in the learned field emerges across shapes, enabling\ntasks such as co-segmentation and correspondence, which we demonstrate in\nseveral applications of these general-purpose, hierarchical, and consistent 3D\nfeature fields. Check our Webpage!\nhttps://research.nvidia.com/labs/toronto-ai/partfield-release/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11455", "pdf": "https://arxiv.org/pdf/2504.11455", "abs": "https://arxiv.org/abs/2504.11455", "authors": ["Junke Wang", "Zhi Tian", "Xun Wang", "Xinyu Zhang", "Weilin Huang", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL", "categories": ["cs.CV"], "comment": "technical report, work in progress", "summary": "This work presents SimpleAR, a vanilla autoregressive visual generation\nframework without complex architecure modifications. Through careful\nexploration of training and inference optimization, we demonstrate that: 1)\nwith only 0.5B parameters, our model can generate 1024x1024 resolution images\nwith high fidelity, and achieve competitive results on challenging\ntext-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both\nsupervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO)\ntraining could lead to significant improvements on generation aesthectics and\nprompt alignment; and 3) when optimized with inference acceleraton techniques\nlike vLLM, the time for SimpleAR to generate an 1024x1024 image could be\nreduced to around 14 seconds. By sharing these findings and open-sourcing the\ncode, we hope to reveal the potential of autoregressive visual generation and\nencourage more participation in this research field. Code is available at\nhttps://github.com/wdrink/SimpleAR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference optimization"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization", "alignment"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10807", "pdf": "https://arxiv.org/pdf/2504.10807", "abs": "https://arxiv.org/abs/2504.10807", "authors": ["Huseyin Tuna Erdinc", "Yunlin Zeng", "Abhinav Prakash Gahlot", "Felix J. Herrmann"], "title": "Power-scaled Bayesian Inference with Score-based Generative mModels", "categories": ["cs.LG", "cs.CV", "physics.geo-ph"], "comment": "8 pages, 4 figures", "summary": "We propose a score-based generative algorithm for sampling from power-scaled\npriors and likelihoods within the Bayesian inference framework. Our algorithm\nenables flexible control over prior-likelihood influence without requiring\nretraining for different power-scaling configurations. Specifically, we focus\non synthesizing seismic velocity models conditioned on imaged seismic. Our\nmethod enables sensitivity analysis by sampling from intermediate power\nposteriors, allowing us to assess the relative influence of the prior and\nlikelihood on samples of the posterior distribution. Through a comprehensive\nset of experiments, we evaluate the effects of varying the power parameter in\ndifferent settings: applying it solely to the prior, to the likelihood of a\nBayesian formulation, and to both simultaneously. The results show that\nincreasing the power of the likelihood up to a certain threshold improves the\nfidelity of posterior samples to the conditioning data (e.g., seismic images),\nwhile decreasing the prior power promotes greater structural diversity among\nsamples. Moreover, we find that moderate scaling of the likelihood leads to a\nreduced shot data residual, confirming its utility in posterior refinement.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10857", "pdf": "https://arxiv.org/pdf/2504.10857", "abs": "https://arxiv.org/abs/2504.10857", "authors": ["Shun Iwase", "Zubair Irshad", "Katherine Liu", "Vitor Guizilini", "Robert Lee", "Takuya Ikeda", "Ayako Amma", "Koichi Nishiwaki", "Kris Kitani", "Rares Ambrus", "Sergey Zakharov"], "title": "ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping", "categories": ["cs.RO", "cs.CV"], "comment": "Published at CVPR 2025, Webpage: https://sh8.io/#/zerograsp", "summary": "Robotic grasping is a cornerstone capability of embodied systems. Many\nmethods directly output grasps from partial information without modeling the\ngeometry of the scene, leading to suboptimal motion and even collisions. To\naddress these issues, we introduce ZeroGrasp, a novel framework that\nsimultaneously performs 3D reconstruction and grasp pose prediction in near\nreal-time. A key insight of our method is that occlusion reasoning and modeling\nthe spatial relationships between objects is beneficial for both accurate\nreconstruction and grasping. We couple our method with a novel large-scale\nsynthetic dataset, which comprises 1M photo-realistic images, high-resolution\n3D reconstructions and 11.3B physically-valid grasp pose annotations for 12K\nobjects from the Objaverse-LVIS dataset. We evaluate ZeroGrasp on the\nGraspNet-1B benchmark as well as through real-world robot experiments.\nZeroGrasp achieves state-of-the-art performance and generalizes to novel\nreal-world objects by leveraging synthetic data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11195", "pdf": "https://arxiv.org/pdf/2504.11195", "abs": "https://arxiv.org/abs/2504.11195", "authors": ["Lijun Sheng", "Jian Liang", "Zilei Wang", "Ran He"], "title": "R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": "CVPR 2025", "summary": "Vision-language models (VLMs), such as CLIP, have gained significant\npopularity as foundation models, with numerous fine-tuning methods developed to\nenhance performance on downstream tasks. However, due to their inherent\nvulnerability and the common practice of selecting from a limited set of\nopen-source models, VLMs suffer from a higher risk of adversarial attacks than\ntraditional vision models. Existing defense techniques typically rely on\nadversarial fine-tuning during training, which requires labeled data and lacks\nof flexibility for downstream tasks. To address these limitations, we propose\nrobust test-time prompt tuning (R-TPT), which mitigates the impact of\nadversarial attacks during the inference stage. We first reformulate the\nclassic marginal entropy objective by eliminating the term that introduces\nconflicts under adversarial conditions, retaining only the pointwise entropy\nminimization. Furthermore, we introduce a plug-and-play reliability-based\nweighted ensembling strategy, which aggregates useful information from reliable\naugmented views to strengthen the defense. R-TPT enhances defense against\nadversarial attacks without requiring labeled training data while offering high\nflexibility for inference tasks. Extensive experiments on widely used\nbenchmarks with various attacks demonstrate the effectiveness of R-TPT. The\ncode is available in https://github.com/TomSheng21/R-TPT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11257", "pdf": "https://arxiv.org/pdf/2504.11257", "abs": "https://arxiv.org/abs/2504.11257", "authors": ["Xinyi Liu", "Xiaoyi Zhang", "Ziyun Zhang", "Yan Lu"], "title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation.In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11438", "pdf": "https://arxiv.org/pdf/2504.11438", "abs": "https://arxiv.org/abs/2504.11438", "authors": ["Lewis Clifton", "Xin Tian", "Duangdao Palasuwan", "Phandee Watanaboonyongcharoen", "Ponlapat Rojnuckarin", "Nantheera Anantrasirichai"], "title": "Mamba-Based Ensemble learning for White Blood Cell Classification", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "White blood cell (WBC) classification assists in assessing immune health and\ndiagnosing various diseases, yet manual classification is labor-intensive and\nprone to inconsistencies. Recent advancements in deep learning have shown\npromise over traditional methods; however, challenges such as data imbalance\nand the computational demands of modern technologies, such as Transformer-based\nmodels which do not scale well with input size, limit their practical\napplication. This paper introduces a novel framework that leverages Mamba\nmodels integrated with ensemble learning to improve WBC classification. Mamba\nmodels, known for their linear complexity, provide a scalable alternative to\nTransformer-based approaches, making them suitable for deployment in\nresource-constrained environments. Additionally, we introduce a new WBC\ndataset, Chula-WBC-8, for benchmarking. Our approach not only validates the\neffectiveness of Mamba models in this domain but also demonstrates their\npotential to significantly enhance classification efficiency without\ncompromising accuracy. The source code can be found at\nhttps://github.com/LewisClifton/Mamba-WBC-Classification.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
