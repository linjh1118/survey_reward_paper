{"id": "2504.13828", "pdf": "https://arxiv.org/pdf/2504.13828", "abs": "https://arxiv.org/abs/2504.13828", "authors": ["Shijie Xia", "Yiwei Qin", "Xuefeng Li", "Yan Ma", "Run-Ze Fan", "Steffi Chern", "Haoyang Zou", "Fan Zhou", "Xiangkun Hu", "Jiahe Jin", "Yanheng He", "Yixin Ye", "Yixiu Liu", "Pengfei Liu"], "title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The first generation of Large Language Models - what might be called \"Act I\"\nof generative AI (2020-2023) - achieved remarkable success through massive\nparameter and data scaling, yet exhibited fundamental limitations in knowledge\nlatency, shallow reasoning, and constrained cognitive processes. During this\nera, prompt engineering emerged as our primary interface with AI, enabling\ndialogue-level communication through natural language. We now witness the\nemergence of \"Act II\" (2024-present), where models are transitioning from\nknowledge-retrieval systems (in latent space) to thought-construction engines\nthrough test-time scaling techniques. This new paradigm establishes a\nmind-level connection with AI through language-based thoughts. In this paper,\nwe clarify the conceptual foundations of cognition engineering and explain why\nthis moment is critical for its development. We systematically break down these\nadvanced approaches through comprehensive tutorials and optimized\nimplementations, democratizing access to cognition engineering and enabling\nevery practitioner to participate in AI's second act. We provide a regularly\nupdated collection of papers on test-time scaling in the GitHub Repository:\nhttps://github.com/GAIR-NLP/cognition-engineering", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time", "scaling"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13626", "pdf": "https://arxiv.org/pdf/2504.13626", "abs": "https://arxiv.org/abs/2504.13626", "authors": ["Yule Liu", "Jingyi Zheng", "Zhen Sun", "Zifan Peng", "Wenhan Dong", "Zeyang Sha", "Shiwen Cui", "Weiqiang Wang", "Xinlei He"], "title": "Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities in multiple tasks. However, LRMs typically suffer from\n\"overthinking\" problems, where models generate significantly redundant\nreasoning steps while bringing limited performance gains. Existing work relies\non fine-tuning to mitigate overthinking, which requires additional data,\nunconventional training setups, risky safety misalignment, and poor\ngeneralization.\n  Through empirical analysis, we reveal an important characteristic of LRM\nbehaviors that placing external CoTs generated by smaller models between the\nthinking token ($\\texttt{<think>}$ and $\\texttt{</think>)}$ can effectively\nmanipulate the model to generate fewer thoughts. Building on these insights, we\npropose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass\nunnecessary intermediate steps and reduce computational costs significantly. We\nconduct extensive experiments to validate the utility and efficiency of\nThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code\ndataset, ThoughtMani keeps the original performance and reduces output token\ncounts by approximately 30%, with little overhead from the CoT generator.\nFurthermore, we find that ThoughtMani enhances safety alignment by an average\nof 10%. Since model vendors typically serve models of different sizes\nsimultaneously, ThoughtMani provides an effective way to construct more\nefficient and accessible LRMs for real-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13682", "pdf": "https://arxiv.org/pdf/2504.13682", "abs": "https://arxiv.org/abs/2504.13682", "authors": ["Mengyuan Li", "Changhong Fu", "Ziyu Lu", "Zijie Zhang", "Haobo Zuo", "Liangliang Yao"], "title": "AnyTSR: Any-Scale Thermal Super-Resolution for UAV", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Thermal imaging can greatly enhance the application of intelligent unmanned\naerial vehicles (UAV) in challenging environments. However, the inherent low\nresolution of thermal sensors leads to insufficient details and blurred\nboundaries. Super-resolution (SR) offers a promising solution to address this\nissue, while most existing SR methods are designed for fixed-scale SR. They are\ncomputationally expensive and inflexible in practical applications. To address\nabove issues, this work proposes a novel any-scale thermal SR method (AnyTSR)\nfor UAV within a single model. Specifically, a new image encoder is proposed to\nexplicitly assign specific feature code to enable more accurate and flexible\nrepresentation. Additionally, by effectively embedding coordinate offset\ninformation into the local feature ensemble, an innovative any-scale upsampler\nis proposed to better understand spatial relationships and reduce artifacts.\nMoreover, a novel dataset (UAV-TSR), covering both land and water scenes, is\nconstructed for thermal SR tasks. Experimental results demonstrate that the\nproposed method consistently outperforms state-of-the-art methods across all\nscaling factors as well as generates more accurate and detailed high-resolution\nimages. The code is located at https://github.com/vision4robotics/AnyTSR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13214", "pdf": "https://arxiv.org/pdf/2504.13214", "abs": "https://arxiv.org/abs/2504.13214", "authors": ["Andrew Kiruluta"], "title": "Wavelet-based Variational Autoencoders for High-Resolution Image Generation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Variational Autoencoders (VAEs) are powerful generative models capable of\nlearning compact latent representations. However, conventional VAEs often\ngenerate relatively blurry images due to their assumption of an isotropic\nGaussian latent space and constraints in capturing high-frequency details. In\nthis paper, we explore a novel wavelet-based approach (Wavelet-VAE) in which\nthe latent space is constructed using multi-scale Haar wavelet coefficients. We\npropose a comprehensive method to encode the image features into multi-scale\ndetail and approximation coefficients and introduce a learnable noise parameter\nto maintain stochasticity. We thoroughly discuss how to reformulate the\nreparameterization trick, address the KL divergence term, and integrate wavelet\nsparsity principles into the training objective. Our experimental evaluation on\nCIFAR-10 and other high-resolution datasets demonstrates that the Wavelet-VAE\nimproves visual fidelity and recovers higher-resolution details compared to\nconventional VAEs. We conclude with a discussion of advantages, potential\nlimitations, and future research directions for wavelet-based generative\nmodeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13224", "pdf": "https://arxiv.org/pdf/2504.13224", "abs": "https://arxiv.org/abs/2504.13224", "authors": ["Fuwei Liu"], "title": "ICAS: IP Adapter and ControlNet-based Attention Structure for Multi-Subject Style Transfer Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Generating multi-subject stylized images remains a significant challenge due\nto the ambiguity in defining style attributes (e.g., color, texture,\natmosphere, and structure) and the difficulty in consistently applying them\nacross multiple subjects. Although recent diffusion-based text-to-image models\nhave achieved remarkable progress, existing methods typically rely on\ncomputationally expensive inversion procedures or large-scale stylized\ndatasets. Moreover, these methods often struggle with maintaining multi-subject\nsemantic fidelity and are limited by high inference costs. To address these\nlimitations, we propose ICAS (IP-Adapter and ControlNet-based Attention\nStructure), a novel framework for efficient and controllable multi-subject\nstyle transfer. Instead of full-model tuning, ICAS adaptively fine-tunes only\nthe content injection branch of a pre-trained diffusion model, thereby\npreserving identity-specific semantics while enhancing style controllability.\nBy combining IP-Adapter for adaptive style injection with ControlNet for\nstructural conditioning, our framework ensures faithful global layout\npreservation alongside accurate local style synthesis. Furthermore, ICAS\nintroduces a cyclic multi-subject content embedding mechanism, which enables\neffective style transfer under limited-data settings without the need for\nextensive stylized corpora. Extensive experiments show that ICAS achieves\nsuperior performance in structure preservation, style consistency, and\ninference efficiency, establishing a new paradigm for multi-subject style\ntransfer in real-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13339", "pdf": "https://arxiv.org/pdf/2504.13339", "abs": "https://arxiv.org/abs/2504.13339", "authors": ["Landon Dyken", "Andres Sewell", "Will Usher", "Steve Petruzza", "Sidharth Kumar"], "title": "Volume Encoding Gaussians: Transfer Function-Agnostic 3D Gaussians for Volume Rendering", "categories": ["cs.GR"], "comment": null, "summary": "While HPC resources are increasingly being used to produce adaptively refined\nor unstructured volume datasets, current research in applying machine\nlearning-based representation to visualization has largely ignored this type of\ndata. To address this, we introduce Volume Encoding Gaussians (VEG), a novel 3D\nGaussian-based representation for scientific volume visualization focused on\nunstructured volumes. Unlike prior 3D Gaussian Splatting (3DGS) methods that\nstore view-dependent color and opacity for each Gaussian, VEG decouple the\nvisual appearance from the data representation by encoding only scalar values,\nenabling transfer-function-agnostic rendering of 3DGS models for interactive\nscientific visualization. VEG are directly initialized from volume datasets,\neliminating the need for structure-from-motion pipelines like COLMAP. To ensure\ncomplete scalar field coverage, we introduce an opacity-guided training\nstrategy, using differentiable rendering with multiple transfer functions to\noptimize our data representation. This allows VEG to preserve fine features\nacross the full scalar range of a dataset while remaining independent of any\nspecific transfer function. Each Gaussian is scaled and rotated to adapt to\nlocal geometry, allowing for efficient representation of unstructured meshes\nwithout storing mesh connectivity and while using far fewer primitives. Across\na diverse set of data, VEG achieve high reconstruction quality, compress large\nvolume datasets by up to 3600x, and support lightning-fast rendering on\ncommodity GPUs, enabling interactive visualization of large-scale structured\nand unstructured volumes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13227", "pdf": "https://arxiv.org/pdf/2504.13227", "abs": "https://arxiv.org/abs/2504.13227", "authors": ["Weijie Shi", "Jipeng Zhang", "Yaguang Wu", "Jingzhi Fang", "Ruiyuan Zhang", "Jiajie Xu", "Jia Zhu", "Hao Chen", "Yao Zhao", "Sirui Han", "Xiaofang Zhou"], "title": "DIDS: Domain Impact-aware Data Sampling for Large Language Model Training", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are commonly trained on multi-domain datasets,\nwhere domain sampling strategies significantly impact model performance due to\nvarying domain importance across downstream tasks. Existing approaches for\noptimizing domain-level sampling strategies struggle with maintaining\nintra-domain consistency and accurately measuring domain impact. In this paper,\nwe present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain\nconsistency, a gradient clustering algorithm is proposed to group training data\nbased on their learning effects, where a proxy language model and\ndimensionality reduction are employed to reduce computational overhead. To\naccurately measure domain impact, we develop a Fisher Information Matrix (FIM)\nguided metric that quantifies how domain-specific parameter updates affect the\nmodel's output distributions on downstream tasks, with theoretical guarantees.\nFurthermore, to determine optimal sampling ratios, DIDS combines both the\nFIM-guided domain impact assessment and loss learning trajectories that\nindicate domain-specific potential, while accounting for diminishing marginal\nreturns. Extensive experiments demonstrate that DIDS achieves 3.4% higher\naverage performance while maintaining comparable training efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13282", "pdf": "https://arxiv.org/pdf/2504.13282", "abs": "https://arxiv.org/abs/2504.13282", "authors": ["Jiang-Xin Shi", "Tong Wei", "Yu-Feng Li"], "title": "LIFT+: Lightweight Fine-Tuning for Long-Tail Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The fine-tuning paradigm has emerged as a prominent approach for addressing\nlong-tail learning tasks in the era of foundation models. However, the impact\nof fine-tuning strategies on long-tail learning performance remains unexplored.\nIn this work, we disclose that existing paradigms exhibit a profound misuse of\nfine-tuning methods, leaving significant room for improvement in both\nefficiency and accuracy. Specifically, we reveal that heavy fine-tuning\n(fine-tuning a large proportion of model parameters) can lead to non-negligible\nperformance deterioration on tail classes, whereas lightweight fine-tuning\ndemonstrates superior effectiveness. Through comprehensive theoretical and\nempirical validation, we identify this phenomenon as stemming from inconsistent\nclass conditional distributions induced by heavy fine-tuning. Building on this\ninsight, we propose LIFT+, an innovative lightweight fine-tuning framework to\noptimize consistent class conditions. Furthermore, LIFT+ incorporates\nsemantic-aware initialization, minimalist data augmentation, and test-time\nensembling to enhance adaptation and generalization of foundation models. Our\nframework provides an efficient and accurate pipeline that facilitates fast\nconvergence and model compactness. Extensive experiments demonstrate that LIFT+\nsignificantly reduces both training epochs (from $\\sim$100 to $\\leq$15) and\nlearned parameters (less than 1%), while surpassing state-of-the-art approaches\nby a considerable margin. The source code is available at\nhttps://github.com/shijxcs/LIFT-plus.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13261", "pdf": "https://arxiv.org/pdf/2504.13261", "abs": "https://arxiv.org/abs/2504.13261", "authors": ["Dong Wang"], "title": "CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.SI"], "comment": "12 pages, 1 figure, 3 tables", "summary": "Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT\nhas significantly impacted foreign language education, yet their pedagogical\ngrammar competence remains under-assessed. This paper introduces CPG-EVAL, the\nfirst dedicated benchmark specifically designed to evaluate LLMs' knowledge of\npedagogical grammar within the context of foreign language instruction.\nMethodology: The benchmark comprises five tasks designed to assess grammar\nrecognition, fine-grained grammatical distinction, categorical discrimination,\nand resistance to linguistic interference. Findings: Smaller-scale models can\nsucceed in single language instance tasks, but struggle with multiple instance\ntasks and interference from confusing instances. Larger-scale models show\nbetter resistance to interference but still have significant room for accuracy\nimprovement. The evaluation indicates the need for better instructional\nalignment and more rigorous benchmarks, to effectively guide the deployment of\nLLMs in educational contexts. Value: This study offers the first specialized,\ntheory-driven, multi-tiered benchmark framework for systematically evaluating\nLLMs' pedagogical grammar competence in Chinese language teaching contexts.\nCPG-EVAL not only provides empirical insights for educators, policymakers, and\nmodel developers to better gauge AI's current abilities in educational\nsettings, but also lays the groundwork for future research on improving model\nalignment, enhancing educational suitability, and ensuring informed\ndecision-making concerning LLM integration in foreign language instruction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13297", "pdf": "https://arxiv.org/pdf/2504.13297", "abs": "https://arxiv.org/abs/2504.13297", "authors": ["Andreas Lau Hansen", "Lukas Wanzeck", "Dim P. Papadopoulos"], "title": "Weak Cube R-CNN: Weakly Supervised 3D Detection using only 2D Bounding Boxes", "categories": ["cs.CV", "I.4"], "comment": "14 pages, 5 figures. Accepted for 23rd Scandinavian Conference, SCIA\n  2025, Reykjavik, Iceland", "summary": "Monocular 3D object detection is an essential task in computer vision, and it\nhas several applications in robotics and virtual reality. However, 3D object\ndetectors are typically trained in a fully supervised way, relying extensively\non 3D labeled data, which is labor-intensive and costly to annotate. This work\nfocuses on weakly-supervised 3D detection to reduce data needs using a\nmonocular method that leverages a singlecamera system over expensive LiDAR\nsensors or multi-camera setups. We propose a general model Weak Cube R-CNN,\nwhich can predict objects in 3D at inference time, requiring only 2D box\nannotations for training by exploiting the relationship between 2D projections\nof 3D cubes. Our proposed method utilizes pre-trained frozen foundation 2D\nmodels to estimate depth and orientation information on a training set. We use\nthese estimated values as pseudo-ground truths during training. We design loss\nfunctions that avoid 3D labels by incorporating information from the external\nmodels into the loss. In this way, we aim to implicitly transfer knowledge from\nthese large foundation 2D models without having access to 3D bounding box\nannotations. Experimental results on the SUN RGB-D dataset show increased\nperformance in accuracy compared to an annotation time equalized Cube R-CNN\nbaseline. While not precise for centimetre-level measurements, this method\nprovides a strong foundation for further research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13436", "pdf": "https://arxiv.org/pdf/2504.13436", "abs": "https://arxiv.org/abs/2504.13436", "authors": ["YoungWoo Kim", "Jaehong Lee", "Duksu Kim"], "title": "RT-HDIST: Ray-Tracing Core-based Hausdorff Distance Computation", "categories": ["cs.GR", "cs.CG"], "comment": "8 pages, 7 figures", "summary": "The Hausdorff distance is a fundamental metric with widespread applications\nacross various fields. However, its computation remains computationally\nexpensive, especially for large-scale datasets. In this work, we present\nRT-HDIST, the first Hausdorff distance algorithm accelerated by ray-tracing\ncores (RT-cores). By reformulating the Hausdorff distance problem as a series\nof nearest-neighbor searches and introducing a novel quantized index space,\nRT-HDIST achieves significant reductions in computational overhead while\nmaintaining exact results. Extensive benchmarks demonstrate up to a\ntwo-order-of-magnitude speedup over prior state-of-the-art methods,\nunderscoring RT-HDIST's potential for real-time and large-scale applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13448", "pdf": "https://arxiv.org/pdf/2504.13448", "abs": "https://arxiv.org/abs/2504.13448", "authors": ["Daniela Ushizima", "Guilherme Melo dos Santos", "Zineb Sordo", "Ronald Pandolfi", "Jeffrey Donatelli"], "title": "Ascribe New Dimensions to Scientific Data Visualization with VR", "categories": ["cs.GR", "cs.AI", "cs.CE"], "comment": null, "summary": "For over half a century, the computer mouse has been the primary tool for\ninteracting with digital data, yet it remains a limiting factor in exploring\ncomplex, multi-scale scientific images. Traditional 2D visualization methods\nhinder intuitive analysis of inherently 3D structures. Virtual Reality (VR)\noffers a transformative alternative, providing immersive, interactive\nenvironments that enhance data comprehension. This article introduces\nASCRIBE-VR, a VR platform of Autonomous Solutions for Computational Research\nwith Immersive Browsing \\& Exploration, which integrates AI-driven algorithms\nwith scientific images. ASCRIBE-VR enables multimodal analysis, structural\nassessments, and immersive visualization, supporting scientific visualization\nof advanced datasets such as X-ray CT, Magnetic Resonance, and synthetic 3D\nimaging. Our VR tools, compatible with Meta Quest, can consume the output of\nour AI-based segmentation and iterative feedback processes to enable seamless\nexploration of large-scale 3D images. By merging AI-generated results with VR\nvisualization, ASCRIBE-VR enhances scientific discovery, bridging the gap\nbetween computational analysis and human intuition in materials research,\nconnecting human-in-the-loop with digital twins.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13367", "pdf": "https://arxiv.org/pdf/2504.13367", "abs": "https://arxiv.org/abs/2504.13367", "authors": ["Xiao Pu", "Michael Saxon", "Wenyue Hua", "William Yang Wang"], "title": "THOUGHTTERMINATOR: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning models have demonstrated impressive performance on difficult tasks\nthat traditional language models struggle at. However, many are plagued with\nthe problem of overthinking--generating large amounts of unnecessary tokens\nwhich don't improve accuracy on a question. We introduce approximate measures\nof problem-level difficulty and demonstrate that a clear relationship between\nproblem difficulty and optimal token spend exists, and evaluate how well\ncalibrated a variety of reasoning models are in terms of efficiently allocating\nthe optimal token count. We find that in general, reasoning models are poorly\ncalibrated, particularly on easy problems. To evaluate calibration on easy\nquestions we introduce DUMB500, a dataset of extremely easy math, reasoning,\ncode, and task problems, and jointly evaluate reasoning model on these simple\nexamples and extremely difficult examples from existing frontier benchmarks on\nthe same task domain. Finally, we introduce THOUGHTTERMINATOR, a training-free\nblack box decoding technique that significantly improves reasoning model\ncalibration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13365", "pdf": "https://arxiv.org/pdf/2504.13365", "abs": "https://arxiv.org/abs/2504.13365", "authors": ["Long Li", "Jiajia Li", "Dong Chen", "Lina Pu", "Haibo Yao", "Yanbo Huang"], "title": "VLLFL: A Vision-Language Model Based Lightweight Federated Learning Framework for Smart Agriculture", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "In modern smart agriculture, object detection plays a crucial role by\nenabling automation, precision farming, and monitoring of resources. From\nidentifying crop health and pest infestations to optimizing harvesting\nprocesses, accurate object detection enhances both productivity and\nsustainability. However, training object detection models often requires\nlarge-scale data collection and raises privacy concerns, particularly when\nsensitive agricultural data is distributed across farms. To address these\nchallenges, we propose VLLFL, a vision-language model-based lightweight\nfederated learning framework (VLLFL). It harnesses the generalization and\ncontext-aware detection capabilities of the vision-language model (VLM) and\nleverages the privacy-preserving nature of federated learning. By training a\ncompact prompt generator to boost the performance of the VLM deployed across\ndifferent farms, VLLFL preserves privacy while reducing communication overhead.\nExperimental results demonstrate that VLLFL achieves 14.53% improvement in the\nperformance of VLM while reducing 99.3% communication overhead. Spanning tasks\nfrom identifying a wide variety of fruits to detecting harmful animals in\nagriculture, the proposed framework offers an efficient, scalable, and\nprivacy-preserving solution specifically tailored to agricultural applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13392", "pdf": "https://arxiv.org/pdf/2504.13392", "abs": "https://arxiv.org/abs/2504.13392", "authors": ["Evans Xu Han", "Alice Qian Zhang", "Hong Shen", "Haiyi Zhu", "Paul Pu Liang", "Jane Hsieh"], "title": "POET: Supporting Prompting Creativity and Personalization with Automated Expansion of Text-to-Image Generation", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "State-of-the-art visual generative AI tools hold immense potential to assist\nusers in the early ideation stages of creative tasks -- offering the ability to\ngenerate (rather than search for) novel and unprecedented (instead of existing)\nimages of considerable quality that also adhere to boundless combinations of\nuser specifications. However, many large-scale text-to-image systems are\ndesigned for broad applicability, yielding conventional output that may limit\ncreative exploration. They also employ interaction methods that may be\ndifficult for beginners. Given that creative end users often operate in\ndiverse, context-specific ways that are often unpredictable, more variation and\npersonalization are necessary. We introduce POET, a real-time interactive tool\nthat (1) automatically discovers dimensions of homogeneity in text-to-image\ngenerative models, (2) expands these dimensions to diversify the output space\nof generated images, and (3) learns from user feedback to personalize\nexpansions. An evaluation with 28 users spanning four creative task domains\ndemonstrated POET's ability to generate results with higher perceived diversity\nand help users reach satisfaction in fewer prompts during creative tasks,\nthereby prompting them to deliberate and reflect more on a wider range of\npossible produced results during the co-creative process. Focusing on visual\ncreativity, POET offers a first glimpse of how interaction techniques of future\ntext-to-image generation tools may support and align with more pluralistic\nvalues and the needs of end users during the ideation stages of their work.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13393", "pdf": "https://arxiv.org/pdf/2504.13393", "abs": "https://arxiv.org/abs/2504.13393", "authors": ["S M Rayeed", "Alyson East", "Samuel Stevens", "Sydne Record", "Charles V Stewart"], "title": "BeetleVerse: A study on taxonomic classification of ground beetles", "categories": ["cs.CV"], "comment": null, "summary": "Ground beetles are a highly sensitive and speciose biological indicator,\nmaking them vital for monitoring biodiversity. However, they are currently an\nunderutilized resource due to the manual effort required by taxonomic experts\nto perform challenging species differentiations based on subtle morphological\ndifferences, precluding widespread applications. In this paper, we evaluate 12\nvision models on taxonomic classification across four diverse, long-tailed\ndatasets spanning over 230 genera and 1769 species, with images ranging from\ncontrolled laboratory settings to challenging field-collected (in-situ)\nphotographs. We further explore taxonomic classification in two important\nreal-world contexts: sample efficiency and domain adaptation. Our results show\nthat the Vision and Language Transformer combined with an MLP head is the best\nperforming model, with 97\\% accuracy at genus and 94\\% at species level. Sample\nefficiency analysis shows that we can reduce train data requirements by up to\n50\\% with minimal compromise in performance. The domain adaptation experiments\nreveal significant challenges when transferring models from lab to in-situ\nimages, highlighting a critical domain gap. Overall, our study lays a\nfoundation for large-scale automated taxonomic classification of beetles, and\nbeyond that, advances sample-efficient learning and cross-domain adaptation for\ndiverse long-tailed ecological datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13399", "pdf": "https://arxiv.org/pdf/2504.13399", "abs": "https://arxiv.org/abs/2504.13399", "authors": ["Shashank Shriram", "Srinivasa Perisetla", "Aryan Keskar", "Harsha Krishnaswamy", "Tonko Emil Westerhof Bossen", "Andreas MÃ¸gelmose", "Ross Greer"], "title": "Towards a Multi-Agent Vision-Language System for Zero-Shot Novel Hazardous Object Detection for Autonomous Driving Safety", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting anomalous hazards in visual data, particularly in video streams, is\na critical challenge in autonomous driving. Existing models often struggle with\nunpredictable, out-of-label hazards due to their reliance on predefined object\ncategories. In this paper, we propose a multimodal approach that integrates\nvision-language reasoning with zero-shot object detection to improve hazard\nidentification and explanation. Our pipeline consists of a Vision-Language\nModel (VLM), a Large Language Model (LLM), in order to detect hazardous objects\nwithin a traffic scene. We refine object detection by incorporating OpenAI's\nCLIP model to match predicted hazards with bounding box annotations, improving\nlocalization accuracy. To assess model performance, we create a ground truth\ndataset by denoising and extending the foundational COOOL\n(Challenge-of-Out-of-Label) anomaly detection benchmark dataset with complete\nnatural language descriptions for hazard annotations. We define a means of\nhazard detection and labeling evaluation on the extended dataset using cosine\nsimilarity. This evaluation considers the semantic similarity between the\npredicted hazard description and the annotated ground truth for each video.\nAdditionally, we release a set of tools for structuring and managing\nlarge-scale hazard detection datasets. Our findings highlight the strengths and\nlimitations of current vision-language-based approaches, offering insights into\nfuture improvements in autonomous hazard detection systems. Our models,\nscripts, and data can be found at https://github.com/mi3labucm/COOOLER.git", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety", "accuracy"], "score": 5}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13500", "pdf": "https://arxiv.org/pdf/2504.13500", "abs": "https://arxiv.org/abs/2504.13500", "authors": ["Jianing Wang", "Jin Jiang", "Yang Liu", "Mengdi Zhang", "Xunliang Cai"], "title": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce a new \\emph{process prejudge} strategy in LLM\nreasoning to demonstrate that bootstrapping with process prejudge allows the\nLLM to adaptively anticipate the errors encountered when advancing the\nsubsequent reasoning steps, similar to people sometimes pausing to think about\nwhat mistakes may occur and how to avoid them, rather than relying solely on\ntrial and error. Specifically, we define a prejudge node in the rationale,\nwhich represents a reasoning step, with at least one step that follows the\nprejudge node that has no paths toward the correct answer. To synthesize the\nprejudge reasoning process, we present an automated reasoning framework with a\ndynamic tree-searching strategy. This framework requires only one LLM to\nperform answer judging, response critiquing, prejudge generation, and thought\ncompletion. Furthermore, we develop a two-phase training mechanism with\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to further enhance\nthe reasoning capabilities of LLMs. Experimental results from competition-level\ncomplex reasoning demonstrate that our method can teach the model to prejudge\nbefore thinking and significantly enhance the reasoning ability of LLMs. Code\nand data is released at https://github.com/wjn1996/Prejudge-Before-Think.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13534", "pdf": "https://arxiv.org/pdf/2504.13534", "abs": "https://arxiv.org/abs/2504.13534", "authors": ["Feiyang Li", "Peng Fang", "Zhan Shi", "Arijit Khan", "Fang Wang", "Dan Feng", "Weihao Wang", "Xin Zhang", "Yongjian Cui"], "title": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While chain-of-thought (CoT) reasoning improves the performance of large\nlanguage models (LLMs) in complex tasks, it still has two main challenges: the\nlow reliability of relying solely on LLMs to generate reasoning chains and the\ninterference of natural language reasoning chains on the inference logic of\nLLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework\nwith three key designs: (i) Knowledge Graph-driven CoT Generation, featuring\nknowledge graphs to modulate reasoning chain generation of LLMs, thereby\nenhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which\nincorporates retrieval-augmented generation (RAG) into knowledge graphs to\nretrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable\ninformation; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to\nexecute reasoning tasks in pseudo-programs with greater logical rigor. We\nconduct a comprehensive evaluation on nine public datasets, covering three\nreasoning problems. Compared with the-state-of-the-art methods, CoT-RAG\nexhibits a significant accuracy improvement, ranging from 4.0% to 23.0%.\nFurthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable\naccuracy and efficient execution, highlighting its strong practical\napplicability and scalability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13419", "pdf": "https://arxiv.org/pdf/2504.13419", "abs": "https://arxiv.org/abs/2504.13419", "authors": ["Wenyu Li", "Sidun Liu", "Peng Qiao", "Yong Dou"], "title": "Mono3R: Exploiting Monocular Cues for Geometric 3D Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in data-driven geometric multi-view 3D reconstruction\nfoundation models (e.g., DUSt3R) have shown remarkable performance across\nvarious 3D vision tasks, facilitated by the release of large-scale,\nhigh-quality 3D datasets. However, as we observed, constrained by their\nmatching-based principles, the reconstruction quality of existing models\nsuffers significant degradation in challenging regions with limited matching\ncues, particularly in weakly textured areas and low-light conditions. To\nmitigate these limitations, we propose to harness the inherent robustness of\nmonocular geometry estimation to compensate for the inherent shortcomings of\nmatching-based methods. Specifically, we introduce a monocular-guided\nrefinement module that integrates monocular geometric priors into multi-view\nreconstruction frameworks. This integration substantially enhances the\nrobustness of multi-view reconstruction systems, leading to high-quality\nfeed-forward reconstructions. Comprehensive experiments across multiple\nbenchmarks demonstrate that our method achieves substantial improvements in\nboth mutli-view camera pose estimation and point cloud accuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13428", "pdf": "https://arxiv.org/pdf/2504.13428", "abs": "https://arxiv.org/abs/2504.13428", "authors": ["Qi'ao Xu", "Pengfei Wang", "Yanjun Li", "Tianwen Qian", "Xiaoling Wang"], "title": "HSACNet: Hierarchical Scale-Aware Consistency Regularized Semi-Supervised Change Detection", "categories": ["cs.CV"], "comment": "7 pages, 8 figures, accepted by ICME 2025", "summary": "Semi-supervised change detection (SSCD) aims to detect changes between\nbi-temporal remote sensing images by utilizing limited labeled data and\nabundant unlabeled data. Existing methods struggle in complex scenarios,\nexhibiting poor performance when confronted with noisy data. They typically\nneglect intra-layer multi-scale features while emphasizing inter-layer fusion,\nharming the integrity of change objects with different scales. In this paper,\nwe propose HSACNet, a Hierarchical Scale-Aware Consistency regularized Network\nfor SSCD. Specifically, we integrate Segment Anything Model 2 (SAM2), using its\nHiera backbone as the encoder to extract inter-layer multi-scale features and\napplying adapters for parameter-efficient fine-tuning. Moreover, we design a\nScale-Aware Differential Attention Module (SADAM) that can precisely capture\nintra-layer multi-scale change features and suppress noise. Additionally, a\ndual-augmentation consistency regularization strategy is adopted to effectively\nutilize the unlabeled data. Extensive experiments across four CD benchmarks\ndemonstrate that our HSACNet achieves state-of-the-art performance, with\nreduced parameters and computational cost.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13442", "pdf": "https://arxiv.org/pdf/2504.13442", "abs": "https://arxiv.org/abs/2504.13442", "authors": ["Zhenyu Yu", "Mohd. Yamani Idna Idris", "Pei Wang"], "title": "SatelliteCalculator: A Multi-Task Vision Foundation Model for Quantitative Remote Sensing Inversion", "categories": ["cs.CV"], "comment": null, "summary": "Quantitative remote sensing inversion plays a critical role in environmental\nmonitoring, enabling the estimation of key ecological variables such as\nvegetation indices, canopy structure, and carbon stock. Although vision\nfoundation models have achieved remarkable progress in classification and\nsegmentation tasks, their application to physically interpretable regression\nremains largely unexplored. Furthermore, the multi-spectral nature and\ngeospatial heterogeneity of remote sensing data pose significant challenges for\ngeneralization and transferability. To address these issues, we introduce\nSatelliteCalculator, the first vision foundation model tailored for\nquantitative remote sensing inversion. By leveraging physically defined index\nformulas, we automatically construct a large-scale dataset of over one million\npaired samples across eight core ecological indicators. The model integrates a\nfrozen Swin Transformer backbone with a prompt-guided architecture, featuring\ncross-attentive adapters and lightweight task-specific MLP decoders.\nExperiments on the Open-Canopy benchmark demonstrate that SatelliteCalculator\nachieves competitive accuracy across all tasks while significantly reducing\ninference cost. Our results validate the feasibility of applying foundation\nmodels to quantitative inversion, and provide a scalable framework for\ntask-adaptive remote sensing estimation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13460", "pdf": "https://arxiv.org/pdf/2504.13460", "abs": "https://arxiv.org/abs/2504.13460", "authors": ["Hongwei Ji", "Wulian Yun", "Mengshi Qi", "Huadong Ma"], "title": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13469", "pdf": "https://arxiv.org/pdf/2504.13469", "abs": "https://arxiv.org/abs/2504.13469", "authors": ["YangChen Zeng"], "title": "HMPE:HeatMap Embedding for Efficient Transformer-Based Small Object Detection", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Current Transformer-based methods for small object detection continue\nemerging, yet they have still exhibited significant shortcomings. This paper\nintroduces HeatMap Position Embedding (HMPE), a novel Transformer Optimization\ntechnique that enhances object detection performance by dynamically integrating\npositional encoding with semantic detection information through heatmap-guided\nadaptive learning.We also innovatively visualize the HMPE method, offering\nclear visualization of embedded information for parameter fine-tuning.We then\ncreate Multi-Scale ObjectBox-Heatmap Fusion Encoder (MOHFE) and HeatMap Induced\nHigh-Quality Queries for Decoder (HIDQ) modules. These are designed for the\nencoder and decoder, respectively, to generate high-quality queries and reduce\nbackground noise queries.Using both heatmap embedding and Linear-Snake\nConv(LSConv) feature engineering, we enhance the embedding of massively diverse\nsmall object categories and reduced the decoder multihead layers, thereby\naccelerating both inference and training.In the generalization experiments, our\napproach outperforme the baseline mAP by 1.9% on the small object dataset (NWPU\nVHR-10) and by 1.2% on the general dataset (PASCAL VOC). By employing\nHMPE-enhanced embedding, we are able to reduce the number of decoder layers\nfrom eight to a minimum of three, significantly decreasing both inference and\ntraining costs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13561", "pdf": "https://arxiv.org/pdf/2504.13561", "abs": "https://arxiv.org/abs/2504.13561", "authors": ["Yang Wu", "Yun Zhu", "Kaihua Zhang", "Jianjun Qian", "Jin Xie", "Jian Yang"], "title": "WeatherGen: A Unified Diverse Weather Generator for LiDAR Point Clouds via Spider Mamba Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "3D scene perception demands a large amount of adverse-weather LiDAR data, yet\nthe cost of LiDAR data collection presents a significant scaling-up challenge.\nTo this end, a series of LiDAR simulators have been proposed. Yet, they can\nonly simulate a single adverse weather with a single physical model, and the\nfidelity of the generated data is quite limited. This paper presents\nWeatherGen, the first unified diverse-weather LiDAR data diffusion generation\nframework, significantly improving fidelity. Specifically, we first design a\nmap-based data producer, which can provide a vast amount of high-quality\ndiverse-weather data for training purposes. Then, we utilize the\ndiffusion-denoising paradigm to construct a diffusion model. Among them, we\npropose a spider mamba generator to restore the disturbed diverse weather data\ngradually. The spider mamba models the feature interactions by scanning the\nLiDAR beam circle or central ray, excellently maintaining the physical\nstructure of the LiDAR data. Subsequently, following the generator to transfer\nreal-world knowledge, we design a latent feature aligner. Afterward, we devise\na contrastive learning-based controller, which equips weather control signals\nwith compact semantic knowledge through language supervision, guiding the\ndiffusion model to generate more discriminative data. Extensive evaluations\ndemonstrate the high generation quality of WeatherGen. Through WeatherGen, we\nconstruct the mini-weather dataset, promoting the performance of the downstream\ntask under adverse weather conditions. Code is available:\nhttps://github.com/wuyang98/weathergen", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13590", "pdf": "https://arxiv.org/pdf/2504.13590", "abs": "https://arxiv.org/abs/2504.13590", "authors": ["Alexander Rusnak", "FrÃ©dÃ©ric Kaplan"], "title": "HAECcity: Open-Vocabulary Scene Understanding of City-Scale Point Clouds with Superpoint Graph Clustering", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication through the upcoming CVPR Workshop on open\n  scene understanding with foundation models (OPENSUN3D)", "summary": "Traditional 3D scene understanding techniques are generally predicated on\nhand-annotated label sets, but in recent years a new class of open-vocabulary\n3D scene understanding techniques has emerged. Despite the success of this\nparadigm on small scenes, existing approaches cannot scale efficiently to\ncity-scale 3D datasets. In this paper, we present Hierarchical vocab-Agnostic\nExpert Clustering (HAEC), after the latin word for 'these', a superpoint graph\nclustering based approach which utilizes a novel mixture of experts graph\ntransformer for its backbone. We administer this highly scalable approach to\nthe first application of open-vocabulary scene understanding on the SensatUrban\ncity-scale dataset. We also demonstrate a synthetic labeling pipeline which is\nderived entirely from the raw point clouds with no hand-annotation. Our\ntechnique can help unlock complex operations on dense urban 3D scenes and open\na new path forward in the processing of digital twins.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13359", "pdf": "https://arxiv.org/pdf/2504.13359", "abs": "https://arxiv.org/abs/2504.13359", "authors": ["Mehmet Hamza Erol", "Batu El", "Mirac Suzgun", "Mert Yuksekgonul", "James Zou"], "title": "Cost-of-Pass: An Economic Framework for Evaluating Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "Code is available at: https://github.com/mhamzaerol/Cost-of-Pass", "summary": "The widespread adoption of AI systems in the economy hinges on their ability\nto generate economic value that outweighs their inference costs. Evaluating\nthis tradeoff requires metrics that account for both performance and costs. We\npropose a framework grounded in production theory for evaluating language\nmodels by combining accuracy and inference cost. We introduce \"cost-of-pass\",\nthe expected monetary cost of generating a correct solution. We then define the\n\"frontier cost-of-pass\" as the minimum cost-of-pass achievable across available\nmodels or the \"human-expert, using the approximate cost of hiring an expert.\nOur analysis reveals distinct economic insights. First, lightweight models are\nmost cost-effective for basic quantitative tasks, large models for\nknowledge-intensive ones, and reasoning models for complex quantitative\nproblems, despite higher per-token costs. Second, tracking this frontier\ncost-of-pass over the past year reveals significant progress, particularly for\ncomplex quantitative tasks where the cost has roughly halved every few months.\nThird, to trace key innovations driving this progress, we examine\ncounterfactual frontiers: estimates of cost-efficiency without specific model\nclasses. We find that innovations in lightweight, large, and reasoning models\nhave been essential for pushing the frontier in basic quantitative,\nknowledge-intensive, and complex quantitative tasks, respectively. Finally, we\nassess the cost-reductions afforded by common inference-time techniques like\nmajority voting and self-refinement, finding that their marginal accuracy gains\nrarely justify their costs. Our findings underscore that complementary\nmodel-level innovations are the primary drivers of cost-efficiency, and our\neconomic framework provides a principled tool for measuring this progress and\nguiding deployment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13638", "pdf": "https://arxiv.org/pdf/2504.13638", "abs": "https://arxiv.org/abs/2504.13638", "authors": ["Yang Zhang", "Jingyi Cao", "Yanan You", "Yuanyuan Qiao"], "title": "DenSe-AdViT: A novel Vision Transformer for Dense SAR Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformer (ViT) has achieved remarkable results in object detection\nfor synthetic aperture radar (SAR) images, owing to its exceptional ability to\nextract global features. However, it struggles with the extraction of\nmulti-scale local features, leading to limited performance in detecting small\ntargets, especially when they are densely arranged. Therefore, we propose\nDensity-Sensitive Vision Transformer with Adaptive Tokens (DenSe-AdViT) for\ndense SAR target detection. We design a Density-Aware Module (DAM) as a\npreliminary component that generates a density tensor based on target\ndistribution. It is guided by a meticulously crafted objective metric, enabling\nprecise and effective capture of the spatial distribution and density of\nobjects. To integrate the multi-scale information enhanced by convolutional\nneural networks (CNNs) with the global features derived from the Transformer,\nDensity-Enhanced Fusion Module (DEFM) is proposed. It effectively refines\nattention toward target-survival regions with the assist of density mask and\nthe multiple sources features. Notably, our DenSe-AdViT achieves 79.8% mAP on\nthe RSDD dataset and 92.5% on the SIVED dataset, both of which feature a large\nnumber of densely distributed vehicle targets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13480", "pdf": "https://arxiv.org/pdf/2504.13480", "abs": "https://arxiv.org/abs/2504.13480", "authors": ["Minsu Koh", "Beom-Chul Park", "Heejo Kong", "Seong-Whan Lee"], "title": "Integrating Locality-Aware Attention with Transformers for General Geometry PDEs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by IJCNN 2025", "summary": "Neural operators have emerged as promising frameworks for learning mappings\ngoverned by partial differential equations (PDEs), serving as data-driven\nalternatives to traditional numerical methods. While methods such as the\nFourier neural operator (FNO) have demonstrated notable performance, their\nreliance on uniform grids restricts their applicability to complex geometries\nand irregular meshes. Recently, Transformer-based neural operators with linear\nattention mechanisms have shown potential in overcoming these limitations for\nlarge-scale PDE simulations. However, these approaches predominantly emphasize\nglobal feature aggregation, often overlooking fine-scale dynamics and localized\nPDE behaviors essential for accurate solutions. To address these challenges, we\npropose the Locality-Aware Attention Transformer (LA2Former), which leverages\nK-nearest neighbors for dynamic patchifying and integrates global-local\nattention for enhanced PDE modeling. By combining linear attention for\nefficient global context encoding with pairwise attention for capturing\nintricate local interactions, LA2Former achieves an optimal balance between\ncomputational efficiency and predictive accuracy. Extensive evaluations across\nsix benchmark datasets demonstrate that LA2Former improves predictive accuracy\nby over 50% relative to existing linear attention methods, while also\noutperforming full pairwise attention under optimal conditions. This work\nunderscores the critical importance of localized feature learning in advancing\nTransformer-based neural operators for solving PDEs on complex and irregular\ndomains.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13756", "pdf": "https://arxiv.org/pdf/2504.13756", "abs": "https://arxiv.org/abs/2504.13756", "authors": ["Dmitrii Kharlapenko", "Stepan Shabalin", "Fazl Barez", "Arthur Conmy", "Neel Nanda"], "title": "Scaling sparse feature circuit finding for in-context learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse autoencoders (SAEs) are a popular tool for interpreting large language\nmodel activations, but their utility in addressing open questions in\ninterpretability remains unclear. In this work, we demonstrate their\neffectiveness by using SAEs to deepen our understanding of the mechanism behind\nin-context learning (ICL). We identify abstract SAE features that (i) encode\nthe model's knowledge of which task to execute and (ii) whose latent vectors\ncausally induce the task zero-shot. This aligns with prior work showing that\nICL is mediated by task vectors. We further demonstrate that these task vectors\nare well approximated by a sparse sum of SAE latents, including these\ntask-execution features. To explore the ICL mechanism, we adapt the sparse\nfeature circuits methodology of Marks et al. (2024) to work for the much larger\nGemma-1 2B model, with 30 times as many parameters, and to the more complex\ntask of ICL. Through circuit finding, we discover task-detecting features with\ncorresponding SAE latents that activate earlier in the prompt, that detect when\ntasks have been performed. They are causally linked with task-execution\nfeatures through the attention and MLP sublayers.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13754", "pdf": "https://arxiv.org/pdf/2504.13754", "abs": "https://arxiv.org/abs/2504.13754", "authors": ["Zhu Zhu", "Shuo Jiang", "Jingyuan Zheng", "Yawen Li", "Yifei Chen", "Manli Zhao", "Weizhong Gu", "Feiwei Qin", "Jinhu Wang", "Gang Yu"], "title": "Towards Accurate and Interpretable Neuroblastoma Diagnosis via Contrastive Multi-scale Pathological Image Analysis", "categories": ["cs.CV", "cs.AI"], "comment": "14pages, 8 figures", "summary": "Neuroblastoma, adrenal-derived, is among the most common pediatric solid\nmalignancies, characterized by significant clinical heterogeneity. Timely and\naccurate pathological diagnosis from hematoxylin and eosin-stained whole slide\nimages is critical for patient prognosis. However, current diagnostic practices\nprimarily rely on subjective manual examination by pathologists, leading to\ninconsistent accuracy. Existing automated whole slide image classification\nmethods encounter challenges such as poor interpretability, limited feature\nextraction capabilities, and high computational costs, restricting their\npractical clinical deployment. To overcome these limitations, we propose\nCMSwinKAN, a contrastive-learning-based multi-scale feature fusion model\ntailored for pathological image classification, which enhances the Swin\nTransformer architecture by integrating a Kernel Activation Network within its\nmultilayer perceptron and classification head modules, significantly improving\nboth interpretability and accuracy. By fusing multi-scale features and\nleveraging contrastive learning strategies, CMSwinKAN mimics clinicians'\ncomprehensive approach, effectively capturing global and local tissue\ncharacteristics. Additionally, we introduce a heuristic soft voting mechanism\nguided by clinical insights to seamlessly bridge patch-level predictions to\nwhole slide image-level classifications. We validate CMSwinKAN on the PpNTs\ndataset, which was collaboratively established with our partner hospital and\nthe publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN\nperforms better than existing state-of-the-art pathology-specific models\npre-trained on large datasets. Our source code is available at\nhttps://github.com/JSLiam94/CMSwinKAN.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13820", "pdf": "https://arxiv.org/pdf/2504.13820", "abs": "https://arxiv.org/abs/2504.13820", "authors": ["Yang Yue", "Yulin Wang", "Chenxin Tao", "Pan Liu", "Shiji Song", "Gao Huang"], "title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation Learning", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Humans can develop internal world models that encode common sense knowledge,\ntelling them how the world works and predicting the consequences of their\nactions. This concept has emerged as a promising direction for establishing\ngeneral-purpose machine-learning models in recent preliminary works, e.g., for\nvisual representation learning. In this paper, we present CheXWorld, the first\neffort towards a self-supervised world model for radiographic images.\nSpecifically, our work develops a unified framework that simultaneously models\nthree aspects of medical knowledge essential for qualified radiologists,\nincluding 1) local anatomical structures describing the fine-grained\ncharacteristics of local tissues (e.g., architectures, shapes, and textures);\n2) global anatomical layouts describing the global organization of the human\nbody (e.g., layouts of organs and skeletons); and 3) domain variations that\nencourage CheXWorld to model the transitions across different appearance\ndomains of radiographs (e.g., varying clarity, contrast, and exposure caused by\ncollecting radiographs from different hospitals, devices, or patients).\nEmpirically, we design tailored qualitative and quantitative analyses,\nrevealing that CheXWorld successfully captures these three dimensions of\nmedical knowledge. Furthermore, transfer learning experiments across eight\nmedical image classification and segmentation benchmarks showcase that\nCheXWorld significantly outperforms existing SSL methods and large-scale\nmedical foundation models. Code & pre-trained models are available at\nhttps://github.com/LeapLabTHU/CheXWorld.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13390", "pdf": "https://arxiv.org/pdf/2504.13390", "abs": "https://arxiv.org/abs/2504.13390", "authors": ["Mahrokh Najaf", "Gregory Ongie"], "title": "Accelerated Optimization of Implicit Neural Representations for CT Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "IEEE ISBI 2025", "summary": "Inspired by their success in solving challenging inverse problems in computer\nvision, implicit neural representations (INRs) have been recently proposed for\nreconstruction in low-dose/sparse-view X-ray computed tomography (CT). An INR\nrepresents a CT image as a small-scale neural network that takes spatial\ncoordinates as inputs and outputs attenuation values. Fitting an INR to\nsinogram data is similar to classical model-based iterative reconstruction\nmethods. However, training INRs with losses and gradient-based algorithms can\nbe prohibitively slow, taking many thousands of iterations to converge. This\npaper investigates strategies to accelerate the optimization of INRs for CT\nreconstruction. In particular, we propose two approaches: (1) using a modified\nloss function with improved conditioning, and (2) an algorithm based on the\nalternating direction method of multipliers. We illustrate that both of these\napproaches significantly accelerate INR-based reconstruction of a synthetic\nbreast CT phantom in a sparse-view setting.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
{"id": "2504.13553", "pdf": "https://arxiv.org/pdf/2504.13553", "abs": "https://arxiv.org/abs/2504.13553", "authors": ["Yihao Ouyang", "Xunheng Kuang", "Mengjia Xiong", "Zhida Wang", "Yuanquan Wang"], "title": "A Novel Hybrid Approach for Retinal Vessel Segmentation with Dynamic Long-Range Dependency and Multi-Scale Retinal Edge Fusion Enhancement", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate retinal vessel segmentation provides essential structural\ninformation for ophthalmic image analysis. However, existing methods struggle\nwith challenges such as multi-scale vessel variability, complex curvatures, and\nambiguous boundaries. While Convolutional Neural Networks (CNNs),\nTransformer-based models and Mamba-based architectures have advanced the field,\nthey often suffer from vascular discontinuities or edge feature ambiguity. To\naddress these limitations, we propose a novel hybrid framework that\nsynergistically integrates CNNs and Mamba for high-precision retinal vessel\nsegmentation. Our approach introduces three key innovations: 1) The proposed\nHigh-Resolution Edge Fuse Network is a high-resolution preserving hybrid\nsegmentation framework that combines a multi-scale backbone with the\nMulti-scale Retina Edge Fusion (MREF) module to enhance edge features, ensuring\naccurate and robust vessel segmentation. 2) The Dynamic Snake Visual State\nSpace block combines Dynamic Snake Convolution with Mamba to adaptively capture\nvessel curvature details and long-range dependencies. An improved\neight-directional 2D Snake-Selective Scan mechanism and a dynamic weighting\nstrategy enhance the perception of complex vascular topologies. 3) The MREF\nmodule enhances boundary precision through multi-scale edge feature\naggregation, suppressing noise while emphasizing critical vessel structures\nacross scales. Experiments on three public datasets demonstrate that our method\nachieves state-of-the-art performance, particularly in maintaining vascular\ncontinuity and effectively segmenting vessels in low-contrast regions. This\nwork provides a robust method for clinical applications requiring accurate\nretinal vessel analysis. The code is available at\nhttps://github.com/frank-oy/HREFNet.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-21.jsonl"}
