{"id": "2504.14597", "pdf": "https://arxiv.org/pdf/2504.14597", "abs": "https://arxiv.org/abs/2504.14597", "authors": ["Lingrui Mei", "Shenghua Liu", "Yiwei Wang", "Baolong Bi", "Yuyao Ge", "Jun Wan", "Yurong Wu", "Xueqi Cheng"], "title": "a1: Steep Test-time Scaling Law via Environment Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made remarkable breakthroughs in reasoning,\nyet continue to struggle with hallucinations, logical errors, and inability to\nself-correct during complex multi-step tasks. Current approaches like\nchain-of-thought prompting offer limited reasoning capabilities that fail when\nprecise step validation is required. We propose Environment Augmented\nGeneration (EAG), a framework that enhances LLM reasoning through: (1)\nreal-time environmental feedback validating each reasoning step, (2) dynamic\nbranch exploration for investigating alternative solution paths when faced with\nerrors, and (3) experience-based learning from successful reasoning\ntrajectories. Unlike existing methods, EAG enables deliberate backtracking and\nstrategic replanning through tight integration of execution feedback with\nbranching exploration. Our a1-32B model achieves state-of-the-art performance\namong similar-sized models across all benchmarks, matching larger models like\no1 on competition mathematics while outperforming comparable models by up to\n24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern:\ninitial token investment in environment interaction yields substantial\nlong-term performance dividends, with advantages amplifying proportionally to\ntask complexity. EAG's theoretical framework demonstrates how environment\ninteractivity and systematic branch exploration together establish a new\nparadigm for reliable machine reasoning, particularly for problems requiring\nprecise multi-step calculation and logical verification.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scaling law", "o1"], "score": 4}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15253", "pdf": "https://arxiv.org/pdf/2504.15253", "abs": "https://arxiv.org/abs/2504.15253", "authors": ["Yilun Zhou", "Austin Xu", "Peifeng Wang", "Caiming Xiong", "Shafiq Joty"], "title": "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators", "categories": ["cs.CL", "cs.LG"], "comment": "The first two authors contributed equally. The codebase is at\n  https://github.com/SalesforceAIResearch/jetts-benchmark", "summary": "Scaling test-time computation, or affording a generator large language model\n(LLM) extra compute during inference, typically employs the help of external\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\nmodels trained to generate evaluations and critiques (explanations) in natural\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\njudge empirical successes, their effectiveness as evaluators in test-time\nscaling settings is largely unknown. In this paper, we introduce the Judge\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\nperformance in three domains (math reasoning, code generation, and instruction\nfollowing) under three task settings: response reranking, step-level beam\nsearch, and critique-based response refinement. We evaluate 10 different judge\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\nparameters). Our benchmark shows that while judges are competitive with outcome\nreward models in reranking, they are consistently worse than process reward\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\ntheir natural language critiques are currently ineffective in guiding the\ngenerator towards better responses.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "beam search"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "code generation"], "score": 3}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.13995", "pdf": "https://arxiv.org/pdf/2504.13995", "abs": "https://arxiv.org/abs/2504.13995", "authors": ["Andrea Amaduzzi", "Pierluigi Zama Ramirez", "Giuseppe Lisanti", "Samuele Salti", "Luigi Di Stefano"], "title": "Scaling LLaNA: Advancing NeRF-Language Understanding Through Large-Scale Training", "categories": ["cs.CV"], "comment": "Under submission. Project page at\n  https://andreamaduzzi.github.io/llana/", "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have shown\nremarkable capabilities in understanding both images and 3D data, yet these\nmodalities face inherent limitations in comprehensively representing object\ngeometry and appearance. Neural Radiance Fields (NeRFs) have emerged as a\npromising alternative, encoding both geometric and photorealistic properties\nwithin the weights of a simple Multi-Layer Perceptron (MLP). This work\ninvestigates the feasibility and effectiveness of ingesting NeRFs into an MLLM.\nWe introduce LLaNA, the first MLLM able to perform new tasks such as NeRF\ncaptioning and Q\\&A, by directly processing the weights of a NeRF's MLP.\nNotably, LLaNA is able to extract information about the represented objects\nwithout the need to render images or materialize 3D data structures. In\naddition, we build the first large-scale NeRF-language dataset, composed by\nmore than 300K NeRFs trained on ShapeNet and Objaverse, with paired textual\nannotations that enable various NeRF-language tasks. Based on this dataset, we\ndevelop a benchmark to evaluate the NeRF understanding capability of our\nmethod. Results show that directly processing NeRF weights leads to better\nperformance on NeRF-Language tasks compared to approaches that rely on either\n2D or 3D representations derived from NeRFs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14089", "pdf": "https://arxiv.org/pdf/2504.14089", "abs": "https://arxiv.org/abs/2504.14089", "authors": ["Kang He", "Kaushik Roy"], "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "multi-step reasoning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14108", "pdf": "https://arxiv.org/pdf/2504.14108", "abs": "https://arxiv.org/abs/2504.14108", "authors": ["Zhenyu Yu", "Mohd Yamani Idna Idris", "Pei Wang", "Yuelong Xia"], "title": "Point-Driven Interactive Text and Image Layer Editing Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We present DanceText, a training-free framework for multilingual text editing\nin images, designed to support complex geometric transformations and achieve\nseamless foreground-background integration. While diffusion-based generative\nmodels have shown promise in text-guided image synthesis, they often lack\ncontrollability and fail to preserve layout consistency under non-trivial\nmanipulations such as rotation, translation, scaling, and warping. To address\nthese limitations, DanceText introduces a layered editing strategy that\nseparates text from the background, allowing geometric transformations to be\nperformed in a modular and controllable manner. A depth-aware module is further\nproposed to align appearance and perspective between the transformed text and\nthe reconstructed background, enhancing photorealism and spatial consistency.\nImportantly, DanceText adopts a fully training-free design by integrating\npretrained modules, allowing flexible deployment without task-specific\nfine-tuning. Extensive experiments on the AnyWord-3M benchmark demonstrate that\nour method achieves superior performance in visual quality, especially under\nlarge-scale and complex transformation scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14225", "pdf": "https://arxiv.org/pdf/2504.14225", "abs": "https://arxiv.org/abs/2504.14225", "authors": ["Bowen Jiang", "Zhuoqun Hao", "Young-Min Cho", "Bryan Li", "Yuan Yuan", "Sihao Chen", "Lyle Ungar", "Camillo J. Taylor", "Dan Roth"], "title": "Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as personalized assistants for\nusers across a wide range of tasks -- from offering writing support to\ndelivering tailored recommendations or consultations. Over time, the\ninteraction history between a user and an LLM can provide extensive information\nabout an individual's traits and preferences. However, open questions remain on\nhow well LLMs today can effectively leverage such history to (1) internalize\nthe user's inherent traits and preferences, (2) track how the user profiling\nand preferences evolve over time, and (3) generate personalized responses\naccordingly in new scenarios.\n  In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features\ncurated user profiles with over 180 simulated user-LLM interaction histories,\neach containing up to 60 sessions of multi-turn conversations across 15\nreal-world tasks that require personalization. Given an in-situ user query,\ni.e. query issued by the user from the first-person perspective, we evaluate\nLLM chatbots' ability to identify the most suitable response according to the\ncurrent state of the user's profile. We observe that current LLMs still\nstruggle to recognize the dynamic evolution in users' profiles over time\nthrough direct prompting approaches. As a consequence, LLMs often fail to\ndeliver responses that align with users' current situations and preferences,\nwith frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0\nachieving only around 50% overall accuracy, suggesting room for improvement. We\nhope that PERSONAMEM, along with the user profile and conversation simulation\npipeline, can facilitate future research in the development of truly user-aware\nchatbots. Code and data are available at github.com/bowen-upenn/PersonaMem.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "o1"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14492", "pdf": "https://arxiv.org/pdf/2504.14492", "abs": "https://arxiv.org/abs/2504.14492", "authors": ["Yichen Li", "Zhiting Fan", "Ruizhe Chen", "Xiaotang Gai", "Luqi Gong", "Yan Zhang", "Zuozhu Liu"], "title": "FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are prone to capturing biases from training\ncorpus, leading to potential negative social impacts. Existing prompt-based\ndebiasing methods exhibit instability due to their sensitivity to prompt\nchanges, while fine-tuning-based techniques incur substantial computational\noverhead and catastrophic forgetting. In this paper, we propose FairSteer, a\nnovel inference-time debiasing framework without requiring customized prompt\ndesign or model retraining. Motivated by the linear representation hypothesis,\nour preliminary investigation demonstrates that fairness-related features can\nbe encoded into separable directions in the hidden activation space. FairSteer\noperates in three steps: biased activation detection, debiasing steering vector\n(DSV) computation, and dynamic activation steering. Specifically, it first\ntrains a lightweight linear classifier to detect bias signatures in\nactivations, and then computes DSVs as intervention directions derived from\nsmall contrastive prompt pairs. Subsequently, it performs debiasing by\nadjusting activations with DSVs in the inference stage. Comprehensive\nevaluation with six LLMs demonstrates the superiority of FairSteer across\nquestion-answering, counterfactual input evaluation and open-ended text\ngeneration tasks. Code will be released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "inference time"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14669", "pdf": "https://arxiv.org/pdf/2504.14669", "abs": "https://arxiv.org/abs/2504.14669", "authors": ["Wei Zou", "Sen Yang", "Yu Bao", "Shujian Huang", "Jiajun Chen", "Shanbo Cheng"], "title": "Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data", "categories": ["cs.CL"], "comment": "11 pages, 4 figures", "summary": "The rise of Large Language Models (LLMs) has reshaped machine translation\n(MT), but multilingual MT still relies heavily on parallel data for supervised\nfine-tuning (SFT), facing challenges like data scarcity for low-resource\nlanguages and catastrophic forgetting. To address these issues, we propose\nTRANS-ZERO, a self-play framework that leverages only monolingual data and the\nintrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic\nMonte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong\ntranslation performance that rivals supervised methods. Experiments demonstrate\nthat this approach not only matches the performance of models trained on\nlarge-scale parallel data but also excels in non-English translation\ndirections. Further analysis reveals that G-MCTS itself significantly enhances\ntranslation quality by exploring semantically consistent candidates through\niterative translations, providing a robust foundation for the framework's\nsuccuss.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "MCTS"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14825", "pdf": "https://arxiv.org/pdf/2504.14825", "abs": "https://arxiv.org/abs/2504.14825", "authors": ["Zhoujie Qian"], "title": "ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Transformers (ViTs) have revolutionized computer vision by leveraging\nself-attention to model long-range dependencies. However, ViTs face challenges\nsuch as high computational costs due to the quadratic scaling of self-attention\nand the requirement of a large amount of training data. To address these\nlimitations, we propose the Efficient Convolutional Vision Transformer (ECViT),\na hybrid architecture that effectively combines the strengths of CNNs and\nTransformers. ECViT introduces inductive biases such as locality and\ntranslation invariance, inherent to Convolutional Neural Networks (CNNs) into\nthe Transformer framework by extracting patches from low-level features and\nenhancing the encoder with convolutional operations. Additionally, it\nincorporates local-attention and a pyramid structure to enable efficient\nmulti-scale feature extraction and representation. Experimental results\ndemonstrate that ECViT achieves an optimal balance between performance and\nefficiency, outperforming state-of-the-art models on various image\nclassification tasks while maintaining low computational and storage\nrequirements. ECViT offers an ideal solution for applications that prioritize\nhigh efficiency without compromising performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14920", "pdf": "https://arxiv.org/pdf/2504.14920", "abs": "https://arxiv.org/abs/2504.14920", "authors": ["Geng Li", "Jinglin Xu", "Yunzhen Zhao", "Yuxin Peng"], "title": "DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 (Hightlight). Project page with code:\n  https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025", "summary": "Humans can effortlessly locate desired objects in cluttered environments,\nrelying on a cognitive mechanism known as visual search to efficiently filter\nout irrelevant information and focus on task-related regions. Inspired by this\nprocess, we propose Dyfo (Dynamic Focus), a training-free dynamic focusing\nvisual search method that enhances fine-grained visual understanding in large\nmultimodal models (LMMs). Unlike existing approaches which require additional\nmodules or data collection, Dyfo leverages a bidirectional interaction between\nLMMs and visual experts, using a Monte Carlo Tree Search (MCTS) algorithm to\nsimulate human-like focus adjustments. This enables LMMs to focus on key visual\nregions while filtering out irrelevant content, without introducing additional\ntraining caused by vocabulary expansion or the integration of specialized\nlocalization modules. Experimental results demonstrate that Dyfo significantly\nimproves fine-grained visual understanding and reduces hallucination issues in\nLMMs, achieving superior performance across both fixed and dynamic resolution\nmodels. The code is available at https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search", "MCTS"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14928", "pdf": "https://arxiv.org/pdf/2504.14928", "abs": "https://arxiv.org/abs/2504.14928", "authors": ["Yao Shi", "Rongkeng Liang", "Yong Xu"], "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement", "dialogue"], "score": 3}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.13915", "pdf": "https://arxiv.org/pdf/2504.13915", "abs": "https://arxiv.org/abs/2504.13915", "authors": ["Dibyadip Chatterjee", "Edoardo Remelli", "Yale Song", "Bugra Tekin", "Abhay Mittal", "Bharat Bhatnagar", "Necati Cihan Camg√∂z", "Shreyas Hampali", "Eric Sauser", "Shugao Ma", "Angela Yao", "Fadime Sener"], "title": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video Understanding", "categories": ["cs.CV"], "comment": "13 pages, 5 figures; https://dibschat.github.io/ProVideLLM", "summary": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue", "fine-grained"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.13987", "pdf": "https://arxiv.org/pdf/2504.13987", "abs": "https://arxiv.org/abs/2504.13987", "authors": ["Tariq Berrada Ifriqi", "Adriana Romero-Soriano", "Michal Drozdzal", "Jakob Verbeek", "Karteek Alahari"], "title": "Entropy Rectifying Guidance for Diffusion and Flow Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Guidance techniques are commonly used in diffusion and flow models to improve\nimage quality and consistency for conditional generative tasks such as\nclass-conditional and text-to-image generation. In particular, classifier-free\nguidance (CFG) -- the most widely adopted guidance technique -- contrasts\nconditional and unconditional predictions to improve the generated images. This\nresults, however, in trade-offs across quality, diversity and consistency,\nimproving some at the expense of others. While recent work has shown that it is\npossible to disentangle these factors to some extent, such methods come with an\noverhead of requiring an additional (weaker) model, or require more forward\npasses per sampling step. In this paper, we propose Entropy Rectifying Guidance\n(ERG), a simple and effective guidance mechanism based on inference-time\nchanges in the attention mechanism of state-of-the-art diffusion transformer\narchitectures, which allows for simultaneous improvements over image quality,\ndiversity and prompt consistency. ERG is more general than CFG and similar\nguidance techniques, as it extends to unconditional sampling. ERG results in\nsignificant improvements in various generation tasks such as text-to-image,\nclass-conditional and unconditional image generation. We also show that ERG can\nbe seamlessly combined with other recent guidance methods such as CADS and APG,\nfurther boosting generation performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14075", "pdf": "https://arxiv.org/pdf/2504.14075", "abs": "https://arxiv.org/abs/2504.14075", "authors": ["Wei Dong", "Yan Min", "Han Zhou", "Jun Chen"], "title": "Towards Scale-Aware Low-Light Enhancement via Structure-Guided Transformer Design", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 NTIRE Workshop, Structure prior,\n  CNN-Transformer, LLIE", "summary": "Current Low-light Image Enhancement (LLIE) techniques predominantly rely on\neither direct Low-Light (LL) to Normal-Light (NL) mappings or guidance from\nsemantic features or illumination maps. Nonetheless, the intrinsic\nill-posedness of LLIE and the difficulty in retrieving robust semantics from\nheavily corrupted images hinder their effectiveness in extremely low-light\nenvironments. To tackle this challenge, we present SG-LLIE, a new multi-scale\nCNN-Transformer hybrid framework guided by structure priors. Different from\nemploying pre-trained models for the extraction of semantics or illumination\nmaps, we choose to extract robust structure priors based on\nillumination-invariant edge detectors. Moreover, we develop a CNN-Transformer\nHybrid Structure-Guided Feature Extractor (HSGFE) module at each scale with in\nthe UNet encoder-decoder architecture. Besides the CNN blocks which excels in\nmulti-scale feature extraction and fusion, we introduce a Structure-Guided\nTransformer Block (SGTB) in each HSGFE that incorporates structural priors to\nmodulate the enhancement process. Extensive experiments show that our method\nachieves state-of-the-art performance on several LLIE benchmarks in both\nquantitative metrics and visual quality. Our solution ranks second in the NTIRE\n2025 Low-Light Enhancement Challenge. Code is released at\nhttps://github.com/minyan8/imagine.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14066", "pdf": "https://arxiv.org/pdf/2504.14066", "abs": "https://arxiv.org/abs/2504.14066", "authors": ["Laerdon Kim"], "title": "A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to CLPsych Workshop, NAACL 2025", "summary": "We present a baseline for the CLPsych 2025 A.1 task: classifying self-states\nin mental health data taken from Reddit. We use few-shot learning with a 4-bit\nquantized Gemma 2 9B model and a data preprocessing step which first identifies\nrelevant sentences indicating self-state evidence, and then performs a binary\nclassification to determine whether the sentence is evidence of an adaptive or\nmaladaptive self-state. This system outperforms our other method which relies\non an LLM to highlight spans of variable length independently. We attribute the\nperformance of our model to the benefits of this sentence chunking step for two\nreasons: partitioning posts into sentences 1) broadly matches the granularity\nat which self-states were human-annotated and 2) simplifies the task for our\nlanguage model to a binary classification problem. Our system places third out\nof fourteen systems submitted for Task A.1, achieving a test-time recall of\n0.579.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14092", "pdf": "https://arxiv.org/pdf/2504.14092", "abs": "https://arxiv.org/abs/2504.14092", "authors": ["Wei Dong", "Han Zhou", "Seyed Amirreza Mousavi", "Jun Chen"], "title": "Retinex-guided Histogram Transformer for Mask-free Shadow Removal", "categories": ["cs.CV"], "comment": "Accpeted by CVPR 2025 NTIRE Workshop, Retinex Guidance, Histogram\n  Transformer", "summary": "While deep learning methods have achieved notable progress in shadow removal,\nmany existing approaches rely on shadow masks that are difficult to obtain,\nlimiting their generalization to real-world scenes. In this work, we propose\nReHiT, an efficient mask-free shadow removal framework based on a hybrid\nCNN-Transformer architecture guided by Retinex theory. We first introduce a\ndual-branch pipeline to separately model reflectance and illumination\ncomponents, and each is restored by our developed Illumination-Guided Hybrid\nCNN-Transformer (IG-HCT) module. Second, besides the CNN-based blocks that are\ncapable of learning residual dense features and performing multi-scale semantic\nfusion, multi-scale semantic fusion, we develop the Illumination-Guided\nHistogram Transformer Block (IGHB) to effectively handle non-uniform\nillumination and spatially complex shadows. Extensive experiments on several\nbenchmark datasets validate the effectiveness of our approach over existing\nmask-free methods. Trained solely on the NTIRE 2025 Shadow Removal Challenge\ndataset, our solution delivers competitive results with one of the smallest\nparameter sizes and fastest inference speeds among top-ranked entries,\nhighlighting its applicability for real-world applications with limited\ncomputational resources. The code is available at\nhttps://github.com/dongw22/oath.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14373", "pdf": "https://arxiv.org/pdf/2504.14373", "abs": "https://arxiv.org/abs/2504.14373", "authors": ["Chen Guo", "Zhuo Su", "Jian Wang", "Shuang Li", "Xu Chang", "Zhaohu Li", "Yang Zhao", "Guidong Wang", "Ruqi Huang"], "title": "SEGA: Drivable 3D Gaussian Head Avatar from a Single Image", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Creating photorealistic 3D head avatars from limited input has become\nincreasingly important for applications in virtual reality, telepresence, and\ndigital entertainment. While recent advances like neural rendering and 3D\nGaussian splatting have enabled high-quality digital human avatar creation and\nanimation, most methods rely on multiple images or multi-view inputs, limiting\ntheir practicality for real-world use. In this paper, we propose SEGA, a novel\napproach for Single-imagE-based 3D drivable Gaussian head Avatar creation that\ncombines generalized prior models with a new hierarchical UV-space Gaussian\nSplatting framework. SEGA seamlessly combines priors derived from large-scale\n2D datasets with 3D priors learned from multi-view, multi-expression, and\nmulti-ID data, achieving robust generalization to unseen identities while\nensuring 3D consistency across novel viewpoints and expressions. We further\npresent a hierarchical UV-space Gaussian Splatting framework that leverages\nFLAME-based structural priors and employs a dual-branch architecture to\ndisentangle dynamic and static facial components effectively. The dynamic\nbranch encodes expression-driven fine details, while the static branch focuses\non expression-invariant regions, enabling efficient parameter inference and\nprecomputation. This design maximizes the utility of limited 3D data and\nachieves real-time performance for animation and rendering. Additionally, SEGA\nperforms person-specific fine-tuning to further enhance the fidelity and\nrealism of the generated avatars. Experiments show our method outperforms\nstate-of-the-art approaches in generalization ability, identity preservation,\nand expression realism, advancing one-shot avatar creation for practical\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14165", "pdf": "https://arxiv.org/pdf/2504.14165", "abs": "https://arxiv.org/abs/2504.14165", "authors": ["Ziyan Zhang", "Yang Hou", "Chen Gong", "Zhenghua Li"], "title": "Self-Correction Makes LLMs Better Parsers", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success across various\nnatural language processing (NLP) tasks. However, recent studies suggest that\nthey still face challenges in performing fundamental NLP tasks essential for\ndeep language understanding, particularly syntactic parsing. In this paper, we\nconduct an in-depth analysis of LLM parsing capabilities, delving into the\nspecific shortcomings of their parsing results. We find that LLMs may stem from\nlimitations to fully leverage grammar rules in existing treebanks, which\nrestricts their capability to generate valid syntactic structures. To help LLMs\nacquire knowledge without additional training, we propose a self-correction\nmethod that leverages grammar rules from existing treebanks to guide LLMs in\ncorrecting previous errors. Specifically, we automatically detect potential\nerrors and dynamically search for relevant rules, offering hints and examples\nto guide LLMs in making corrections themselves. Experimental results on three\ndatasets with various LLMs, demonstrate that our method significantly improves\nperformance in both in-domain and cross-domain settings on the English and\nChinese datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14212", "pdf": "https://arxiv.org/pdf/2504.14212", "abs": "https://arxiv.org/abs/2504.14212", "authors": ["Takuma Udagawa", "Yang Zhao", "Hiroshi Kanayama", "Bishwaranjan Bhattacharjee"], "title": "Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) acquire general linguistic knowledge from\nmassive-scale pretraining. However, pretraining data mainly comprised of\nweb-crawled texts contain undesirable social biases which can be perpetuated or\neven amplified by LLMs. In this study, we propose an efficient yet effective\nannotation pipeline to investigate social biases in the pretraining corpora.\nOur pipeline consists of protected attribute detection to identify diverse\ndemographics, followed by regard classification to analyze the language\npolarity towards each attribute. Through our experiments, we demonstrate the\neffect of our bias analysis and mitigation measures, focusing on Common Crawl\nas the most representative pretraining corpus.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14139", "pdf": "https://arxiv.org/pdf/2504.14139", "abs": "https://arxiv.org/abs/2504.14139", "authors": ["Hai Pham-Ngoc", "De Nguyen-Van", "Dung Vu-Tien", "Phuong Le-Hong"], "title": "ThyroidEffi 1.0: A Cost-Effective System for High-Performance Multi-Class Thyroid Carcinoma Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Background: Automated classification of thyroid fine needle aspiration biopsy\n(FNAB) images faces challenges in limited data, inter-observer variability, and\ncomputational cost. Efficient, interpretable models are crucial for clinical\nsupport. Objective: To develop and externally validate a deep learning system\nfor the multi-class classification of thyroid FNAB images into three key\ncategories that directly guide post-biopsy treatment decisions in Vietnam:\nbenign (B2), suspicious for malignancy (B5), and malignant (B6), while\nachieving high diagnostic accuracy with low computational overhead. Methods:\nOur framework features: (1) YOLOv10-based cell cluster detection for\ninformative sub-region extraction and noise reduction; (2) a curriculum\nlearning-inspired protocol sequencing localized crops to full images for\nmulti-scale feature capture; (3) adaptive lightweight EfficientNetB0 (4\nmillions parameters) selection balancing performance and efficiency; and (4) a\nTransformer-inspired module for multi-scale, multi-region analysis. External\nvalidation used 1,015 independent FNAB images. Results: ThyroidEffi Basic\nachieved a macro F1 of 89.19\\% and AUCs of 0.98 (B2), 0.95 (B5), and 0.96 (B6)\non the internal test set. External validation yielded AUCs of 0.9495 (B2),\n0.7436 (B5), and 0.8396 (B6). ThyroidEffi Premium improved macro F1 to 89.77\\%.\nGrad-CAM highlighted key diagnostic regions, confirming interpretability. The\nsystem processed 1000 cases in 30 seconds, demonstrating feasibility on widely\naccessible hardware like a 12-core CPU. Conclusions: This work demonstrates\nthat high-accuracy, interpretable thyroid FNAB image classification is\nachievable with minimal computational demands.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14221", "pdf": "https://arxiv.org/pdf/2504.14221", "abs": "https://arxiv.org/abs/2504.14221", "authors": ["Wenbing Zhu", "Lidong Wang", "Ziqing Zhou", "Chengjie Wang", "Yurui Pan", "Ruoyi Zhang", "Zhuhao Chen", "Linjie Cheng", "Bin-Bin Gao", "Jiangning Zhang", "Zhenye Gan", "Yuxie Wang", "Yulong Chen", "Shuguang Qian", "Mingmin Chi", "Bo Peng", "Lizhuang Ma"], "title": "Real-IAD D3: A Real-World 2D/Pseudo-3D/3D Dataset for Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": "13 pages. Dataset and code: https://realiad4ad.github.io/Real-IAD D3", "summary": "The increasing complexity of industrial anomaly detection (IAD) has\npositioned multimodal detection methods as a focal area of machine vision\nresearch. However, dedicated multimodal datasets specifically tailored for IAD\nremain limited. Pioneering datasets like MVTec 3D have laid essential\ngroundwork in multimodal IAD by incorporating RGB+3D data, but still face\nchallenges in bridging the gap with real industrial environments due to\nlimitations in scale and resolution. To address these challenges, we introduce\nReal-IAD D3, a high-precision multimodal dataset that uniquely incorporates an\nadditional pseudo3D modality generated through photometric stereo, alongside\nhigh-resolution RGB images and micrometer-level 3D point clouds. Real-IAD D3\nfeatures finer defects, diverse anomalies, and greater scale across 20\ncategories, providing a challenging benchmark for multimodal IAD Additionally,\nwe introduce an effective approach that integrates RGB, point cloud, and\npseudo-3D depth information to leverage the complementary strengths of each\nmodality, enhancing detection performance. Our experiments highlight the\nimportance of these modalities in boosting detection robustness and overall IAD\nperformance. The dataset and code are publicly accessible for research purposes\nat https://realiad4ad.github.io/Real-IAD D3", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14366", "pdf": "https://arxiv.org/pdf/2504.14366", "abs": "https://arxiv.org/abs/2504.14366", "authors": ["Patrick Haller", "Jonas Golde", "Alan Akbik"], "title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge distillation is a widely used technique for compressing large\nlanguage models (LLMs) by training a smaller student model to mimic a larger\nteacher model. Typically, both the teacher and student are Transformer-based\narchitectures, leveraging softmax attention for sequence modeling. However, the\nquadratic complexity of self-attention at inference time remains a significant\nbottleneck, motivating the exploration of subquadratic alternatives such as\nstructured state-space models (SSMs), linear attention, and recurrent\narchitectures. In this work, we systematically evaluate the transferability of\nknowledge distillation from a Transformer teacher to nine subquadratic student\narchitectures. Our study aims to determine which subquadratic model best aligns\nwith the teacher's learned representations and how different architectural\nconstraints influence the distillation process. We also investigate the impact\nof intelligent initialization strategies, including matrix mixing and\nquery-key-value (QKV) copying, on the adaptation process. Our empirical results\non multiple NLP benchmarks provide insights into the trade-offs between\nefficiency and performance, highlighting key factors for successful knowledge\ntransfer to subquadratic architectures.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14238", "pdf": "https://arxiv.org/pdf/2504.14238", "abs": "https://arxiv.org/abs/2504.14238", "authors": ["Lu Pan", "Yu-Hsuan Huang", "Hongxia Xie", "Cheng Zhang", "Hongwei Zhao", "Hong-Han Shuai", "Wen-Huang Cheng"], "title": "Single Document Image Highlight Removal via A Large-Scale Real-World Dataset and A Location-Aware Network", "categories": ["cs.CV"], "comment": "main paper with 8 pages, conference", "summary": "Reflective documents often suffer from specular highlights under ambient\nlighting, severely hindering text readability and degrading overall visual\nquality. Although recent deep learning methods show promise in highlight\nremoval, they remain suboptimal for document images, primarily due to the lack\nof dedicated datasets and tailored architectural designs. To tackle these\nchallenges, we present DocHR14K, a large-scale real-world dataset comprising\n14,902 high-resolution image pairs across six document categories and various\nlighting conditions. To the best of our knowledge, this is the first\nhigh-resolution dataset for document highlight removal that captures a wide\nrange of real-world lighting conditions. Additionally, motivated by the\nobservation that the residual map between highlighted and clean images\nnaturally reveals the spatial structure of highlight regions, we propose a\nsimple yet effective Highlight Location Prior (HLP) to estimate highlight masks\nwithout human annotations. Building on this prior, we present the\nLocation-Aware Laplacian Pyramid Highlight Removal Network (L2HRNet), which\neffectively removes highlights by leveraging estimated priors and incorporates\ndiffusion module to restore details. Extensive experiments demonstrate that\nDocHR14K improves highlight removal under diverse lighting conditions. Our\nL2HRNet achieves state-of-the-art performance across three benchmark datasets,\nincluding a 5.01\\% increase in PSNR and a 13.17\\% reduction in RMSE on\nDocHR14K.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14462", "pdf": "https://arxiv.org/pdf/2504.14462", "abs": "https://arxiv.org/abs/2504.14462", "authors": ["Armin Toroghi", "Willis Guo", "Scott Sanner"], "title": "CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has redefined the AI landscape,\nparticularly due to their ability to encode factual and commonsense knowledge,\nand their outstanding performance in tasks requiring reasoning. Despite these\nadvances, hallucinations and reasoning errors remain a significant barrier to\ntheir deployment in high-stakes settings. In this work, we observe that even\nthe most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning\nerrors and hallucinations on tasks requiring commonsense reasoning over\nobscure, long-tail entities. To investigate this limitation, we present a new\ndataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that\nconsists of 3,300 queries from question answering and claim verification tasks\nand covers a diverse range of commonsense reasoning skills. We remark that\nCoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset\nsince the support of knowledge required to answer its queries is present in the\nWikidata knowledge graph. However, as opposed to existing KGQA benchmarks that\nmerely focus on factoid questions, our CoLoTa queries also require commonsense\nreasoning. Our experiments with strong LLM-based KGQA methodologies indicate\ntheir severe inability to answer queries involving commonsense reasoning.\nHence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM\ncommonsense reasoning capabilities and their robustness to hallucinations on\nlong-tail entities and (ii) the commonsense reasoning capabilities of KGQA\nmethods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14249", "pdf": "https://arxiv.org/pdf/2504.14249", "abs": "https://arxiv.org/abs/2504.14249", "authors": ["Bin Ren", "Eduard Zamfir", "Zongwei Wu", "Yawei Li", "Yidi Li", "Danda Pani Paudel", "Radu Timofte", "Ming-Hsuan Yang", "Luc Van Gool", "Nicu Sebe"], "title": "Any Image Restoration via Efficient Spatial-Frequency Degradation Adaptation", "categories": ["cs.CV"], "comment": "Efficient All in One Image Restoration", "summary": "Restoring any degraded image efficiently via just one model has become\nincreasingly significant and impactful, especially with the proliferation of\nmobile devices. Traditional solutions typically involve training dedicated\nmodels per degradation, resulting in inefficiency and redundancy. More recent\napproaches either introduce additional modules to learn visual prompts,\nsignificantly increasing model size, or incorporate cross-modal transfer from\nlarge language models trained on vast datasets, adding complexity to the system\narchitecture. In contrast, our approach, termed AnyIR, takes a unified path\nthat leverages inherent similarity across various degradations to enable both\nefficient and comprehensive restoration through a joint embedding mechanism,\nwithout scaling up the model or relying on large language models.Specifically,\nwe examine the sub-latent space of each input, identifying key components and\nreweighting them first in a gated manner. To fuse the intrinsic degradation\nawareness and the contextualized attention, a spatial-frequency parallel fusion\nstrategy is proposed for enhancing spatial-aware local-global interactions and\nenriching the restoration details from the frequency perspective. Extensive\nbenchmarking in the all-in-one restoration setting confirms AnyIR's SOTA\nperformance, reducing model complexity by around 82\\% in parameters and 85\\% in\nFLOPs. Our code will be available at our Project page\n(https://amazingren.github.io/AnyIR/)", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14260", "pdf": "https://arxiv.org/pdf/2504.14260", "abs": "https://arxiv.org/abs/2504.14260", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "Cross-attention for State-based model RWKV-7", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce CrossWKV, a novel cross-attention mechanism for the state-based\nRWKV-7 model, designed to enhance the expressive power of text-to-image\ngeneration. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)\narchitecture, CrossWKV integrates text and image modalities in a single pass,\nutilizing a generalized delta rule with vector-valued gating and low-rank\nadaptations (LoRA) to achieve superior cross-modal alignment. Unlike\nTransformer-based models, CrossWKV's non-diagonal, input-dependent transition\nmatrix enables it to represent complex functions beyond the $\\mathrm{TC}^0$\ncomplexity class, including all regular languages, as demonstrated by its\nability to perform state-tracking tasks like $S_5$ permutation modeling.\nEvaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B\nand ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and\na CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance\nwhile offering robust generalization across diverse prompts. The model's\nenhanced expressivity, combined with constant memory usage and linear scaling,\npositions it as a powerful solution for advanced cross-modal tasks, with\npotential applications in high-resolution generation and dynamic state\nmanipulation.Code at https://github.com/TorchRWKV/flash-linear-attention", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14619", "pdf": "https://arxiv.org/pdf/2504.14619", "abs": "https://arxiv.org/abs/2504.14619", "authors": ["Yuri Balashov", "Alex Balashov", "Shiho Fukuda Koski"], "title": "Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations", "categories": ["cs.CL"], "comment": "28 pages, 4 figures. Accepted at the MT Summit, University of Geneva,\n  June 2025", "summary": "This is the first in a series of papers exploring the rapidly expanding new\nopportunities arising from recent progress in language technologies for\nindividual translators and language service providers with modest resources.\nThe advent of advanced neural machine translation systems, large language\nmodels, and their integration into workflows via computer-assisted translation\ntools and translation management systems have reshaped the translation\nlandscape. These advancements enable not only translation but also quality\nevaluation, error spotting, glossary generation, and adaptation to\ndomain-specific needs, creating new technical opportunities for freelancers. In\nthis series, we aim to empower translators with actionable methods to harness\nthese advancements. Our approach emphasizes Translation Analytics, a suite of\nevaluation techniques traditionally reserved for large-scale industry\napplications but now becoming increasingly available for smaller-scale users.\nThis first paper introduces a practical framework for adapting automatic\nevaluation metrics -- such as BLEU, chrF, TER, and COMET -- to freelancers'\nneeds. We illustrate the potential of these metrics using a trilingual corpus\nderived from a real-world project in the medical domain and provide statistical\nanalysis correlating human evaluations with automatic scores. Our findings\nemphasize the importance of proactive engagement with emerging technologies to\nnot only adapt but thrive in the evolving professional environment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14289", "pdf": "https://arxiv.org/pdf/2504.14289", "abs": "https://arxiv.org/abs/2504.14289", "authors": ["Shang Zhang", "Yujie Cui", "Ruoyan Xiong", "Huanbin Zhang"], "title": "ISTD-YOLO: A Multi-Scale Lightweight High-Performance Infrared Small Target Detection Algorithm", "categories": ["cs.CV"], "comment": null, "summary": "Aiming at the detection difficulties of infrared images such as complex\nbackground, low signal-to-noise ratio, small target size and weak brightness, a\nlightweight infrared small target detection algorithm ISTD-YOLO based on\nimproved YOLOv7 was proposed. Firstly, the YOLOv7 network structure was\nlightweight reconstructed, and a three-scale lightweight network architecture\nwas designed. Then, the ELAN-W module of the model neck network is replaced by\nVoV-GSCSP to reduce the computational cost and the complexity of the network\nstructure. Secondly, a parameter-free attention mechanism was introduced into\nthe neck network to enhance the relevance of local con-text information.\nFinally, the Normalized Wasserstein Distance (NWD) was used to optimize the\ncommonly used IoU index to enhance the localization and detection accuracy of\nsmall targets. Experimental results show that compared with YOLOv7 and the\ncurrent mainstream algorithms, ISTD-YOLO can effectively improve the detection\neffect, and all indicators are effectively improved, which can achieve\nhigh-quality detection of infrared small targets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14306", "pdf": "https://arxiv.org/pdf/2504.14306", "abs": "https://arxiv.org/abs/2504.14306", "authors": ["Yitao Zhao", "Sen Lei", "Nanqing Liu", "Heng-Chao Li", "Turgay Celik", "Qing Zhu"], "title": "Exploring Generalizable Pre-training for Real-world Change Detection via Geometric Estimation", "categories": ["cs.CV"], "comment": "Submitted to IEEE TGRS", "summary": "As an essential procedure in earth observation system, change detection (CD)\naims to reveal the spatial-temporal evolution of the observation regions. A key\nprerequisite for existing change detection algorithms is aligned geo-references\nbetween multi-temporal images by fine-grained registration. However, in the\nmajority of real-world scenarios, a prior manual registration is required\nbetween the original images, which significantly increases the complexity of\nthe CD workflow. In this paper, we proposed a self-supervision motivated CD\nframework with geometric estimation, called \"MatchCD\". Specifically, the\nproposed MatchCD framework utilizes the zero-shot capability to optimize the\nencoder with self-supervised contrastive representation, which is reused in the\ndownstream image registration and change detection to simultaneously handle the\nbi-temporal unalignment and object change issues. Moreover, unlike the\nconventional change detection requiring segmenting the full-frame image into\nsmall patches, our MatchCD framework can directly process the original\nlarge-scale image (e.g., 6K*4K resolutions) with promising performance. The\nperformance in multiple complex scenarios with significant geometric distortion\ndemonstrates the effectiveness of our proposed framework.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14692", "pdf": "https://arxiv.org/pdf/2504.14692", "abs": "https://arxiv.org/abs/2504.14692", "authors": ["Songtao Jiang", "Yuan Wang", "Sibo Song", "Yan Zhang", "Zijie Meng", "Bohan Lei", "Jian Wu", "Jimeng Sun", "Zuozhu Liu"], "title": "OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding", "categories": ["cs.CL"], "comment": null, "summary": "The practical deployment of medical vision-language models (Med-VLMs)\nnecessitates seamless integration of textual data with diverse visual\nmodalities, including 2D/3D images and videos, yet existing models typically\nemploy separate encoders for different modalities. To address this limitation,\nwe present OmniV-Med, a unified framework for multimodal medical understanding.\nOur technical contributions are threefold: First, we construct\nOmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K\ninstructional samples spanning 14 medical image modalities and 11 clinical\ntasks. Second, we devise a rotary position-adaptive encoder that processes\nmulti-resolution 2D/3D images and videos within a unified architecture,\ndiverging from conventional modality-specific encoders. Third, we introduce a\nmedical-aware token pruning mechanism that exploits spatial-temporal redundancy\nin volumetric data (e.g., consecutive CT slices) and medical videos,\neffectively reducing 60\\% of visual tokens without performance degradation.\nEmpirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art\nperformance on 7 benchmarks spanning 2D/3D medical imaging and video\nunderstanding tasks. Notably, our lightweight variant (OmniV-Med-1.5B) attains\ncomparable performance while requiring only 8 RTX3090 GPUs for training and\nsupporting efficient long-video inference. Data, code and model will be\nreleased.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14992", "pdf": "https://arxiv.org/pdf/2504.14992", "abs": "https://arxiv.org/abs/2504.14992", "authors": ["Bohong Wu", "Shen Yan", "Sijun Zhang", "Jianqiao Lu", "Yutao Zeng", "Ya Wang", "Xun Zhou"], "title": "Efficient Pretraining Length Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14470", "pdf": "https://arxiv.org/pdf/2504.14470", "abs": "https://arxiv.org/abs/2504.14470", "authors": ["Jingjing Ren", "Wenbo Li", "Zhongdao Wang", "Haoze Sun", "Bangzhen Liu", "Haoyu Chen", "Jiaqi Xu", "Aoxue Li", "Shifeng Zhang", "Bin Shao", "Yong Guo", "Lei Zhu"], "title": "Turbo2K: Towards Ultra-Efficient and High-Quality 2K Video Synthesis", "categories": ["cs.CV"], "comment": "Webpage at https://jingjingrenabc.github.io/turbo2k/", "summary": "Demand for 2K video synthesis is rising with increasing consumer expectations\nfor ultra-clear visuals. While diffusion transformers (DiTs) have demonstrated\nremarkable capabilities in high-quality video generation, scaling them to 2K\nresolution remains computationally prohibitive due to quadratic growth in\nmemory and processing costs. In this work, we propose Turbo2K, an efficient and\npractical framework for generating detail-rich 2K videos while significantly\nimproving training and inference efficiency. First, Turbo2K operates in a\nhighly compressed latent space, reducing computational complexity and memory\nfootprint, making high-resolution video synthesis feasible. However, the high\ncompression ratio of the VAE and limited model size impose constraints on\ngenerative quality. To mitigate this, we introduce a knowledge distillation\nstrategy that enables a smaller student model to inherit the generative\ncapacity of a larger, more powerful teacher model. Our analysis reveals that,\ndespite differences in latent spaces and architectures, DiTs exhibit structural\nsimilarities in their internal representations, facilitating effective\nknowledge transfer. Second, we design a hierarchical two-stage synthesis\nframework that first generates multi-level feature at lower resolutions before\nguiding high-resolution video generation. This approach ensures structural\ncoherence and fine-grained detail refinement while eliminating redundant\nencoding-decoding overhead, further enhancing computational efficiency.Turbo2K\nachieves state-of-the-art efficiency, generating 5-second, 24fps, 2K videos\nwith significantly reduced computational cost. Compared to existing methods,\nTurbo2K is up to 20$\\times$ faster for inference, making high-resolution video\ngeneration more scalable and practical for real-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15133", "pdf": "https://arxiv.org/pdf/2504.15133", "abs": "https://arxiv.org/abs/2504.15133", "authors": ["Ziwen Xu", "Shuxun Wang", "Kewei Xu", "Haoming Xu", "Mengru Wang", "Xinle Deng", "Yunzhi Yao", "Guozhou Zheng", "Huajun Chen", "Ningyu Zhang"], "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "Work in progress. Demo:\n  https://zjunlp.github.io/project/EasyEdit2/video; code:\n  https://github.com/zjunlp/EasyEdit", "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "safety"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14516", "pdf": "https://arxiv.org/pdf/2504.14516", "abs": "https://arxiv.org/abs/2504.14516", "authors": ["Weirong Chen", "Ganlin Zhang", "Felix Wimbauer", "Rui Wang", "Nikita Araslanov", "Andrea Vedaldi", "Daniel Cremers"], "title": "Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction", "categories": ["cs.CV"], "comment": "Project page: https://wrchen530.github.io/projects/batrack/", "summary": "Traditional SLAM systems, which rely on bundle adjustment, struggle with\nhighly dynamic scenes commonly found in casual videos. Such videos entangle the\nmotion of dynamic elements, undermining the assumption of static environments\nrequired by traditional systems. Existing techniques either filter out dynamic\nelements or model their motion independently. However, the former often results\nin incomplete reconstructions, whereas the latter can lead to inconsistent\nmotion estimates. Taking a novel approach, this work leverages a 3D point\ntracker to separate the camera-induced motion from the observed motion of\ndynamic objects. By considering only the camera-induced component, bundle\nadjustment can operate reliably on all scene elements as a result. We further\nensure depth consistency across video frames with lightweight post-processing\nbased on scale maps. Our framework combines the core of traditional SLAM --\nbundle adjustment -- with a robust learning-based 3D tracker front-end.\nIntegrating motion decomposition, bundle adjustment and depth refinement, our\nunified framework, BA-Track, accurately tracks the camera motion and produces\ntemporally coherent and scale-consistent dense reconstructions, accommodating\nboth static and dynamic elements. Our experiments on challenging datasets\nreveal significant improvements in camera pose estimation and 3D reconstruction\naccuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15205", "pdf": "https://arxiv.org/pdf/2504.15205", "abs": "https://arxiv.org/abs/2504.15205", "authors": ["Nandan Thakur", "Ronak Pradeep", "Shivani Upadhyay", "Daniel Campos", "Nick Craswell", "Jimmy Lin"], "title": "Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted at SIGIR 2025 (short)", "summary": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to\ngenerate answers with citations from source documents containing \"ground\ntruth\", thereby reducing system hallucinations. A crucial factor in RAG\nevaluation is \"support\", whether the information in the cited documents\nsupports the answer. To this end, we conducted a large-scale comparative study\nof 45 participant submissions on 36 topics to the TREC 2024 RAG Track,\ncomparing an automatic LLM judge (GPT-4o) against human judges for support\nassessment. We considered two conditions: (1) fully manual assessments from\nscratch and (2) manual assessments with post-editing of LLM predictions. Our\nresults indicate that for 56% of the manual from-scratch assessments, human and\nGPT-4o predictions match perfectly (on a three-level scale), increasing to 72%\nin the manual with post-editing condition. Furthermore, by carefully analyzing\nthe disagreements in an unbiased study, we found that an independent human\njudge correlates better with GPT-4o than a human judge, suggesting that LLM\njudges can be a reliable alternative for support assessment. To conclude, we\nprovide a qualitative analysis of human and GPT-4o errors to help guide future\niterations of support assessment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15220", "pdf": "https://arxiv.org/pdf/2504.15220", "abs": "https://arxiv.org/abs/2504.15220", "authors": ["Juli√°n Cendrero", "Julio Gonzalo", "Ivar Zapata"], "title": "Fully Bayesian Approaches to Topics over Time", "categories": ["cs.CL", "cs.LG"], "comment": "25 pages", "summary": "The Topics over Time (ToT) model captures thematic changes in timestamped\ndatasets by explicitly modeling publication dates jointly with word\nco-occurrence patterns. However, ToT was not approached in a fully Bayesian\nfashion, a flaw that makes it susceptible to stability problems. To address\nthis issue, we propose a fully Bayesian Topics over Time (BToT) model via the\nintroduction of a conjugate prior to the Beta distribution. This prior acts as\na regularization that prevents the online version of the algorithm from\nunstable updates when a topic is poorly represented in a mini-batch. The\ncharacteristics of this prior to the Beta distribution are studied here for the\nfirst time. Still, this model suffers from a difference in scale between the\nsingle-time observations and the multiplicity of words per document. A\nvariation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a\nsolution. In WBToT, publication dates are repeated a certain number of times\nper document, which balances the relative influence of words and timestamps\nalong the inference process. We have tested our models on two datasets: a\ncollection of over 200 years of US state-of-the-union (SOTU) addresses and a\nlarge-scale COVID-19 Twitter corpus of 10 million tweets. The results show that\nWBToT captures events better than Latent Dirichlet Allocation and other SOTA\ntopic models like BERTopic: the median absolute deviation of the topic presence\nover time is reduced by $51\\%$ and $34\\%$, respectively. Our experiments also\ndemonstrate the superior coherence of WBToT over BToT, which highlights the\nimportance of balancing the time and word modalities. Finally, we illustrate\nthe stability of the online optimization algorithm in WBToT, which allows the\napplication of WBToT to problems that are intractable for standard ToT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15236", "pdf": "https://arxiv.org/pdf/2504.15236", "abs": "https://arxiv.org/abs/2504.15236", "authors": ["Saffron Huang", "Esin Durmus", "Miles McCain", "Kunal Handa", "Alex Tamkin", "Jerry Hong", "Michael Stern", "Arushi Somani", "Xiuruo Zhang", "Deep Ganguli"], "title": "Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "44 pages", "summary": "AI assistants can impart value judgments that shape people's decisions and\nworldviews, yet little is known empirically about what values these systems\nrely on in practice. To address this, we develop a bottom-up,\nprivacy-preserving method to extract the values (normative considerations\nstated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit\nin hundreds of thousands of real-world interactions. We empirically discover\nand taxonomize 3,307 AI values and study how they vary by context. We find that\nClaude expresses many practical and epistemic values, and typically supports\nprosocial human values while resisting values like \"moral nihilism\". While some\nvalues appear consistently across contexts (e.g. \"transparency\"), many are more\nspecialized and context-dependent, reflecting the diversity of human\ninterlocutors and their varied contexts. For example, \"harm prevention\" emerges\nwhen Claude resists users, \"historical accuracy\" when responding to queries\nabout controversial events, \"healthy boundaries\" when asked for relationship\nadvice, and \"human agency\" in technology ethics discussions. By providing the\nfirst large-scale empirical mapping of AI values in deployment, our work\ncreates a foundation for more grounded evaluation and design of values in AI\nsystems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14553", "pdf": "https://arxiv.org/pdf/2504.14553", "abs": "https://arxiv.org/abs/2504.14553", "authors": ["Weijun Zhuang", "Qizhang Li", "Xin Li", "Ming Liu", "Xiaopeng Hong", "Feng Gao", "Fan Yang", "Wangmeng Zuo"], "title": "Grounding-MD: Grounded Video-language Pre-training for Open-World Moment Detection", "categories": ["cs.CV"], "comment": null, "summary": "Temporal Action Detection and Moment Retrieval constitute two pivotal tasks\nin video understanding, focusing on precisely localizing temporal segments\ncorresponding to specific actions or events. Recent advancements introduced\nMoment Detection to unify these two tasks, yet existing approaches remain\nconfined to closed-set scenarios, limiting their applicability in open-world\ncontexts. To bridge this gap, we present Grounding-MD, an innovative, grounded\nvideo-language pre-training framework tailored for open-world moment detection.\nOur framework incorporates an arbitrary number of open-ended natural language\nqueries through a structured prompt mechanism, enabling flexible and scalable\nmoment detection. Grounding-MD leverages a Cross-Modality Fusion Encoder and a\nText-Guided Fusion Decoder to facilitate comprehensive video-text alignment and\nenable effective cross-task collaboration. Through large-scale pre-training on\ntemporal action detection and moment retrieval datasets, Grounding-MD\ndemonstrates exceptional semantic representation learning capabilities,\neffectively handling diverse and complex query conditions. Comprehensive\nevaluations across four benchmark datasets including ActivityNet, THUMOS14,\nActivityNet-Captions, and Charades-STA demonstrate that Grounding-MD\nestablishes new state-of-the-art performance in zero-shot and supervised\nsettings in open-world moment detection scenarios. All source code and trained\nmodels will be released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14582", "pdf": "https://arxiv.org/pdf/2504.14582", "abs": "https://arxiv.org/abs/2504.14582", "authors": ["Zheng Chen", "Kai Liu", "Jue Gong", "Jingkai Wang", "Lei Sun", "Zongwei Wu", "Radu Timofte", "Yulun Zhang", "Xiangyu Kong", "Xiaoxuan Yu", "Hyunhee Park", "Suejin Han", "Hakjae Jeon", "Dafeng Zhang", "Hyung-Ju Chun", "Donghun Ryou", "Inju Ha", "Bohyung Han", "Lu Zhao", "Yuyi Zhang", "Pengyu Yan", "Jiawei Hu", "Pengwei Liu", "Fengjun Guo", "Hongyuan Yu", "Pufan Xu", "Zhijuan Huang", "Shuyuan Cui", "Peng Guo", "Jiahui Liu", "Dongkai Zhang", "Heng Zhang", "Huiyuan Fu", "Huadong Ma", "Yanhui Guo", "Sisi Tian", "Xin Liu", "Jinwen Liang", "Jie Liu", "Jie Tang", "Gangshan Wu", "Zeyu Xiao", "Zhuoyuan Li", "Yinxiang Zhang", "Wenxuan Cai", "Vijayalaxmi Ashok Aralikatti", "Nikhil Akalwadi", "G Gyaneshwar Rao", "Chaitra Desai", "Ramesh Ashok Tabib", "Uma Mudenagudi", "Marcos V. Conde", "Alejandro Merino", "Bruno Longarela", "Javier Abad", "Weijun Yuan", "Zhan Li", "Zhanglu Chen", "Boyang Yao", "Aagam Jain", "Milan Kumar Singh", "Ankit Kumar", "Shubh Kawa", "Divyavardhan Singh", "Anjali Sarvaiya", "Kishor Upla", "Raghavendra Ramachandra", "Chia-Ming Lee", "Yu-Fan Lin", "Chih-Chung Hsu", "Risheek V Hiremath", "Yashaswini Palani", "Yuxuan Jiang", "Qiang Zhu", "Siyue Teng", "Fan Zhang", "Shuyuan Zhu", "Bing Zeng", "David Bull", "Jingwei Liao", "Yuqing Yang", "Wenda Shao", "Junyi Zhao", "Qisheng Xu", "Kele Xu", "Sunder Ali Khowaja", "Ik Hyun Lee", "Snehal Singh Tomar", "Rajarshi Ray", "Klaus Mueller", "Sachin Chaudhary", "Surya Vashisth", "Akshay Dudhane", "Praful Hambarde", "Satya Naryan Tazi", "Prashant Patil", "Santosh Kumar Vipparthi", "Subrahmanyam Murala", "Bilel Benjdira", "Anas M. Ali", "Wadii Boulila", "Zahra Moammeri", "Ahmad Mahmoudi-Aznaveh", "Ali Karbasi", "Hossein Motamednia", "Liangyan Li", "Guanhua Zhao", "Kevin Le", "Yimo Ning", "Haoxuan Huang", "Jun Chen"], "title": "NTIRE 2025 Challenge on Image Super-Resolution ($\\times$4): Methods and Results", "categories": ["cs.CV"], "comment": "NTIRE 2025 webpage: https://www.cvlai.net/ntire/2025. Code:\n  https://github.com/zhengchen1999/NTIRE2025_ImageSR_x4", "summary": "This paper presents the NTIRE 2025 image super-resolution ($\\times$4)\nchallenge, one of the associated competitions of the 10th NTIRE Workshop at\nCVPR 2025. The challenge aims to recover high-resolution (HR) images from\nlow-resolution (LR) counterparts generated through bicubic downsampling with a\n$\\times$4 scaling factor. The objective is to develop effective network designs\nor solutions that achieve state-of-the-art SR performance. To reflect the dual\nobjectives of image SR research, the challenge includes two sub-tracks: (1) a\nrestoration track, emphasizes pixel-wise accuracy and ranks submissions based\non PSNR; (2) a perceptual track, focuses on visual realism and ranks results by\na perceptual score. A total of 286 participants registered for the competition,\nwith 25 teams submitting valid entries. This report summarizes the challenge\ndesign, datasets, evaluation protocol, the main results, and methods of each\nteam. The challenge serves as a benchmark to advance the state of the art and\nfoster progress in image SR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14583", "pdf": "https://arxiv.org/pdf/2504.14583", "abs": "https://arxiv.org/abs/2504.14583", "authors": ["Akshit Gupta", "Remko Uijlenhoet"], "title": "Using street view imagery and deep generative modeling for estimating the health of urban forests", "categories": ["cs.CV", "cs.CY"], "comment": "Accepted at ICLR 2025 Workshop", "summary": "Healthy urban forests comprising of diverse trees and shrubs play a crucial\nrole in mitigating climate change. They provide several key advantages such as\nproviding shade for energy conservation, and intercepting rainfall to reduce\nflood runoff and soil erosion. Traditional approaches for monitoring the health\nof urban forests require instrumented inspection techniques, often involving a\nhigh amount of human labor and subjective evaluations. As a result, they are\nnot scalable for cities which lack extensive resources. Recent approaches\ninvolving multi-spectral imaging data based on terrestrial sensing and\nsatellites, are constrained respectively with challenges related to dedicated\ndeployments and limited spatial resolutions. In this work, we propose an\nalternative approach for monitoring the urban forests using simplified inputs:\nstreet view imagery, tree inventory data and meteorological conditions. We\npropose to use image-to-image translation networks to estimate two urban forest\nhealth parameters, namely, NDVI and CTD. Finally, we aim to compare the\ngenerated results with ground truth data using an onsite campaign utilizing\nhandheld multi-spectral and thermal imaging sensors. With the advent and\nexpansion of street view imagery platforms such as Google Street View and\nMapillary, this approach should enable effective management of urban forests\nfor the authorities in cities at scale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14626", "pdf": "https://arxiv.org/pdf/2504.14626", "abs": "https://arxiv.org/abs/2504.14626", "authors": ["Santanu Roy", "Shweta Singh", "Palak Sahu", "Ashvath Suresh", "Debashish Das"], "title": "MSAD-Net: Multiscale and Spatial Attention-based Dense Network for Lung Cancer Classification", "categories": ["cs.CV"], "comment": null, "summary": "Lung cancer, a severe form of malignant tumor that originates in the tissues\nof the lungs, can be fatal if not detected in its early stages. It ranks among\nthe top causes of cancer-related mortality worldwide. Detecting lung cancer\nmanually using chest X-Ray image or Computational Tomography (CT) scans image\nposes significant challenges for radiologists. Hence, there is a need for\nautomatic diagnosis system of lung cancers from radiology images. With the\nrecent emergence of deep learning, particularly through Convolutional Neural\nNetworks (CNNs), the automated detection of lung cancer has become a much\nsimpler task. Nevertheless, numerous researchers have addressed that the\nperformance of conventional CNNs may be hindered due to class imbalance issue,\nwhich is prevalent in medical images. In this research work, we have proposed a\nnovel CNN architecture ``Multi-Scale Dense Network (MSD-Net)''\n(trained-from-scratch). The novelties we bring in the proposed model are (I) We\nintroduce novel dense modules in the 4th block and 5th block of the CNN model.\nWe have leveraged 3 depthwise separable convolutional (DWSC) layers, and one\n1x1 convolutional layer in each dense module, in order to reduce complexity of\nthe model considerably. (II) Additionally, we have incorporated one skip\nconnection from 3rd block to 5th block and one parallel branch connection from\n4th block to Global Average Pooling (GAP) layer. We have utilized dilated\nconvolutional layer (with dilation rate=2) in the last parallel branch in order\nto extract multi-scale features. Extensive experiments reveal that our proposed\nmodel has outperformed latest CNN model ConvNext-Tiny, recent trend Vision\nTransformer (ViT), Pooling-based ViT (PiT), and other existing models by\nsignificant margins.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.13955", "pdf": "https://arxiv.org/pdf/2504.13955", "abs": "https://arxiv.org/abs/2504.13955", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "title": "Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "68T50", "I.2.7; H.5.2"], "comment": "14 pages, 6 figures", "summary": "The advancement of AI systems for mental health support is hindered by\nlimited access to therapeutic conversation data, particularly for trauma\ntreatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset\nof 3,000 therapy conversations based on Prolonged Exposure therapy protocols\nfor Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique\ncases, each explored through six conversational perspectives that mirror the\nprogression of therapy from initial anxiety to peak distress to emotional\nprocessing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,\n49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10\ntrauma-related behaviors using deterministic and probabilistic generation\nmethods. Analysis reveals realistic distributions of trauma types (witnessing\nviolence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse\n20.8%). Clinical experts validated the dataset's therapeutic fidelity,\nhighlighting its emotional depth while suggesting refinements for greater\nauthenticity. We also developed an emotional trajectory benchmark with\nstandardized metrics for evaluating model responses. This privacy-preserving\ndataset addresses critical gaps in trauma-focused mental health data, offering\na valuable resource for advancing both patient-facing applications and\nclinician training tools.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.13989", "pdf": "https://arxiv.org/pdf/2504.13989", "abs": "https://arxiv.org/abs/2504.13989", "authors": ["Lucas Maisonnave", "Cyril Moineau", "Olivier Bichler", "Fabrice Rastello"], "title": "Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "dimension"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14191", "pdf": "https://arxiv.org/pdf/2504.14191", "abs": "https://arxiv.org/abs/2504.14191", "authors": ["Yansheng Qiu", "Haoquan Zhang", "Zhaopan Xu", "Ming Li", "Diping Song", "Zheng Wang", "Kaipeng Zhang"], "title": "AI Idea Bench 2025: AI Research Idea Generation Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large-scale Language Models (LLMs) have revolutionized human-AI interaction\nand achieved significant success in the generation of novel ideas. However,\ncurrent assessments of idea generation overlook crucial factors such as\nknowledge leakage in LLMs, the absence of open-ended benchmarks with grounded\ntruth, and the limited scope of feasibility analysis constrained by prompt\ndesign. These limitations hinder the potential of uncovering groundbreaking\nresearch ideas. In this paper, we present AI Idea Bench 2025, a framework\ndesigned to quantitatively evaluate and compare the ideas generated by LLMs\nwithin the domain of AI research from diverse perspectives. The framework\ncomprises a comprehensive dataset of 3,495 AI papers and their associated\ninspired works, along with a robust evaluation methodology. This evaluation\nsystem gauges idea quality in two dimensions: alignment with the ground-truth\ncontent of the original papers and judgment based on general reference\nmaterial. AI Idea Bench 2025's benchmarking system stands to be an invaluable\nresource for assessing and comparing idea-generation techniques, thereby\nfacilitating the automation of scientific discovery.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14847", "pdf": "https://arxiv.org/pdf/2504.14847", "abs": "https://arxiv.org/abs/2504.14847", "authors": ["Xixi Wan", "Aihua Zheng", "Zi Wang", "Bo Jiang", "Jin Tang", "Jixin Ma"], "title": "Reliable Multi-Modal Object Re-Identification via Modality-Aware Graph Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal data provides abundant and diverse object information, crucial\nfor effective modal interactions in Re-Identification (ReID) tasks. However,\nexisting approaches often overlook the quality variations in local features and\nfail to fully leverage the complementary information across modalities,\nparticularly in the case of low-quality features. In this paper, we propose to\naddress this issue by leveraging a novel graph reasoning model, termed the\nModality-aware Graph Reasoning Network (MGRNet). Specifically, we first\nconstruct modality-aware graphs to enhance the extraction of fine-grained local\ndetails by effectively capturing and modeling the relationships between\npatches. Subsequently, the selective graph nodes swap operation is employed to\nalleviate the adverse effects of low-quality local features by considering both\nlocal and global information, enhancing the representation of discriminative\ninformation. Finally, the swapped modality-aware graphs are fed into the\nlocal-aware graph reasoning module, which propagates multi-modal information to\nyield a reliable feature representation. Another advantage of the proposed\ngraph reasoning approach is its ability to reconstruct missing modal\ninformation by exploiting inherent structural relationships, thereby minimizing\ndisparities between different modalities. Experimental results on four\nbenchmarks (RGBNT201, Market1501-MM, RGBNT100, MSVR310) indicate that the\nproposed method achieves state-of-the-art performance in multi-modal object\nReID. The code for our method will be available upon acceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14260", "pdf": "https://arxiv.org/pdf/2504.14260", "abs": "https://arxiv.org/abs/2504.14260", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "Cross-attention for State-based model RWKV-7", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce CrossWKV, a novel cross-attention mechanism for the state-based\nRWKV-7 model, designed to enhance the expressive power of text-to-image\ngeneration. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)\narchitecture, CrossWKV integrates text and image modalities in a single pass,\nutilizing a generalized delta rule with vector-valued gating and low-rank\nadaptations (LoRA) to achieve superior cross-modal alignment. Unlike\nTransformer-based models, CrossWKV's non-diagonal, input-dependent transition\nmatrix enables it to represent complex functions beyond the $\\mathrm{TC}^0$\ncomplexity class, including all regular languages, as demonstrated by its\nability to perform state-tracking tasks like $S_5$ permutation modeling.\nEvaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B\nand ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and\na CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance\nwhile offering robust generalization across diverse prompts. The model's\nenhanced expressivity, combined with constant memory usage and linear scaling,\npositions it as a powerful solution for advanced cross-modal tasks, with\npotential applications in high-resolution generation and dynamic state\nmanipulation.Code at https://github.com/TorchRWKV/flash-linear-attention", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14860", "pdf": "https://arxiv.org/pdf/2504.14860", "abs": "https://arxiv.org/abs/2504.14860", "authors": ["Ziyi Liu", "Yangcen Liu"], "title": "Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025: IEEE Conference on Computer Vision and Pattern Recognition", "summary": "Weakly-supervised Temporal Action Localization (WTAL) has achieved notable\nsuccess but still suffers from a lack of temporal annotations, leading to a\nperformance and framework gap compared with fully-supervised methods. While\nrecent approaches employ pseudo labels for training, three key challenges:\ngenerating high-quality pseudo labels, making full use of different priors, and\noptimizing training methods with noisy labels remain unresolved. Due to these\nperspectives, we propose PseudoFormer, a novel two-branch framework that\nbridges the gap between weakly and fully-supervised Temporal Action\nLocalization (TAL). We first introduce RickerFusion, which maps all predicted\naction proposals to a global shared space to generate pseudo labels with better\nquality. Subsequently, we leverage both snippet-level and proposal-level labels\nwith different priors from the weak branch to train the regression-based model\nin the full branch. Finally, the uncertainty mask and iterative refinement\nmechanism are applied for training with noisy pseudo labels. PseudoFormer\nachieves state-of-the-art WTAL results on the two commonly used benchmarks,\nTHUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate\nthe contribution of each component of our method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14875", "pdf": "https://arxiv.org/pdf/2504.14875", "abs": "https://arxiv.org/abs/2504.14875", "authors": ["Chris Dongjoo Kim", "Jihwan Moon", "Sangwoo Moon", "Heeseung Yun", "Sihaeng Lee", "Aniruddha Kembhavi", "Soonyoung Lee", "Gunhee Kim", "Sangho Lee", "Christopher Clark"], "title": "ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025 (main conference)", "summary": "The rapid growth of video-text data presents challenges in storage and\ncomputation during training. Online learning, which processes streaming data in\nreal-time, offers a promising solution to these issues while also allowing\nswift adaptations in scenarios demanding real-time responsiveness. One strategy\nto enhance the efficiency and effectiveness of learning involves identifying\nand prioritizing data that enhances performance on target downstream tasks. We\npropose Relevance and Specificity-based online filtering framework (ReSpec)\nthat selects data based on four criteria: (i) modality alignment for clean\ndata, (ii) task relevance for target focused data, (iii) specificity for\ninformative and detailed data, and (iv) efficiency for low-latency processing.\nRelevance is determined by the probabilistic alignment of incoming data with\ndownstream tasks, while specificity employs the distance to a root embedding\nrepresenting the least specific data as an efficient proxy for informativeness.\nBy establishing reference points from target task data, ReSpec filters incoming\ndata in real-time, eliminating the need for extensive storage and compute.\nEvaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains\nstate-of-the-art performance on five zeroshot video retrieval tasks, using as\nlittle as 5% of the data while incurring minimal compute. The source code is\navailable at https://github.com/cdjkim/ReSpec.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14640", "pdf": "https://arxiv.org/pdf/2504.14640", "abs": "https://arxiv.org/abs/2504.14640", "authors": ["Yuheng Huang", "Lei Ma", "Keizaburo Nishikino", "Takumi Akazaki"], "title": "Risk Assessment Framework for Code LLMs via Leveraging Internal States", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "To appear in the 33rd ACM International Conference on the Foundations\n  of Software Engineering (FSE Companion'25 Industry Track), June 23-28, 2025,\n  Trondheim, Norway. This work was supported by Fujitsu Limited", "summary": "The pre-training paradigm plays a key role in the success of Large Language\nModels (LLMs), which have been recognized as one of the most significant\nadvancements of AI recently. Building on these breakthroughs, code LLMs with\nadvanced coding capabilities bring huge impacts on software engineering,\nshowing the tendency to become an essential part of developers' daily routines.\nHowever, the current code LLMs still face serious challenges related to\ntrustworthiness, as they can generate incorrect, insecure, or unreliable code.\nRecent exploratory studies find that it can be promising to detect such risky\noutputs by analyzing LLMs' internal states, akin to how the human brain\nunconsciously recognizes its own mistakes. Yet, most of these approaches are\nlimited to narrow sub-domains of LLM operations and fall short of achieving\nindustry-level scalability and practicability. To address these challenges, in\nthis paper, we propose PtTrust, a two-stage risk assessment framework for code\nLLM based on internal state pre-training, designed to integrate seamlessly with\nthe existing infrastructure of software companies. The core idea is that the\nrisk assessment framework could also undergo a pre-training process similar to\nLLMs. Specifically, PtTrust first performs unsupervised pre-training on\nlarge-scale unlabeled source code to learn general representations of LLM\nstates. Then, it uses a small, labeled dataset to train a risk predictor. We\ndemonstrate the effectiveness of PtTrust through fine-grained, code line-level\nrisk assessment and demonstrate that it generalizes across tasks and different\nprogramming languages. Further experiments also reveal that PtTrust provides\nhighly intuitive and interpretable features, fostering greater user trust. We\nbelieve PtTrust makes a promising step toward scalable and trustworthy\nassurance for code LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14858", "pdf": "https://arxiv.org/pdf/2504.14858", "abs": "https://arxiv.org/abs/2504.14858", "authors": ["Jiaqi Wei", "Hao Zhou", "Xiang Zhang", "Di Zhang", "Zijie Qiu", "Wei Wei", "Jinzhe Li", "Wanli Ouyang", "Siqi Sun"], "title": "AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a foundational paradigm\nfor knowledge-grounded text generation. However, existing RAG pipelines often\nfail to ensure that the reasoning trajectories align with the evidential\nconstraints imposed by retrieved content. In this paper, we reframe RAG as a\nproblem of retrieval-aware reasoning and identify a core challenge: reasoning\nmisalignment-the mismatch between a model's reasoning trajectory and the\nretrieved evidence. To address this challenge, we propose AlignRAG, a novel\ntest-time framework that mitigates reasoning misalignment through iterative\nCritique-Driven Alignment (CDA) steps. In contrast to prior approaches that\nrely on static training or post-hoc selection, AlignRAG actively refines\nreasoning trajectories during inference by enforcing fine-grained alignment\nwith evidence. Our framework introduces a new paradigm for retrieval-aware\nreasoning by: (1) constructing context-rich training corpora; (2) generating\ncontrastive critiques from preference-aware reasoning trajectories; (3)\ntraining a dedicated \\textit{Critic Language Model (CLM)} to identify reasoning\nmisalignments; and (4) applying CDA steps to optimize reasoning trajectories\niteratively. Empirical results demonstrate that AlignRAG consistently\noutperforms all baselines and could integrate as a plug-and-play module into\nexisting RAG pipelines without further changes. By reconceptualizing RAG as a\nstructured reasoning trajectory and establishing the test-time framework for\ncorrecting reasoning misalignments in RAG, AlignRAG provides practical\nadvancements for retrieval-aware generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14952", "pdf": "https://arxiv.org/pdf/2504.14952", "abs": "https://arxiv.org/abs/2504.14952", "authors": ["Qianyu Zhu", "Junjie Wang", "Jeremiah Hu", "Jia Ai", "Yong Lee"], "title": "PIV-FlowDiffuser:Transfer-learning-based denoising diffusion models for PIV", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Deep learning algorithms have significantly reduced the computational time\nand improved the spatial resolution of particle image velocimetry~(PIV).\nHowever, the models trained on synthetic datasets might have a degraded\nperformance on practical particle images due to domain gaps. As a result,\nspecial residual patterns are often observed for the vector fields of deep\nlearning-based estimators. To reduce the special noise step-by-step, we employ\na denoising diffusion model~(FlowDiffuser) for PIV analysis. And the\ndata-hungry iterative denoising diffusion model is trained via a transfer\nlearning strategy, resulting in our PIV-FlowDiffuser method. Specifically, (1)\npre-training a FlowDiffuser model with multiple optical flow datasets of the\ncomputer vision community, such as Sintel, KITTI, etc; (2) fine-tuning the\npre-trained model on synthetic PIV datasets. Note that the PIV images are\nupsampled by a factor of two to resolve the small-scale turbulent flow\nstructures. The visualized results indicate that our PIV-FlowDiffuser\neffectively suppresses the noise patterns. Therefore, the denoising diffusion\nmodel reduces the average end-point error~($AEE$) by 59.4% over RAFT256-PIV\nbaseline on the classic Cai's dataset. Besides, PIV-FlowDiffuser exhibits\nenhanced generalization performance on unseen particle images due to transfer\nlearning. Overall, this study highlights the transfer-learning-based denoising\ndiffusion models for PIV. And a detailed implementation is recommended for\ninterested readers in the repository\nhttps://github.com/Zhu-Qianyu/PIV-FlowDiffuser.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14904", "pdf": "https://arxiv.org/pdf/2504.14904", "abs": "https://arxiv.org/abs/2504.14904", "authors": ["Xingyu Lu", "Tianke Zhang", "Chang Meng", "Xiaobei Wang", "Jinpeng Wang", "YiFan Zhang", "Shisong Tang", "Changyi Liu", "Haojie Ding", "Kaiyu Jiang", "Kaiyu Tang", "Bin Wen", "Hai-Tao Zheng", "Fan Yang", "Tingting Gao", "Di Zhang", "Kun Gai"], "title": "VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.MM"], "comment": "20 pages, 6 figures", "summary": "Exponentially growing short video platforms (SVPs) face significant\nchallenges in moderating content detrimental to users' mental health,\nparticularly for minors. The dissemination of such content on SVPs can lead to\ncatastrophic societal consequences. Although substantial efforts have been\ndedicated to moderating such content, existing methods suffer from critical\nlimitations: (1) Manual review is prone to human bias and incurs high\noperational costs. (2) Automated methods, though efficient, lack nuanced\ncontent understanding, resulting in lower accuracy. (3) Industrial moderation\nregulations struggle to adapt to rapidly evolving trends due to long update\ncycles. In this paper, we annotate the first SVP content moderation benchmark\nwith authentic user/reviewer feedback to fill the absence of benchmark in this\nfield. Then we evaluate various methods on the benchmark to verify the\nexistence of the aforementioned limitations. We further propose our common-law\ncontent moderation framework named KuaiMod to address these challenges. KuaiMod\nconsists of three components: training data construction, offline adaptation,\nand online deployment & refinement. Leveraging large vision language model\n(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video\ntoxicity based on sparse user feedback and fosters dynamic moderation policy\nwith rapid update speed and high accuracy. Offline experiments and large-scale\nonline A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the\nbest moderation performance on our benchmark. The deployment of KuaiMod reduces\nthe user reporting rate by 20% and its application in video recommendation\nincreases both Daily Active User (DAU) and APP Usage Time (AUT) on several\nKuaishou scenarios. We have open-sourced our benchmark at\nhttps://kuaimod.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14977", "pdf": "https://arxiv.org/pdf/2504.14977", "abs": "https://arxiv.org/abs/2504.14977", "authors": ["Jingkai Zhou", "Yifan Wu", "Shikai Li", "Min Wei", "Chao Fan", "Weihua Chen", "Wei Jiang", "Fan Wang"], "title": "RealisDance-DiT: Simple yet Strong Baseline towards Controllable Character Animation in the Wild", "categories": ["cs.CV"], "comment": "Project Page:\n  https://thefoxofsky.github.io/project_pages_new/RealisDance-DiT/index", "summary": "Controllable character animation remains a challenging problem, particularly\nin handling rare poses, stylized characters, character-object interactions,\ncomplex illumination, and dynamic scenes. To tackle these issues, prior work\nhas largely focused on injecting pose and appearance guidance via elaborate\nbypass networks, but often struggles to generalize to open-world scenarios. In\nthis paper, we propose a new perspective that, as long as the foundation model\nis powerful enough, straightforward model modifications with flexible\nfine-tuning strategies can largely address the above challenges, taking a step\ntowards controllable character animation in the wild. Specifically, we\nintroduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our\nsufficient analysis reveals that the widely adopted Reference Net design is\nsuboptimal for large-scale DiT models. Instead, we demonstrate that minimal\nmodifications to the foundation model architecture yield a surprisingly strong\nbaseline. We further propose the low-noise warmup and \"large batches and small\niterations\" strategies to accelerate model convergence during fine-tuning while\nmaximally preserving the priors of the foundation model. In addition, we\nintroduce a new test dataset that captures diverse real-world challenges,\ncomplementing existing benchmarks such as TikTok dataset and UBC fashion video\ndataset, to comprehensively evaluate the proposed method. Extensive experiments\nshow that RealisDance-DiT outperforms existing methods by a large margin.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14945", "pdf": "https://arxiv.org/pdf/2504.14945", "abs": "https://arxiv.org/abs/2504.14945", "authors": ["Jianhao Yan", "Yafu Li", "Zican Hu", "Zhi Wang", "Ganqu Cui", "Xiaoye Qu", "Yu Cheng", "Yue Zhang"], "title": "Learning to Reason under Off-Policy Guidance", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15254", "pdf": "https://arxiv.org/pdf/2504.15254", "abs": "https://arxiv.org/abs/2504.15254", "authors": ["Anirudh Khatry", "Robert Zhang", "Jia Pan", "Ziteng Wang", "Qiaochu Chen", "Greg Durrett", "Isil Dillig"], "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15270", "pdf": "https://arxiv.org/pdf/2504.15270", "abs": "https://arxiv.org/abs/2504.15270", "authors": ["Ji Qi", "Yuan Yao", "Yushi Bai", "Bin Xu", "Juanzi Li", "Zhiyuan Liu", "Tat-Seng Chua"], "title": "An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Multimodal Models (LMMs) uniformly perceive video frames, creating\ncomputational inefficiency for videos with inherently varying temporal\ninformation density. This paper present \\textbf{Quicksviewer}, an LMM with new\nperceiving paradigm that partitions a video of nonuniform density into varying\ncubes using Gumbel Softmax, followed by a unified resampling for each cube to\nachieve efficient video understanding. This simple and intuitive approach\ndynamically compress video online based on its temporal density, significantly\nreducing spatiotemporal redundancy (overall 45$\\times$ compression rate), while\nenabling efficient training with large receptive field. We train the model from\na language backbone through three progressive stages, each incorporating\nlengthy videos on average of 420s/1fps thanks to the perceiving efficiency.\nWith only 0.8M total video-text samples for training, our model outperforms the\ndirect baseline employing a fixed partitioning strategy by a maximum of 8.72 in\naccuracy, demonstrating the effectiveness in performance. On Video-MME,\nQuicksviewer achieves SOTA under modest sequence lengths using just up to 5\\%\nof tokens per frame required by baselines. With this paradigm, scaling up the\nnumber of input frames reveals a clear power law of the model capabilities. It\nis also empirically verified that the segments generated by the cubing network\ncan help for analyzing continuous events in videos.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15108", "pdf": "https://arxiv.org/pdf/2504.15108", "abs": "https://arxiv.org/abs/2504.15108", "authors": ["Zhenzhen Xiao", "Heng Liu", "Bingwen Hu"], "title": "Unwarping Screen Content Images via Structure-texture Enhancement Network and Transformation Self-estimation", "categories": ["cs.CV"], "comment": null, "summary": "While existing implicit neural network-based image unwarping methods perform\nwell on natural images, they struggle to handle screen content images (SCIs),\nwhich often contain large geometric distortions, text, symbols, and sharp\nedges. To address this, we propose a structure-texture enhancement network\n(STEN) with transformation self-estimation for SCI warping. STEN integrates a\nB-spline implicit neural representation module and a transformation error\nestimation and self-correction algorithm. It comprises two branches: the\nstructure estimation branch (SEB), which enhances local aggregation and global\ndependency modeling, and the texture estimation branch (TEB), which improves\ntexture detail synthesis using B-spline implicit neural representation.\nAdditionally, the transformation self-estimation module autonomously estimates\nthe transformation error and corrects the coordinate transformation matrix,\neffectively handling real-world image distortions. Extensive experiments on\npublic SCI datasets demonstrate that our approach significantly outperforms\nstate-of-the-art methods. Comparisons on well-known natural image datasets also\nshow the potential of our approach for natural image distortion.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15155", "pdf": "https://arxiv.org/pdf/2504.15155", "abs": "https://arxiv.org/abs/2504.15155", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Dynamic 3D KAN Convolution with Adaptive Grid Optimization for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks face several challenges in hyperspectral image\nclassification, including high-dimensional data, sparse distribution of ground\nobjects, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To more efficiently adapt to\nground object distributions while extracting image features without introducing\nexcessive parameters and skipping redundant information, this paper proposes\nKANet based on an improved 3D-DenseNet model, consisting of 3D KAN Conv and an\nadaptive grid update mechanism. By introducing learnable univariate B-spline\nfunctions on network edges, specifically by flattening three-dimensional\nneighborhoods into vectors and applying B-spline-parameterized nonlinear\nactivation functions to replace the fixed linear weights of traditional 3D\nconvolutional kernels, we precisely capture complex spectral-spatial nonlinear\nrelationships in hyperspectral data. Simultaneously, through a dynamic grid\nadjustment mechanism, we adaptively update the grid point positions of\nB-splines based on the statistical characteristics of input data, optimizing\nthe resolution of spline functions to match the non-uniform distribution of\nspectral features, significantly improving the model's accuracy in\nhigh-dimensional data modeling and parameter efficiency, effectively\nalleviating the curse of dimensionality. This characteristic demonstrates\nsuperior neural scaling laws compared to traditional convolutional neural\nnetworks and reduces overfitting risks in small-sample and high-noise\nscenarios. KANet enhances model representation capability through a 3D dynamic\nexpert convolution system without increasing network depth or width. The\nproposed method demonstrates superior performance on IN, UP, and KSC datasets,\noutperforming mainstream hyperspectral image classification approaches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15165", "pdf": "https://arxiv.org/pdf/2504.15165", "abs": "https://arxiv.org/abs/2504.15165", "authors": ["Liu Wenbin"], "title": "An Efficient Aerial Image Detection with Variable Receptive Fields", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aerial object detection using unmanned aerial vehicles (UAVs) faces critical\nchallenges including sub-10px targets, dense occlusions, and stringent\ncomputational constraints. Existing detectors struggle to balance accuracy and\nefficiency due to rigid receptive fields and redundant architectures. To\naddress these limitations, we propose Variable Receptive Field DETR (VRF-DETR),\na transformer-based detector incorporating three key components: 1) Multi-Scale\nContext Fusion (MSCF) module that dynamically recalibrates features through\nadaptive spatial attention and gated multi-scale fusion, 2) Gated Convolution\n(GConv) layer enabling parameter-efficient local-context modeling via depthwise\nseparable operations and dynamic gating, and 3) Gated Multi-scale Fusion (GMCF)\nBottleneck that hierarchically disentangles occluded objects through cascaded\nglobal-local interactions. Experiments on VisDrone2019 demonstrate VRF-DETR\nachieves 51.4\\% mAP\\textsubscript{50} and 31.8\\% mAP\\textsubscript{50:95} with\nonly 13.5M parameters. This work establishes a new efficiency-accuracy Pareto\nfrontier for UAV-based detection tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15170", "pdf": "https://arxiv.org/pdf/2504.15170", "abs": "https://arxiv.org/abs/2504.15170", "authors": ["Chengxi Han", "Xiaoyu Su", "Zhiqiang Wei", "Meiqi Hu", "Yichu Xu"], "title": "HSANET: A Hybrid Self-Cross Attention Network For Remote Sensing Change Detection", "categories": ["cs.CV"], "comment": null, "summary": "The remote sensing image change detection task is an essential method for\nlarge-scale monitoring. We propose HSANet, a network that uses hierarchical\nconvolution to extract multi-scale features. It incorporates hybrid\nself-attention and cross-attention mechanisms to learn and fuse global and\ncross-scale information. This enables HSANet to capture global context at\ndifferent scales and integrate cross-scale features, refining edge details and\nimproving detection performance. We will also open-source our model code:\nhttps://github.com/ChengxiHAN/HSANet.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15270", "pdf": "https://arxiv.org/pdf/2504.15270", "abs": "https://arxiv.org/abs/2504.15270", "authors": ["Ji Qi", "Yuan Yao", "Yushi Bai", "Bin Xu", "Juanzi Li", "Zhiyuan Liu", "Tat-Seng Chua"], "title": "An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Multimodal Models (LMMs) uniformly perceive video frames, creating\ncomputational inefficiency for videos with inherently varying temporal\ninformation density. This paper present \\textbf{Quicksviewer}, an LMM with new\nperceiving paradigm that partitions a video of nonuniform density into varying\ncubes using Gumbel Softmax, followed by a unified resampling for each cube to\nachieve efficient video understanding. This simple and intuitive approach\ndynamically compress video online based on its temporal density, significantly\nreducing spatiotemporal redundancy (overall 45$\\times$ compression rate), while\nenabling efficient training with large receptive field. We train the model from\na language backbone through three progressive stages, each incorporating\nlengthy videos on average of 420s/1fps thanks to the perceiving efficiency.\nWith only 0.8M total video-text samples for training, our model outperforms the\ndirect baseline employing a fixed partitioning strategy by a maximum of 8.72 in\naccuracy, demonstrating the effectiveness in performance. On Video-MME,\nQuicksviewer achieves SOTA under modest sequence lengths using just up to 5\\%\nof tokens per frame required by baselines. With this paradigm, scaling up the\nnumber of input frames reveals a clear power law of the model capabilities. It\nis also empirically verified that the segments generated by the cubing network\ncan help for analyzing continuous events in videos.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15271", "pdf": "https://arxiv.org/pdf/2504.15271", "abs": "https://arxiv.org/abs/2504.15271", "authors": ["Guo Chen", "Zhiqi Li", "Shihao Wang", "Jindong Jiang", "Yicheng Liu", "Lidong Lu", "De-An Huang", "Wonmin Byeon", "Matthieu Le", "Tuomas Rintamaki", "Tyler Poon", "Max Ehrlich", "Tuomas Rintamaki", "Tyler Poon", "Tong Lu", "Limin Wang", "Bryan Catanzaro", "Jan Kautz", "Andrew Tao", "Zhiding Yu", "Guilin Liu"], "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)\nfor long-context multimodal learning. Our work addresses the challenges in long\nvideo comprehension and high-resolution image understanding, introducing a\ngeneralist framework for both tasks. The proposed training framework\nincorporates Automatic Degrade Sampling and Image Area Preservation, two\ntechniques that preserve contextual integrity and visual details. The framework\nalso includes numerous efficiency optimizations in the pipeline for\nlong-context data training. Finally, we propose Eagle-Video-110K, a novel\ndataset that integrates both story-level and clip-level annotations,\nfacilitating long-video understanding. Eagle 2.5 demonstrates substantial\nimprovements on long-context multimodal benchmarks, providing a robust solution\nto the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B\nachieves 72.4% on Video-MME with 512 input frames, matching the results of\ntop-tier commercial model such as GPT-4o and large-scale open-source models\nlike Qwen2.5-VL-72B and InternVL2.5-78B.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15281", "pdf": "https://arxiv.org/pdf/2504.15281", "abs": "https://arxiv.org/abs/2504.15281", "authors": ["Cailin Zhuang", "Yaoqi Hu", "Xuanyang Zhang", "Wei Cheng", "Jiacheng Bao", "Shengqi Liu", "Yiying Yang", "Xianfang Zeng", "Gang Yu", "Ming Li"], "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians", "categories": ["cs.CV"], "comment": "16 pages; Project page: https://styleme3d.github.io/", "summary": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction\nbut struggles with stylized scenarios (e.g., cartoons, games) due to fragmented\ntextures, semantic misalignment, and limited adaptability to abstract\naesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer\nthat integrates multi-modal style conditioning, multi-level semantic alignment,\nand perceptual quality enhancement. Our key insights include: (1) optimizing\nonly RGB attributes preserves geometric integrity during stylization; (2)\ndisentangling low-, medium-, and high-level semantics is critical for coherent\nstyle transfer; (3) scalability across isolated objects and complex scenes is\nessential for practical deployment. StyleMe3D introduces four novel components:\nDynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent\nspace for semantic alignment; Contrastive Style Descriptor (CSD) for localized,\ncontent-aware texture transfer; Simultaneously Optimized Scale (SOS) to\ndecouple style details and structural coherence; and 3D Gaussian Quality\nAssessment (3DG-QA), a differentiable aesthetic prior trained on human-rated\ndata to suppress artifacts and enhance visual harmony. Evaluated on NeRF\nsynthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D\noutperforms state-of-the-art methods in preserving geometric details (e.g.,\ncarvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,\ncoherent lighting in landscapes), while maintaining real-time rendering. This\nwork bridges photorealistic 3D GS and artistic stylization, unlocking\napplications in gaming, virtual worlds, and digital art.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14373", "pdf": "https://arxiv.org/pdf/2504.14373", "abs": "https://arxiv.org/abs/2504.14373", "authors": ["Chen Guo", "Zhuo Su", "Jian Wang", "Shuang Li", "Xu Chang", "Zhaohu Li", "Yang Zhao", "Guidong Wang", "Ruqi Huang"], "title": "SEGA: Drivable 3D Gaussian Head Avatar from a Single Image", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Creating photorealistic 3D head avatars from limited input has become\nincreasingly important for applications in virtual reality, telepresence, and\ndigital entertainment. While recent advances like neural rendering and 3D\nGaussian splatting have enabled high-quality digital human avatar creation and\nanimation, most methods rely on multiple images or multi-view inputs, limiting\ntheir practicality for real-world use. In this paper, we propose SEGA, a novel\napproach for Single-imagE-based 3D drivable Gaussian head Avatar creation that\ncombines generalized prior models with a new hierarchical UV-space Gaussian\nSplatting framework. SEGA seamlessly combines priors derived from large-scale\n2D datasets with 3D priors learned from multi-view, multi-expression, and\nmulti-ID data, achieving robust generalization to unseen identities while\nensuring 3D consistency across novel viewpoints and expressions. We further\npresent a hierarchical UV-space Gaussian Splatting framework that leverages\nFLAME-based structural priors and employs a dual-branch architecture to\ndisentangle dynamic and static facial components effectively. The dynamic\nbranch encodes expression-driven fine details, while the static branch focuses\non expression-invariant regions, enabling efficient parameter inference and\nprecomputation. This design maximizes the utility of limited 3D data and\nachieves real-time performance for animation and rendering. Additionally, SEGA\nperforms person-specific fine-tuning to further enhance the fidelity and\nrealism of the generated avatars. Experiments show our method outperforms\nstate-of-the-art approaches in generalization ability, identity preservation,\nand expression realism, advancing one-shot avatar creation for practical\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14409", "pdf": "https://arxiv.org/pdf/2504.14409", "abs": "https://arxiv.org/abs/2504.14409", "authors": ["Christopher Ick", "Gordon Wichern", "Yoshiki Masuyama", "Fran√ßois G. Germain", "Jonathan Le Roux"], "title": "Data Augmentation Using Neural Acoustic Fields With Retrieval-Augmented Pre-training", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "comment": "Presented at ICASSP 2025 GenDA Workshop", "summary": "This report details MERL's system for room impulse response (RIR) estimation\nsubmitted to the Generative Data Augmentation Workshop at ICASSP 2025 for\nAugmenting RIR Data (Task 1) and Improving Speaker Distance Estimation (Task\n2). We first pre-train a neural acoustic field conditioned by room geometry on\nan external large-scale dataset in which pairs of RIRs and the geometries are\nprovided. The neural acoustic field is then adapted to each target room by\nusing the enrollment data, where we leverage either the provided room\ngeometries or geometries retrieved from the external dataset, depending on\navailability. Lastly, we predict the RIRs for each pair of source and receiver\nlocations specified by Task 1, and use these RIRs to train the speaker distance\nestimation model in Task 2.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14588", "pdf": "https://arxiv.org/pdf/2504.14588", "abs": "https://arxiv.org/abs/2504.14588", "authors": ["Wenke Xia", "Ruoxuan Feng", "Dong Wang", "Di Hu"], "title": "Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Building a generalizable self-correction system is crucial for robots to\nrecover from failures. Despite advancements in Multimodal Large Language Models\n(MLLMs) that empower robots with semantic reflection ability for failure,\ntranslating semantic reflection into how to correct fine-grained robotic\nactions remains a significant challenge. To address this gap, we build the\nPhoenix framework, which leverages motion instruction as a bridge to connect\nhigh-level semantic reflection with low-level robotic action correction. In\nthis motion-based self-reflection framework, we start with a dual-process\nmotion adjustment mechanism with MLLMs to translate the semantic reflection\ninto coarse-grained motion instruction adjustment. To leverage this motion\ninstruction for guiding how to correct fine-grained robotic actions, a\nmulti-task motion-conditioned diffusion policy is proposed to integrate visual\nobservations for high-frequency robotic action correction. By combining these\ntwo models, we could shift the demand for generalization capability from the\nlow-level manipulation policy to the MLLMs-driven motion adjustment model and\nfacilitate precise, fine-grained robotic action correction. Utilizing this\nframework, we further develop a lifelong learning method to automatically\nimprove the model's capability from interactions with dynamic environments. The\nexperiments conducted in both the RoboMimic simulation and real-world scenarios\nprove the superior generalization and robustness of our framework across a\nvariety of manipulation tasks. Our code is released at\n\\href{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14662", "pdf": "https://arxiv.org/pdf/2504.14662", "abs": "https://arxiv.org/abs/2504.14662", "authors": ["Yeoreum Lee", "Jinwook Jung", "Sungyong Baik"], "title": "Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": "ICLR 2025", "summary": "Large-scale deep learning models with a pretraining-finetuning paradigm have\nled to a surge of numerous task-specific models fine-tuned from a common\npre-trained model. Recently, several research efforts have been made on merging\nthese large models into a single multi-task model, particularly with simple\narithmetic on parameters. Such merging methodology faces a central challenge:\ninterference between model parameters fine-tuned on different tasks. Few recent\nworks have focused on designing a new fine-tuning scheme that can lead to small\nparameter interference, however at the cost of the performance of each\ntask-specific fine-tuned model and thereby limiting that of a merged model. To\nimprove the performance of a merged model, we note that a fine-tuning scheme\nshould aim for (1) smaller parameter interference and (2) better performance of\neach fine-tuned model on the corresponding task. In this work, we aim to design\na new fine-tuning objective function to work towards these two goals. In the\ncourse of this process, we find such objective function to be strikingly\nsimilar to sharpness-aware minimization (SAM) objective function, which aims to\nachieve generalization by finding flat minima. Drawing upon our observation, we\npropose to fine-tune pre-trained models via sharpness-aware minimization. The\nexperimental and theoretical results showcase the effectiveness and\northogonality of our proposed approach, improving performance upon various\nmerging and fine-tuning methods. Our code is available at\nhttps://github.com/baiklab/SAFT-Merge.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14815", "pdf": "https://arxiv.org/pdf/2504.14815", "abs": "https://arxiv.org/abs/2504.14815", "authors": ["Xiaoyong Yuan", "Xiaolong Ma", "Linke Guo", "Lan Zhang"], "title": "What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "comment": "17 pages, 15 figures", "summary": "Diffusion models (DMs) have revolutionized text-to-image generation, enabling\nthe creation of highly realistic and customized images from text prompts. With\nthe rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users\ncan now customize powerful pre-trained models using minimal computational\nresources. However, the widespread sharing of fine-tuned DMs on open platforms\nraises growing ethical and legal concerns, as these models may inadvertently or\ndeliberately generate sensitive or unauthorized content, such as copyrighted\nmaterial, private individuals, or harmful content. Despite the increasing\nregulatory attention on generative AI, there are currently no practical tools\nfor systematically auditing these models before deployment. In this paper, we\naddress the problem of concept auditing: determining whether a fine-tuned DM\nhas learned to generate a specific target concept. Existing approaches\ntypically rely on prompt-based input crafting and output-based image\nclassification but suffer from critical limitations, including prompt\nuncertainty, concept drift, and poor scalability. To overcome these challenges,\nwe introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric\nconcept auditing framework. By treating the DM as the object of inspection,\nPAIA enables direct analysis of internal model behavior, bypassing the need for\noptimized prompts or generated images. We evaluate PAIA on 320 controlled model\nand 690 real-world community models sourced from a public DM sharing platform.\nPAIA achieves over 90% detection accuracy while reducing auditing time by\n18-40x compared to existing baselines. To our knowledge, PAIA is the first\nscalable and practical solution for pre-deployment concept auditing of\ndiffusion models, providing a practical foundation for safer and more\ntransparent diffusion model sharing.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14906", "pdf": "https://arxiv.org/pdf/2504.14906", "abs": "https://arxiv.org/abs/2504.14906", "authors": ["Huadai Liu", "Tianyi Luo", "Qikai Jiang", "Kaicheng Luo", "Peiwen Sun", "Jialei Wan", "Rongjie Huang", "Qian Chen", "Wen Wang", "Xiangtai Li", "Shiliang Zhang", "Zhijie Yan", "Zhou Zhao", "Wei Xue"], "title": "OmniAudio: Generating Spatial Audio from 360-Degree Video", "categories": ["eess.AS", "cs.CV", "cs.SD"], "comment": "Work in Progress", "summary": "Traditional video-to-audio generation techniques primarily focus on\nfield-of-view (FoV) video and non-spatial audio, often missing the spatial cues\nnecessary for accurately representing sound sources in 3D environments. To\naddress this limitation, we introduce a novel task, 360V2SA, to generate\nspatial audio from 360-degree videos, specifically producing First-order\nAmbisonics (FOA) audio - a standard format for representing 3D spatial audio\nthat captures sound directionality and enables realistic 3D audio reproduction.\nWe first create Sphere360, a novel dataset tailored for this task that is\ncurated from real-world data. We also design an efficient semi-automated\npipeline for collecting and cleaning paired video-audio data. To generate\nspatial audio from 360-degree video, we propose a novel framework OmniAudio,\nwhich leverages self-supervised pre-training using both spatial audio data (in\nFOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a\ndual-branch framework that utilizes both panoramic and FoV video inputs to\ncapture comprehensive local and global information from 360-degree videos.\nExperimental results demonstrate that OmniAudio achieves state-of-the-art\nperformance across both objective and subjective metrics on Sphere360. Code and\ndatasets will be released at https://github.com/liuhuadai/OmniAudio. The demo\npage is available at https://OmniAudio-360V2SA.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15133", "pdf": "https://arxiv.org/pdf/2504.15133", "abs": "https://arxiv.org/abs/2504.15133", "authors": ["Ziwen Xu", "Shuxun Wang", "Kewei Xu", "Haoming Xu", "Mengru Wang", "Xinle Deng", "Yunzhi Yao", "Guozhou Zheng", "Huajun Chen", "Ningyu Zhang"], "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "Work in progress. Demo:\n  https://zjunlp.github.io/project/EasyEdit2/video; code:\n  https://github.com/zjunlp/EasyEdit", "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "safety"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
