{"id": "2504.18058", "pdf": "https://arxiv.org/pdf/2504.18058", "abs": "https://arxiv.org/abs/2504.18058", "authors": ["Sijia Cheng", "Wen-Yu Chang", "Yun-Nung Chen"], "title": "Exploring Personality-Aware Interactions in Salesperson Dialogue Agents", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IWSDS 2025", "summary": "The integration of dialogue agents into the sales domain requires a deep\nunderstanding of how these systems interact with users possessing diverse\npersonas. This study explores the influence of user personas, defined using the\nMyers-Briggs Type Indicator (MBTI), on the interaction quality and performance\nof sales-oriented dialogue agents. Through large-scale testing and analysis, we\nassess the pre-trained agent's effectiveness, adaptability, and personalization\ncapabilities across a wide range of MBTI-defined user types. Our findings\nreveal significant patterns in interaction dynamics, task completion rates, and\ndialogue naturalness, underscoring the future potential for dialogue agents to\nrefine their strategies to better align with varying personality traits. This\nwork not only provides actionable insights for building more adaptive and\nuser-centric conversational systems in the sales domain but also contributes\nbroadly to the field by releasing persona-defined user simulators. These\nsimulators, unconstrained by domain, offer valuable tools for future research\nand demonstrate the potential for scaling personalized dialogue systems across\ndiverse applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18070", "pdf": "https://arxiv.org/pdf/2504.18070", "abs": "https://arxiv.org/abs/2504.18070", "authors": ["Jingjin Wang"], "title": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths", "categories": ["cs.CL", "cs.AI"], "comment": "Code and data to be released at:\n  https://github.com/ReLink-Inc/PropRAG", "summary": "Retrieval Augmented Generation (RAG) has become the standard non-parametric\napproach for equipping Large Language Models (LLMs) with up-to-date knowledge\nand mitigating catastrophic forgetting common in continual learning. However,\nstandard RAG, relying on independent passage retrieval, fails to capture the\ninterconnected nature of human memory crucial for complex reasoning\n(associativity) and contextual understanding (sense-making). While structured\nRAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples,\nthe inherent context loss limits fidelity. We introduce PropRAG, a framework\nleveraging contextually rich propositions and a novel beam search algorithm\nover proposition paths to explicitly discover multi-step reasoning chains.\nCrucially, PropRAG's online retrieval process operates entirely without\ninvoking generative LLMs, relying instead on efficient graph traversal and\npre-computed embeddings. This avoids online LLM inference costs and potential\ninconsistencies during evidence gathering. LLMs are used effectively offline\nfor high-quality proposition extraction and post-retrieval for answer\ngeneration. PropRAG achieves state-of-the-art zero-shot Recall@5 results on\nPopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside\ntop F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through\nricher representation and explicit, LLM-free online path finding, PropRAG\nadvances non-parametric continual learning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning", "beam search"], "score": 2}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18114", "pdf": "https://arxiv.org/pdf/2504.18114", "abs": "https://arxiv.org/abs/2504.18114", "authors": ["Atharva Kulkarni", "Yuan Zhang", "Joel Ruben Antony Moniz", "Xiou Ge", "Bo-Hsiang Tseng", "Dhivya Piraviperumal", "Swabha Swayamdipta", "Hong Yu"], "title": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "factuality", "reliability"], "score": 3}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18012", "pdf": "https://arxiv.org/pdf/2504.18012", "abs": "https://arxiv.org/abs/2504.18012", "authors": ["Zhuang Yu", "Shiliang Sun", "Jing Zhao", "Tengfei Song", "Hao Yang"], "title": "Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Machine Translation (MMT) aims to improve translation quality by\nleveraging auxiliary modalities such as images alongside textual input. While\nrecent advances in large-scale pre-trained language and vision models have\nsignificantly benefited unimodal natural language processing tasks, their\neffectiveness and role in MMT remain underexplored. In this work, we conduct a\nsystematic study on the impact of pre-trained encoders and decoders in\nmultimodal translation models. Specifically, we analyze how different training\nstrategies, from training from scratch to using pre-trained and partially\nfrozen components, affect translation performance under a unified MMT\nframework. Experiments are carried out on the Multi30K and CoMMuTE dataset\nacross English-German and English-French translation tasks. Our results reveal\nthat pre-training plays a crucial yet asymmetrical role in multimodal settings:\npre-trained decoders consistently yield more fluent and accurate outputs, while\npre-trained encoders show varied effects depending on the quality of\nvisual-text alignment. Furthermore, we provide insights into the interplay\nbetween modality fusion and pre-trained components, offering guidance for\nfuture architecture design in multimodal translation systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.17954", "pdf": "https://arxiv.org/pdf/2504.17954", "abs": "https://arxiv.org/abs/2504.17954", "authors": ["Kaiyuan Tang", "Siyuan Yao", "Chaoli Wang"], "title": "iVR-GS: Inverse Volume Rendering for Explorable Visualization via Editable 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted by IEEE Transactions on Visualization and Computer Graphics\n  (TVCG)", "summary": "In volume visualization, users can interactively explore the\nthree-dimensional data by specifying color and opacity mappings in the transfer\nfunction (TF) or adjusting lighting parameters, facilitating meaningful\ninterpretation of the underlying structure. However, rendering large-scale\nvolumes demands powerful GPUs and high-speed memory access for real-time\nperformance. While existing novel view synthesis (NVS) methods offer faster\nrendering speeds with lower hardware requirements, the visible parts of a\nreconstructed scene are fixed and constrained by preset TF settings,\nsignificantly limiting user exploration. This paper introduces inverse volume\nrendering via Gaussian splatting (iVR-GS), an innovative NVS method that\nreduces the rendering cost while enabling scene editing for interactive volume\nexploration. Specifically, we compose multiple iVR-GS models associated with\nbasic TFs covering disjoint visible parts to make the entire volumetric scene\nvisible. Each basic model contains a collection of 3D editable Gaussians, where\neach Gaussian is a 3D spatial point that supports real-time scene rendering and\nediting. We demonstrate the superior reconstruction quality and composability\nof iVR-GS against other NVS solutions (Plenoxels, CCNeRF, and base 3DGS) on\nvarious volume datasets. The code is available at\nhttps://github.com/TouKaienn/iVR-GS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18001", "pdf": "https://arxiv.org/pdf/2504.18001", "abs": "https://arxiv.org/abs/2504.18001", "authors": ["Daniel Zavorotny", "Qi Wu", "David Bauer", "Kwan-Liu Ma"], "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for Interactive Visualization of Tera-Scale Data", "categories": ["cs.GR"], "comment": "11 pages, 11 figures, EGPGV25", "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.17822", "pdf": "https://arxiv.org/pdf/2504.17822", "abs": "https://arxiv.org/abs/2504.17822", "authors": ["Wenwen Li", "Chia-Yu Hsu", "Sizhe Wang", "Zhining Gu", "Yili Yang", "Brendan M. Rogers", "Anna Liljedahl"], "title": "A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Retrogressive Thaw Slumps (RTS) in Arctic regions are distinct permafrost\nlandforms with significant environmental impacts. Mapping these RTS is crucial\nbecause their appearance serves as a clear indication of permafrost thaw.\nHowever, their small scale compared to other landform features, vague\nboundaries, and spatiotemporal variation pose significant challenges for\naccurate detection. In this paper, we employed a state-of-the-art deep learning\nmodel, the Cascade Mask R-CNN with a multi-scale vision transformer-based\nbackbone, to delineate RTS features across the Arctic. Two new strategies were\nintroduced to optimize multimodal learning and enhance the model's predictive\nperformance: (1) a feature-level, residual cross-modality attention fusion\nstrategy, which effectively integrates feature maps from multiple modalities to\ncapture complementary information and improve the model's ability to understand\ncomplex patterns and relationships within the data; (2) pre-trained unimodal\nlearning followed by multimodal fine-tuning to alleviate high computing demand\nwhile achieving strong model performance. Experimental results demonstrated\nthat our approach outperformed existing models adopting data-level fusion,\nfeature-level convolutional fusion, and various attention fusion strategies,\nproviding valuable insights into the efficient utilization of multimodal data\nfor RTS mapping. This research contributes to our understanding of permafrost\nlandforms and their environmental implications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.17826", "pdf": "https://arxiv.org/pdf/2504.17826", "abs": "https://arxiv.org/abs/2504.17826", "authors": ["Kaicheng Pang", "Xingxing Zou", "Waikeung Wong"], "title": "FashionM3: Multimodal, Multitask, and Multiround Fashion Assistant based on Unified Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fashion styling and personalized recommendations are pivotal in modern\nretail, contributing substantial economic value in the fashion industry. With\nthe advent of vision-language models (VLM), new opportunities have emerged to\nenhance retailing through natural language and visual interactions. This work\nproposes FashionM3, a multimodal, multitask, and multiround fashion assistant,\nbuilt upon a VLM fine-tuned for fashion-specific tasks. It helps users discover\nsatisfying outfits by offering multiple capabilities including personalized\nrecommendation, alternative suggestion, product image generation, and virtual\ntry-on simulation. Fine-tuned on the novel FashionRec dataset, comprising\n331,124 multimodal dialogue samples across basic, personalized, and alternative\nrecommendation tasks, FashionM3 delivers contextually personalized suggestions\nwith iterative refinement through multiround interactions. Quantitative and\nqualitative evaluations, alongside user studies, demonstrate FashionM3's\nsuperior performance in recommendation effectiveness and practical value as a\nfashion assistant.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue"], "score": 2}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.17990", "pdf": "https://arxiv.org/pdf/2504.17990", "abs": "https://arxiv.org/abs/2504.17990", "authors": ["Yabing Wang", "Zhuotao Tian", "Qingpei Guo", "Zheng Qin", "Sanping Zhou", "Ming Yang", "Le Wang"], "title": "From Mapping to Composing: A Two-Stage Framework for Zero-shot Composed Image Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Composed Image Retrieval (CIR) is a challenging multimodal task that\nretrieves a target image based on a reference image and accompanying\nmodification text. Due to the high cost of annotating CIR triplet datasets,\nzero-shot (ZS) CIR has gained traction as a promising alternative. Existing\nstudies mainly focus on projection-based methods, which map an image to a\nsingle pseudo-word token. However, these methods face three critical\nchallenges: (1) insufficient pseudo-word token representation capacity, (2)\ndiscrepancies between training and inference phases, and (3) reliance on\nlarge-scale synthetic data. To address these issues, we propose a two-stage\nframework where the training is accomplished from mapping to composing. In the\nfirst stage, we enhance image-to-pseudo-word token learning by introducing a\nvisual semantic injection module and a soft text alignment objective, enabling\nthe token to capture richer and fine-grained image information. In the second\nstage, we optimize the text encoder using a small amount of synthetic triplet\ndata, enabling it to effectively extract compositional semantics by combining\npseudo-word tokens with modification text for accurate target image retrieval.\nThe strong visual-to-pseudo mapping established in the first stage provides a\nsolid foundation for the second stage, making our approach compatible with both\nhigh- and low-quality synthetic data, and capable of achieving significant\nperformance gains with only a small amount of synthetic data. Extensive\nexperiments were conducted on three public datasets, achieving superior\nperformance compared to existing approaches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.17996", "pdf": "https://arxiv.org/pdf/2504.17996", "abs": "https://arxiv.org/abs/2504.17996", "authors": ["Yuanbing Ouyang", "Yizhuo Liang", "Qingpeng Li", "Xinfei Guo", "Yiming Luo", "Di Wu", "Hao Wang", "Yushan Pan"], "title": "Back to Fundamentals: Low-Level Visual Features Guided Progressive Token Pruning", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformers (ViTs) excel in semantic segmentation but demand\nsignificant computation, posing challenges for deployment on\nresource-constrained devices. Existing token pruning methods often overlook\nfundamental visual data characteristics. This study introduces 'LVTP', a\nprogressive token pruning framework guided by multi-scale Tsallis entropy and\nlow-level visual features with twice clustering. It integrates high-level\nsemantics and basic visual attributes for precise segmentation. A novel dynamic\nscoring mechanism using multi-scale Tsallis entropy weighting overcomes\nlimitations of traditional single-parameter entropy. The framework also\nincorporates low-level feature analysis to preserve critical edge information\nwhile optimizing computational cost. As a plug-and-play module, it requires no\narchitectural changes or additional training. Evaluations across multiple\ndatasets show 20%-45% computational reductions with negligible performance\nloss, outperforming existing methods in balancing cost and accuracy, especially\nin complex edge regions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18020", "pdf": "https://arxiv.org/pdf/2504.18020", "abs": "https://arxiv.org/abs/2504.18020", "authors": ["Guyue Hu", "Siyuan Song", "Yukun Kang", "Zhu Yin", "Gangming Zhao", "Chenglong Li", "Jin Tang"], "title": "Federated Client-tailored Adapter for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Medical image segmentation in X-ray images is beneficial for computer-aided\ndiagnosis and lesion localization. Existing methods mainly fall into a\ncentralized learning paradigm, which is inapplicable in the practical medical\nscenario that only has access to distributed data islands. Federated Learning\nhas the potential to offer a distributed solution but struggles with heavy\ntraining instability due to client-wise domain heterogeneity (including\ndistribution diversity and class imbalance). In this paper, we propose a novel\nFederated Client-tailored Adapter (FCA) framework for medical image\nsegmentation, which achieves stable and client-tailored adaptive segmentation\nwithout sharing sensitive local data. Specifically, the federated adapter stirs\nuniversal knowledge in off-the-shelf medical foundation models to stabilize the\nfederated training process. In addition, we develop two client-tailored\nfederated updating strategies that adaptively decompose the adapter into common\nand individual components, then globally and independently update the parameter\ngroups associated with common client-invariant and individual client-specific\nunits, respectively. They further stabilize the heterogeneous federated\nlearning process and realize optimal client-tailored instead of sub-optimal\nglobal-compromised segmentation models. Extensive experiments on three\nlarge-scale datasets demonstrate the effectiveness and superiority of the\nproposed FCA framework for federated medical segmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18046", "pdf": "https://arxiv.org/pdf/2504.18046", "abs": "https://arxiv.org/abs/2504.18046", "authors": ["Guohao Huo", "Zibo Lin", "Zitong Wang", "Ruiting Dai", "Hao Tang"], "title": "DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular Fundus Image Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Ophthalmic diseases pose a significant global health challenge, yet\ntraditional diagnosis methods and existing single-eye deep learning approaches\noften fail to account for binocular pathological correlations. To address this,\nwe propose DMS-Net, a dual-modal multi-scale Siamese network for binocular\nfundus image classification. Our framework leverages weight-shared Siamese\nResNet-152 backbones to extract deep semantic features from paired fundus\nimages. To tackle challenges such as lesion boundary ambiguity and scattered\npathological distributions, we introduce a Multi-Scale Context-Aware Module\n(MSCAM) that integrates adaptive pooling and attention mechanisms for\nmulti-resolution feature aggregation. Additionally, a Dual-Modal Feature Fusion\n(DMFF) module enhances cross-modal interaction through spatial-semantic\nrecalibration and bidirectional attention, effectively combining global context\nand local edge features. Evaluated on the ODIR-5K dataset, DMS-Net achieves\nstate-of-the-art performance with 80.5% accuracy, 86.1% recall, and 83.8%\nCohen's kappa, demonstrating superior capability in detecting symmetric\npathologies and advancing clinical decision-making for ocular diseases.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "kappa", "accuracy"], "score": 3}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18049", "pdf": "https://arxiv.org/pdf/2504.18049", "abs": "https://arxiv.org/abs/2504.18049", "authors": ["Xin Li", "Wenhui Zhu", "Peijie Qiu", "Oana M. Dumitrascu", "Amal Youssef", "Yalin Wang"], "title": "A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In the field of medical imaging, the advent of deep learning, especially the\napplication of convolutional neural networks (CNNs) has revolutionized the\nanalysis and interpretation of medical images. Nevertheless, deep learning\nmethods usually rely on large amounts of labeled data. In medical imaging\nresearch, the acquisition of high-quality labels is both expensive and\ndifficult. The introduction of Vision Transformers (ViT) and self-supervised\nlearning provides a pre-training strategy that utilizes abundant unlabeled\ndata, effectively alleviating the label acquisition challenge while broadening\nthe breadth of data utilization. However, ViT's high computational density and\nsubstantial demand for computing power, coupled with the lack of localization\ncharacteristics of its operations on image patches, limit its efficiency and\napplicability in many application scenarios. In this study, we employ\nnn-MobileNet, a lightweight CNN framework, to implement a BERT-style\nself-supervised learning approach. We pre-train the network on the unlabeled\nretinal fundus images from the UK Biobank to improve downstream application\nperformance. We validate the results of the pre-trained model on Alzheimer's\ndisease (AD), Parkinson's disease (PD), and various retinal diseases\nidentification. The results show that our approach can significantly improve\nperformance in the downstream tasks. In summary, this study combines the\nbenefits of CNNs with the capabilities of advanced self-supervised learning in\nhandling large-scale unlabeled data, demonstrating the potential of CNNs in the\npresence of label scarcity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18127", "pdf": "https://arxiv.org/pdf/2504.18127", "abs": "https://arxiv.org/abs/2504.18127", "authors": ["Jingfan Yang", "Hu Gao", "Ying Zhang", "Depeng Dang"], "title": "Salient Region-Guided Spacecraft Image Arbitrary-Scale Super-Resolution Network", "categories": ["cs.CV"], "comment": null, "summary": "Spacecraft image super-resolution seeks to enhance low-resolution spacecraft\nimages into high-resolution ones. Although existing arbitrary-scale\nsuper-resolution methods perform well on general images, they tend to overlook\nthe difference in features between the spacecraft core region and the large\nblack space background, introducing irrelevant noise. In this paper, we propose\na salient region-guided spacecraft image arbitrary-scale super-resolution\nnetwork (SGSASR), which uses features from the spacecraft core salient regions\nto guide latent modulation and achieve arbitrary-scale super-resolution.\nSpecifically, we design a spacecraft core region recognition block (SCRRB) that\nidentifies the core salient regions in spacecraft images using a pre-trained\nsaliency detection model. Furthermore, we present an adaptive-weighted feature\nfusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft\ncore region features with general image features by dynamic weight parameter to\nenhance the response of the core salient regions. Experimental results\ndemonstrate that the proposed SGSASR outperforms state-of-the-art approaches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18136", "pdf": "https://arxiv.org/pdf/2504.18136", "abs": "https://arxiv.org/abs/2504.18136", "authors": ["Liugang Lu", "Dabin He", "Congxiang Liu", "Zhixiang Deng"], "title": "MASF-YOLO: An Improved YOLOv11 Network for Small Object Detection on Drone View", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of Unmanned Aerial Vehicle (UAV) and computer\nvision technologies, object detection from UAV perspectives has emerged as a\nprominent research area. However, challenges for detection brought by the\nextremely small proportion of target pixels, significant scale variations of\nobjects, and complex background information in UAV images have greatly limited\nthe practical applications of UAV. To address these challenges, we propose a\nnovel object detection network Multi-scale Context Aggregation and\nScale-adaptive Fusion YOLO (MASF-YOLO), which is developed based on YOLOv11.\nFirstly, to tackle the difficulty of detecting small objects in UAV images, we\ndesign a Multi-scale Feature Aggregation Module (MFAM), which significantly\nimproves the detection accuracy of small objects through parallel multi-scale\nconvolutions and feature fusion. Secondly, to mitigate the interference of\nbackground noise, we propose an Improved Efficient Multi-scale Attention Module\n(IEMA), which enhances the focus on target regions through feature grouping,\nparallel sub-networks, and cross-spatial learning. Thirdly, we introduce a\nDimension-Aware Selective Integration Module (DASI), which further enhances\nmulti-scale feature fusion capabilities by adaptively weighting and fusing\nlow-dimensional features and high-dimensional features. Finally, we conducted\nextensive performance evaluations of our proposed method on the VisDrone2019\ndataset. Compared to YOLOv11-s, MASFYOLO-s achieves improvements of 4.6% in\nmAP@0.5 and 3.5% in mAP@0.5:0.95 on the VisDrone2019 validation set.\nRemarkably, MASF-YOLO-s outperforms YOLOv11-m while requiring only\napproximately 60% of its parameters and 65% of its computational cost.\nFurthermore, comparative experiments with state-of-the-art detectors confirm\nthat MASF-YOLO-s maintains a clear competitive advantage in both detection\naccuracy and model efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "dimension"], "score": 3}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18152", "pdf": "https://arxiv.org/pdf/2504.18152", "abs": "https://arxiv.org/abs/2504.18152", "authors": ["Yi-Xing Peng", "Qize Yang", "Yu-Ming Tang", "Shenghao Fu", "Kun-Yu Lin", "Xihan Wei", "Wei-Shi Zheng"], "title": "ActionArt: Advancing Multimodal Large Models for Fine-Grained Human-Centric Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Fine-grained understanding of human actions and poses in videos is essential\nfor human-centric AI applications. In this work, we introduce ActionArt, a\nfine-grained video-caption dataset designed to advance research in\nhuman-centric multimodal understanding. Our dataset comprises thousands of\nvideos capturing a broad spectrum of human actions, human-object interactions,\nand diverse scenarios, each accompanied by detailed annotations that\nmeticulously label every limb movement. We develop eight sub-tasks to evaluate\nthe fine-grained understanding capabilities of existing large multimodal models\nacross different dimensions. Experimental results indicate that, while current\nlarge multimodal models perform commendably on various tasks, they often fall\nshort in achieving fine-grained understanding. We attribute this limitation to\nthe scarcity of meticulously annotated data, which is both costly and difficult\nto scale manually. Since manual annotations are costly and hard to scale, we\npropose proxy tasks to enhance the model perception ability in both spatial and\ntemporal dimensions. These proxy tasks are carefully crafted to be driven by\ndata automatically generated from existing MLLMs, thereby reducing the reliance\non costly manual labels. Experimental results show that the proposed proxy\ntasks significantly narrow the gap toward the performance achieved with\nmanually annotated fine-grained data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18158", "pdf": "https://arxiv.org/pdf/2504.18158", "abs": "https://arxiv.org/abs/2504.18158", "authors": ["Jiahao Zhang", "Bowen Wang", "Hong Liu", "Liangzhi Li", "Yuta Nakashima", "Hajime Nagahara"], "title": "E-InMeMo: Enhanced Prompting for Visual In-Context Learning", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Large-scale models trained on extensive datasets have become the standard due\nto their strong generalizability across diverse tasks. In-context learning\n(ICL), widely used in natural language processing, leverages these models by\nproviding task-specific prompts without modifying their parameters. This\nparadigm is increasingly being adapted for computer vision, where models\nreceive an input-output image pair, known as an in-context pair, alongside a\nquery image to illustrate the desired output. However, the success of visual\nICL largely hinges on the quality of these prompts. To address this, we propose\nEnhanced Instruct Me More (E-InMeMo), a novel approach that incorporates\nlearnable perturbations into in-context pairs to optimize prompting. Through\nextensive experiments on standard vision tasks, E-InMeMo demonstrates superior\nperformance over existing state-of-the-art methods. Notably, it improves mIoU\nscores by 7.99 for foreground segmentation and by 17.04 for single object\ndetection when compared to the baseline without learnable prompts. These\nresults highlight E-InMeMo as a lightweight yet effective strategy for\nenhancing visual ICL. Code is publicly available at:\nhttps://github.com/Jackieam/E-InMeMo", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18190", "pdf": "https://arxiv.org/pdf/2504.18190", "abs": "https://arxiv.org/abs/2504.18190", "authors": ["Brunó B. Englert", "Tommie Kerssies", "Gijs Dubbelman"], "title": "What is the Added Value of UDA in the VFM Era?", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised Domain Adaptation (UDA) can improve a perception model's\ngeneralization to an unlabeled target domain starting from a labeled source\ndomain. UDA using Vision Foundation Models (VFMs) with synthetic source data\ncan achieve generalization performance comparable to fully-supervised learning\nwith real target data. However, because VFMs have strong generalization from\ntheir pre-training, more straightforward, source-only fine-tuning can also\nperform well on the target. As data scenarios used in academic research are not\nnecessarily representative for real-world applications, it is currently unclear\n(a) how UDA behaves with more representative and diverse data and (b) if\nsource-only fine-tuning of VFMs can perform equally well in these scenarios.\nOur research aims to close these gaps and, similar to previous studies, we\nfocus on semantic segmentation as a representative perception task. We assess\nUDA for synth-to-real and real-to-real use cases with different source and\ntarget data combinations. We also investigate the effect of using a small\namount of labeled target data in UDA. We clarify that while these scenarios are\nmore realistic, they are not necessarily more challenging. Our results show\nthat, when using stronger synthetic source data, UDA's improvement over\nsource-only fine-tuning of VFMs reduces from +8 mIoU to +2 mIoU, and when using\nmore diverse real source data, UDA has no added value. However, UDA\ngeneralization is always higher in all synthetic data scenarios than\nsource-only fine-tuning and, when including only 1/16 of Cityscapes labels,\nsynthetic UDA obtains the same state-of-the-art segmentation quality of 85 mIoU\nas a fully-supervised model using all labels. Considering the mixed results, we\ndiscuss how UDA can best support robust autonomous driving at scale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18256", "pdf": "https://arxiv.org/pdf/2504.18256", "abs": "https://arxiv.org/abs/2504.18256", "authors": ["Elena Plekhanova", "Damien Robert", "Johannes Dollinger", "Emilia Arens", "Philipp Brun", "Jan Dirk Wegner", "Niklaus Zimmermann"], "title": "SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in Ecology", "categories": ["cs.CV"], "comment": "CVPR 2025, EarthVision workshop", "summary": "With the exacerbation of the biodiversity and climate crises, macroecological\npursuits such as global biodiversity mapping become more urgent. Remote sensing\noffers a wealth of Earth observation data for ecological studies, but the\nscarcity of labeled datasets remains a major challenge. Recently,\nself-supervised learning has enabled learning representations from unlabeled\ndata, triggering the development of pretrained geospatial models with\ngeneralizable features. However, these models are often trained on datasets\nbiased toward areas of high human activity, leaving entire ecological regions\nunderrepresented. Additionally, while some datasets attempt to address\nseasonality through multi-date imagery, they typically follow calendar seasons\nrather than local phenological cycles. To better capture vegetation seasonality\nat a global scale, we propose a simple phenology-informed sampling strategy and\nintroduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we\ntrain an existing model with a season-contrastive objective. We compare\nrepresentations learned from SSL4Eco against other datasets on diverse\necological downstream tasks and demonstrate that our straightforward sampling\nmethod consistently improves representation quality, highlighting the\nimportance of dataset construction. The model pretrained on SSL4Eco reaches\nstate of the art performance on 7 out of 8 downstream tasks spanning\n(multi-label) classification and regression. We release our code, data, and\nmodel weights to support macroecological and computer vision research at\nhttps://github.com/PlekhanovaElena/ssl4eco.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18325", "pdf": "https://arxiv.org/pdf/2504.18325", "abs": "https://arxiv.org/abs/2504.18325", "authors": ["Dongxin Lyu", "Han Huang", "Cheng Tan", "Zimu Li"], "title": "Depth3DLane: Monocular 3D Lane Detection via Depth Prior Distillation", "categories": ["cs.CV"], "comment": "Submitting to ICCV2025", "summary": "Monocular 3D lane detection is challenging due to the difficulty in capturing\ndepth information from single-camera images. A common strategy involves\ntransforming front-view (FV) images into bird's-eye-view (BEV) space through\ninverse perspective mapping (IPM), facilitating lane detection using BEV\nfeatures. However, IPM's flat-ground assumption and loss of contextual\ninformation lead to inaccuracies in reconstructing 3D information, especially\nheight. In this paper, we introduce a BEV-based framework to address these\nlimitations and improve 3D lane detection accuracy. Our approach incorporates a\nHierarchical Depth-Aware Head that provides multi-scale depth features,\nmitigating the flat-ground assumption by enhancing spatial awareness across\nvarying depths. Additionally, we leverage Depth Prior Distillation to transfer\nsemantic depth knowledge from a teacher model, capturing richer structural and\ncontextual information for complex lane structures. To further refine lane\ncontinuity and ensure smooth lane reconstruction, we introduce a Conditional\nRandom Field module that enforces spatial coherence in lane predictions.\nExtensive experiments validate that our method achieves state-of-the-art\nperformance in terms of z-axis error and outperforms other methods in the field\nin overall performance. The code is released at:\nhttps://anonymous.4open.science/r/Depth3DLane-DCDD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18361", "pdf": "https://arxiv.org/pdf/2504.18361", "abs": "https://arxiv.org/abs/2504.18361", "authors": ["Haozhen Yan", "Yan Hong", "Jiahui Zhan", "Yikun Ji", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Jianfu Zhang"], "title": "COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation Localization", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Recent advancements in image manipulation have achieved unprecedented\nprogress in generating photorealistic content, but also simultaneously\neliminating barriers to arbitrary manipulation and editing, raising concerns\nabout multimedia authenticity and cybersecurity. However, existing Image\nManipulation Detection and Localization (IMDL) methodologies predominantly\nfocus on splicing or copy-move forgeries, lacking dedicated benchmarks for\ninpainting-based manipulations. To bridge this gap, we present COCOInpaint, a\ncomprehensive benchmark specifically designed for inpainting detection, with\nthree key contributions: 1) High-quality inpainting samples generated by six\nstate-of-the-art inpainting models, 2) Diverse generation scenarios enabled by\nfour mask generation strategies with optional text guidance, and 3) Large-scale\ncoverage with 258,266 inpainted images with rich semantic diversity. Our\nbenchmark is constructed to emphasize intrinsic inconsistencies between\ninpainted and authentic regions, rather than superficial semantic artifacts\nsuch as object shapes. We establish a rigorous evaluation protocol using three\nstandard metrics to assess existing IMDL approaches. The dataset will be made\npublicly available to facilitate future research in this area.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.17865", "pdf": "https://arxiv.org/pdf/2504.17865", "abs": "https://arxiv.org/abs/2504.17865", "authors": ["Charles J. Carver", "Hadleigh Schwartz", "Toma Itagaki", "Zachary Englhardt", "Kechen Liu", "Megan Graciela Nauli Manik", "Chun-Cheng Chang", "Vikram Iyer", "Brian Plancher", "Xia Zhou"], "title": "Set Phasers to Stun: Beaming Power and Control to Mobile Robots with Laser Light", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 7 figures, submitted to IROS 2025", "summary": "We present Phaser, a flexible system that directs narrow-beam laser light to\nmoving robots for concurrent wireless power delivery and communication. We\ndesign a semi-automatic calibration procedure to enable fusion of\nstereo-vision-based 3D robot tracking with high-power beam steering, and a\nlow-power optical communication scheme that reuses the laser light as a data\nchannel. We fabricate a Phaser prototype using off-the-shelf hardware and\nevaluate its performance with battery-free autonomous robots. Phaser delivers\noptical power densities of over 110 mW/cm$^2$ and error-free data to mobile\nrobots at multi-meter ranges, with on-board decoding drawing 0.3 mA (97\\% less\ncurrent than Bluetooth Low Energy). We demonstrate Phaser fully powering\ngram-scale battery-free robots to nearly 2x higher speeds than prior work while\nsimultaneously controlling them to navigate around obstacles and along paths.\nCode, an open-source design guide, and a demonstration video of Phaser is\navailable at https://mobilex.cs.columbia.edu/phaser.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.17954", "pdf": "https://arxiv.org/pdf/2504.17954", "abs": "https://arxiv.org/abs/2504.17954", "authors": ["Kaiyuan Tang", "Siyuan Yao", "Chaoli Wang"], "title": "iVR-GS: Inverse Volume Rendering for Explorable Visualization via Editable 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted by IEEE Transactions on Visualization and Computer Graphics\n  (TVCG)", "summary": "In volume visualization, users can interactively explore the\nthree-dimensional data by specifying color and opacity mappings in the transfer\nfunction (TF) or adjusting lighting parameters, facilitating meaningful\ninterpretation of the underlying structure. However, rendering large-scale\nvolumes demands powerful GPUs and high-speed memory access for real-time\nperformance. While existing novel view synthesis (NVS) methods offer faster\nrendering speeds with lower hardware requirements, the visible parts of a\nreconstructed scene are fixed and constrained by preset TF settings,\nsignificantly limiting user exploration. This paper introduces inverse volume\nrendering via Gaussian splatting (iVR-GS), an innovative NVS method that\nreduces the rendering cost while enabling scene editing for interactive volume\nexploration. Specifically, we compose multiple iVR-GS models associated with\nbasic TFs covering disjoint visible parts to make the entire volumetric scene\nvisible. Each basic model contains a collection of 3D editable Gaussians, where\neach Gaussian is a 3D spatial point that supports real-time scene rendering and\nediting. We demonstrate the superior reconstruction quality and composability\nof iVR-GS against other NVS solutions (Plenoxels, CCNeRF, and base 3DGS) on\nvarious volume datasets. The code is available at\nhttps://github.com/TouKaienn/iVR-GS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-28.jsonl"}
{"id": "2504.18400", "pdf": "https://arxiv.org/pdf/2504.18400", "abs": "https://arxiv.org/abs/2504.18400", "authors": ["Yui Lo", "Yuqian Chen", "Dongnan Liu", "Leo Zekelman", "Jarrett Rushmore", "Yogesh Rathi", "Nikos Makris", "Alexandra J. Golby", "Fan Zhang", "Weidong Cai", "Lauren J. O'Donnell"], "title": "A Multimodal Deep Learning Approach for White Matter Shape Prediction in Diffusion MRI Tractography", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "21 pages, 3 figures, 6 tables", "summary": "Shape measures have emerged as promising descriptors of white matter\ntractography, offering complementary insights into anatomical variability and\nassociations with cognitive and clinical phenotypes. However, conventional\nmethods for computing shape measures are computationally expensive and\ntime-consuming for large-scale datasets due to reliance on voxel-based\nrepresentations. We propose Tract2Shape, a novel multimodal deep learning\nframework that leverages geometric (point cloud) and scalar (tabular) features\nto predict ten white matter tractography shape measures. To enhance model\nefficiency, we utilize a dimensionality reduction algorithm for the model to\npredict five primary shape components. The model is trained and evaluated on\ntwo independently acquired datasets, the HCP-YA dataset, and the PPMI dataset.\nWe evaluate the performance of Tract2Shape by training and testing it on the\nHCP-YA dataset and comparing the results with state-of-the-art models. To\nfurther assess its robustness and generalization ability, we also test\nTract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep\nlearning models across all ten shape measures, achieving the highest average\nPearson's r and the lowest nMSE on the HCP-YA dataset. The ablation study shows\nthat both multimodal input and PCA contribute to performance gains. On the\nunseen testing PPMI dataset, Tract2Shape maintains a high Pearson's r and low\nnMSE, demonstrating strong generalizability in cross-dataset evaluation.\nTract2Shape enables fast, accurate, and generalizable prediction of white\nmatter shape measures from tractography data, supporting scalable analysis\nacross datasets. This framework lays a promising foundation for future\nlarge-scale white matter shape analysis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-28.jsonl"}
