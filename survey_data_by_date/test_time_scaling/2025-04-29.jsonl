{"id": "2504.18992", "pdf": "https://arxiv.org/pdf/2504.18992", "abs": "https://arxiv.org/abs/2504.18992", "authors": ["Sanwoo Lee", "Jiahao Liu", "Qifan Wang", "Jingang Wang", "Xunliang Cai", "Yunfang Wu"], "title": "Dynamic Fisher-weighted Model Merging via Bayesian Optimization", "categories": ["cs.CL"], "comment": null, "summary": "The fine-tuning of pre-trained language models has resulted in the widespread\navailability of task-specific models. Model merging offers an efficient way to\ncreate multi-task models by combining these fine-tuned models at the parameter\nlevel, without the need for training data or joint training on multiple\ndatasets. Existing merging approaches typically involve scaling the parameters\nmodel-wise or integrating parameter importance parameter-wise. Both approaches\nexhibit their own weaknesses, leading to a notable performance gap compared to\nmulti-task fine-tuning. In this paper, we unify these seemingly distinct\nstrategies into a more general merging framework, and introduce Dynamic\nFisher-weighted Merging (DF-Merge). Specifically, candidate models are\nassociated with a set of coefficients that linearly scale their fine-tuned\nparameters. Bayesian optimization is applied to dynamically adjust these\ncoefficients, aiming to maximize overall performance on validation sets. Each\niteration of this process integrates parameter importance based on the Fisher\ninformation conditioned by the coefficients. Experimental results show that\nDF-Merge outperforms strong baselines across models of different sizes and a\nvariety of tasks. Our analysis shows that the effectiveness of DF-Merge arises\nfrom the unified view of merging and that near-optimal performance is\nachievable in a few iterations, even with minimal validation data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19811", "pdf": "https://arxiv.org/pdf/2504.19811", "abs": "https://arxiv.org/abs/2504.19811", "authors": ["Takuya Tamura", "Taro Yano", "Masafumi Enomoto", "Masafumi Oyamada"], "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance", "categories": ["cs.CL"], "comment": null, "summary": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships - i.e., which models are derived\nor merged from which parents. In this work, we propose a novel\nLineage-Regularized Matrix Factorization (LRMF) framework that encodes\nancestral ties among LLMs via a graph Laplacian regularizer. By leveraging\nmulti-hop parent-child connections, LRMF consistently outperforms conventional\nmatrix factorization and collaborative filtering methods in both instance-level\nand benchmark-level performance prediction. Our large-scale study includes\n2,934 publicly available Hugging Face models and 21,000+ instances across 6\nmajor benchmarks, showing that lineage constraints yield up to 7-10 percentage\npoints higher correlation with actual performance compared to baselines.\nMoreover, LRMF effectively addresses the cold-start problem, providing accurate\nestimates for newly derived or merged models even with minimal data. This\nlineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation"], "score": 2}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19898", "pdf": "https://arxiv.org/pdf/2504.19898", "abs": "https://arxiv.org/abs/2504.19898", "authors": ["Mingqian He", "Fei Zhao", "Chonggang Lu", "Ziyan Liu", "Yue Wang", "Haofu Qian"], "title": "GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets", "categories": ["cs.CL"], "comment": null, "summary": "As a fundamental task in machine learning, text classification plays a\ncrucial role in many areas. With the rapid scaling of Large Language Models\n(LLMs), particularly through reinforcement learning (RL), there is a growing\nneed for more capable discriminators. Consequently, advances in classification\nare becoming increasingly vital for enhancing the overall capabilities of LLMs.\nTraditional discriminative methods map text to labels but overlook LLMs'\nintrinsic generative strengths. Generative classification addresses this by\nprompting the model to directly output labels. However, existing studies still\nrely on simple SFT alone, seldom probing the interplay between training and\ninference prompts, and no work has systematically leveraged RL for generative\ntext classifiers and unified SFT, RL, and inference-time prompting in one\nframework. We bridge this gap with GenCLS++, a framework that jointly optimizes\nSFT and RL while systematically exploring five high-level strategy\ndimensions-in-context learning variants, category definitions, explicit\nuncertainty labels, semantically irrelevant numeric labels, and\nperplexity-based decoding-during both training and inference. After an SFT\n\"policy warm-up,\" we apply RL with a simple rule-based reward, yielding sizable\nextra gains. Across seven datasets, GenCLS++ achieves an average accuracy\nimprovement of 3.46% relative to the naive SFT baseline; on public datasets,\nthis improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that\nbenefit from explicit thinking processes, we find that classification tasks\nperform better without such reasoning steps. These insights into the role of\nexplicit reasoning provide valuable guidance for future LLM applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.20026", "pdf": "https://arxiv.org/pdf/2504.20026", "abs": "https://arxiv.org/abs/2504.20026", "authors": ["Zhengqin Li", "Dilin Wang", "Ka Chen", "Zhaoyang Lv", "Thu Nguyen-Phuoc", "Milim Lee", "Jia-Bin Huang", "Lei Xiao", "Cheng Zhang", "Yufeng Zhu", "Carl S. Marshall", "Yufeng Ren", "Richard Newcombe", "Zhao Dong"], "title": "LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "We present Large Inverse Rendering Model (LIRM), a transformer architecture\nthat jointly reconstructs high-quality shape, materials, and radiance fields\nwith view-dependent effects in less than a second. Our model builds upon the\nrecent Large Reconstruction Models (LRMs) that achieve state-of-the-art\nsparse-view reconstruction quality. However, existing LRMs struggle to\nreconstruct unseen parts accurately and cannot recover glossy appearance or\ngenerate relightable 3D contents that can be consumed by standard Graphics\nengines. To address these limitations, we make three key technical\ncontributions to build a more practical multi-view 3D reconstruction framework.\nFirst, we introduce an update model that allows us to progressively add more\ninput views to improve our reconstruction. Second, we propose a hexa-plane\nneural SDF representation to better recover detailed textures, geometry and\nmaterial parameters. Third, we develop a novel neural directional-embedding\nmechanism to handle view-dependent effects. Trained on a large-scale shape and\nmaterial dataset with a tailored coarse-to-fine training scheme, our model\nachieves compelling results. It compares favorably to optimization-based\ndense-view inverse rendering methods in terms of geometry and relighting\naccuracy, while requiring only a fraction of the inference time.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.18773", "pdf": "https://arxiv.org/pdf/2504.18773", "abs": "https://arxiv.org/abs/2504.18773", "authors": ["Zhiheng Tu", "Xinjian Huang", "Yong He", "Ruiyang Zhou", "Bo Du", "Weitao Wu"], "title": "Depth as Points: Center Point-based Depth Estimation", "categories": ["cs.CV"], "comment": "Depth Esitimation, Key-points, Virtual Datasets, Autonomous Driving", "summary": "The perception of vehicles and pedestrians in urban scenarios is crucial for\nautonomous driving. This process typically involves complicated data\ncollection, imposes high computational and hardware demands. To address these\nlimitations, we first develop a highly efficient method for generating virtual\ndatasets, which enables the creation of task- and scenario-specific datasets in\na short time. Leveraging this method, we construct the virtual depth estimation\ndataset VirDepth, a large-scale, multi-task autonomous driving dataset.\nSubsequently, we propose CenterDepth, a lightweight architecture for monocular\ndepth estimation that ensures high operational efficiency and exhibits superior\nperformance in depth estimation tasks with highly imbalanced height-scale\ndistributions. CenterDepth integrates global semantic information through the\ninnovative Center FC-CRFs algorithm, aggregates multi-scale features based on\nobject key points, and enables detection-based depth estimation of targets.\nExperiments demonstrate that our proposed method achieves superior performance\nin terms of both computational speed and prediction accuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.18762", "pdf": "https://arxiv.org/pdf/2504.18762", "abs": "https://arxiv.org/abs/2504.18762", "authors": ["Ojasw Upadhyay", "Abishek Saravankumar", "Ayman Ismail"], "title": "SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures, 4 tables", "summary": "Large Language Models (LLMs) are powerful but often require extensive\nfine-tuning and large datasets for specialized domains like law.\nGeneral-purpose pre-training may not capture legal nuances, and acquiring\nsufficient legal data is challenging. We introduce SynLexLM, a novel approach\nto efficiently pre-train a legal LLM. Our method employs curriculum learning,\nprogressing from simple to complex legal texts and queries, combined with\nsynthetic data augmentation using models like Gemini Pro to address data\nscarcity. We aim to achieve improved performance on legal benchmarks\n(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned\nversions. Preliminary work involves generating synthetic QA pairs reflecting\nlegal reasoning. This work aims to enhance legal document analysis and research\ntools, potentially democratizing access to advanced legal AI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19401", "pdf": "https://arxiv.org/pdf/2504.19401", "abs": "https://arxiv.org/abs/2504.19401", "authors": ["Shuo Wang", "Tong Ren", "Nan Cheng", "Li Zhang", "Rong Wang"], "title": "Innovative Integration of 4D Cardiovascular Reconstruction and Hologram: A New Visualization Tool for Coronary Artery Bypass Grafting Planning", "categories": ["physics.med-ph", "cs.CV", "cs.GR", "eess.IV", "J.3; I.3.8"], "comment": "35 pages, 9 figures", "summary": "Background: Coronary artery bypass grafting (CABG) planning requires advanced\nspatial visualization and consideration of coronary artery depth,\ncalcification, and pericardial adhesions. Objective: To develop and evaluate a\ndynamic cardiovascular holographic visualization tool for preoperative CABG\nplanning. Methods: Using 4D cardiac computed tomography angiography data from\n14 CABG candidates, we developed a semi-automated workflow for time-resolved\nsegmentation of cardiac structures, epicardial adipose tissue (EAT), and\ncoronary arteries with calcium scoring. The workflow incorporated methods for\ncardiac segmentation, coronary calcification quantification, visualization of\ncoronary depth within EAT, and pericardial adhesion assessment through motion\nanalysis. Dynamic cardiovascular holograms were displayed using the Looking\nGlass platform. Thirteen cardiac surgeons evaluated the tool using a Likert\nscale. Additionally, pericardial adhesion scores from holograms of 21 patients\n(including seven undergoing secondary cardiac surgeries) were compared with\nintraoperative findings. Results: Surgeons rated the visualization tool highly\nfor preoperative planning utility (mean Likert score: 4.57/5.0). Hologram-based\npericardial adhesion scoring strongly correlated with intraoperative findings\n(r=0.786, P<0.001). Conclusion: This study establishes a visualization\nframework for CABG planning that produces clinically relevant dynamic holograms\nfrom patient-specific data, with clinical feedback confirming its effectiveness\nfor preoperative planning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.18838", "pdf": "https://arxiv.org/pdf/2504.18838", "abs": "https://arxiv.org/abs/2504.18838", "authors": ["Yixin Cao", "Shibo Hong", "Xinze Li", "Jiahao Ying", "Yubo Ma", "Haiyuan Liang", "Yantao Liu", "Zijun Yao", "Xiaozhi Wang", "Dan Huang", "Wenxuan Zhang", "Lifu Huang", "Muhao Chen", "Lei Hou", "Qianru Sun", "Xingjun Ma", "Zuxuan Wu", "Min-Yen Kan", "David Lo", "Qi Zhang", "Heng Ji", "Jing Jiang", "Juanzi Li", "Aixin Sun", "Xuanjing Huang", "Tat-Seng Chua", "Yu-Gang Jiang"], "title": "Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are advancing at an amazing speed and have\nbecome indispensable across academia, industry, and daily applications. To keep\npace with the status quo, this survey probes the core challenges that the rise\nof LLMs poses for evaluation. We identify and analyze two pivotal transitions:\n(i) from task-specific to capability-based evaluation, which reorganizes\nbenchmarks around core competencies such as knowledge, reasoning, instruction\nfollowing, multi-modal understanding, and safety; and (ii) from manual to\nautomated evaluation, encompassing dynamic dataset curation and\n\"LLM-as-a-judge\" scoring.\n  Yet, even with these transitions, a crucial obstacle persists: the evaluation\ngeneralization issue. Bounded test sets cannot scale alongside models whose\nabilities grow seemingly without limit. We will dissect this issue, along with\nthe core challenges of the above two transitions, from the perspectives of\nmethods, datasets, evaluators, and metrics. Due to the fast evolving of this\nfield, we will maintain a living GitHub repository (links are in each section)\nto crowd-source updates and corrections, and warmly invite contributors and\ncollaborators.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety"], "score": 3}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.18800", "pdf": "https://arxiv.org/pdf/2504.18800", "abs": "https://arxiv.org/abs/2504.18800", "authors": ["Ryo Takizawa", "Satoshi Kodera", "Tempei Kabayama", "Ryo Matsuoka", "Yuta Ando", "Yuto Nakamura", "Haruki Settai", "Norihiko Takeda"], "title": "Video CLIP Model for Multi-View Echocardiography Interpretation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Echocardiography involves recording videos of the heart using ultrasound,\nenabling clinicians to evaluate its condition. Recent advances in large-scale\nvision-language models (VLMs) have garnered attention for automating the\ninterpretation of echocardiographic videos. However, most existing VLMs\nproposed for medical interpretation thus far rely on single-frame (i.e., image)\ninputs. Consequently, these image-based models often exhibit lower diagnostic\naccuracy for conditions identifiable through cardiac motion. Moreover,\nechocardiographic videos are recorded from various views that depend on the\ndirection of ultrasound emission, and certain views are more suitable than\nothers for interpreting specific conditions. Incorporating multiple views could\npotentially yield further improvements in accuracy. In this study, we developed\na video-language model that takes five different views and full video sequences\nas input, training it on pairs of echocardiographic videos and clinical reports\nfrom 60,747 cases. Our experiments demonstrate that this expanded approach\nachieves higher interpretation accuracy than models trained with only\nsingle-view videos or with still images.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.18857", "pdf": "https://arxiv.org/pdf/2504.18857", "abs": "https://arxiv.org/abs/2504.18857", "authors": ["Yi Lu", "Wanxu Zhao", "Xin Zhou", "Chenxin An", "Chenglong Wang", "Shuo Li", "Yuming Yang", "Jun Zhao", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle to process and generate coherent\ncontext when the number of input tokens exceeds the pre-trained length. Recent\nadvancements in long-context extension have significantly expanded the context\nwindow of LLMs but require expensive overhead to train the large-scale models\nwith longer context. In this work, we propose Dimension-Wise Positional\nEmbeddings Manipulation (DPE), a training-free framework to extrapolate the\ncontext window of LLMs by diving into RoPE's different hidden dimensions.\nInstead of manipulating all dimensions equally, DPE detects the effective\nlength for every dimension and finds the key dimensions for context extension.\nWe reuse the original position indices with their embeddings from the\npre-trained model and manipulate the key dimensions' position indices to their\nmost effective lengths. In this way, DPE adjusts the pre-trained models with\nminimal modifications while ensuring that each dimension reaches its optimal\nstate for extrapolation. DPE significantly surpasses well-known baselines such\nas YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of\n128k tokens without continual training and integrates seamlessly with Flash\nAttention 2. In addition to its impressive extrapolation capability, DPE also\ndramatically improves the models' performance within training length, such as\nLlama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When\ncompared with commercial models, Llama 3.1 70B with DPE even achieves better\nperformance than GPT-4-128K.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.18938", "pdf": "https://arxiv.org/pdf/2504.18938", "abs": "https://arxiv.org/abs/2504.18938", "authors": ["Junhong Liang", "Yu Zhou"], "title": "MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction", "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens\nin sentences. While Large Language Models (LLMs) have shown remarkable success\nin identifying and rectifying potential errors, they often struggle with\nmaintaining consistent output lengths and adapting to domain-specific\ncorrections. Furthermore, existing CSC task impose rigid constraints requiring\ninput and output lengths to be identical, limiting their applicability. In this\nwork, we extend traditional CSC to variable-length correction scenarios,\nincluding Chinese Splitting Error Correction (CSEC) and ASR N-best Error\nCorrection. To address domain adaptation and length consistency, we propose\nMTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection\nmechanism. Our approach constructs a retrieval database from domain-specific\ntraining data and dictionaries, fine-tuning retrievers to optimize performance\nfor error-containing inputs. Additionally, we introduce a multi-source\ncombination strategy with iterative length reflection to ensure output length\nfidelity. Experiments across diverse domain datasets demonstrate that our\nmethod significantly outperforms current approaches in correction quality,\nparticularly in handling domain-specific and variable-length error correction\ntasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19095", "pdf": "https://arxiv.org/pdf/2504.19095", "abs": "https://arxiv.org/abs/2504.19095", "authors": ["Jikai Wang", "Juntao Li", "Lijun Wu", "Min Zhang"], "title": "Efficient Reasoning for LLMs through Speculative Chain-of-Thought", "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have\nrecently attracted widespread attention due to their impressive task-solving\nabilities. However, the enormous model size and the generation of lengthy\nthought chains introduce significant reasoning costs and response latency.\nExisting methods for efficient reasoning mainly focus on reducing the number of\nmodel parameters or shortening the chain-of-thought length. In this paper, we\nintroduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency\nfrom another perspective by accelerated average reasoning speed through large\nand small model collaboration. SCoT conducts thought-level drafting using a\nlightweight draft model. Then it selects the best CoT draft and corrects the\nerror cases with the target model. The proposed thinking behavior alignment\nimproves the efficiency of drafting and the draft selection strategy maintains\nthe prediction accuracy for complex problems. Experimental results on GSM8K,\nMATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces\nreasoning latency by 48\\%$\\sim$66\\% for Deepseek-R1-Distill-Qwen-32B while\nachieving near-target-model-level performance. Our code is available at\nhttps://github.com/Jikai0Wang/Speculative_CoT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19080", "pdf": "https://arxiv.org/pdf/2504.19080", "abs": "https://arxiv.org/abs/2504.19080", "authors": ["Zhenkai Qin", "Jiaquan Liang", "Qiao Fang"], "title": "MIA-Mind: A Multidimensional Interactive Attention Mechanism Based on MindSpore", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Attention mechanisms have significantly advanced deep learning by enhancing\nfeature representation through selective focus. However, existing approaches\noften independently model channel importance and spatial saliency, overlooking\ntheir inherent interdependence and limiting their effectiveness. To address\nthis limitation, we propose MIA-Mind, a lightweight and modular\nMultidimensional Interactive Attention Mechanism, built upon the MindSpore\nframework. MIA-Mind jointly models spatial and channel features through a\nunified cross-attentive fusion strategy, enabling fine-grained feature\nrecalibration with minimal computational overhead. Extensive experiments are\nconducted on three representative datasets: on CIFAR-10, MIA-Mind achieves an\naccuracy of 82.9\\%; on ISBI2012, it achieves an accuracy of 78.7\\%; and on\nCIC-IDS2017, it achieves an accuracy of 91.9\\%. These results validate the\nversatility, lightweight design, and generalization ability of MIA-Mind across\nheterogeneous tasks. Future work will explore the extension of MIA-Mind to\nlarge-scale datasets, the development of ada,ptive attention fusion strategies,\nand distributed deployment to further enhance scalability and robustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19110", "pdf": "https://arxiv.org/pdf/2504.19110", "abs": "https://arxiv.org/abs/2504.19110", "authors": ["Huajian Xin", "Luming Li", "Xiaoran Jin", "Jacques Fleuriot", "Wenda Li"], "title": "APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries", "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has shown promise in formal\ntheorem proving, yet existing benchmarks remain limited to isolated, static\nproof tasks, failing to capture the iterative, engineering-intensive workflows\nof real-world formal mathematics libraries. Motivated by analogous advances in\nsoftware engineering, we introduce the paradigm of Automated Proof Engineering\n(APE), which aims to automate proof engineering tasks such as feature addition,\nproof refactoring, and bug fixing using LLMs. To facilitate research in this\ndirection, we present APE-Bench I, the first realistic benchmark built from\nreal-world commit histories of Mathlib4, featuring diverse file-level tasks\ndescribed in natural language and verified via a hybrid approach combining the\nLean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable\nparallel verification infrastructure optimized for proof checking across\nmultiple versions of Mathlib. Empirical results on state-of-the-art LLMs\ndemonstrate strong performance on localized edits but substantial degradation\non handling complex proof engineering. This work lays the foundation for\ndeveloping agentic workflows in proof engineering, with future benchmarks\ntargeting multi-file coordination, project-scale verification, and autonomous\nagents capable of planning, editing, and repairing formal libraries.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19086", "pdf": "https://arxiv.org/pdf/2504.19086", "abs": "https://arxiv.org/abs/2504.19086", "authors": ["Xiaoran Xu", "Jiangang Yang", "Wenyue Chong", "Wenhui Shi", "Shichu Sun", "Jing Xing", "Jian Liu"], "title": "Boosting Single-domain Generalized Object Detection via Vision-Language Knowledge Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Single-Domain Generalized Object Detection~(S-DGOD) aims to train an object\ndetector on a single source domain while generalizing well to diverse unseen\ntarget domains, making it suitable for multimedia applications that involve\nvarious domain shifts, such as intelligent video surveillance and VR/AR\ntechnologies. With the success of large-scale Vision-Language Models, recent\nS-DGOD approaches exploit pre-trained vision-language knowledge to guide\ninvariant feature learning across visual domains. However, the utilized\nknowledge remains at a coarse-grained level~(e.g., the textual description of\nadverse weather paired with the image) and serves as an implicit regularization\nfor guidance, struggling to learn accurate region- and object-level features in\nvarying domains. In this work, we propose a new cross-modal feature learning\nmethod, which can capture generalized and discriminative regional features for\nS-DGOD tasks. The core of our method is the mechanism of Cross-modal and\nRegion-aware Feature Interaction, which simultaneously learns both inter-modal\nand intra-modal regional invariance through dynamic interactions between\nfine-grained textual and visual features. Moreover, we design a simple but\neffective strategy called Cross-domain Proposal Refining and Mixing, which\naligns the position of region proposals across multiple domains and diversifies\nthem, enhancing the localization ability of detectors in unseen scenarios. Our\nmethod achieves new state-of-the-art results on S-DGOD benchmark datasets, with\nimprovements of +8.8\\%~mPC on Cityscapes-C and +7.9\\%~mPC on DWD over\nbaselines, demonstrating its efficacy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19162", "pdf": "https://arxiv.org/pdf/2504.19162", "abs": "https://arxiv.org/abs/2504.19162", "authors": ["Jiaqi Chen", "Bang Zhang", "Ruotian Ma", "Peisong Wang", "Xiaodan Liang", "Zhaopeng Tu", "Xiaolong Li", "Kwan-Yee K. Wong"], "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project: https://chen-judge.github.io/SPC/", "summary": "Evaluating the step-by-step reliability of large language model (LLM)\nreasoning, such as Chain-of-Thought, remains challenging due to the difficulty\nand cost of obtaining high-quality step-level supervision. In this paper, we\nintroduce Self-Play Critic (SPC), a novel approach where a critic model evolves\nits ability to assess reasoning steps through adversarial self-play games,\neliminating the need for manual step-level annotation. SPC involves fine-tuning\ntwo copies of a base model to play two roles, namely a \"sneaky generator\" that\ndeliberately produces erroneous steps designed to be difficult to detect, and a\n\"critic\" that analyzes the correctness of reasoning steps. These two models\nengage in an adversarial game in which the generator aims to fool the critic,\nwhile the critic model seeks to identify the generator's errors. Using\nreinforcement learning based on the game outcomes, the models iteratively\nimprove; the winner of each confrontation receives a positive reward and the\nloser receives a negative reward, driving continuous self-evolution.\nExperiments on three reasoning process benchmarks (ProcessBench, PRM800K,\nDeltaBench) demonstrate that our SPC progressively enhances its error detection\ncapabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and\nsurpasses strong baselines, including distilled R1 model. Furthermore, applying\nSPC to guide the test-time search of diverse LLMs significantly improves their\nmathematical reasoning performance on MATH500 and AIME2024, outperforming\nstate-of-the-art process reward models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "reliability", "accuracy", "mathematical reasoning"], "score": 4}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19127", "pdf": "https://arxiv.org/pdf/2504.19127", "abs": "https://arxiv.org/abs/2504.19127", "authors": ["Jialang Lu", "Huayu Zhao", "Huiyu Zhai", "Xingxing Yang", "Shini Han"], "title": "DeepSPG: Exploring Deep Semantic Prior Guidance for Low-light Image Enhancement with Multimodal Learning", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by ICMR 2025 Main track. Code is available at\n  https://github.com/Wenyuzhy/DeepSPG", "summary": "There has long been a belief that high-level semantics learning can benefit\nvarious downstream computer vision tasks. However, in the low-light image\nenhancement (LLIE) community, existing methods learn a brutal mapping between\nlow-light and normal-light domains without considering the semantic information\nof different regions, especially in those extremely dark regions that suffer\nfrom severe information loss. To address this issue, we propose a new deep\nsemantic prior-guided framework (DeepSPG) based on Retinex image decomposition\nfor LLIE to explore informative semantic knowledge via a pre-trained semantic\nsegmentation model and multimodal learning. Notably, we incorporate both\nimage-level semantic prior and text-level semantic prior and thus formulate a\nmultimodal learning framework with combinatorial deep semantic prior guidance\nfor LLIE. Specifically, we incorporate semantic knowledge to guide the\nenhancement process via three designs: an image-level semantic prior guidance\nby leveraging hierarchical semantic features from a pre-trained semantic\nsegmentation model; a text-level semantic prior guidance by integrating natural\nlanguage semantic constraints via a pre-trained vision-language model; a\nmulti-scale semantic-aware structure that facilitates effective semantic\nfeature incorporation. Eventually, our proposed DeepSPG demonstrates superior\nperformance compared to state-of-the-art methods across five benchmark\ndatasets. The implementation details and code are publicly available at\nhttps://github.com/Wenyuzhy/DeepSPG.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19136", "pdf": "https://arxiv.org/pdf/2504.19136", "abs": "https://arxiv.org/abs/2504.19136", "authors": ["Huiling Zheng", "Xian Zhong", "Bin Liu", "Yi Xiao", "Bihan Wen", "Xiaofeng Li"], "title": "PAD: Phase-Amplitude Decoupling Fusion for Multi-Modal Land Cover Classification", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "13 pages, 8 figures", "summary": "The fusion of Synthetic Aperture Radar (SAR) and RGB imagery for land cover\nclassification remains challenging due to modality heterogeneity and the\nunderutilization of spectral complementarity. Existing methods often fail to\ndecouple shared structural features from modality-specific radiometric\nattributes, leading to feature conflicts and information loss. To address this\nissue, we propose Phase-Amplitude Decoupling (PAD), a frequency-aware framework\nthat separates phase (modality-shared) and amplitude (modality-specific)\ncomponents in the Fourier domain. Specifically, PAD consists of two key\ncomponents: 1) Phase Spectrum Correction (PSC), which aligns cross-modal phase\nfeatures through convolution-guided scaling to enhance geometric consistency,\nand 2) Amplitude Spectrum Fusion (ASF), which dynamically integrates\nhigh-frequency details and low-frequency structures using frequency-adaptive\nmultilayer perceptrons. This approach leverages SAR's sensitivity to\nmorphological features and RGB's spectral richness. Extensive experiments on\nWHU-OPT-SAR and DDHR-SK datasets demonstrate state-of-the-art performance. Our\nwork establishes a new paradigm for physics-aware multi-modal fusion in remote\nsensing. The code will be available at https://github.com/RanFeng2/PAD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19267", "pdf": "https://arxiv.org/pdf/2504.19267", "abs": "https://arxiv.org/abs/2504.19267", "authors": ["Mohamed Gado", "Towhid Taliee", "Muhammad Memon", "Dmitry Ignatov", "Radu Timofte"], "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Visual storytelling is an interdisciplinary field combining computer vision\nand natural language processing to generate cohesive narratives from sequences\nof images. This paper presents a novel approach that leverages recent\nadvancements in multimodal models, specifically adapting transformer-based\narchitectures and large multimodal models, for the visual storytelling task.\nLeveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT\nmodel produces visually grounded, contextually appropriate narratives. We\naddress the limitations of traditional evaluation metrics, such as BLEU,\nMETEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we\nutilize RoViST and GROOVIST, novel reference-free metrics designed to assess\nvisual storytelling, focusing on visual grounding, coherence, and\nnon-redundancy. These metrics provide a more nuanced evaluation of narrative\nquality, aligning closely with human judgment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19161", "pdf": "https://arxiv.org/pdf/2504.19161", "abs": "https://arxiv.org/abs/2504.19161", "authors": ["Zheng Fang", "Kangjun Liu", "Ke Chen", "Qingyu Liu", "Jianguo Zhang", "Lingyang Song", "Yaowei Wang"], "title": "RadioFormer: A Multiple-Granularity Radio Map Estimation Transformer with 1\\textpertenthousand Spatial Sampling", "categories": ["cs.CV"], "comment": null, "summary": "The task of radio map estimation aims to generate a dense representation of\nelectromagnetic spectrum quantities, such as the received signal strength at\neach grid point within a geographic region, based on measurements from a subset\nof spatially distributed nodes (represented as pixels). Recently, deep vision\nmodels such as the U-Net have been adapted to radio map estimation, whose\neffectiveness can be guaranteed with sufficient spatial observations (typically\n0.01% to 1% of pixels) in each map, to model local dependency of observed\nsignal power. However, such a setting of sufficient measurements can be less\npractical in real-world scenarios, where extreme sparsity in spatial sampling\ncan be widely encountered. To address this challenge, we propose RadioFormer, a\nnovel multiple-granularity transformer designed to handle the constraints posed\nby spatial sparse observations. Our RadioFormer, through a dual-stream\nself-attention (DSA) module, can respectively discover the correlation of\npixel-wise observed signal power and also learn patch-wise buildings'\ngeometries in a style of multiple granularities, which are integrated into\nmulti-scale representations of radio maps by a cross stream cross-attention\n(CCA) module. Extensive experiments on the public RadioMapSeer dataset\ndemonstrate that RadioFormer outperforms state-of-the-art methods in radio map\nestimation while maintaining the lowest computational cost. Furthermore, the\nproposed approach exhibits exceptional generalization capabilities and robust\nzero-shot performance, underscoring its potential to advance radio map\nestimation in a more practical setting with very limited observation nodes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation"], "score": 2}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19198", "pdf": "https://arxiv.org/pdf/2504.19198", "abs": "https://arxiv.org/abs/2504.19198", "authors": ["Lingtao Peng", "Liheng Bian"], "title": "Adaptive Dual-domain Learning for Underwater Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by AAAI 2025", "summary": "Recently, learning-based Underwater Image Enhancement (UIE) methods have\ndemonstrated promising performance. However, existing learning-based methods\nstill face two challenges. 1) They rarely consider the inconsistent degradation\nlevels in different spatial regions and spectral bands simultaneously. 2) They\ntreat all regions equally, ignoring that the regions with high-frequency\ndetails are more difficult to reconstruct. To address these challenges, we\npropose a novel UIE method based on spatial-spectral dual-domain adaptive\nlearning, termed SS-UIE. Specifically, we first introduce a spatial-wise\nMulti-scale Cycle Selective Scan (MCSS) module and a Spectral-Wise\nSelf-Attention (SWSA) module, both with linear complexity, and combine them in\nparallel to form a basic Spatial-Spectral block (SS-block). Benefiting from the\nglobal receptive field of MCSS and SWSA, SS-block can effectively model the\ndegradation levels of different spatial regions and spectral bands, thereby\nenabling degradation level-based dual-domain adaptive UIE. By stacking multiple\nSS-blocks, we build our SS-UIE network. Additionally, a Frequency-Wise Loss\n(FWL) is introduced to narrow the frequency-wise discrepancy and reinforce the\nmodel's attention on the regions with high-frequency details. Extensive\nexperiments validate that the SS-UIE technique outperforms state-of-the-art UIE\nmethods while requiring cheaper computational and memory costs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19395", "pdf": "https://arxiv.org/pdf/2504.19395", "abs": "https://arxiv.org/abs/2504.19395", "authors": ["Zhouxiang Fang", "Aayush Mishra", "Muhan Gao", "Anqi Liu", "Daniel Khashabi"], "title": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via Substitution Ciphers", "categories": ["cs.CL"], "comment": null, "summary": "Recent works have suggested that In-Context Learning (ICL) operates in dual\nmodes, i.e. task retrieval (remember learned patterns from pre-training) and\ntask learning (inference-time ``learning'' from demonstrations). However,\ndisentangling these the two modes remains a challenging goal. We introduce ICL\nCIPHERS, a class of task reformulations based on substitution ciphers borrowed\nfrom classic cryptography. In this approach, a subset of tokens in the\nin-context inputs are substituted with other (irrelevant) tokens, rendering\nEnglish sentences less comprehensible to human eye. However, by design, there\nis a latent, fixed pattern to this substitution, making it reversible. This\nbijective (reversible) cipher ensures that the task remains a well-defined task\nin some abstract sense, despite the transformations. It is a curious question\nif LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires\ndeciphering the latent cipher. We show that LLMs are better at solving ICL\nCIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,\nproviding a novel approach to quantify ``learning'' in ICL. While this gap is\nsmall, it is consistent across the board on four datasets and six models.\nFinally, we examine LLMs' internal representations and identify evidence in\ntheir ability to decode the ciphered inputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19223", "pdf": "https://arxiv.org/pdf/2504.19223", "abs": "https://arxiv.org/abs/2504.19223", "authors": ["Alexander Baumann", "Leonardo Ayala", "Silvia Seidlitz", "Jan Sellner", "Alexander Studier-Fischer", "Berkin Ã–zdemir", "Lena Maier-Hein", "Slobodan Ilic"], "title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Spectral imaging offers promising applications across diverse domains,\nincluding medicine and urban scene understanding, and is already established as\na critical modality in remote sensing. However, variability in channel\ndimensionality and captured wavelengths among spectral cameras impede the\ndevelopment of AI-driven methodologies, leading to camera-specific models with\nlimited generalizability and inadequate cross-camera applicability. To address\nthis bottleneck, we introduce $\\textbf{CARL}$, a model for\n$\\textbf{C}$amera-$\\textbf{A}$gnostic $\\textbf{R}$epresentation\n$\\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging\nmodalities. To enable the conversion of a spectral image with any channel\ndimensionality to a camera-agnostic embedding, we introduce wavelength\npositional encoding and a self-attention-cross-attention mechanism to compress\nspectral information into learned query representations. Spectral-spatial\npre-training is achieved with a novel spectral self-supervised JEPA-inspired\nstrategy tailored to CARL. Large-scale experiments across the domains of\nmedical imaging, autonomous driving, and satellite imaging demonstrate our\nmodel's unique robustness to spectral heterogeneity, outperforming on datasets\nwith simulated and real-world cross-camera spectral variations. The scalability\nand versatility of the proposed approach position our model as a backbone for\nfuture spectral foundation models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19258", "pdf": "https://arxiv.org/pdf/2504.19258", "abs": "https://arxiv.org/abs/2504.19258", "authors": ["Shuhao Kang", "Martin Y. Liao", "Yan Xia", "Olaf Wysocki", "Boris Jutzi", "Daniel Cremers"], "title": "OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion", "categories": ["cs.CV", "cs.RO"], "comment": "Technical report. 15 pages, 9 figures", "summary": "LiDAR place recognition is a critical capability for autonomous navigation\nand cross-modal localization in large-scale outdoor environments. Existing\napproaches predominantly depend on pre-built 3D dense maps or aerial imagery,\nwhich impose significant storage overhead and lack real-time adaptability. In\nthis paper, we propose OPAL, a novel network for LiDAR place recognition that\nleverages OpenStreetMap as a lightweight and up-to-date prior. Our key\ninnovation lies in bridging the domain disparity between sparse LiDAR scans and\nstructured OSM data through two carefully designed components: a cross-modal\nvisibility mask that identifies maximal observable regions from both modalities\nto guide feature learning, and an adaptive radial fusion module that\ndynamically consolidates multiscale radial features into discriminative global\ndescriptors. Extensive experiments on the augmented KITTI and KITTI-360\ndatasets demonstrate OPAL's superiority, achieving 15.98% higher recall at @1m\nthreshold for top-1 retrieved matches while operating at 12x faster inference\nspeeds compared to state-of-the-art approaches. Code and datasets are publicly\navailable at: https://github.com/WHU-USI3DV/OPAL .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19565", "pdf": "https://arxiv.org/pdf/2504.19565", "abs": "https://arxiv.org/abs/2504.19565", "authors": ["Meng Xiao", "Xunxin Cai", "Chengrui Wang", "Yuanchun Zhou"], "title": "m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training", "categories": ["cs.CL", "cs.AI", "q-bio.QM"], "comment": "22 pages, Large Language Model, Agentic AI, Dataset Distillation,\n  Multi-agent Collaboration", "summary": "The rapid progress of large language models (LLMs) in biomedical research has\nunderscored the limitations of existing open-source annotated scientific\ncorpora, which are often insufficient in quantity and quality. Addressing the\nchallenge posed by the complex hierarchy of biomedical knowledge, we propose a\nknowledge-driven, multi-agent framework for scientific corpus distillation\ntailored for LLM training in the biomedical domain. Central to our approach is\na collaborative multi-agent architecture, where specialized agents, each guided\nby the Medical Subject Headings (MeSH) hierarchy, work in concert to\nautonomously extract, synthesize, and self-evaluate high-quality textual data\nfrom vast scientific literature. These agents collectively generate and refine\ndomain-specific question-answer pairs, ensuring comprehensive coverage and\nconsistency with biomedical ontologies while minimizing manual involvement.\nExtensive experimental results show that language models trained on our\nmulti-agent distilled datasets achieve notable improvements in biomedical\nquestion-answering tasks, outperforming both strong life sciences LLM baselines\nand advanced proprietary models. Notably, our AI-Ready dataset enables\nLlama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger\nscale. Detailed ablation studies and case analyses further validate the\neffectiveness and synergy of each agent within the framework, highlighting the\npotential of multi-agent collaboration in biomedical LLM training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19300", "pdf": "https://arxiv.org/pdf/2504.19300", "abs": "https://arxiv.org/abs/2504.19300", "authors": ["Ni Yao", "Xiangyu Liu", "Danyang Sun", "Chuang Han", "Yanting Li", "Jiaofen Nan", "Chengyang Li", "Fubao Zhu", "Weihua Zhou", "Chen Zhao"], "title": "Myocardial Region-guided Feature Aggregation Net for Automatic Coronary artery Segmentation and Stenosis Assessment using Coronary Computed Tomography Angiography", "categories": ["cs.CV"], "comment": "31 pages, 12 figures", "summary": "Coronary artery disease (CAD) remains a leading cause of mortality worldwide,\nrequiring accurate segmentation and stenosis detection using Coronary Computed\nTomography angiography (CCTA). Existing methods struggle with challenges such\nas low contrast, morphological variability and small vessel segmentation. To\naddress these limitations, we propose the Myocardial Region-guided Feature\nAggregation Net, a novel U-shaped dual-encoder architecture that integrates\nanatomical prior knowledge to enhance robustness in coronary artery\nsegmentation. Our framework incorporates three key innovations: (1) a\nMyocardial Region-guided Module that directs attention to coronary regions via\nmyocardial contour expansion and multi-scale feature fusion, (2) a Residual\nFeature Extraction Encoding Module that combines parallel spatial channel\nattention with residual blocks to enhance local-global feature discrimination,\nand (3) a Multi-scale Feature Fusion Module for adaptive aggregation of\nhierarchical vascular features. Additionally, Monte Carlo dropout f quantifies\nprediction uncertainty, supporting clinical interpretability. For stenosis\ndetection, a morphology-based centerline extraction algorithm separates the\nvascular tree into anatomical branches, enabling cross-sectional area\nquantification and stenosis grading. The superiority of MGFA-Net was\ndemonstrated by achieving an Dice score of 85.04%, an accuracy of 84.24%, an\nHD95 of 6.1294 mm, and an improvement of 5.46% in true positive rate for\nstenosis detection compared to3D U-Net. The integrated segmentation-to-stenosis\npipeline provides automated, clinically interpretable CAD assessment, bridging\ndeep learning with anatomical prior knowledge for precision medicine. Our code\nis publicly available at http://github.com/chenzhao2023/MGFA_CCTA", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19327", "pdf": "https://arxiv.org/pdf/2504.19327", "abs": "https://arxiv.org/abs/2504.19327", "authors": ["Moulik Choraria", "Xinbo Wu", "Akhil Bhimaraju", "Nitesh Sekhar", "Yue Wu", "Xu Zhang", "Prateek Singhal", "Lav R. Varshney"], "title": "Platonic Grounding for Efficient Multimodal Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The hyperscaling of data and parameter count in Transformer-based models is\nyielding diminishing performance improvement, especially when weighed against\ntraining costs. Such plateauing indicates the importance of methods for more\nefficient finetuning and inference, while retaining similar performance. This\nis especially relevant for multimodal learning paradigms, where inference costs\nof processing multimodal tokens can determine the model's practical viability.\nAt the same time, research on representations and mechanistic interpretability\nhas improved our understanding of the inner workings of Transformer-based\nmodels; one such line of work reveals an implicit alignment in the deeper\nlayers of pretrained models, across modalities. Taking inspiration from this,\nwe motivate and propose a simple modification to existing multimodal frameworks\nthat rely on aligning pretrained models. We demonstrate that our approach\nmaintains and, in some cases, even improves performance of baseline methods\nwhile achieving significant gains in both training and inference-time compute.\nOur work also has implications for combining pretrained models into larger\nsystems efficiently.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19347", "pdf": "https://arxiv.org/pdf/2504.19347", "abs": "https://arxiv.org/abs/2504.19347", "authors": ["Rayson Laroca", "Marcelo dos Santos", "David Menotti"], "title": "Improving Small Drone Detection Through Multi-Scale Processing and Data Augmentation", "categories": ["cs.CV"], "comment": "Accepted for presentation at the International Joint Conference on\n  Neural Networks (IJCNN) 2025", "summary": "Detecting small drones, often indistinguishable from birds, is crucial for\nmodern surveillance. This work introduces a drone detection methodology built\nupon the medium-sized YOLOv11 object detection model. To enhance its\nperformance on small targets, we implemented a multi-scale approach in which\nthe input image is processed both as a whole and in segmented parts, with\nsubsequent prediction aggregation. We also utilized a copy-paste data\naugmentation technique to enrich the training dataset with diverse drone and\nbird examples. Finally, we implemented a post-processing technique that\nleverages frame-to-frame consistency to mitigate missed detections. The\nproposed approach attained a top-3 ranking in the 8th WOSDETC Drone-vsBird\nDetection Grand Challenge, held at the 2025 International Joint Conference on\nNeural Networks (IJCNN), showcasing its capability to detect drones in complex\nenvironments effectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19432", "pdf": "https://arxiv.org/pdf/2504.19432", "abs": "https://arxiv.org/abs/2504.19432", "authors": ["Zhe Dong", "Yuzhe Sun", "Tianzhu Liu", "Wangmeng Zuo", "Yanfeng Gu"], "title": "EarthMapper: Visual Autoregressive Models for Controllable Bidirectional Satellite-Map Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Satellite imagery and maps, as two fundamental data modalities in remote\nsensing, offer direct observations of the Earth's surface and\nhuman-interpretable geographic abstractions, respectively. The task of\nbidirectional translation between satellite images and maps (BSMT) holds\nsignificant potential for applications in urban planning and disaster response.\nHowever, this task presents two major challenges: first, the absence of precise\npixel-wise alignment between the two modalities substantially complicates the\ntranslation process; second, it requires achieving both high-level abstraction\nof geographic features and high-quality visual synthesis, which further\nelevates the technical complexity. To address these limitations, we introduce\nEarthMapper, a novel autoregressive framework for controllable bidirectional\nsatellite-map translation. EarthMapper employs geographic coordinate embeddings\nto anchor generation, ensuring region-specific adaptability, and leverages\nmulti-scale feature alignment within a geo-conditioned joint scale\nautoregression (GJSA) process to unify bidirectional translation in a single\ntraining cycle. A semantic infusion (SI) mechanism is introduced to enhance\nfeature-level consistency, while a key point adaptive guidance (KPAG) mechanism\nis proposed to dynamically balance diversity and precision during inference. We\nfurther contribute CNSatMap, a large-scale dataset comprising 302,132 precisely\naligned satellite-map pairs across 38 Chinese cities, enabling robust\nbenchmarking. Extensive experiments on CNSatMap and the New York dataset\ndemonstrate EarthMapper's superior performance, achieving significant\nimprovements in visual realism, semantic consistency, and structural fidelity\nover state-of-the-art methods. Additionally, EarthMapper excels in zero-shot\ntasks like in-painting, out-painting and coordinate-conditional generation,\nunderscoring its versatility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.20013", "pdf": "https://arxiv.org/pdf/2504.20013", "abs": "https://arxiv.org/abs/2504.20013", "authors": ["Beizhe Hu", "Qiang Sheng", "Juan Cao", "Yang Li", "Danding Wang"], "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation", "categories": ["cs.CL", "cs.CY", "cs.IR"], "comment": "ACM SIGIR 2025 Full Paper", "summary": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation"], "score": 2}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.20039", "pdf": "https://arxiv.org/pdf/2504.20039", "abs": "https://arxiv.org/abs/2504.20039", "authors": ["Roman Garipov", "Fedor Velikonivtsev", "Ruslan Svirschevski", "Vage Egiazarian", "Max Ryabinin"], "title": "AutoJudge: Judge Decoding Without Manual Annotation", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint, Work in progress", "summary": "We introduce AutoJudge, a framework that accelerates large language model\n(LLM) inference with task-specific lossy speculative decoding. Instead of\nmatching the original model output distribution token-by-token, we identify\nwhich of the generated tokens affect the downstream quality of the generated\nresponse, relaxing the guarantee so that the \"unimportant\" tokens can be\ngenerated faster. Our approach relies on a semi-greedy search algorithm to test\nwhich of the mismatches between target and draft model should be corrected to\npreserve quality, and which ones may be skipped. We then train a lightweight\nclassifier based on existing LLM embeddings to predict, at inference time,\nwhich mismatching tokens can be safely accepted without compromising the final\nanswer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\naccepted tokens per verification cycle with under 1% degradation in answer\naccuracy compared to standard speculative decoding and over 2x with small loss\nin accuracy. When applied to the LiveCodeBench benchmark, our approach\nautomatically detects other, programming-specific important tokens and shows\nsimilar speedups, demonstrating its ability to generalize across tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19478", "pdf": "https://arxiv.org/pdf/2504.19478", "abs": "https://arxiv.org/abs/2504.19478", "authors": ["Weitao Feng", "Hang Zhou", "Jing Liao", "Li Cheng", "Wenbo Zhou"], "title": "CasaGPT: Cuboid Arrangement and Scene Assembly for Interior Design", "categories": ["cs.CV"], "comment": null, "summary": "We present a novel approach for indoor scene synthesis, which learns to\narrange decomposed cuboid primitives to represent 3D objects within a scene.\nUnlike conventional methods that use bounding boxes to determine the placement\nand scale of 3D objects, our approach leverages cuboids as a straightforward\nyet highly effective alternative for modeling objects. This allows for compact\nscene generation while minimizing object intersections. Our approach, coined\nCasaGPT for Cuboid Arrangement and Scene Assembly, employs an autoregressive\nmodel to sequentially arrange cuboids, producing physically plausible scenes.\nBy applying rejection sampling during the fine-tuning stage to filter out\nscenes with object collisions, our model further reduces intersections and\nenhances scene quality. Additionally, we introduce a refined dataset,\n3DFRONT-NC, which eliminates significant noise presented in the original\ndataset, 3D-FRONT. Extensive experiments on the 3D-FRONT dataset as well as our\ndataset demonstrate that our approach consistently outperforms the\nstate-of-the-art methods, enhancing the realism of generated scenes, and\nproviding a promising direction for 3D scene synthesis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19506", "pdf": "https://arxiv.org/pdf/2504.19506", "abs": "https://arxiv.org/abs/2504.19506", "authors": ["Xinyang Li", "Chengjie Yi", "Jiawei Lai", "Mingbao Lin", "Yansong Qu", "Shengchuan Zhang", "Liujuan Cao"], "title": "SynergyAmodal: Deocclude Anything with Text Control", "categories": ["cs.CV"], "comment": "17 pages", "summary": "Image deocclusion (or amodal completion) aims to recover the invisible\nregions (\\ie, shape and appearance) of occluded instances in images. Despite\nrecent advances, the scarcity of high-quality data that balances diversity,\nplausibility, and fidelity remains a major obstacle. To address this challenge,\nwe identify three critical elements: leveraging in-the-wild image data for\ndiversity, incorporating human expertise for plausibility, and utilizing\ngenerative priors for fidelity. We propose SynergyAmodal, a novel framework for\nco-synthesizing in-the-wild amodal datasets with comprehensive shape and\nappearance annotations, which integrates these elements through a tripartite\ndata-human-model collaboration. First, we design an occlusion-grounded\nself-supervised learning algorithm to harness the diversity of in-the-wild\nimage data, fine-tuning an inpainting diffusion model into a partial completion\ndiffusion model. Second, we establish a co-synthesis pipeline to iteratively\nfilter, refine, select, and annotate the initial deocclusion results of the\npartial completion diffusion model, ensuring plausibility and fidelity through\nhuman expert guidance and prior model constraints. This pipeline generates a\nhigh-quality paired amodal dataset with extensive category and scale diversity,\ncomprising approximately 16K pairs. Finally, we train a full completion\ndiffusion model on the synthesized dataset, incorporating text prompts as\nconditioning signals. Extensive experiments demonstrate the effectiveness of\nour framework in achieving zero-shot generalization and textual\ncontrollability. Our code, dataset, and models will be made publicly available\nat https://github.com/imlixinyang/SynergyAmodal.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19514", "pdf": "https://arxiv.org/pdf/2504.19514", "abs": "https://arxiv.org/abs/2504.19514", "authors": ["Rong Gao", "Xin Liu", "Zhuozhao Hu", "Bohao Xing", "Baiqiang Xia", "Zitong Yu", "Heikki KÃ¤lviÃ¤inen"], "title": "FSBench: A Figure Skating Benchmark for Advancing Artistic Sports Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Figure skating, known as the \"Art on Ice,\" is among the most artistic sports,\nchallenging to understand due to its blend of technical elements (like jumps\nand spins) and overall artistic expression. Existing figure skating datasets\nmainly focus on single tasks, such as action recognition or scoring, lacking\ncomprehensive annotations for both technical and artistic evaluation. Current\nsports research is largely centered on ball games, with limited relevance to\nartistic sports like figure skating. To address this, we introduce FSAnno, a\nlarge-scale dataset advancing artistic sports understanding through figure\nskating. FSAnno includes an open-access training and test dataset, alongside a\nbenchmark dataset, FSBench, for fair model evaluation. FSBench consists of\nFSBench-Text, with multiple-choice questions and explanations, and\nFSBench-Motion, containing multimodal data and Question and Answer (QA) pairs,\nsupporting tasks from technical analysis to performance commentary. Initial\ntests on FSBench reveal significant limitations in existing models'\nunderstanding of artistic sports. We hope FSBench will become a key tool for\nevaluating and enhancing model comprehension of figure skating.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19546", "pdf": "https://arxiv.org/pdf/2504.19546", "abs": "https://arxiv.org/abs/2504.19546", "authors": ["Tong Xiao", "Qunming Wang", "Ping Lu", "Tenghai Huang", "Xiaohua Tong", "Peter M. Atkinson"], "title": "Crowd Detection Using Very-Fine-Resolution Satellite Imagery", "categories": ["cs.CV"], "comment": "17 pages, 12 figures, 5 tables", "summary": "Accurate crowd detection (CD) is critical for public safety and historical\npattern analysis, yet existing methods relying on ground and aerial imagery\nsuffer from limited spatio-temporal coverage. The development of\nvery-fine-resolution (VFR) satellite sensor imagery (e.g., ~0.3 m spatial\nresolution) provides unprecedented opportunities for large-scale crowd activity\nanalysis, but it has never been considered for this task. To address this gap,\nwe proposed CrowdSat-Net, a novel point-based convolutional neural network,\nwhich features two innovative components: Dual-Context Progressive Attention\nNetwork (DCPAN) to improve feature representation of individuals by aggregating\nscene context and local individual characteristics, and High-Frequency Guided\nDeformable Upsampler (HFGDU) that recovers high-frequency information during\nupsampling through frequency-domain guided deformable convolutions. To validate\nthe effectiveness of CrowdSat-Net, we developed CrowdSat, the first VFR\nsatellite imagery dataset designed specifically for CD tasks, comprising over\n120k manually labeled individuals from multi-source satellite platforms\n(Beijing-3N, Jilin-1 Gaofen-04A and Google Earth) across China. In the\nexperiments, CrowdSat-Net was compared with five state-of-the-art point-based\nCD methods (originally designed for ground or aerial imagery) using CrowdSat\nand achieved the largest F1-score of 66.12% and Precision of 73.23%, surpassing\nthe second-best method by 1.71% and 2.42%, respectively. Moreover, extensive\nablation experiments validated the importance of the DCPAN and HFGDU modules.\nFurthermore, cross-regional evaluation further demonstrated the spatial\ngeneralizability of CrowdSat-Net. This research advances CD capability by\nproviding both a newly developed network architecture for CD and a pioneering\nbenchmark dataset to facilitate future CD development.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety"], "score": 4}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19557", "pdf": "https://arxiv.org/pdf/2504.19557", "abs": "https://arxiv.org/abs/2504.19557", "authors": ["Mohammad Altillawi", "Fengyi Shen", "Liudi Yang", "Sai Manoj Prakhya", "Ziyuan Liu"], "title": "CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel View Synthesis in Autonomous Driving Scenes", "categories": ["cs.CV"], "comment": "Accepted in 2025 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW)", "summary": "Current point-based approaches encounter limitations in scalability and\nrendering quality when using large 3D point cloud maps because using them\ndirectly for novel view synthesis (NVS) leads to degraded visualizations. We\nidentify the primary issue behind these low-quality renderings as a visibility\nmismatch between geometry and appearance, stemming from using these two\nmodalities together. To address this problem, we present CE-NPBG, a new\napproach for novel view synthesis (NVS) in large-scale autonomous driving\nscenes. Our method is a neural point-based technique that leverages two\nmodalities: posed images (cameras) and synchronized raw 3D point clouds\n(LiDAR). We first employ a connectivity relationship graph between appearance\nand geometry, which retrieves points from a large 3D point cloud map observed\nfrom the current camera perspective and uses them for rendering. By leveraging\nthis connectivity, our method significantly improves rendering quality and\nenhances run-time and scalability by using only a small subset of points from\nthe large 3D point cloud map. Our approach associates neural descriptors with\nthe points and uses them to synthesize views. To enhance the encoding of these\ndescriptors and elevate rendering quality, we propose a joint adversarial and\npoint rasterization training. During training, we pair an image-synthesizer\nnetwork with a multi-resolution discriminator. At inference, we decouple them\nand use the image-synthesizer to generate novel views. We also integrate our\nproposal into the recent 3D Gaussian Splatting work to highlight its benefits\nfor improved rendering and scalability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19483", "pdf": "https://arxiv.org/pdf/2504.19483", "abs": "https://arxiv.org/abs/2504.19483", "authors": ["Bertram HÃ¸jer", "Oliver Jarvis", "Stefan Heinrich"], "title": "Improving Reasoning Performance in Large Language Models via Representation Engineering", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Has been accepted at \"The Thirteenth International Conference on\n  Learning Representations (ICLR 2025)\" Link to publication:\n  https://openreview.net/forum?id=IssPhpUsKt", "summary": "Recent advancements in large language models (LLMs) have resulted in\nincreasingly anthropomorphic language concerning the ability of LLMs to reason.\nWhether reasoning in LLMs should be understood to be inherently different is,\nhowever, widely debated. We propose utilizing a representation engineering\napproach wherein model activations are read from the residual stream of an LLM\nwhen processing a reasoning task. The activations are used to derive a control\nvector that is applied to the model as an inference-time intervention,\nmodulating the representational space of the model, to improve performance on\nthe specified task. We publish the code for deriving control vectors and\nanalyzing model representations. The method allows us to improve performance on\nreasoning benchmarks and assess how control vectors influence the final logit\ndistribution of a model via metrics such as KL divergence and entropy. We apply\ncontrol vectors to Mistral-7B-Instruct and a range of Pythia models on an\ninductive, a deductive and mathematical reasoning task. We show that an LLM\ncan, to a certain degree, be controlled to improve its perceived reasoning\nability by modulating activations. The intervention is dependent upon the\nability to reliably extract the model's typical state when correctly solving a\ntask. Our results suggest that reasoning performance can be modulated in the\nsame manner as other information-processing tasks performed by LLMs and\ndemonstrate that we are capable of improving performance on specific tasks via\na simple intervention on the residual stream with no additional training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19583", "pdf": "https://arxiv.org/pdf/2504.19583", "abs": "https://arxiv.org/abs/2504.19583", "authors": ["Hanlu Zhang", "Yumeng Ma", "Shuo Wang", "Guiran Liu", "Binrong Zhu"], "title": "Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "This paper proposes a parameter collaborative optimization algorithm for\nlarge language models, enhanced with graph spectral analysis. The goal is to\nimprove both fine-tuning efficiency and structural awareness during training.\nIn the proposed method, the parameters of a pre-trained language model are\ntreated as nodes in a graph. A weighted graph is constructed, and Laplacian\nspectral decomposition is applied to enable frequency-domain modeling and\nstructural representation of the parameter space. Based on this structure, a\njoint loss function is designed. It combines the task loss with a spectral\nregularization term to facilitate collaborative updates among parameters. In\naddition, a spectral filtering mechanism is introduced during the optimization\nphase. This mechanism adjusts gradients in a structure-aware manner, enhancing\nthe model's training stability and convergence behavior. The method is\nevaluated on multiple tasks, including traditional fine-tuning comparisons,\nfew-shot generalization tests, and convergence speed analysis. In all settings,\nthe proposed approach demonstrates superior performance. The experimental\nresults confirm that the spectral collaborative optimization framework\neffectively reduces parameter perturbations and improves fine-tuning quality\nwhile preserving overall model performance. This work contributes significantly\nto the field of artificial intelligence by advancing parameter-efficient\ntraining methodologies for large-scale models, reinforcing the importance of\nstructural signal processing in deep learning optimization, and offering a\nrobust, generalizable framework for enhancing language model adaptability and\nperformance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19614", "pdf": "https://arxiv.org/pdf/2504.19614", "abs": "https://arxiv.org/abs/2504.19614", "authors": ["Junpeng Jiang", "Gangyi Hong", "Miao Zhang", "Hengtong Hu", "Kun Zhan", "Rui Shao", "Liqiang Nie"], "title": "DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Collecting multi-view driving scenario videos to enhance the performance of\n3D visual perception tasks presents significant challenges and incurs\nsubstantial costs, making generative models for realistic data an appealing\nalternative. Yet, the videos generated by recent works suffer from poor quality\nand spatiotemporal consistency, undermining their utility in advancing\nperception tasks under driving scenarios. To address this gap, we propose DiVE,\na diffusion transformer-based generative framework meticulously engineered to\nproduce high-fidelity, temporally coherent, and cross-view consistent\nmulti-view videos, aligning seamlessly with bird's-eye view layouts and textual\ndescriptions. DiVE leverages a unified cross-attention and a SketchFormer to\nexert precise control over multimodal data, while incorporating a view-inflated\nattention mechanism that adds no extra parameters, thereby guaranteeing\nconsistency across views. Despite these advancements, synthesizing\nhigh-resolution videos under multimodal constraints introduces dual challenges:\ninvestigating the optimal classifier-free guidance coniguration under intricate\nmulti-condition inputs and mitigating excessive computational latency in\nhigh-resolution rendering--both of which remain underexplored in prior\nresearches. To resolve these limitations, we introduce two innovations:\nMulti-Control Auxiliary Branch Distillation, which streamlines multi-condition\nCFG selection while circumventing high computational overhead, and Resolution\nProgressive Sampling, a training-free acceleration strategy that staggers\nresolution scaling to reduce high latency due to high resolution. These\ninnovations collectively achieve a 2.62x speedup with minimal quality\ndegradation. Evaluated on the nuScenes dataset, DiVE achieves SOTA performance\nin multi-view video generation, yielding photorealistic outputs with\nexceptional temporal and cross-view coherence.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19687", "pdf": "https://arxiv.org/pdf/2504.19687", "abs": "https://arxiv.org/abs/2504.19687", "authors": ["Baoshun Shi", "Bing Chen", "Shaolei Zhang", "Huazhu Fu", "Zhanli Hu"], "title": "Prompt Guiding Multi-Scale Adaptive Sparse Representation-driven Network for Low-Dose CT MAR", "categories": ["cs.CV"], "comment": null, "summary": "Low-dose CT (LDCT) is capable of reducing X-ray radiation exposure, but it\nwill potentially degrade image quality, even yields metal artifacts at the case\nof metallic implants. For simultaneous LDCT reconstruction and metal artifact\nreduction (LDMAR), existing deep learning-based efforts face two main\nlimitations: i) the network design neglects multi-scale and within-scale\ninformation; ii) training a distinct model for each dose necessitates\nsignificant storage space for multiple doses. To fill these gaps, we propose a\nprompt guiding multi-scale adaptive sparse representation-driven network,\nabbreviated as PMSRNet, for LDMAR task. Specifically, we construct PMSRNet\ninspired from multi-scale sparsifying frames, and it can simultaneously employ\nwithin-scale characteristics and cross-scale complementarity owing to an\nelaborated prompt guiding scale-adaptive threshold generator (PSATG) and a\nbuilt multi-scale coefficient fusion module (MSFuM). The PSATG can adaptively\ncapture multiple contextual information to generate more faithful thresholds,\nachieved by fusing features from local, regional, and global levels.\nFurthermore, we elaborate a model interpretable dual domain LDMAR framework\ncalled PDuMSRNet, and train single model with a prompt guiding strategy for\nmultiple dose levels. We build a prompt guiding module, whose input contains\ndose level, metal mask and input instance, to provide various guiding\ninformation, allowing a single model to accommodate various CT dose settings.\nExtensive experiments at various dose levels demonstrate that the proposed\nmethods outperform the state-of-the-art LDMAR methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19737", "pdf": "https://arxiv.org/pdf/2504.19737", "abs": "https://arxiv.org/abs/2504.19737", "authors": ["Abhishek Kuriyal", "Elliot Vincent", "Mathieu Aubry", "Loic Landrieu"], "title": "CoDEx: Combining Domain Expertise for Spatial Generalization in Satellite Image Analysis", "categories": ["cs.CV"], "comment": "CVPR 2025 EarthVision Workshop", "summary": "Global variations in terrain appearance raise a major challenge for satellite\nimage analysis, leading to poor model performance when training on locations\nthat differ from those encountered at test time. This remains true even with\nrecent large global datasets. To address this challenge, we propose a novel\ndomain-generalization framework for satellite images. Instead of trying to\nlearn a single generalizable model, we train one expert model per training\ndomain, while learning experts' similarity and encouraging similar experts to\nbe consistent. A model selection module then identifies the most suitable\nexperts for a given test sample and aggregates their predictions. Experiments\non four datasets (DynamicEarthNet, MUDS, OSCD, and FMoW) demonstrate consistent\ngains over existing domain generalization and adaptation methods. Our code is\npublicly available at https://github.com/Abhishek19009/CoDEx.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19839", "pdf": "https://arxiv.org/pdf/2504.19839", "abs": "https://arxiv.org/abs/2504.19839", "authors": ["Yulong Guo", "Zilun Zhang", "Yongheng Shang", "Tiancheng Zhao", "Shuiguang Deng", "Yingchun Yang", "Jianwei Yin"], "title": "SRMF: A Data Augmentation and Multimodal Fusion Approach for Long-Tail UHR Satellite Image Segmentation", "categories": ["cs.CV"], "comment": "None", "summary": "The long-tail problem presents a significant challenge to the advancement of\nsemantic segmentation in ultra-high-resolution (UHR) satellite imagery. While\nprevious efforts in UHR semantic segmentation have largely focused on\nmulti-branch network architectures that emphasize multi-scale feature\nextraction and fusion, they have often overlooked the importance of addressing\nthe long-tail issue. In contrast to prior UHR methods that focused on\nindependent feature extraction, we emphasize data augmentation and multimodal\nfeature fusion to alleviate the long-tail problem. In this paper, we introduce\nSRMF, a novel framework for semantic segmentation in UHR satellite imagery. Our\napproach addresses the long-tail class distribution by incorporating a\nmulti-scale cropping technique alongside a data augmentation strategy based on\nsemantic reordering and resampling. To further enhance model performance, we\npropose a multimodal fusion-based general representation knowledge injection\nmethod, which, for the first time, fuses text and visual features without the\nneed for individual region text descriptions, extracting more robust features.\nExtensive experiments on the URUR, GID, and FBP datasets demonstrate that our\nmethod improves mIoU by 3.33\\%, 0.66\\%, and 0.98\\%, respectively, achieving\nstate-of-the-art performance. Code is available at:\nhttps://github.com/BinSpa/SRMF.git.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19900", "pdf": "https://arxiv.org/pdf/2504.19900", "abs": "https://arxiv.org/abs/2504.19900", "authors": ["Han Chen", "Anne L. Martel"], "title": "Breast Cancer Detection from Multi-View Screening Mammograms with Visual Prompt Tuning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate detection of breast cancer from high-resolution mammograms is\ncrucial for early diagnosis and effective treatment planning. Previous studies\nhave shown the potential of using single-view mammograms for breast cancer\ndetection. However, incorporating multi-view data can provide more\ncomprehensive insights. Multi-view classification, especially in medical\nimaging, presents unique challenges, particularly when dealing with\nlarge-scale, high-resolution data. In this work, we propose a novel Multi-view\nVisual Prompt Tuning Network (MVPT-NET) for analyzing multiple screening\nmammograms. We first pretrain a robust single-view classification model on\nhigh-resolution mammograms and then innovatively adapt multi-view feature\nlearning into a task-specific prompt tuning process. This technique selectively\ntunes a minimal set of trainable parameters (7\\%) while retaining the\nrobustness of the pre-trained single-view model, enabling efficient integration\nof multi-view data without the need for aggressive downsampling. Our approach\noffers an efficient alternative to traditional feature fusion methods,\nproviding a more robust, scalable, and efficient solution for high-resolution\nmammogram analysis. Experimental results on a large multi-institution dataset\ndemonstrate that our method outperforms conventional approaches while\nmaintaining detection efficiency, achieving an AUROC of 0.852 for\ndistinguishing between Benign, DCIS, and Invasive classes. This work highlights\nthe potential of MVPT-NET for medical imaging tasks and provides a scalable\nsolution for integrating multi-view data in breast cancer detection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.18591", "pdf": "https://arxiv.org/pdf/2504.18591", "abs": "https://arxiv.org/abs/2504.18591", "authors": ["Giovanni Catalani", "Michael Bauerheim", "FrÃ©dÃ©ric Tost", "Xavier Bertrand", "Joseph Morlier"], "title": "Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent advances in Neural Fields have enabled powerful,\ndiscretization-invariant methods for learning neural operators that approximate\nsolutions of Partial Differential Equations (PDEs) on general geometries.\nBuilding on these developments, we introduce enf2enf, an encoder--decoder\nmethodology for predicting steady-state Partial Differential Equations with\nnon-parameterized geometric variability, based on recently proposed Equivariant\nNeural Field architectures. In enf2enf, input geometries are encoded into\nlatent point cloud embeddings that inherently preserve geometric grounding and\ncapture local phenomena. The resulting representations are then combined with\nglobal parameters and directly decoded into continuous output fields, thus\nefficiently modeling the coupling between geometry and physics. By leveraging\nthe inductive biases of locality and translation invariance, our approach is\nable to capture fine-scale physical features as well as complex shape\nvariations, thereby enhancing generalization and physical compliance. Extensive\nexperiments on a high-fidelity aerodynamic dataset, a hyper-elastic material\nbenchmark, and multi-element airfoil geometries, demonstrate that the proposed\nmodel achieves superior or competitive performance compared to state-of-the-art\ngraph based, operator learning, and neural field methods. Notably, our method\nsupports real time inference and zero-shot super-resolution, enabling efficient\ntraining on low-resolution meshes while maintaining high accuracy on full-scale\ndiscretizations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.18829", "pdf": "https://arxiv.org/pdf/2504.18829", "abs": "https://arxiv.org/abs/2504.18829", "authors": ["Jiayi Chen", "Yubin Ke", "Lin Peng", "He Wang"], "title": "Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by Robotics: Science and Systems (RSS 2025)", "summary": "Generalizable dexterous grasping with suitable grasp types is a fundamental\nskill for intelligent robots. Developing such skills requires a large-scale and\nhigh-quality dataset that covers numerous grasp types (i.e., at least those\ncategorized by the GRASP taxonomy), but collecting such data is extremely\nchallenging. Existing automatic grasp synthesis methods are often limited to\nspecific grasp types or object categories, hindering scalability. This work\nproposes an efficient pipeline capable of synthesizing contact-rich,\npenetration-free, and physically plausible grasps for any grasp type, object,\nand articulated hand. Starting from a single human-annotated template for each\nhand and grasp type, our pipeline tackles the complicated synthesis problem\nwith two stages: optimize the object to fit the hand template first, and then\nlocally refine the hand to fit the object in simulation. To validate the\nsynthesized grasps, we introduce a contact-aware control strategy that allows\nthe hand to apply the appropriate force at each contact point to the object.\nThose validated grasps can also be used as new grasp templates to facilitate\nfuture synthesis. Experiments show that our method significantly outperforms\nprevious type-unaware grasp synthesis baselines in simulation. Using our\nalgorithm, we construct a dataset containing 10.7k objects and 9.5M grasps,\ncovering 31 grasp types in the GRASP taxonomy. Finally, we train a\ntype-conditional generative model that successfully performs the desired grasp\ntype from single-view object point clouds, achieving an 82.3% success rate in\nreal-world experiments. Project page: https://pku-epic.github.io/Dexonomy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19267", "pdf": "https://arxiv.org/pdf/2504.19267", "abs": "https://arxiv.org/abs/2504.19267", "authors": ["Mohamed Gado", "Towhid Taliee", "Muhammad Memon", "Dmitry Ignatov", "Radu Timofte"], "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Visual storytelling is an interdisciplinary field combining computer vision\nand natural language processing to generate cohesive narratives from sequences\nof images. This paper presents a novel approach that leverages recent\nadvancements in multimodal models, specifically adapting transformer-based\narchitectures and large multimodal models, for the visual storytelling task.\nLeveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT\nmodel produces visually grounded, contextually appropriate narratives. We\naddress the limitations of traditional evaluation metrics, such as BLEU,\nMETEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we\nutilize RoViST and GROOVIST, novel reference-free metrics designed to assess\nvisual storytelling, focusing on visual grounding, coherence, and\nnon-redundancy. These metrics provide a more nuanced evaluation of narrative\nquality, aligning closely with human judgment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19401", "pdf": "https://arxiv.org/pdf/2504.19401", "abs": "https://arxiv.org/abs/2504.19401", "authors": ["Shuo Wang", "Tong Ren", "Nan Cheng", "Li Zhang", "Rong Wang"], "title": "Innovative Integration of 4D Cardiovascular Reconstruction and Hologram: A New Visualization Tool for Coronary Artery Bypass Grafting Planning", "categories": ["physics.med-ph", "cs.CV", "cs.GR", "eess.IV", "J.3; I.3.8"], "comment": "35 pages, 9 figures", "summary": "Background: Coronary artery bypass grafting (CABG) planning requires advanced\nspatial visualization and consideration of coronary artery depth,\ncalcification, and pericardial adhesions. Objective: To develop and evaluate a\ndynamic cardiovascular holographic visualization tool for preoperative CABG\nplanning. Methods: Using 4D cardiac computed tomography angiography data from\n14 CABG candidates, we developed a semi-automated workflow for time-resolved\nsegmentation of cardiac structures, epicardial adipose tissue (EAT), and\ncoronary arteries with calcium scoring. The workflow incorporated methods for\ncardiac segmentation, coronary calcification quantification, visualization of\ncoronary depth within EAT, and pericardial adhesion assessment through motion\nanalysis. Dynamic cardiovascular holograms were displayed using the Looking\nGlass platform. Thirteen cardiac surgeons evaluated the tool using a Likert\nscale. Additionally, pericardial adhesion scores from holograms of 21 patients\n(including seven undergoing secondary cardiac surgeries) were compared with\nintraoperative findings. Results: Surgeons rated the visualization tool highly\nfor preoperative planning utility (mean Likert score: 4.57/5.0). Hologram-based\npericardial adhesion scoring strongly correlated with intraoperative findings\n(r=0.786, P<0.001). Conclusion: This study establishes a visualization\nframework for CABG planning that produces clinically relevant dynamic holograms\nfrom patient-specific data, with clinical feedback confirming its effectiveness\nfor preoperative planning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19822", "pdf": "https://arxiv.org/pdf/2504.19822", "abs": "https://arxiv.org/abs/2504.19822", "authors": ["Minjong Cheon"], "title": "MjÃ¶lnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.ao-ph"], "comment": null, "summary": "Recent advances in AI-based weather forecasting models, such as FourCastNet,\nPangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep\nlearning to emulate complex atmospheric dynamics. Building on this momentum, we\npropose Mj\\\"olnir, a novel deep learning-based framework for global lightning\nflash density parameterization. Trained on ERA5 atmospheric predictors and\nWorld Wide Lightning Location Network (WWLLN) observations at a daily temporal\nresolution and 1 degree spatial resolution, Mj\\\"olnir captures the nonlinear\nmapping between large-scale environmental conditions and lightning activity.\nThe model architecture is based on the InceptionNeXt backbone with SENet, and a\nmulti-task learning strategy to simultaneously predict lightning occurrence and\nmagnitude. Extensive evaluations yield that Mollnir accurately reproduces the\nglobal distribution, seasonal variability, and regional characteristics of\nlightning activity, achieving a global Pearson correlation coefficient of 0.96\nfor annual mean fields. These results suggest that Mj\\\"olnir serves not only as\nan effective data-driven global lightning parameterization but also as a\npromising AI-based scheme for next-generation Earth system models (AI-ESMs).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
{"id": "2504.19854", "pdf": "https://arxiv.org/pdf/2504.19854", "abs": "https://arxiv.org/abs/2504.19854", "authors": ["Chia-Yu Hung", "Qi Sun", "Pengfei Hong", "Amir Zadeh", "Chuan Li", "U-Xuan Tan", "Navonil Majumder", "Soujanya Poria"], "title": "NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Existing Visual-Language-Action (VLA) models have shown promising performance\nin zero-shot scenarios, demonstrating impressive task execution and reasoning\ncapabilities. However, a significant challenge arises from the limitations of\nvisual encoding, which can result in failures during tasks such as object\ngrasping. Moreover, these models typically suffer from high computational\noverhead due to their large sizes, often exceeding 7B parameters. While these\nmodels excel in reasoning and task planning, the substantial computational\noverhead they incur makes them impractical for real-time robotic environments,\nwhere speed and efficiency are paramount. To address the limitations of\nexisting VLA models, we propose NORA, a 3B-parameter model designed to reduce\ncomputational overhead while maintaining strong task performance. NORA adopts\nthe Qwen-2.5-VL-3B multimodal model as its backbone, leveraging its superior\nvisual-semantic understanding to enhance visual reasoning and action grounding.\nAdditionally, our \\model{} is trained on 970k real-world robot demonstrations\nand equipped with the FAST+ tokenizer for efficient action sequence generation.\nExperimental results demonstrate that NORA outperforms existing large-scale VLA\nmodels, achieving better task performance with significantly reduced\ncomputational overhead, making it a more practical solution for real-time\nrobotic autonomy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-04-29.jsonl"}
