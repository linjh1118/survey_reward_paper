{"id": "2505.01812", "pdf": "https://arxiv.org/pdf/2505.01812", "abs": "https://arxiv.org/abs/2505.01812", "authors": ["Core Francisco Park", "Zechen Zhang", "Hidenori Tanaka"], "title": "$\\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Humans and intelligent animals can effortlessly internalize new information\n(\"news\") and accurately extract the implications for performing downstream\ntasks. While large language models (LLMs) can achieve this through in-context\nlearning (ICL) when the news is explicitly given as context, fine-tuning\nremains challenging for the models to consolidate learning in weights. In this\npaper, we introduce $\\textit{New News}$, a dataset composed of hypothetical yet\nplausible news spanning multiple domains (mathematics, coding, discoveries,\nleaderboards, events), accompanied by downstream evaluation questions whose\ncorrect answers critically depend on understanding and internalizing the news.\nWe first demonstrate a substantial gap between naive fine-tuning and in-context\nlearning (FT-ICL gap) on our news dataset. To address this gap, we explore a\nsuite of self-play data generation protocols -- paraphrases, implications and\nSelf-QAs -- designed to distill the knowledge from the model with context into\nthe weights of the model without the context, which we term $\\textit{System-2\nFine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance\nacross data domains and model scales with the Qwen 2.5 family of models. Our\nresults demonstrate that the self-QA protocol of Sys2-FT significantly improves\nmodels' in-weight learning of the news. Furthermore, we discover the\n$\\textit{contexual shadowing effect}$, where training with the news $\\textit{in\ncontext}$ followed by its rephrases or QAs degrade learning of the news.\nFinally, we show preliminary evidence of an emerging scaling law of Sys2-FT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scaling law"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02172", "pdf": "https://arxiv.org/pdf/2505.02172", "abs": "https://arxiv.org/abs/2505.02172", "authors": ["Chuck Arvin"], "title": "Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization", "categories": ["cs.CL"], "comment": "Presented as a short paper at International Conference on Artificial\n  Intelligence and Law 2025 (Chicago, IL)", "summary": "As large language models (LLMs) continue to advance in capabilities, it is\nessential to assess how they perform on established benchmarks. In this study,\nwe present a suite of experiments to assess the performance of modern LLMs\n(ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for\nidentifying case holdings. Our experiments demonstrate ``scaling effects'' -\nperformance on this task improves with model size, with more capable models\nlike GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720\nrespectively. These scores are competitive with the best published results on\nthis dataset, and do not require any technically sophisticated model training,\nfine-tuning or few-shot prompting. To ensure that these strong results are not\ndue to memorization of judicial opinions contained in the training data, we\ndevelop and utilize a novel citation anonymization test that preserves semantic\nmeaning while ensuring case names and citations are fictitious. Models maintain\nstrong performance under these conditions (macro F1 of 0.728), suggesting the\nperformance is not due to rote memorization. These findings demonstrate both\nthe promise and current limitations of LLMs for legal tasks with important\nimplications for the development and measurement of automated legal analytics\nand legal benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02266", "pdf": "https://arxiv.org/pdf/2505.02266", "abs": "https://arxiv.org/abs/2505.02266", "authors": ["Henry Ndubuaku", "Mouad Talhi"], "title": "Parameter-Efficient Transformer Embeddings", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07 (Primary) 68T50 (Secondary)"], "comment": "7 pages, 2 tables. Code available at https://github.com/HMUNACHI/pete", "summary": "Embedding layers in transformer-based NLP models typically account for the\nlargest share of model parameters, scaling with vocabulary size but not\nyielding performance gains proportional to scale. We propose an alternative\napproach in which token embedding vectors are first generated\ndeterministically, directly from the token IDs using a Fourier expansion of\ntheir normalized values, followed by a lightweight multilayer perceptron (MLP)\nthat captures higher-order interactions. We train standard transformers and our\narchitecture on natural language inference tasks (SNLI and MNLI), and evaluate\nzero-shot performance on sentence textual similarity (STS-B). Our results\ndemonstrate that the proposed method achieves competitive performance using\nsignificantly fewer parameters, trains faster, and operates effectively without\nthe need for dropout. This proof-of-concept study highlights the potential for\nscalable, memory-efficient language models and motivates further large-scale\nexperimentation based on our findings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02686", "pdf": "https://arxiv.org/pdf/2505.02686", "abs": "https://arxiv.org/abs/2505.02686", "authors": ["Xiaobao Wu"], "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models", "categories": ["cs.CL"], "comment": "35 Pages", "summary": "Recent developments in Large Language Models (LLMs) have shifted from\npre-training scaling to post-training and test-time scaling. Across these\ndevelopments, a key unified paradigm has arisen: Learning from Rewards, where\nreward signals act as the guiding stars to steer LLM behavior. It has\nunderpinned a wide range of prevalent techniques, such as reinforcement\nlearning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc\ncorrection. Crucially, this paradigm enables the transition from passive\nlearning from static data to active learning from dynamic feedback. This endows\nLLMs with aligned preferences and deep reasoning capabilities. In this survey,\nwe present a comprehensive overview of the paradigm of learning from rewards.\nWe categorize and analyze the strategies under this paradigm across training,\ninference, and post-inference stages. We further discuss the benchmarks for\nreward models and the primary applications. Finally we highlight the challenges\nand future directions. We maintain a paper collection at\nhttps://github.com/bobxwu/learning-from-rewards-llm-papers.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "DPO"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01435", "pdf": "https://arxiv.org/pdf/2505.01435", "abs": "https://arxiv.org/abs/2505.01435", "authors": ["Carlo Siebenschuh", "Kyle Hippe", "Ozan Gokdemir", "Alexander Brace", "Arham Khan", "Khalid Hossain", "Yadu Babuji", "Nicholas Chia", "Venkatram Vishwanath", "Rick Stevens", "Arvind Ramanathan", "Ian Foster", "Robert Underwood"], "title": "AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine", "categories": ["cs.IR", "cs.CL", "cs.DC", "cs.LG"], "comment": "This paper has been accepted at the The Eighth Annual Conference on\n  Machine Learning and Systems (MLSys 2025)", "summary": "Language models for scientific tasks are trained on text from scientific\npublications, most distributed as PDFs that require parsing. PDF parsing\napproaches range from inexpensive heuristics (for simple documents) to\ncomputationally intensive ML-driven systems (for complex or degraded ones). The\nchoice of the \"best\" parser for a particular document depends on its\ncomputational cost and the accuracy of its output. To address these issues, we\nintroduce an Adaptive Parallel PDF Parsing and Resource Scaling Engine\n(AdaParse), a data-driven strategy for assigning an appropriate parser to each\ndocument. We enlist scientists to select preferred parser outputs and\nincorporate this information through direct preference optimization (DPO) into\nAdaParse, thereby aligning its selection process with human judgment. AdaParse\nthen incorporates hardware requirements and predicted accuracy of each parser\nto orchestrate computational resources efficiently for large-scale parsing\ncampaigns. We demonstrate that AdaParse, when compared to state-of-the-art\nparsers, improves throughput by $17\\times$ while still achieving comparable\naccuracy (0.2 percent better) on a benchmark set of 1000 scientific documents.\nAdaParse's combination of high accuracy and parallel scalability makes it\nfeasible to parse large-scale scientific document corpora to support the\ndevelopment of high-quality, trillion-token-scale text datasets. The\nimplementation is available at https://github.com/7shoe/AdaParse/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02325", "pdf": "https://arxiv.org/pdf/2505.02325", "abs": "https://arxiv.org/abs/2505.02325", "authors": ["Zhichuan Wang", "Yang Zhou", "Jinhai Xiang", "Yulong Wang", "Xinwei He"], "title": "TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval via Testing-time Distribution Alignment", "categories": ["cs.CV"], "comment": "Accepted by ICMR 2025", "summary": "Learning discriminative 3D representations that generalize well to unknown\ntesting categories is an emerging requirement for many real-world 3D\napplications. Existing well-established methods often struggle to attain this\ngoal due to insufficient 3D training data from broader concepts. Meanwhile,\npre-trained large vision-language models (e.g., CLIP) have shown remarkable\nzero-shot generalization capabilities. Yet, they are limited in extracting\nsuitable 3D representations due to substantial gaps between their 2D training\nand 3D testing distributions. To address these challenges, we propose\nTesting-time Distribution Alignment (TeDA), a novel framework that adapts a\npretrained 2D vision-language model CLIP for unknown 3D object retrieval at\ntest time. To our knowledge, it is the first work that studies the test-time\nadaptation of a vision-language model for 3D feature learning. TeDA projects 3D\nobjects into multi-view images, extracts features using CLIP, and refines 3D\nquery embeddings with an iterative optimization strategy by confident\nquery-target sample pairs in a self-boosting manner. Additionally, TeDA\nintegrates textual descriptions generated by a multimodal language model\n(InternVL) to enhance 3D object understanding, leveraging CLIP's aligned\nfeature space to fuse visual and textual cues. Extensive experiments on four\nopen-set 3D object retrieval benchmarks demonstrate that TeDA greatly\noutperforms state-of-the-art methods, even those requiring extensive training.\nWe also experimented with depth maps on Objaverse-LVIS, further validating its\neffectiveness. Code is available at https://github.com/wangzhichuan123/TeDA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02388", "pdf": "https://arxiv.org/pdf/2505.02388", "abs": "https://arxiv.org/abs/2505.02388", "authors": ["Huangyue Yu", "Baoxiong Jia", "Yixin Chen", "Yandan Yang", "Puhao Li", "Rongpeng Su", "Jiaxin Li", "Qing Li", "Wei Liang", "Song-Chun Zhu", "Tengyu Liu", "Siyuan Huang"], "title": "MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "CVPR 2025", "summary": "Embodied AI (EAI) research requires high-quality, diverse 3D scenes to\neffectively support skill acquisition, sim-to-real transfer, and\ngeneralization. Achieving these quality standards, however, necessitates the\nprecise replication of real-world object diversity. Existing datasets\ndemonstrate that this process heavily relies on artist-driven designs, which\ndemand substantial human effort and present significant scalability challenges.\nTo scalably produce realistic and interactive 3D scenes, we first present\nMetaScenes, a large-scale, simulatable 3D scene dataset constructed from\nreal-world scans, which includes 15366 objects spanning 831 fine-grained\ncategories. Then, we introduce Scan2Sim, a robust multi-modal alignment model,\nwhich enables the automated, high-quality replacement of assets, thereby\neliminating the reliance on artist-driven designs for scaling 3D scenes. We\nfurther propose two benchmarks to evaluate MetaScenes: a detailed scene\nsynthesis task focused on small item layouts for robotic manipulation and a\ndomain transfer task in vision-and-language navigation (VLN) to validate\ncross-domain transfer. Results confirm MetaScene's potential to enhance EAI by\nsupporting more generalizable agent learning and sim-to-real applications,\nintroducing new possibilities for EAI research. Project website:\nhttps://meta-scenes.github.io/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01456", "pdf": "https://arxiv.org/pdf/2505.01456", "abs": "https://arxiv.org/abs/2505.01456", "authors": ["Vaidehi Patil", "Yi-Lin Sung", "Peter Hase", "Jie Peng", "Tianlong Chen", "Mohit Bansal"], "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "The dataset and code are publicly available at\n  https://github.com/Vaidehi99/UnLOK-VQA", "summary": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety"], "score": 4}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02017", "pdf": "https://arxiv.org/pdf/2505.02017", "abs": "https://arxiv.org/abs/2505.02017", "authors": ["Yingrong Fang", "Qitong Wang", "Wei Wang"], "title": "Aokana: A GPU-Driven Voxel Rendering Framework for Open World Games", "categories": ["cs.GR"], "comment": null, "summary": "Voxels are among the most popular 3D geometric representations today. Due to\ntheir intuitiveness and ease-of-editing, voxels have been widely adopted in\nstylized games and low-cost independent games. However, the high storage cost\nof voxels, along with the significant time overhead associated with large-scale\nvoxel rendering, limits the further development of open-world voxel games. In\nthis paper, we introduce Aokana, a GPU-Driven Voxel Rendering Framework for\nOpen World Games. Aokana is based on a Sparse Voxel Directed Acyclic Graph\n(SVDAG). It incorporates a Level-of-Details (LOD) mechanism and a streaming\nsystem, enabling seamless map loading as players traverse the open-world game\nenvironment. We also designed a corresponding high-performance GPU-driven voxel\nrendering pipeline to support real-time rendering of the voxel scenes that\ncontain tens of billions of voxels. Aokana can be directly applied to existing\ngame engines and easily integrated with mesh-based rendering methods,\ndemonstrating its practical applicability in game development. Experimental\nevaluations show that, with increasing voxel scene resolution, Aokana can\nreduce memory usage by up to ninefold and achieves rendering speeds up to 4.8\ntimes faster than those of previous state-of-the-art approaches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01560", "pdf": "https://arxiv.org/pdf/2505.01560", "abs": "https://arxiv.org/abs/2505.01560", "authors": ["Vicent Briva Iglesias", "Gokhan Dogru"], "title": "AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) and multi-agent orchestration are touted as the\nnext leap in machine translation (MT), but their benefits relative to\nconventional neural MT (NMT) remain unclear. This paper offers an empirical\nreality check. We benchmark five paradigms, Google Translate (strong NMT\nbaseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM),\nand two GPT-4o-powered agentic workflows (sequential three-stage and iterative\nrefinement), on test data drawn from a legal contract and news prose in three\nEnglish-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is\nperformed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with\nexpert ratings of adequacy and fluency; efficiency with total input-plus-output\ntoken counts mapped to April 2025 pricing.\n  Automatic scores still favour the mature NMT system, which ranks first in\nseven of twelve metric-language combinations; o1-preview ties or places second\nin most remaining cases, while both multi-agent workflows trail. Human\nevaluation reverses part of this narrative: o1-preview produces the most\nadequate and fluent output in five of six comparisons, and the iterative agent\nedges ahead once, indicating that reasoning layers capture semantic nuance\nundervalued by surface metrics. Yet these qualitative gains carry steep costs.\nThe sequential agent consumes roughly five times, and the iterative agent\nfifteen times, the tokens used by NMT or single-pass LLMs.\n  We advocate multidimensional, cost-aware evaluation protocols and highlight\nresearch directions that could tip the balance: leaner coordination strategies,\nselective agent activation, and hybrid pipelines combining single-pass LLMs\nwith targeted agent intervention.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01548", "pdf": "https://arxiv.org/pdf/2505.01548", "abs": "https://arxiv.org/abs/2505.01548", "authors": ["Zhen Yao", "Xiaowen Ying", "Mooi Choo Chuah"], "title": "Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional Motion-enhanced Event Representation", "categories": ["cs.CV"], "comment": "12 pages, 9 figures", "summary": "Event cameras capture motion dynamics, offering a unique modality with great\npotential in various computer vision tasks. However, RGB-Event fusion faces\nthree intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal\nmisalignment. Existing voxel grid representations neglect temporal correlations\nbetween consecutive event windows, and their formulation with simple\naccumulation of asynchronous and sparse events is incompatible with the\nsynchronous and dense nature of RGB modality. To tackle these challenges, we\npropose a novel event representation, Motion-enhanced Event Tensor (MET), which\ntransforms sparse event voxels into a dense and temporally coherent form by\nleveraging dense optical flows and event temporal features. In addition, we\nintroduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a\nTemporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to\nmitigate modal misalignment, while bidirectional flow aggregation and temporal\nfusion mechanisms resolve spatiotemporal misalignment. Experimental results on\ntwo large-scale datasets demonstrate that our framework significantly\noutperforms state-of-the-art RGB-Event semantic segmentation approaches. Our\ncode is available at: https://github.com/zyaocoder/BRENet.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01595", "pdf": "https://arxiv.org/pdf/2505.01595", "abs": "https://arxiv.org/abs/2505.01595", "authors": ["Liaoyaqi Wang", "Zhengping Jiang", "Anqi Liu", "Benjamin Van Durme"], "title": "Always Tell Me The Odds: Fine-grained Conditional Probability Estimation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present a state-of-the-art model for fine-grained probability estimation\nof propositions conditioned on context. Recent advances in large language\nmodels (LLMs) have significantly enhanced their reasoning capabilities,\nparticularly on well-defined tasks with complete information. However, LLMs\ncontinue to struggle with making accurate and well-calibrated probabilistic\npredictions under uncertainty or partial information. While incorporating\nuncertainty into model predictions often boosts performance, obtaining reliable\nestimates of that uncertainty remains understudied. In particular, LLM\nprobability estimates tend to be coarse and biased towards more frequent\nnumbers. Through a combination of human and synthetic data creation and\nassessment, scaling to larger models, and better supervision, we propose a set\nof strong and precise probability estimation models. We conduct systematic\nevaluations across tasks that rely on conditional probability estimation and\nshow that our approach consistently outperforms existing fine-tuned and\nprompting-based methods by a large margin.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01693", "pdf": "https://arxiv.org/pdf/2505.01693", "abs": "https://arxiv.org/abs/2505.01693", "authors": ["Brian Wong", "Kaito Tanaka"], "title": "High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers", "categories": ["cs.CL"], "comment": null, "summary": "Automated labeling of chest X-ray reports is essential for enabling\ndownstream tasks such as training image-based diagnostic models, population\nhealth studies, and clinical decision support. However, the high variability,\ncomplexity, and prevalence of negation and uncertainty in these free-text\nreports pose significant challenges for traditional Natural Language Processing\nmethods. While large language models (LLMs) demonstrate strong text\nunderstanding, their direct application for large-scale, efficient labeling is\nlimited by computational cost and speed. This paper introduces DeBERTa-RAD, a\nnovel two-stage framework that combines the power of state-of-the-art LLM\npseudo-labeling with efficient DeBERTa-based knowledge distillation for\naccurate and fast chest X-ray report labeling. We leverage an advanced LLM to\ngenerate high-quality pseudo-labels, including certainty statuses, for a large\ncorpus of reports. Subsequently, a DeBERTa-Base model is trained on this\npseudo-labeled data using a tailored knowledge distillation strategy. Evaluated\non the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a\nstate-of-the-art Macro F1 score of 0.9120, significantly outperforming\nestablished rule-based systems, fine-tuned transformer models, and direct LLM\ninference, while maintaining a practical inference speed suitable for\nhigh-throughput applications. Our analysis shows particular strength in\nhandling uncertain findings. This work demonstrates a promising path to\novercome data annotation bottlenecks and achieve high-performance medical text\nprocessing through the strategic combination of LLM capabilities and efficient\nstudent models trained via distillation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01486", "pdf": "https://arxiv.org/pdf/2505.01486", "abs": "https://arxiv.org/abs/2505.01486", "authors": ["Mingfeng Tang", "Ziyuan Xie", "Ke Xie", "Hui Huang", "Jianwei Hu", "Ningna Wang", "Xiaohu Guo"], "title": "Aerial Path Online Planning for Urban Scene Updation", "categories": ["cs.RO", "cs.GR"], "comment": null, "summary": "We present the first scene-update aerial path planning algorithm specifically\ndesigned for detecting and updating change areas in urban environments. While\nexisting methods for large-scale 3D urban scene reconstruction focus on\nachieving high accuracy and completeness, they are inefficient for scenarios\nrequiring periodic updates, as they often re-explore and reconstruct entire\nscenes, wasting significant time and resources on unchanged areas. To address\nthis limitation, our method leverages prior reconstructions and change\nprobability statistics to guide UAVs in detecting and focusing on areas likely\nto have changed. Our approach introduces a novel changeability heuristic to\nevaluate the likelihood of changes, driving the planning of two flight paths: a\nprior path informed by static priors and a dynamic real-time path that adapts\nto newly detected changes. The framework integrates surface sampling and\ncandidate view generation strategies, ensuring efficient coverage of change\nareas with minimal redundancy. Extensive experiments on real-world urban\ndatasets demonstrate that our method significantly reduces flight time and\ncomputational overhead, while maintaining high-quality updates comparable to\nfull-scene re-exploration and reconstruction. These contributions pave the way\nfor efficient, scalable, and adaptive UAV-based scene updates in complex urban\nenvironments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01583", "pdf": "https://arxiv.org/pdf/2505.01583", "abs": "https://arxiv.org/abs/2505.01583", "authors": ["Jen-Hao Cheng", "Vivian Wang", "Huayu Wang", "Huapeng Zhou", "Yi-Hao Peng", "Hou-I Liu", "Hsiang-Wei Huang", "Kuang-Ming Chen", "Cheng-Yen Yang", "Wenhao Chai", "Yi-Ling Chen", "Vibhav Vineet", "Qin Cai", "Jenq-Neng Hwang"], "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding causal event relationships and achieving fine-grained temporal\ngrounding in videos remain challenging for vision-language models. Existing\nmethods either compress video tokens to reduce temporal resolution, or treat\nvideos as unsegmented streams, which obscures fine-grained event boundaries and\nlimits the modeling of causal dependencies. We propose TEMPURA (Temporal Event\nMasked Prediction and Understanding for Reasoning in Action), a two-stage\ntraining framework that enhances video temporal understanding. TEMPURA first\napplies masked event prediction reasoning to reconstruct missing events and\ngenerate step-by-step causal explanations from dense event annotations, drawing\ninspiration from effective infilling techniques. TEMPURA then learns to perform\nvideo segmentation and dense captioning to decompose videos into\nnon-overlapping events with detailed, timestamp-aligned descriptions. We train\nTEMPURA on VER, a large-scale dataset curated by us that comprises 1M training\ninstances and 500K videos with temporally aligned event descriptions and\nstructured reasoning steps. Experiments on temporal grounding and highlight\ndetection benchmarks demonstrate that TEMPURA outperforms strong baseline\nmodels, confirming that integrating causal reasoning with fine-grained temporal\nsegmentation leads to improved video understanding.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01656", "pdf": "https://arxiv.org/pdf/2505.01656", "abs": "https://arxiv.org/abs/2505.01656", "authors": ["Chenyang Fan", "Xujie Zhu", "Taige Luo", "Sheng Xu", "Zhulin Chen", "Hongxin Yang"], "title": "A Novel WaveInst-based Network for Tree Trunk Structure Extraction and Pattern Analysis in Forest Inventory", "categories": ["cs.CV"], "comment": null, "summary": "The pattern analysis of tree structure holds significant scientific value for\ngenetic breeding and forestry management. The current trunk and branch\nextraction technologies are mainly LiDAR-based or UAV-based. The former\napproaches obtain high-precision 3D data, but its equipment cost is high and\nthe three-dimensional (3D) data processing is complex. The latter approaches\nefficiently capture canopy information, but they miss the 3-D structure of\ntrees. In order to deal with the branch information extraction from the complex\nbackground interference and occlusion, this work proposes a novel WaveInst\ninstance segmentation framework, involving a discrete wavelet transform, to\nenhance multi-scale edge information for accurately improving tree structure\nextraction. Experimental results of the proposed model show superior\nperformance on SynthTree43k, CaneTree100, Urban Street and our PoplarDataset.\nMoreover, we present a new Phenotypic dataset PoplarDataset, which is dedicated\nto extract tree structure and pattern analysis from artificial forest. The\nproposed method achieves a mean average precision of 49.6 and 24.3 for the\nstructure extraction of mature and juvenile trees, respectively, surpassing the\nexisting state-of-the-art method by 9.9. Furthermore, by in tegrating the\nsegmentation model within the regression model, we accurately achieve\nsignificant tree grown parameters, such as the location of trees, the\ndiameter-at-breast-height of individual trees, and the plant height, from 2D\nimages directly. This study provides a scientific and plenty of data for tree\nstructure analysis in related to the phenotype research, offering a platform\nfor the significant applications in precision forestry, ecological monitoring,\nand intelligent breeding.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01664", "pdf": "https://arxiv.org/pdf/2505.01664", "abs": "https://arxiv.org/abs/2505.01664", "authors": ["Yi-Ming Zhai", "Chuan-Xian Ren", "Hong Yan"], "title": "Soft-Masked Semi-Dual Optimal Transport for Partial Domain Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual domain adaptation aims to learn discriminative and domain-invariant\nrepresentation for an unlabeled target domain by leveraging knowledge from a\nlabeled source domain. Partial domain adaptation (PDA) is a general and\npractical scenario in which the target label space is a subset of the source\none. The challenges of PDA exist due to not only domain shift but also the\nnon-identical label spaces of domains. In this paper, a Soft-masked Semi-dual\nOptimal Transport (SSOT) method is proposed to deal with the PDA problem.\nSpecifically, the class weights of domains are estimated, and then a reweighed\nsource domain is constructed, which is favorable in conducting\nclass-conditional distribution matching with the target domain. A soft-masked\ntransport distance matrix is constructed by category predictions, which will\nenhance the class-oriented representation ability of optimal transport in the\nshared feature space. To deal with large-scale optimal transport problems, the\nsemi-dual formulation of the entropy-regularized Kantorovich problem is\nemployed since it can be optimized by gradient-based algorithms. Further, a\nneural network is exploited to approximate the Kantorovich potential due to its\nstrong fitting ability. This network parametrization also allows the\ngeneralization of the dual variable outside the supports of the input\ndistribution. The SSOT model is built upon neural networks, which can be\noptimized alternately in an end-to-end manner. Extensive experiments are\nconducted on four benchmark datasets to demonstrate the effectiveness of SSOT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01699", "pdf": "https://arxiv.org/pdf/2505.01699", "abs": "https://arxiv.org/abs/2505.01699", "authors": ["Yifan Liu", "Ruichen Yao", "Yaokun Liu", "Ruohan Zong", "Zelin Li", "Yang Zhang", "Dong Wang"], "title": "Component-Based Fairness in Face Attribute Classification with Bayesian Network-informed Meta Learning", "categories": ["cs.CV", "cs.AI", "I.2.10; K.4.1"], "comment": "Accepted by ACM FAccT 2025", "summary": "The widespread integration of face recognition technologies into various\napplications (e.g., access control and personalized advertising) necessitates a\ncritical emphasis on fairness. While previous efforts have focused on\ndemographic fairness, the fairness of individual biological face components\nremains unexplored. In this paper, we focus on face component fairness, a\nfairness notion defined by biological face features. To our best knowledge, our\nwork is the first work to mitigate bias of face attribute prediction at the\nbiological feature level. In this work, we identify two key challenges in\noptimizing face component fairness: attribute label scarcity and attribute\ninter-dependencies, both of which limit the effectiveness of bias mitigation\nfrom previous approaches. To address these issues, we propose \\textbf{B}ayesian\n\\textbf{N}etwork-informed \\textbf{M}eta \\textbf{R}eweighting (BNMR), which\nincorporates a Bayesian Network calibrator to guide an adaptive\nmeta-learning-based sample reweighting process. During the training process of\nour approach, the Bayesian Network calibrator dynamically tracks model bias and\nencodes prior probabilities for face component attributes to overcome the above\nchallenges. To demonstrate the efficacy of our approach, we conduct extensive\nexperiments on a large-scale real-world human face dataset. Our results show\nthat BNMR is able to consistently outperform recent face bias mitigation\nbaselines. Moreover, our results suggest a positive impact of face component\nfairness on the commonly considered demographic fairness (e.g.,\n\\textit{gender}). Our findings pave the way for new research avenues on face\ncomponent fairness, suggesting that face component fairness could serve as a\npotential surrogate objective for demographic fairness. The code for our work\nis publicly\navailable~\\footnote{https://github.com/yliuaa/BNMR-FairCompFace.git}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01883", "pdf": "https://arxiv.org/pdf/2505.01883", "abs": "https://arxiv.org/abs/2505.01883", "authors": ["Yiwen Lu", "Siheng Xiong", "Zhaowei Li"], "title": "Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams", "categories": ["cs.CL"], "comment": null, "summary": "We present a framework for large-scale sentiment and topic analysis of\nTwitter discourse. Our pipeline begins with targeted data collection using\nconflict-specific keywords, followed by automated sentiment labeling via\nmultiple pre-trained models to improve annotation robustness. We examine the\nrelationship between sentiment and contextual features such as timestamp,\ngeolocation, and lexical content. To identify latent themes, we apply Latent\nDirichlet Allocation (LDA) on partitioned subsets grouped by sentiment and\nmetadata attributes. Finally, we develop an interactive visualization interface\nto support exploration of sentiment trends and topic distributions across time\nand regions. This work contributes a scalable methodology for social media\nanalysis in dynamic geopolitical contexts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01980", "pdf": "https://arxiv.org/pdf/2505.01980", "abs": "https://arxiv.org/abs/2505.01980", "authors": ["Theo Guidroz", "Diego Ardila", "Jimmy Li", "Adam Mansour", "Paul Jhun", "Nina Gonzalez", "Xiang Ji", "Mike Sanchez", "Sujay Kakarmath", "Mathias MJ Bellaiche", "Miguel √Ångel Garrido", "Faruk Ahmed", "Divyansh Choudhary", "Jay Hartford", "Chenwei Xu", "Henry Javier Serrano Echeverria", "Yifan Wang", "Jeff Shaffer", "Eric", "Cao", "Yossi Matias", "Avinatan Hassidim", "Dale R Webster", "Yun Liu", "Sho Fujiwara", "Peggy Bui", "Quang Duong"], "title": "LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load", "categories": ["cs.CL"], "comment": null, "summary": "Information on the web, such as scientific publications and Wikipedia, often\nsurpasses users' reading level. To help address this, we used a self-refinement\napproach to develop a LLM capability for minimally lossy text simplification.\nTo validate our approach, we conducted a randomized study involving 4563\nparticipants and 31 texts spanning 6 broad subject areas: PubMed (biomedical\nscientific articles), biology, law, finance, literature/philosophy, and\naerospace/computer science. Participants were randomized to viewing original or\nsimplified texts in a subject area, and answered multiple-choice questions\n(MCQs) that tested their comprehension of the text. The participants were also\nasked to provide qualitative feedback such as task difficulty. Our results\nindicate that participants who read the simplified text answered more MCQs\ncorrectly than their counterparts who read the original text (3.9% absolute\nincrease, p<0.05). This gain was most striking with PubMed (14.6%), while more\nmoderate gains were observed for finance (5.5%), aerospace/computer science\n(3.8%) domains, and legal (3.5%). Notably, the results were robust to whether\nparticipants could refer back to the text while answering MCQs. The absolute\naccuracy decreased by up to ~9% for both original and simplified setups where\nparticipants could not refer back to the text, but the ~4% overall improvement\npersisted. Finally, participants' self-reported perceived ease based on a\nsimplified NASA Task Load Index was greater for those who read the simplified\ntext (absolute change on a 5-point scale 0.33, p<0.05). This randomized study,\ninvolving an order of magnitude more participants than prior works,\ndemonstrates the potential of LLMs to make complex information easier to\nunderstand. Our work aims to enable a broader audience to better learn and make\nuse of expert knowledge available on the web, improving information\naccessibility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01737", "pdf": "https://arxiv.org/pdf/2505.01737", "abs": "https://arxiv.org/abs/2505.01737", "authors": ["Seong Hyeon Park", "Jinwoo Shin"], "title": "Learning Multi-frame and Monocular Prior for Estimating Geometry in Dynamic Scenes", "categories": ["cs.CV"], "comment": null, "summary": "In monocular videos that capture dynamic scenes, estimating the 3D geometry\nof video contents has been a fundamental challenge in computer vision.\nSpecifically, the task is significantly challenged by the object motion, where\nexisting models are limited to predict only partial attributes of the dynamic\nscenes, such as depth or pointmaps spanning only over a pair of frames. Since\nthese attributes are inherently noisy under multiple frames, test-time global\noptimizations are often employed to fully recover the geometry, which is liable\nto failure and incurs heavy inference costs. To address the challenge, we\npresent a new model, coined MMP, to estimate the geometry in a feed-forward\nmanner, which produces a dynamic pointmap representation that evolves over\nmultiple frames. Specifically, based on the recent Siamese architecture, we\nintroduce a new trajectory encoding module to project point-wise dynamics on\nthe representation for each frame, which can provide significantly improved\nexpressiveness for dynamic scenes. In our experiments, we find MMP can achieve\nstate-of-the-art quality in feed-forward pointmap prediction, e.g., 15.1%\nenhancement in the regression error.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02009", "pdf": "https://arxiv.org/pdf/2505.02009", "abs": "https://arxiv.org/abs/2505.02009", "authors": ["Sai Krishna Mendu", "Harish Yenala", "Aditi Gulati", "Shanu Kumar", "Parag Agrawal"], "title": "Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have become integral to various real-world\napplications, leveraging massive, web-sourced datasets like Common Crawl, C4,\nand FineWeb for pretraining. While these datasets provide linguistic data\nessential for high-quality natural language generation, they often contain\nharmful content, such as hate speech, misinformation, and biased narratives.\nTraining LLMs on such unfiltered data risks perpetuating toxic behaviors,\nspreading misinformation, and amplifying societal biases which can undermine\ntrust in LLM-driven applications and raise ethical concerns about their use.\nThis paper presents a large-scale analysis of inappropriate content across\nthese datasets, offering a comprehensive taxonomy that categorizes harmful\nwebpages into Topical and Toxic based on their intent. We also introduce a\nprompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and\na transformer-based model (HarmFormer) for content filtering. Additionally, we\ncreate a new multi-harm open-ended toxicity benchmark (HAVOC) and provide\ncrucial insights into how models respond to adversarial toxic inputs. Upon\npublishing, we will also opensource our model signal on the entire C4 dataset.\nOur work offers insights into ensuring safer LLM pretraining and serves as a\nresource for Responsible AI (RAI) compliance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01743", "pdf": "https://arxiv.org/pdf/2505.01743", "abs": "https://arxiv.org/abs/2505.01743", "authors": ["Siyang Jiang", "Bufang Yang", "Lilin Xu", "Mu Yuan", "Yeerzhati Abudunuer", "Kaiwei Liu", "Liekang Zeng", "Hongkai Chen", "Zhenyu Yan", "Xiaofan Jiang", "Guoliang Xing"], "title": "An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid advancements in Large Vision Language Models (LVLMs) offer the\npotential to surpass conventional labeling by generating richer, more detailed\ndescriptions of on-device human behavior understanding (HBU) in low-resolution\nvision systems, such as depth, thermal, and infrared. However, existing large\nvision language model (LVLM) approaches are unable to understand low-resolution\ndata well as they are primarily designed for high-resolution data, such as RGB\nimages. A quick fixing approach is to caption a large amount of low-resolution\ndata, but it requires a significant amount of labor-intensive annotation\nefforts. In this paper, we propose a novel, labor-saving system, Llambda,\ndesigned to support low-resolution HBU. The core idea is to leverage limited\nlabeled data and a large amount of unlabeled data to guide LLMs in generating\ninformative captions, which can be combined with raw data to effectively\nfine-tune LVLM models for understanding low-resolution videos in HBU. First, we\npropose a Contrastive-Oriented Data Labeler, which can capture\nbehavior-relevant information from long, low-resolution videos and generate\nhigh-quality pseudo labels for unlabeled data via contrastive learning. Second,\nwe propose a Physical-Knowledge Guided Captioner, which utilizes spatial and\ntemporal consistency checks to mitigate errors in pseudo labels. Therefore, it\ncan improve LLMs' understanding of sequential data and then generate\nhigh-quality video captions. Finally, to ensure on-device deployability, we\nemploy LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data.\nWe evaluate Llambda using a region-scale real-world testbed and three distinct\nlow-resolution datasets, and the experiments show that Llambda outperforms\nseveral state-of-the-art LVLM systems up to $40.03\\%$ on average Bert-Score.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["testbed", "annotation", "consistency"], "score": 3}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02032", "pdf": "https://arxiv.org/pdf/2505.02032", "abs": "https://arxiv.org/abs/2505.02032", "authors": ["Anisia Katinskaia"], "title": "An overview of artificial intelligence in computer-assisted language learning", "categories": ["cs.CL"], "comment": null, "summary": "Computer-assisted language learning -- CALL -- is an established research\nfield. We review how artificial intelligence can be applied to support language\nlearning and teaching. The need for intelligent agents that assist language\nlearners and teachers is increasing: the human teacher's time is a scarce and\ncostly resource, which does not scale with growing demand. Further factors\ncontribute to the need for CALL: pandemics and increasing demand for distance\nlearning, migration of large populations, the need for sustainable and\naffordable support for learning, etc. CALL systems are made up of many\ncomponents that perform various functions, and AI is applied to many different\naspects in CALL, corresponding to their own expansive research areas. Most of\nwhat we find in the research literature and in practical use are prototypes or\npartial implementations -- systems that perform some aspects of the overall\ndesired functionality. Complete solutions -- most of them commercial -- are\nfew, because they require massive resources. Recent advances in AI should\nresult in improvements in CALL, yet there is a lack of surveys that focus on AI\nin the context of this research field. This paper aims to present a perspective\non the AI methods that can be employed for language learning from a position of\na developer of a CALL system. We also aim to connect work from different\ndisciplines, to build bridges for interdisciplinary work.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01746", "pdf": "https://arxiv.org/pdf/2505.01746", "abs": "https://arxiv.org/abs/2505.01746", "authors": ["Xingqun Qi", "Yatian Wang", "Hengyuan Zhang", "Jiahao Pan", "Wei Xue", "Shanghang Zhang", "Wenhan Luo", "Qifeng Liu", "Yike Guo"], "title": "Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion", "categories": ["cs.CV"], "comment": "Accepted as ICLR 2025 (Spotlight)", "summary": "Generating gestures from human speech has gained tremendous progress in\nanimating virtual avatars. While the existing methods enable synthesizing\ngestures cooperated by individual self-talking, they overlook the practicality\nof concurrent gesture modeling with two-person interactive conversations.\nMoreover, the lack of high-quality datasets with concurrent co-speech gestures\nalso limits handling this issue. To fulfill this goal, we first construct a\nlarge-scale concurrent co-speech gesture dataset that contains more than 7M\nframes for diverse two-person interactive posture sequences, dubbed GES-Inter.\nAdditionally, we propose Co$^3$Gesture, a novel framework that enables coherent\nconcurrent co-speech gesture synthesis including two-person interactive\nmovements. Considering the asymmetric body dynamics of two speakers, our\nframework is built upon two cooperative generation branches conditioned on\nseparated speaker audio. Specifically, to enhance the coordination of human\npostures with respect to corresponding speaker audios while interacting with\nthe conversational partner, we present a Temporal Interaction Module (TIM). TIM\ncan effectively model the temporal association representation between two\nspeakers' gesture sequences as interaction guidance and fuse it into the\nconcurrent gesture generation. Then, we devise a mutual attention mechanism to\nfurther holistically boost learning dependencies of interacted concurrent\nmotions, thereby enabling us to generate vivid and coherent gestures. Extensive\nexperiments demonstrate that our method outperforms the state-of-the-art models\non our newly collected GES-Inter dataset. The dataset and source code are\npublicly available at\n\\href{https://mattie-e.github.io/Co3/}{\\textit{https://mattie-e.github.io/Co3/}}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02078", "pdf": "https://arxiv.org/pdf/2505.02078", "abs": "https://arxiv.org/abs/2505.02078", "authors": ["Joy Lim Jia Yin", "Daniel Zhang-Li", "Jifan Yu", "Haoxuan Li", "Shangqing Tu", "Yuanchun Wang", "Zhiyuan Liu", "Huiqin Liu", "Lei Hou", "Juanzi Li", "Bin Xu"], "title": "LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures", "summary": "Evaluating the quality of slide-based multimedia instruction is challenging.\nExisting methods like manual assessment, reference-based metrics, and large\nlanguage model evaluators face limitations in scalability, context capture, or\nbias. In this paper, we introduce LecEval, an automated metric grounded in\nMayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal\nknowledge acquisition in slide-based learning. LecEval assesses effectiveness\nusing four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical\nStructure (LS), and Audience Engagement (AE). We curate a large-scale dataset\nof over 2,000 slides from more than 50 online course videos, annotated with\nfine-grained human ratings across these rubrics. A model trained on this\ndataset demonstrates superior accuracy and adaptability compared to existing\nmetrics, bridging the gap between automated and human assessments. We release\nour dataset and toolkits at https://github.com/JoylimJY/LecEval.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02146", "pdf": "https://arxiv.org/pdf/2505.02146", "abs": "https://arxiv.org/abs/2505.02146", "authors": ["Shouyang Dong", "Yuanbo Wen", "Jun Bi", "Di Huang", "Jiaming Guo", "Jianxing Xu", "Ruibai Xu", "Xinkai Song", "Yifan Hao", "Xuehai Zhou", "Tianshi Chen", "Qi Guo", "Yunji Chen"], "title": "QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach", "categories": ["cs.CL", "cs.LG", "cs.PL"], "comment": "Accepted to OSDI 2025", "summary": "Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been\nwidely deployed in industrial data centers, which requires to develop multiple\nlow-level tensor programs for different platforms. An attractive solution to\nrelieve the programming burden is to transcompile the legacy code of one\nplatform to others. However, current transcompilation techniques struggle with\neither tremendous manual efforts or functional incorrectness, rendering \"Write\nOnce, Run Anywhere\" of tensor programs an open question.\n  We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically\ntranslating tensor programs across DLS via both large language models (LLMs)\nand symbolic program synthesis, i.e., neural-symbolic synthesis. The key\ninsight is leveraging the powerful code generation ability of LLM to make\ncostly search-based symbolic synthesis computationally tractable. Concretely,\nwe propose multiple LLM-assisted compilation passes via pre-defined\nmeta-prompts for program transformation. During each program transformation,\nefficient symbolic program synthesis is employed to repair incorrect code\nsnippets with a limited scale. To attain high performance, we propose a\nhierarchical auto-tuning approach to systematically explore both the parameters\nand sequences of transformation passes. Experiments on 4 DLS with distinct\nprogramming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA,\nAMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler\ncorrectly translates different tensor programs at the accuracy of 95% on\naverage, and the performance of translated programs achieves up to 2.0x over\nvendor-provided manually-optimized libraries. As a result, the programming\nproductivity of DLS is improved by up to 96.0x via transcompiling legacy tensor\nprograms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "code generation"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01805", "pdf": "https://arxiv.org/pdf/2505.01805", "abs": "https://arxiv.org/abs/2505.01805", "authors": ["Yuchang Jiang", "Maxim Neumann"], "title": "Not Every Tree Is a Forest: Benchmarking Forest Types from Satellite Remote Sensing", "categories": ["cs.CV"], "comment": null, "summary": "Developing accurate and reliable models for forest types mapping is critical\nto support efforts for halting deforestation and for biodiversity conservation\n(such as European Union Deforestation Regulation (EUDR)). This work introduces\nForTy, a benchmark for global-scale FORest TYpes mapping using multi-temporal\nsatellite data1. The benchmark comprises 200,000 time series of image patches,\neach consisting of Sentinel-2, Sentinel-1, climate, and elevation data. Each\ntime series captures variations at monthly or seasonal cadence. Per-pixel\nannotations, including forest types and other land use classes, support image\nsegmentation tasks. Unlike most existing land use products that often\ncategorize all forest areas into a single class, our benchmark differentiates\nbetween three forest types classes: natural forest, planted forest, and tree\ncrops. By leveraging multiple public data sources, we achieve global coverage\nwith this benchmark. We evaluate the forest types dataset using several\nbaseline models, including convolution neural networks and transformer-based\nmodels. Additionally, we propose a novel transformer-based model specifically\ndesigned to handle multi-modal, multi-temporal satellite data for forest types\nmapping. Our experimental results demonstrate that the proposed model surpasses\nthe baseline models in performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01823", "pdf": "https://arxiv.org/pdf/2505.01823", "abs": "https://arxiv.org/abs/2505.01823", "authors": ["Nitin Rai", "Arnold W. Schumann", "Nathan Boyd"], "title": "PhytoSynth: Leveraging Multi-modal Generative Models for Crop Disease Data Generation with Novel Benchmarking and Prompt Engineering Approach", "categories": ["cs.CV", "cs.AI", "cs.ET"], "comment": null, "summary": "Collecting large-scale crop disease images in the field is labor-intensive\nand time-consuming. Generative models (GMs) offer an alternative by creating\nsynthetic samples that resemble real-world images. However, existing research\nprimarily relies on Generative Adversarial Networks (GANs)-based image-to-image\ntranslation and lack a comprehensive analysis of computational requirements in\nagriculture. Therefore, this research explores a multi-modal text-to-image\napproach for generating synthetic crop disease images and is the first to\nprovide computational benchmarking in this context. We trained three Stable\nDiffusion (SD) variants-SDXL, SD3.5M (medium), and SD3.5L (large)-and\nfine-tuned them using Dreambooth and Low-Rank Adaptation (LoRA) fine-tuning\ntechniques to enhance generalization. SD3.5M outperformed the others, with an\naverage memory usage of 18 GB, power consumption of 180 W, and total energy use\nof 1.02 kWh/500 images (0.002 kWh per image) during inference task. Our results\ndemonstrate SD3.5M's ability to generate 500 synthetic images from just 36\nin-field samples in 1.5 hours. We recommend SD3.5M for efficient crop disease\ndata generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01837", "pdf": "https://arxiv.org/pdf/2505.01837", "abs": "https://arxiv.org/abs/2505.01837", "authors": ["Xiangru Li", "Wei Song", "Yingda Huang", "Wei Meng", "Le Chang"], "title": "CVVNet: A Cross-Vertical-View Network for Gait Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Gait recognition enables contact-free, long-range person identification that\nis robust to clothing variations and non-cooperative scenarios. While existing\nmethods perform well in controlled indoor environments, they struggle with\ncross-vertical view scenarios, where surveillance angles vary significantly in\nelevation. Our experiments show up to 60\\% accuracy degradation in low-to-high\nvertical view settings due to severe deformations and self-occlusions of key\nanatomical features. Current CNN and self-attention-based methods fail to\neffectively handle these challenges, due to their reliance on single-scale\nconvolutions or simplistic attention mechanisms that lack effective\nmulti-frequency feature integration. To tackle this challenge, we propose\nCVVNet (Cross-Vertical-View Network), a frequency aggregation architecture\nspecifically designed for robust cross-vertical-view gait recognition. CVVNet\nemploys a High-Low Frequency Extraction module (HLFE) that adopts parallel\nmulti-scale convolution/max-pooling path and self-attention path as high- and\nlow-frequency mixers for effective multi-frequency feature extraction from\ninput silhouettes. We also introduce the Dynamic Gated Aggregation (DGA)\nmechanism to adaptively adjust the fusion ratio of high- and low-frequency\nfeatures. The integration of our core Multi-Scale Attention Gated Aggregation\n(MSAGA) module, HLFE and DGA enables CVVNet to effectively handle distortions\nfrom view changes, significantly improving the recognition robustness across\ndifferent vertical views. Experimental results show that our CVVNet achieves\nstate-of-the-art performance, with $8.6\\%$ improvement on DroneGait and $2\\%$\non Gait3D compared with the best existing methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01838", "pdf": "https://arxiv.org/pdf/2505.01838", "abs": "https://arxiv.org/abs/2505.01838", "authors": ["Chenghong Li", "Hongjie Liao", "Yihao Zhi", "Xihe Yang", "Zhengwentai Sun", "Jiahao Chang", "Shuguang Cui", "Xiaoguang Han"], "title": "MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization", "categories": ["cs.CV"], "comment": "project page: https://kevinlee09.github.io/research/MVHumanNet++/.\n  arXiv admin note: substantial text overlap with arXiv:2312.02963", "summary": "In this era, the success of large language models and text-to-image models\ncan be attributed to the driving force of large-scale datasets. However, in the\nrealm of 3D vision, while significant progress has been achieved in\nobject-centric tasks through large-scale datasets like Objaverse and MVImgNet,\nhuman-centric tasks have seen limited advancement, largely due to the absence\nof a comparable large-scale human dataset. To bridge this gap, we present\nMVHumanNet++, a dataset that comprises multi-view human action sequences of\n4,500 human identities. The primary focus of our work is on collecting human\ndata that features a large number of diverse identities and everyday clothing\nusing multi-view human capture systems, which facilitates easily scalable data\ncollection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences\nand 645 million frames with extensive annotations, including human masks,\ncamera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and\ncorresponding textual descriptions. Additionally, the proposed MVHumanNet++\ndataset is enhanced with newly processed normal maps and depth maps,\nsignificantly expanding its applicability and utility for advanced\nhuman-centric research. To explore the potential of our proposed MVHumanNet++\ndataset in various 2D and 3D visual tasks, we conducted several pilot studies\nto demonstrate the performance improvements and effective applications enabled\nby the scale provided by MVHumanNet++. As the current largest-scale 3D human\ndataset, we hope that the release of MVHumanNet++ dataset with annotations will\nfoster further innovations in the domain of 3D human-centric tasks at scale.\nMVHumanNet++ is publicly available at\nhttps://kevinlee09.github.io/research/MVHumanNet++/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01950", "pdf": "https://arxiv.org/pdf/2505.01950", "abs": "https://arxiv.org/abs/2505.01950", "authors": ["Dong Xing", "Xianxun Zhu", "Wei Zhou", "Qika Lin", "Hang Yang", "Yuqing Wang"], "title": "Segment Any RGB-Thermal Model with Language-aided Distillation", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2412.04220 by other authors", "summary": "The recent Segment Anything Model (SAM) demonstrates strong instance\nsegmentation performance across various downstream tasks. However, SAM is\ntrained solely on RGB data, limiting its direct applicability to RGB-thermal\n(RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for\nscene understanding in adverse weather and lighting conditions, such as low\nlight and overexposure, we propose a novel framework, SARTM, which customizes\nthe powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash\nthe potential of SAM while introduce semantic understanding modules for RGB-T\ndata pairs. Specifically, our framework first involves fine tuning the original\nSAM by adding extra LoRA layers, aiming at preserving SAM's strong\ngeneralization and segmentation capabilities for downstream tasks. Secondly, we\nintroduce language information as guidance for training our SARTM. To address\ncross-modal inconsistencies, we introduce a Cross-Modal Knowledge\nDistillation(CMKD) module that effectively achieves modality adaptation while\nmaintaining its generalization capabilities. This semantic module enables the\nminimization of modality gaps and alleviates semantic ambiguity, facilitating\nthe combination of any modality under any visual conditions. Furthermore, we\nenhance the segmentation performance by adjusting the segmentation head of SAM\nand incorporating an auxiliary semantic segmentation head, which integrates\nmulti-scale features for effective fusion. Extensive experiments are conducted\nacross three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900,\nand FMB. Both quantitative and qualitative results consistently demonstrate\nthat the proposed SARTM significantly outperforms state-of-the-art approaches\nacross a variety of conditions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02410", "pdf": "https://arxiv.org/pdf/2505.02410", "abs": "https://arxiv.org/abs/2505.02410", "authors": ["Krzysztof Ociepa", "≈Åukasz Flis", "Krzysztof Wr√≥bel", "Adrian Gwo≈∫dziej", "Remigiusz Kinas"], "title": "Bielik 11B v2 Technical Report", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": null, "summary": "We present Bielik 11B v2, a state-of-the-art language model optimized for\nPolish text processing. Built on the Mistral 7B v0.2 architecture and scaled to\n11B parameters using depth up-scaling, this model demonstrates exceptional\nperformance across Polish language benchmarks while maintaining strong\ncross-lingual capabilities. We introduce two key technical innovations:\nWeighted Instruction Cross-Entropy Loss, which optimizes learning across\ndiverse instruction types by assigning quality-based weights to training\nexamples, and Adaptive Learning Rate, which dynamically adjusts based on\ncontext length. Comprehensive evaluation across multiple benchmarks\ndemonstrates that Bielik 11B v2 outperforms many larger models, including those\nwith 2-6 times more parameters, and significantly surpasses other specialized\nPolish language models on tasks ranging from linguistic understanding to\ncomplex reasoning. The model's parameter efficiency and extensive quantization\noptions enable deployment across various hardware configurations, advancing\nPolish language AI capabilities and establishing new benchmarks for\nresource-efficient language modeling in less-represented languages.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02005", "pdf": "https://arxiv.org/pdf/2505.02005", "abs": "https://arxiv.org/abs/2505.02005", "authors": ["Zhenxing Mi", "Ping Yin", "Xue Xiao", "Dan Xu"], "title": "Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields", "categories": ["cs.CV"], "comment": "15 pages, 9 figures", "summary": "Recent NeRF methods on large-scale scenes have underlined the importance of\nscene decomposition for scalable NeRFs. Although achieving reasonable\nscalability, there are several critical problems remaining unexplored, i.e.,\nlearnable decomposition, modeling scene heterogeneity, and modeling efficiency.\nIn this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash\nExperts (HMoHE) network that addresses these challenges within a unified\nframework. It is a highly scalable NeRF that learns heterogeneous decomposition\nand heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end\nmanner. In our framework, a gating network learns to decomposes scenes and\nallocates 3D points to specialized NeRF experts. This gating network is\nco-optimized with the experts, by our proposed Sparsely Gated Mixture of\nExperts (MoE) NeRF framework. We incorporate a hash-based gating network and\ndistinct heterogeneous hash experts. The hash-based gating efficiently learns\nthe decomposition of the large-scale scene. The distinct heterogeneous hash\nexperts consist of hash grids of different resolution ranges, enabling\neffective learning of the heterogeneous representation of different scene\nparts. These design choices make our framework an end-to-end and highly\nscalable NeRF solution for real-world large-scale scene modeling to achieve\nboth quality and efficiency. We evaluate our accuracy and scalability on\nexisting large-scale NeRF datasets and a new dataset with very large-scale\nscenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our\napproach can be easily scaled to various large-scale scenes and achieve\nstate-of-the-art scene rendering accuracy. Furthermore, our method exhibits\nsignificant efficiency, with an 8x acceleration in training and a 16x\nacceleration in rendering compared to Switch-NeRF. Codes will be released in\nhttps://github.com/MiZhenxing/Switch-NeRF.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02018", "pdf": "https://arxiv.org/pdf/2505.02018", "abs": "https://arxiv.org/abs/2505.02018", "authors": ["Meng-Hao Guo", "Jiajun Xu", "Yi Zhang", "Jiaxi Song", "Haoyang Peng", "Yi-Xuan Deng", "Xinzhi Dong", "Kiyohiro Nakayama", "Zhengyang Geng", "Chen Wang", "Bolin Ni", "Guo-Wei Yang", "Yongming Rao", "Houwen Peng", "Han Hu", "Gordon Wetzstein", "Shi-min Hu"], "title": "R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation", "categories": ["cs.CV"], "comment": "18pages", "summary": "Reasoning stands as a cornerstone of intelligence, enabling the synthesis of\nexisting knowledge to solve complex problems. Despite remarkable progress,\nexisting reasoning benchmarks often fail to rigorously evaluate the nuanced\nreasoning capabilities required for complex, real-world problemsolving,\nparticularly in multi-disciplinary and multimodal contexts. In this paper, we\nintroduce a graduate-level, multi-disciplinary, EnglishChinese benchmark,\ndubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of\nboth language and multimodal models. RBench spans 1,094 questions across 108\nsubjects for language model evaluation and 665 questions across 83 subjects for\nmultimodal model testing in both English and Chinese. These questions are\nmeticulously curated to ensure rigorous difficulty calibration, subject\nbalance, and crosslinguistic alignment, enabling the assessment to be an\nOlympiad-level multi-disciplinary benchmark. We evaluate widely used models,\nincluding OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate\nthat advanced models perform poorly on complex reasoning, especially multimodal\nreasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy\non our multimodal evaluation. Data and code are made publicly available at\nhere.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01433", "pdf": "https://arxiv.org/pdf/2505.01433", "abs": "https://arxiv.org/abs/2505.01433", "authors": ["Cong Qi", "Hanzhang Fang", "Siqi jiang", "Tianxing Hu", "Wei Zhi"], "title": "Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations", "categories": ["q-bio.QM", "cs.CL", "cs.LG"], "comment": null, "summary": "Understanding the binding specificity between T-cell receptors (TCRs) and\npeptide-major histocompatibility complexes (pMHCs) is central to immunotherapy\nand vaccine development. However, current predictive models struggle with\ngeneralization, especially in data-scarce settings and when faced with novel\nepitopes. We present LANTERN (Large lAnguage model-powered TCR-Enhanced\nRecognition Network), a deep learning framework that combines large-scale\nprotein language models with chemical representations of peptides. By encoding\nTCR \\b{eta}-chain sequences using ESM-1b and transforming peptide sequences\ninto SMILES strings processed by MolFormer, LANTERN captures rich biological\nand chemical features critical for TCR-peptide recognition. Through extensive\nbenchmarking against existing models such as ChemBERTa, TITAN, and NetTCR,\nLANTERN demonstrates superior performance, particularly in zero-shot and\nfew-shot learning scenarios. Our model also benefits from a robust negative\nsampling strategy and shows significant clustering improvements via embedding\nanalysis. These results highlight the potential of LANTERN to advance TCR-pMHC\nbinding prediction and support the development of personalized immunotherapies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01636", "pdf": "https://arxiv.org/pdf/2505.01636", "abs": "https://arxiv.org/abs/2505.01636", "authors": ["Amit Rath"], "title": "Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; H.2.8; D.2.13"], "comment": "21 pages, 2 figures", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and task generalization. However, their\napplication to structured data analysis remains fragile due to inconsistencies\nin schema interpretation, misalignment between user intent and model output,\nand limited mechanisms for self-correction when failures occur. This paper\nintroduces the STROT Framework (Structured Task Reasoning and Output\nTransformation), a method for structured prompting and feedback-driven\ntransformation logic generation aimed at improving the reliability and semantic\nalignment of LLM-based analytical workflows. STROT begins with lightweight\nschema introspection and sample-based field classification, enabling dynamic\ncontext construction that captures both the structure and statistical profile\nof the input data. This contextual information is embedded in structured\nprompts that guide the model toward generating task-specific, interpretable\noutputs. To address common failure modes in complex queries, STROT incorporates\na refinement mechanism in which the model iteratively revises its outputs based\non execution feedback and validation signals. Unlike conventional approaches\nthat rely on static prompts or single-shot inference, STROT treats the LLM as a\nreasoning agent embedded within a controlled analysis loop -- capable of\nadjusting its output trajectory through planning and correction. The result is\na robust and reproducible framework for reasoning over structured data with\nLLMs, applicable to diverse data exploration and analysis tasks where\ninterpretability, stability, and correctness are essential.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02075", "pdf": "https://arxiv.org/pdf/2505.02075", "abs": "https://arxiv.org/abs/2505.02075", "authors": ["Volodymyr Havrylov", "Haiwen Huang", "Dan Zhang", "Andreas Geiger"], "title": "Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Vision Foundation Models (VFMs) are large-scale, pre-trained models that\nserve as general-purpose backbones for various computer vision tasks. As VFMs'\npopularity grows, there is an increasing interest in understanding their\neffectiveness for dense prediction tasks. However, VFMs typically produce\nlow-resolution features, limiting their direct applicability in this context.\nOne way to tackle this limitation is by employing a task-agnostic feature\nupsampling module that refines VFM features resolution. To assess the\neffectiveness of this approach, we investigate Interactive Segmentation (IS) as\na novel benchmark for evaluating feature upsampling methods on VFMs. Due to its\ninherent multimodal input, consisting of an image and a set of user-defined\nclicks, as well as its dense mask output, IS creates a challenging environment\nthat demands comprehensive visual scene understanding. Our benchmarking\nexperiments show that selecting appropriate upsampling strategies significantly\nimproves VFM features quality. The code is released at\nhttps://github.com/havrylovv/iSegProbe", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02109", "pdf": "https://arxiv.org/pdf/2505.02109", "abs": "https://arxiv.org/abs/2505.02109", "authors": ["Yingkai Zhang", "Zeqiang Lai", "Tao Zhang", "Ying Fu", "Chenghu Zhou"], "title": "Unaligned RGB Guided Hyperspectral Image Super-Resolution with Spatial-Spectral Concordance", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral images super-resolution aims to improve the spatial resolution,\nyet its performance is often limited at high-resolution ratios. The recent\nadoption of high-resolution reference images for super-resolution is driven by\nthe poor spatial detail found in low-resolution HSIs, presenting it as a\nfavorable method. However, these approaches cannot effectively utilize\ninformation from the reference image, due to the inaccuracy of alignment and\nits inadequate interaction between alignment and fusion modules. In this paper,\nwe introduce a Spatial-Spectral Concordance Hyperspectral Super-Resolution\n(SSC-HSR) framework for unaligned reference RGB guided HSI SR to address the\nissues of inaccurate alignment and poor interactivity of the previous\napproaches. Specifically, to ensure spatial concordance, i.e., align images\nmore accurately across resolutions and refine textures, we construct a\nTwo-Stage Image Alignment with a synthetic generation pipeline in the image\nalignment module, where the fine-tuned optical flow model can produce a more\naccurate optical flow in the first stage and warp model can refine damaged\ntextures in the second stage. To enhance the interaction between alignment and\nfusion modules and ensure spectral concordance during reconstruction, we\npropose a Feature Aggregation module and an Attention Fusion module. In the\nfeature aggregation module, we introduce an Iterative Deformable Feature\nAggregation block to achieve significant feature matching and texture\naggregation with the fusion multi-scale results guidance, iteratively\ngenerating learnable offset. Besides, we introduce two basic spectral-wise\nattention blocks in the attention fusion module to model the inter-spectra\ninteractions. Extensive experiments on three natural or remote-sensing datasets\nshow that our method outperforms state-of-the-art approaches on both\nquantitative and qualitative evaluations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02206", "pdf": "https://arxiv.org/pdf/2505.02206", "abs": "https://arxiv.org/abs/2505.02206", "authors": ["Lei Mao", "Yuanhe Tian", "Yan Song"], "title": "DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "19 pages, 3 figures", "summary": "Genome modeling conventionally treats gene sequence as a language, reflecting\nits structured motifs and long-range dependencies analogous to linguistic units\nand organization principles such as words and syntax. Recent studies utilize\nadvanced neural networks, ranging from convolutional and recurrent models to\nTransformer-based models, to capture contextual information of gene sequence,\nwith the primary goal of obtaining effective gene sequence representations and\nthus enhance the models' understanding of various running gene samples.\nHowever, these approaches often directly apply language modeling techniques to\ngene sequences and do not fully consider the intrinsic information organization\nin them, where they do not consider how units at different granularities\ncontribute to representation. In this paper, we propose DNAZEN, an enhanced\ngenomic representation framework designed to learn from various granularities\nin gene sequences, including small polymers and G-grams that are combinations\nof several contiguous polymers. Specifically, we extract the G-grams from\nlarge-scale genomic corpora through an unsupervised approach to construct the\nG-gram vocabulary, which is used to provide G-grams in the learning process of\nDNA sequences through dynamically matching from running gene samples. A\nTransformer-based G-gram encoder is also proposed and the matched G-grams are\nfed into it to compute their representations and integrated into the encoder\nfor basic unit (E4BU), which is responsible for encoding small units and\nmaintaining the learning and inference process. To further enhance the learning\nprocess, we propose whole G-gram masking to train DNAZEN, where the model\nlargely favors the selection of each entire G-gram to mask rather than an\nordinary masking mechanism performed on basic units. Experiments on benchmark\ndatasets demonstrate the effectiveness of DNAZEN on various downstream tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02176", "pdf": "https://arxiv.org/pdf/2505.02176", "abs": "https://arxiv.org/abs/2505.02176", "authors": ["Samuel Webster", "Adam Czajka"], "title": "Saliency-Guided Training for Fingerprint Presentation Attack Detection", "categories": ["cs.CV"], "comment": "19 pages (8 main, 2 references, 9 appendix), 2 figures, 19 tables (2\n  main, 17 appendix)", "summary": "Saliency-guided training, which directs model learning to important regions\nof images, has demonstrated generalization improvements across various\nbiometric presentation attack detection (PAD) tasks. This paper presents its\nfirst application to fingerprint PAD. We conducted a 50-participant study to\ncreate a dataset of 800 human-annotated fingerprint perceptually-important\nmaps, explored alongside algorithmically-generated \"pseudosaliency,\" including\nminutiae-based, image quality-based, and autoencoder-based saliency maps.\nEvaluating on the 2021 Fingerprint Liveness Detection Competition testing set,\nwe explore various configurations within five distinct training scenarios to\nassess the impact of saliency-guided training on accuracy and generalization.\nOur findings demonstrate the effectiveness of saliency-guided training for\nfingerprint PAD in both limited and large data contexts, and we present a\nconfiguration capable of earning the first place on the LivDet-2021 benchmark.\nOur results highlight saliency-guided training's promise for increased model\ngeneralization capabilities, its effectiveness when data is limited, and its\npotential to scale to larger datasets in fingerprint PAD. All collected\nsaliency data and trained models are released with the paper to support\nreproducible research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02391", "pdf": "https://arxiv.org/pdf/2505.02391", "abs": "https://arxiv.org/abs/2505.02391", "authors": ["Jiarui Yao", "Yifan Hao", "Hanning Zhang", "Hanze Dong", "Wei Xiong", "Nan Jiang", "Tong Zhang"], "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02178", "pdf": "https://arxiv.org/pdf/2505.02178", "abs": "https://arxiv.org/abs/2505.02178", "authors": ["Shubhendu Jena", "Amine Ouasfi", "Mae Younes", "Adnane Boukhayma"], "title": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery", "categories": ["cs.CV"], "comment": "Project page : https://shubhendu-jena.github.io/Sparfels/", "summary": "We present a method for Sparse view reconstruction with surface element\nsplatting that runs within 3 minutes on a consumer grade GPU. While few methods\naddress sparse radiance field learning from noisy or unposed sparse cameras,\nshape recovery remains relatively underexplored in this setting. Several\nradiance and shape learning test-time optimization methods address the sparse\nposed setting by learning data priors or using combinations of external\nmonocular geometry priors. Differently, we propose an efficient and simple\npipeline harnessing a single recent 3D foundation model. We leverage its\nvarious task heads, notably point maps and camera initializations to\ninstantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image\ncorrespondences to guide camera optimization midst 2DGS training. Key to our\ncontribution is a novel formulation of splatted color variance along rays,\nwhich can be computed efficiently. Reducing this moment in training leads to\nmore accurate shape reconstructions. We demonstrate state-of-the-art\nperformances in the sparse uncalibrated setting in reconstruction and novel\nview benchmarks based on established multi-view datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02639", "pdf": "https://arxiv.org/pdf/2505.02639", "abs": "https://arxiv.org/abs/2505.02639", "authors": ["Xuan Lin", "Qingrui Liu", "Hongxin Xiang", "Daojian Zeng", "Xiangxiang Zeng"], "title": "Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted for publication at IJCAI 2025", "summary": "Chemical reaction and retrosynthesis prediction are fundamental tasks in drug\ndiscovery. Recently, large language models (LLMs) have shown potential in many\ndomains. However, directly applying LLMs to these tasks faces two major\nchallenges: (i) lacking a large-scale chemical synthesis-related instruction\ndataset; (ii) ignoring the close correlation between reaction and\nretrosynthesis prediction for the existing fine-tuning strategies. To address\nthese challenges, we propose ChemDual, a novel LLM framework for accurate\nchemical synthesis. Specifically, considering the high cost of data acquisition\nfor reaction and retrosynthesis, ChemDual regards the\nreaction-and-retrosynthesis of molecules as a related\nrecombination-and-fragmentation process and constructs a large-scale of 4.4\nmillion instruction dataset. Furthermore, ChemDual introduces an enhanced\nLLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy,\nto jointly optimize the process of recombination and fragmentation as well as\nthe tasks between reaction and retrosynthesis prediction. Extensive experiments\non Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves\nstate-of-the-art performance in both predictions of reaction and\nretrosynthesis, outperforming the existing conventional single-task approaches\nand the general open-source LLMs. Through molecular docking analysis, ChemDual\ngenerates compounds with diverse and strong protein binding affinity, further\nhighlighting its strong potential in drug design.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02192", "pdf": "https://arxiv.org/pdf/2505.02192", "abs": "https://arxiv.org/abs/2505.02192", "authors": ["Wenchuan Wang", "Mengqi Huang", "Yijing Tu", "Zhendong Mao"], "title": "DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Customized text-to-video generation with pre-trained large-scale models has\nrecently garnered significant attention through focusing on identity and motion\nconsistency. Existing works typically follow the isolated customized paradigm,\nwhere the subject identity or motion dynamics are customized exclusively.\nHowever, this paradigm completely ignores the intrinsic mutual constraints and\nsynergistic interdependencies between identity and motion, resulting in\nidentity-motion conflicts throughout the generation process that systematically\ndegrades. To address this, we introduce DualReal, a novel framework that,\nemploys adaptive joint training to collaboratively construct interdependencies\nbetween dimensions. Specifically, DualReal is composed of two units: (1)\nDual-aware Adaptation dynamically selects a training phase (i.e., identity or\nmotion), learns the current information guided by the frozen dimension prior,\nand employs a regularization strategy to avoid knowledge leakage; (2)\nStageBlender Controller leverages the denoising stages and Diffusion\nTransformer depths to guide different dimensions with adaptive granularity,\navoiding conflicts at various stages and ultimately achieving lossless fusion\nof identity and motion patterns. We constructed a more comprehensive benchmark\nthan existing methods. The experimental results show that DualReal improves\nCLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top\nperformance on nearly all motion quality metrics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "dimension"], "score": 3}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02707", "pdf": "https://arxiv.org/pdf/2505.02707", "abs": "https://arxiv.org/abs/2505.02707", "authors": ["Yemin Shi", "Yu Shu", "Siwei Dong", "Guangyi Liu", "Jaward Sesay", "Jingwen Li", "Zhiting Hu"], "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play", "categories": ["cs.AI", "cs.CL", "cs.SD"], "comment": "18 pages, 7 figures, Website: https://voila.maitrix.org", "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02811", "pdf": "https://arxiv.org/pdf/2505.02811", "abs": "https://arxiv.org/abs/2505.02811", "authors": ["Diji Yang", "Linda Zeng", "Jinmeng Rao", "Yi Zhang"], "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "Proceedings of the 48th International ACM SIGIR 2025", "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance.\n  This paper aims to address these limitations by introducing a new framework,\n\\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and\nmulti-round retrieval capabilities. To train SIM-RAG, we first let a RAG system\nself-practice multi-round retrieval, augmenting existing question-answer pairs\nwith intermediate inner monologue reasoning steps to generate synthetic\ntraining data. For each pair, the system may explore multiple retrieval paths,\nwhich are labeled as successful if they reach the correct answer and\nunsuccessful otherwise. Using this data, we train a lightweight information\nsufficiency Critic. At inference time, the Critic evaluates whether the RAG\nsystem has retrieved sufficient information at each round, guiding retrieval\ndecisions and improving system-level self-awareness through in-context\nreinforcement learning.\n  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an\neffective multi-round RAG solution. Furthermore, this framework is\nsystem-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02255", "pdf": "https://arxiv.org/pdf/2505.02255", "abs": "https://arxiv.org/abs/2505.02255", "authors": ["Jakub WƒÖsala", "Bart≈Çomiej Wrzalski", "Kornelia Noculak", "Yuliia Tarasenko", "Oliwer Krupa", "Jan Koco≈Ñ", "Grzegorz Chodak"], "title": "Enhancing AI Face Realism: Cost-Efficient Quality Improvement in Distilled Diffusion Models with a Fully Synthetic Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "25th International Conference on Computational Science", "summary": "This study presents a novel approach to enhance the cost-to-quality ratio of\nimage generation with diffusion models. We hypothesize that differences between\ndistilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are\nconsistent and, therefore, learnable within a specialized domain, like portrait\ngeneration. We generate a synthetic paired dataset and train a fast\nimage-to-image translation head. Using two sets of low- and high-quality\nsynthetic images, our model is trained to refine the output of a distilled\ngenerator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like\nFLUX.1-dev, which is more computationally intensive. Our results show that the\npipeline, which combines a distilled version of a large generative model with\nour enhancement layer, delivers similar photorealistic portraits to the\nbaseline version with up to an 82% decrease in computational cost compared to\nFLUX.1-dev. This study demonstrates the potential for improving the efficiency\nof AI solutions involving large-scale image generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02830", "pdf": "https://arxiv.org/pdf/2505.02830", "abs": "https://arxiv.org/abs/2505.02830", "authors": ["Qingqiu Li", "Zihang Cui", "Seongsu Bae", "Jilan Xu", "Runtian Yuan", "Yuejie Zhang", "Rui Feng", "Quanli Shen", "Xiaobo Zhang", "Junjun He", "Shujun Wang"], "title": "AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Chest X-rays (CXRs) are the most frequently performed imaging examinations in\nclinical settings. Recent advancements in Large Multimodal Models (LMMs) have\nenabled automated CXR interpretation, enhancing diagnostic accuracy and\nefficiency. However, despite their strong visual understanding, current Medical\nLMMs (MLMMs) still face two major challenges: (1) Insufficient region-level\nunderstanding and interaction, and (2) Limited accuracy and interpretability\ndue to single-step reasoning. In this paper, we empower MLMMs with\nanatomy-centric reasoning capabilities to enhance their interactivity and\nexplainability. Specifically, we first propose an Anatomical Ontology-Guided\nReasoning (AOR) framework, which centers on cross-modal region-level\ninformation to facilitate multi-step reasoning. Next, under the guidance of\nexpert physicians, we develop AOR-Instruction, a large instruction dataset for\nMLMMs training. Our experiments demonstrate AOR's superior performance in both\nVQA and report generation tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02331", "pdf": "https://arxiv.org/pdf/2505.02331", "abs": "https://arxiv.org/abs/2505.02331", "authors": ["Hao Cheng", "Zhiwei Zhao", "Yichao He", "Zhenzhen Hu", "Jia Li", "Meng Wang", "Richang Hong"], "title": "VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection", "categories": ["cs.CV", "cs.SD"], "comment": "Source code and pre-trained models will be available at\n  https://github.com/MSA-LMC/VAEmo", "summary": "Audiovisual emotion recognition (AVER) aims to infer human emotions from\nnonverbal visual-audio (VA) cues, offering modality-complementary and\nlanguage-agnostic advantages. However, AVER remains challenging due to the\ninherent ambiguity of emotional expressions, cross-modal expressive\ndisparities, and the scarcity of reliably annotated data. Recent\nself-supervised AVER approaches have introduced strong multimodal\nrepresentations, yet they predominantly rely on modality-specific encoders and\ncoarse content-level alignment, limiting fine-grained emotional semantic\nmodeling. To address these issues, we propose VAEmo, an efficient two-stage\nframework for emotion-centric joint VA representation learning with external\nknowledge injection. In Stage 1, a unified and lightweight representation\nnetwork is pre-trained on large-scale speaker-centric VA corpora via masked\nreconstruction and contrastive objectives, mitigating the modality gap and\nlearning expressive, complementary representations without emotion labels. In\nStage 2, multimodal large language models automatically generate detailed\naffective descriptions according to our well-designed chain-of-thought\nprompting for only a small subset of VA samples; these rich textual semantics\nare then injected by aligning their corresponding embeddings with VA\nrepresentations through dual-path contrastive learning, further bridging the\nemotion gap. Extensive experiments on multiple downstream AVER benchmarks show\nthat VAEmo achieves state-of-the-art performance with a compact design,\nhighlighting the benefit of unified cross-modal encoding and emotion-aware\nsemantic guidance for efficient, generalizable VA emotion representations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02365", "pdf": "https://arxiv.org/pdf/2505.02365", "abs": "https://arxiv.org/abs/2505.02365", "authors": ["Weihua Yang", "Yicong Zhou"], "title": "Quaternion Multi-focus Color Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Multi-focus color image fusion refers to integrating multiple partially\nfocused color images to create a single all-in-focus color image. However,\nexisting methods struggle with complex real-world scenarios due to limitations\nin handling color information and intricate textures. To address these\nchallenges, this paper proposes a quaternion multi-focus color image fusion\nframework to perform high-quality color image fusion completely in the\nquaternion domain. This framework introduces 1) a quaternion sparse\ndecomposition model to jointly learn fine-scale image details and structure\ninformation of color images in an iterative fashion for high-precision focus\ndetection, 2) a quaternion base-detail fusion strategy to individually fuse\nbase-scale and detail-scale results across multiple color images for preserving\nstructure and detail information, and 3) a quaternion structural similarity\nrefinement strategy to adaptively select optimal patches from initial fusion\nresults and obtain the final fused result for preserving fine details and\nensuring spatially consistent outputs. Extensive experiments demonstrate that\nthe proposed framework outperforms state-of-the-art methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02471", "pdf": "https://arxiv.org/pdf/2505.02471", "abs": "https://arxiv.org/abs/2505.02471", "authors": ["Biao Gong", "Cheng Zou", "Dandan Zheng", "Hu Yu", "Jingdong Chen", "Jianxin Sun", "Junbo Zhao", "Jun Zhou", "Kaixiang Ji", "Lixiang Ru", "Libin Wang", "Qingpei Guo", "Rui Liu", "Weilong Chai", "Xinyu Xiao", "Ziyuan Huang"], "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction", "categories": ["cs.CV"], "comment": "https://github.com/inclusionAI/Ming/tree/main/Ming-unify", "summary": "We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02704", "pdf": "https://arxiv.org/pdf/2505.02704", "abs": "https://arxiv.org/abs/2505.02704", "authors": ["Bojin Wu", "Jing Chen"], "title": "Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery", "categories": ["cs.CV"], "comment": "21 pages, conference", "summary": "We propose a robust method for monocular depth scale recovery. Monocular\ndepth estimation can be divided into two main directions: (1) relative depth\nestimation, which provides normalized or inverse depth without scale\ninformation, and (2) metric depth estimation, which involves recovering depth\nwith absolute scale. To obtain absolute scale information for practical\ndownstream tasks, utilizing textual information to recover the scale of a\nrelative depth map is a highly promising approach. However, since a single\nimage can have multiple descriptions from different perspectives or with\nvarying styles, it has been shown that different textual descriptions can\nsignificantly affect the scale recovery process. To address this issue, our\nmethod, VGLD, stabilizes the influence of textual information by incorporating\nhigh-level semantic information from the corresponding image alongside the\ntextual description. This approach resolves textual ambiguities and robustly\noutputs a set of linear transformation parameters (scalars) that can be\nglobally applied to the relative depth map, ultimately generating depth\npredictions with metric-scale accuracy. We validate our method across several\npopular relative depth models(MiDas, DepthAnything), using both indoor scenes\n(NYUv2) and outdoor scenes (KITTI). Our results demonstrate that VGLD functions\nas a universal alignment module when trained on multiple datasets, achieving\nstrong performance even in zero-shot scenarios. Code is available at:\nhttps://github.com/pakinwu/VGLD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02815", "pdf": "https://arxiv.org/pdf/2505.02815", "abs": "https://arxiv.org/abs/2505.02815", "authors": ["Nicoleta Basoc", "Adrian Cosma", "Andy C«étrun«é", "Emilian R«édoi"], "title": "Database-Agnostic Gait Enrollment using SetTransformers", "categories": ["cs.CV"], "comment": "5 Tables, 6 Figures", "summary": "Gait recognition has emerged as a powerful tool for unobtrusive and\nlong-range identity analysis, with growing relevance in surveillance and\nmonitoring applications. Although recent advances in deep learning and\nlarge-scale datasets have enabled highly accurate recognition under closed-set\nconditions, real-world deployment demands open-set gait enrollment, which means\ndetermining whether a new gait sample corresponds to a known identity or\nrepresents a previously unseen individual. In this work, we introduce a\ntransformer-based framework for open-set gait enrollment that is both\ndataset-agnostic and recognition-architecture-agnostic. Our method leverages a\nSetTransformer to make enrollment decisions based on the embedding of a probe\nsample and a context set drawn from the gallery, without requiring\ntask-specific thresholds or retraining for new environments. By decoupling\nenrollment from the main recognition pipeline, our model is generalized across\ndifferent datasets, gallery sizes, and identity distributions. We propose an\nevaluation protocol that uses existing datasets in different ratios of\nidentities and walks per identity. We instantiate our method using\nskeleton-based gait representations and evaluate it on two benchmark datasets\n(CASIA-B and PsyMo), using embeddings from three state-of-the-art recognition\nmodels (GaitGraph, GaitFormer, and GaitPT). We show that our method is\nflexible, is able to accurately perform enrollment in different scenarios, and\nscales better with data compared to traditional approaches. We will make the\ncode and dataset scenarios publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02830", "pdf": "https://arxiv.org/pdf/2505.02830", "abs": "https://arxiv.org/abs/2505.02830", "authors": ["Qingqiu Li", "Zihang Cui", "Seongsu Bae", "Jilan Xu", "Runtian Yuan", "Yuejie Zhang", "Rui Feng", "Quanli Shen", "Xiaobo Zhang", "Junjun He", "Shujun Wang"], "title": "AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Chest X-rays (CXRs) are the most frequently performed imaging examinations in\nclinical settings. Recent advancements in Large Multimodal Models (LMMs) have\nenabled automated CXR interpretation, enhancing diagnostic accuracy and\nefficiency. However, despite their strong visual understanding, current Medical\nLMMs (MLMMs) still face two major challenges: (1) Insufficient region-level\nunderstanding and interaction, and (2) Limited accuracy and interpretability\ndue to single-step reasoning. In this paper, we empower MLMMs with\nanatomy-centric reasoning capabilities to enhance their interactivity and\nexplainability. Specifically, we first propose an Anatomical Ontology-Guided\nReasoning (AOR) framework, which centers on cross-modal region-level\ninformation to facilitate multi-step reasoning. Next, under the guidance of\nexpert physicians, we develop AOR-Instruction, a large instruction dataset for\nMLMMs training. Our experiments demonstrate AOR's superior performance in both\nVQA and report generation tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02831", "pdf": "https://arxiv.org/pdf/2505.02831", "abs": "https://arxiv.org/abs/2505.02831", "authors": ["Dengyang Jiang", "Mengmeng Wang", "Liuzhuozheng Li", "Lei Zhang", "Haoyu Wang", "Wei Wei", "Guang Dai", "Yanning Zhang", "Jingdong Wang"], "title": "No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves", "categories": ["cs.CV"], "comment": "Self-Representation Alignment for Diffusion Transformers. arXiv admin\n  note: text overlap with arXiv:2410.06940 by other authors", "summary": "Recent studies have demonstrated that learning a meaningful internal\nrepresentation can both accelerate generative training and enhance generation\nquality of the diffusion transformers. However, existing approaches necessitate\nto either introduce an additional and complex representation training framework\nor rely on a large-scale, pre-trained representation foundation model to\nprovide representation guidance during the original generative training\nprocess. In this study, we posit that the unique discriminative process\ninherent to diffusion transformers enables them to offer such guidance without\nrequiring external representation components. We therefore propose\nSelf-Representation A}lignment (SRA), a simple yet straightforward method that\nobtain representation guidance through a self-distillation manner.\nSpecifically, SRA aligns the output latent representation of the diffusion\ntransformer in earlier layer with higher noise to that in later layer with\nlower noise to progressively enhance the overall representation learning during\nonly generative training process. Experimental results indicate that applying\nSRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA\nnot only significantly outperforms approaches relying on auxiliary, complex\nrepresentation training frameworks but also achieves performance comparable to\nmethods that heavily dependent on powerful external representation priors.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02836", "pdf": "https://arxiv.org/pdf/2505.02836", "abs": "https://arxiv.org/abs/2505.02836", "authors": ["Lu Ling", "Chen-Hsuan Lin", "Tsung-Yi Lin", "Yifan Ding", "Yu Zeng", "Yichen Sheng", "Yunhao Ge", "Ming-Yu Liu", "Aniket Bera", "Zhaoshuo Li"], "title": "Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "Synthesizing interactive 3D scenes from text is essential for gaming, virtual\nreality, and embodied AI. However, existing methods face several challenges.\nLearning-based approaches depend on small-scale indoor datasets, limiting the\nscene diversity and layout complexity. While large language models (LLMs) can\nleverage diverse text-domain knowledge, they struggle with spatial realism,\noften producing unnatural object placements that fail to respect common sense.\nOur key insight is that vision perception can bridge this gap by providing\nrealistic spatial guidance that LLMs lack. To this end, we introduce\nScenethesis, a training-free agentic framework that integrates LLM-based scene\nplanning with vision-guided layout refinement. Given a text prompt, Scenethesis\nfirst employs an LLM to draft a coarse layout. A vision module then refines it\nby generating an image guidance and extracting scene structure to capture\ninter-object relations. Next, an optimization module iteratively enforces\naccurate pose alignment and physical plausibility, preventing artifacts like\nobject penetration and instability. Finally, a judge module verifies spatial\ncoherence. Comprehensive experiments show that Scenethesis generates diverse,\nrealistic, and physically plausible 3D interactive scenes, making it valuable\nfor virtual content creation, simulation environments, and embodied AI\nresearch.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01456", "pdf": "https://arxiv.org/pdf/2505.01456", "abs": "https://arxiv.org/abs/2505.01456", "authors": ["Vaidehi Patil", "Yi-Lin Sung", "Peter Hase", "Jie Peng", "Tianlong Chen", "Mohit Bansal"], "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "The dataset and code are publicly available at\n  https://github.com/Vaidehi99/UnLOK-VQA", "summary": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety"], "score": 4}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01709", "pdf": "https://arxiv.org/pdf/2505.01709", "abs": "https://arxiv.org/abs/2505.01709", "authors": ["Kaidong Zhang", "Rongtao Xu", "Pengzhen Ren", "Junfan Lin", "Hefeng Wu", "Liang Lin", "Xiaodan Liang"], "title": "RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "project page: https://abliao.github.io/RoBridge/", "summary": "Operating robots in open-ended scenarios with diverse tasks is a crucial\nresearch and application direction in robotics. While recent progress in\nnatural language processing and large multimodal models has enhanced robots'\nability to understand complex instructions, robot manipulation still faces the\nprocedural skill dilemma and the declarative skill dilemma in open\nenvironments. Existing methods often compromise cognitive and executive\ncapabilities. To address these challenges, in this paper, we propose RoBridge,\na hierarchical intelligent architecture for general robotic manipulation. It\nconsists of a high-level cognitive planner (HCP) based on a large-scale\npre-trained vision-language model (VLM), an invariant operable representation\n(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).\nRoBridge maintains the declarative skill of VLM and unleashes the procedural\nskill of reinforcement learning, effectively bridging the gap between cognition\nand execution. RoBridge demonstrates significant performance improvements over\nexisting baselines, achieving a 75% success rate on new tasks and an 83%\naverage success rate in sim-to-real generalization using only five real-world\ndata samples per task. This work represents a significant step towards\nintegrating cognitive reasoning with physical execution in robotic systems,\noffering a new paradigm for general robotic manipulation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01755", "pdf": "https://arxiv.org/pdf/2505.01755", "abs": "https://arxiv.org/abs/2505.01755", "authors": ["Jiesong Bai", "Yuhao Yin", "Yihang Dong", "Xiaofeng Zhang", "Chi-Man Pun", "Xuhang Chen"], "title": "LensNet: An End-to-End Learning Framework for Empirical Point Spread Function Modeling and Lensless Imaging Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Lensless imaging stands out as a promising alternative to conventional\nlens-based systems, particularly in scenarios demanding ultracompact form\nfactors and cost-effective architectures. However, such systems are\nfundamentally governed by the Point Spread Function (PSF), which dictates how a\npoint source contributes to the final captured signal. Traditional lensless\ntechniques often require explicit calibrations and extensive pre-processing,\nrelying on static or approximate PSF models. These rigid strategies can result\nin limited adaptability to real-world challenges, including noise, system\nimperfections, and dynamic scene variations, thus impeding high-fidelity\nreconstruction. In this paper, we propose LensNet, an end-to-end deep learning\nframework that integrates spatial-domain and frequency-domain representations\nin a unified pipeline. Central to our approach is a learnable Coded Mask\nSimulator (CMS) that enables dynamic, data-driven estimation of the PSF during\ntraining, effectively mitigating the shortcomings of fixed or sparsely\ncalibrated kernels. By embedding a Wiener filtering component, LensNet refines\nglobal structure and restores fine-scale details, thus alleviating the\ndependency on multiple handcrafted pre-processing steps. Extensive experiments\ndemonstrate LensNet's robust performance and superior reconstruction quality\ncompared to state-of-the-art methods, particularly in preserving high-frequency\ndetails and attenuating noise. The proposed framework establishes a novel\nconvergence between physics-based modeling and data-driven learning, paving the\nway for more accurate, flexible, and practical lensless imaging solutions for\napplications ranging from miniature sensors to medical diagnostics. The link of\ncode is https://github.com/baijiesong/Lensnet.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.01831", "pdf": "https://arxiv.org/pdf/2505.01831", "abs": "https://arxiv.org/abs/2505.01831", "authors": ["Haofan Wu", "Yin Huang", "Yuqing Wu", "Qiuyu Yang", "Bingfang Wang", "Li Zhang", "Muhammad Fahadullah Khan", "Ali Zia", "M. Saleh Memon", "Syed Sohail Bukhari", "Abdul Fattah Memon", "Daizong Ji", "Ya Zhang", "Ghulam Mustafa", "Yin Fang"], "title": "Multi-Scale Target-Aware Representation Learning for Fundus Image Enhancement", "categories": ["eess.IV", "cs.CV"], "comment": "Under review at Neural Networks", "summary": "High-quality fundus images provide essential anatomical information for\nclinical screening and ophthalmic disease diagnosis. Yet, due to hardware\nlimitations, operational variability, and patient compliance, fundus images\noften suffer from low resolution and signal-to-noise ratio. Recent years have\nwitnessed promising progress in fundus image enhancement. However, existing\nworks usually focus on restoring structural details or global characteristics\nof fundus images, lacking a unified image enhancement framework to recover\ncomprehensive multi-scale information. Moreover, few methods pinpoint the\ntarget of image enhancement, e.g., lesions, which is crucial for medical\nimage-based diagnosis. To address these challenges, we propose a multi-scale\ntarget-aware representation learning framework (MTRL-FIE) for efficient fundus\nimage enhancement. Specifically, we propose a multi-scale feature encoder (MFE)\nthat employs wavelet decomposition to embed both low-frequency structural\ninformation and high-frequency details. Next, we design a structure-preserving\nhierarchical decoder (SHD) to fuse multi-scale feature embeddings for real\nfundus image restoration. SHD integrates hierarchical fusion and group\nattention mechanisms to achieve adaptive feature fusion while retaining local\nstructural smoothness. Meanwhile, a target-aware feature aggregation (TFA)\nmodule is used to enhance pathological regions and reduce artifacts.\nExperimental results on multiple fundus image datasets demonstrate the\neffectiveness and generalizability of MTRL-FIE for fundus image enhancement.\nCompared to state-of-the-art methods, MTRL-FIE achieves superior enhancement\nperformance with a more lightweight architecture. Furthermore, our approach\ngeneralizes to other ophthalmic image processing tasks without supervised\nfine-tuning, highlighting its potential for clinical applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02001", "pdf": "https://arxiv.org/pdf/2505.02001", "abs": "https://arxiv.org/abs/2505.02001", "authors": ["Vineesh Kumar Reddy Mondem"], "title": "Hybrid Image Resolution Quality Metric (HIRQM):A Comprehensive Perceptual Image Quality Assessment Framework", "categories": ["eess.IV", "cs.CV", "94A08", "I.4.0; I.4.9; I.2.10"], "comment": "19 pages,2 figures,2 tables and biblography with similar papers with\n  some valid information", "summary": "Traditional image quality assessment metrics like Mean Squared Error and\nStructural Similarity Index often fail to reflect perceptual quality under\ncomplex distortions. We propose the Hybrid Image Resolution Quality Metric\n(HIRQM), integrating statistical, multi-scale, and deep learning-based methods\nfor a comprehensive quality evaluation. HIRQM combines three components:\nProbability Density Function for local pixel distribution analysis, Multi-scale\nFeature Similarity for structural integrity across resolutions, and\nHierarchical Deep Image Features using a pre-trained VGG16 network for semantic\nalignment with human perception. A dynamic weighting mechanism adapts component\ncontributions based on image characteristics like brightness and variance,\nenhancing flexibility across distortion types. Our contributions include a\nunified metric and dynamic weighting for better perceptual alignment. Evaluated\non TID2013 and LIVE datasets, HIRQM achieves Pearson and Spearman correlations\nof 0.92 and 0.90, outperforming traditional metrics. It excels in handling\nnoise, blur, and compression artifacts, making it valuable for image processing\napplications like compression and restoration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02628", "pdf": "https://arxiv.org/pdf/2505.02628", "abs": "https://arxiv.org/abs/2505.02628", "authors": ["Yiqun Lin", "Hualiang Wang", "Jixiang Chen", "Jiewen Yang", "Jiarong Guo", "Xiaomeng Li"], "title": "DeepSparse: A Foundation Model for Sparse-View CBCT Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Cone-beam computed tomography (CBCT) is a critical 3D imaging technology in\nthe medical field, while the high radiation exposure required for high-quality\nimaging raises significant concerns, particularly for vulnerable populations.\nSparse-view reconstruction reduces radiation by using fewer X-ray projections\nwhile maintaining image quality, yet existing methods face challenges such as\nhigh computational demands and poor generalizability to different datasets. To\novercome these limitations, we propose DeepSparse, the first foundation model\nfor sparse-view CBCT reconstruction, featuring DiCE (Dual-Dimensional\nCross-Scale Embedding), a novel network that integrates multi-view 2D features\nand multi-scale 3D features. Additionally, we introduce the HyViP (Hybrid View\nSampling Pretraining) framework, which pretrains the model on large datasets\nwith both sparse-view and dense-view projections, and a two-step finetuning\nstrategy to adapt and refine the model for new datasets. Extensive experiments\nand ablation studies demonstrate that our proposed DeepSparse achieves superior\nreconstruction quality compared to state-of-the-art methods, paving the way for\nsafer and more efficient CBCT imaging.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
{"id": "2505.02705", "pdf": "https://arxiv.org/pdf/2505.02705", "abs": "https://arxiv.org/abs/2505.02705", "authors": ["Binghong Chen", "Tingting Chai", "Wei Jiang", "Yuanrong Xu", "Guanglu Zhou", "Xiangqian Wu"], "title": "Multi-View Learning with Context-Guided Receptance for Image Denoising", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by IJCAI 2025, code will be available at\n  https://github.com/Seeker98/CRWKV", "summary": "Image denoising is essential in low-level vision applications such as\nphotography and automated driving. Existing methods struggle with\ndistinguishing complex noise patterns in real-world scenes and consume\nsignificant computational resources due to reliance on Transformer-based\nmodels. In this work, the Context-guided Receptance Weighted Key-Value (\\M)\nmodel is proposed, combining enhanced multi-view feature integration with\nefficient sequence modeling. Our approach introduces the Context-guided Token\nShift (CTS) paradigm, which effectively captures local spatial dependencies and\nenhance the model's ability to model real-world noise distributions.\nAdditionally, the Frequency Mix (FMix) module extracting frequency-domain\nfeatures is designed to isolate noise in high-frequency spectra, and is\nintegrated with spatial representations through a multi-view learning process.\nTo improve computational efficiency, the Bidirectional WKV (BiWKV) mechanism is\nadopted, enabling full pixel-sequence interaction with linear complexity while\novercoming the causal selection constraints. The model is validated on multiple\nreal-world image denoising datasets, outperforming the existing\nstate-of-the-art methods quantitatively and reducing inference time up to 40\\%.\nQualitative results further demonstrate the ability of our model to restore\nfine details in various scenes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-05-06.jsonl"}
