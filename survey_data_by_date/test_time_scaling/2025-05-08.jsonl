{"id": "2505.03973", "pdf": "https://arxiv.org/pdf/2505.03973", "abs": "https://arxiv.org/abs/2505.03973", "authors": ["Jiale Liu", "Yifan Zeng", "Shaokun Zhang", "Chi Zhang", "Malte HÃ¸jmark-Bertelsen", "Marie Normann Gadeberg", "Huazheng Wang", "Qingyun Wu"], "title": "Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale", "categories": ["cs.CL"], "comment": null, "summary": "LLM-based optimization has shown remarkable potential in enhancing agentic\nsystems. However, the conventional approach of prompting LLM optimizer with the\nwhole training trajectories on training dataset in a single pass becomes\nuntenable as datasets grow, leading to context window overflow and degraded\npattern recognition. To address these challenges, we propose Fine-Grained\nOptimization (FGO), a scalable framework that divides large optimization tasks\ninto manageable subsets, performs targeted optimizations, and systematically\ncombines optimized components through progressive merging. Evaluation across\nALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms\nexisting approaches by 1.6-8.6% while reducing average prompt token consumption\nby 56.3%. Our framework provides a practical solution for scaling up LLM-based\noptimization of increasingly sophisticated agent systems. Further analysis\ndemonstrates that FGO achieves the most consistent performance gain in all\ntraining dataset sizes, showcasing its scalability and efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04087", "pdf": "https://arxiv.org/pdf/2505.04087", "abs": "https://arxiv.org/abs/2505.04087", "authors": ["Zixuan Hu", "Yichun Hu", "Ling-Yu Duan"], "title": "SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for Test-Time Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Test-Time adaptation (TTA) aims to enhance model robustness against\ndistribution shifts through rapid model adaptation during inference. While\nexisting TTA methods often rely on entropy-based unsupervised training and\nachieve promising results, the common practice of a single round of entropy\ntraining is typically unable to adequately utilize reliable samples, hindering\nadaptation efficiency. In this paper, we discover augmentation strategies can\neffectively unleash the potential of reliable samples, but the rapidly growing\ncomputational cost impedes their real-time application. To address this\nlimitation, we propose a novel TTA approach named Single-step Ensemble of\nVicinal Augmentations (SEVA), which can take advantage of data augmentations\nwithout increasing the computational burden. Specifically, instead of\nexplicitly utilizing the augmentation strategy to generate new data, SEVA\ndevelops a theoretical framework to explore the impacts of multiple\naugmentations on model adaptation and proposes to optimize an upper bound of\nthe entropy loss to integrate the effects of multiple rounds of augmentation\ntraining into a single step. Furthermore, we discover and verify that using the\nupper bound as the loss is more conducive to the selection mechanism, as it can\neffectively filter out harmful samples that confuse the model. Combining these\ntwo key advantages, the proposed efficient loss and a complementary selection\nstrategy can simultaneously boost the potential of reliable samples and meet\nthe stringent time requirements of TTA. The comprehensive experiments on\nvarious network architectures across challenging testing scenarios demonstrate\nimpressive performances and the broad adaptability of SEVA. The code will be\npublicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.03786", "pdf": "https://arxiv.org/pdf/2505.03786", "abs": "https://arxiv.org/abs/2505.03786", "authors": ["Md Fahim Anjum"], "title": "When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator", "categories": ["cs.LG", "cs.CL"], "comment": "12 pages, 5 figures. Code available at:\n  https://github.com/MDFahimAnjum/llm-planning-with-reasoning", "summary": "Large Language Models (LLM) with reasoning capabilities offer a promising\npath for improving candidate evaluation in planning frameworks, but their\nrelative performance against traditional non-reasoning models remains largely\nunderexplored. In this study, we benchmark a distilled 1.5B parameter reasoning\nmodel (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within\na generator-discriminator LLM planning framework for the text-to-SQL task. For\nthis, we introduce a novel method for extracting soft scores from the\nchain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking\nof candidates. Our central hypothesis is that reasoning models are more\neffective discriminators than non-reasoning LLMs. Our results show that\ndistilled DeepSeek-R1-1.5B achieves up to $87\\%$ higher F1 and $3.7\\%$ better\ndiscrimination accuracy than CodeLlama-7B, as well as $3.7\\%$ higher execution\naccuracy than CodeLlama-13B, despite having significantly fewer parameters.\nFurthermore, we find that there is a limit to the logical capabilities of\nreasoning models, and only providing more context or allowing more compute\nbudget for reasoning is not enough to improve their discrimination performance.\nFinally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find\ngeneration more challenging than discrimination and may underperform as\ngenerators compared to smaller non-reasoning LLMs. Our work highlights the\npotential of reasoning models as discriminators in agentic frameworks, far\noutweighing their capabilities as generators, offering insights into their\noptimal role within LLM planning infrastructures.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "reasoning model"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.03788", "pdf": "https://arxiv.org/pdf/2505.03788", "abs": "https://arxiv.org/abs/2505.03788", "authors": ["Trilok Padhi", "Ramneet Kaur", "Adam D. Cobb", "Manoj Acharya", "Anirban Roy", "Colin Samplawski", "Brian Matejek", "Alexander M. Berenbeim", "Nathaniel D. Bastian", "Susmit Jha"], "title": "Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce a novel approach for calibrating uncertainty quantification (UQ)\ntailored for multi-modal large language models (LLMs). Existing\nstate-of-the-art UQ methods rely on consistency among multiple responses\ngenerated by the LLM on an input query under diverse settings. However, these\napproaches often report higher confidence in scenarios where the LLM is\nconsistently incorrect. This leads to a poorly calibrated confidence with\nrespect to accuracy. To address this, we leverage cross-modal consistency in\naddition to self-consistency to improve the calibration of the multi-modal\nmodels. Specifically, we ground the textual responses to the visual inputs. The\nconfidence from the grounding model is used to calibrate the overall\nconfidence. Given that using a grounding model adds its own uncertainty in the\npipeline, we apply temperature scaling - a widely accepted parametric\ncalibration technique - to calibrate the grounding model's confidence in the\naccuracy of generated responses. We evaluate the proposed approach across\nmultiple multi-modal tasks, such as medical question answering (Slake) and\nvisual question answering (VQAv2), considering multi-modal models such as\nLLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework\nachieves significantly improved calibration on both tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.03910", "pdf": "https://arxiv.org/pdf/2505.03910", "abs": "https://arxiv.org/abs/2505.03910", "authors": ["Gianluca Manzo", "Julia Ive"], "title": "Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty", "categories": ["cs.CL"], "comment": null, "summary": "Automating chest radiograph interpretation using Deep Learning (DL) models\nhas the potential to significantly improve clinical workflows, decision-making,\nand large-scale health screening. However, in medical settings, merely\noptimising predictive performance is insufficient, as the quantification of\nuncertainty is equally crucial. This paper investigates the relationship\nbetween predictive uncertainty, derived from Bayesian Deep Learning\napproximations, and human/linguistic uncertainty, as estimated from free-text\nradiology reports labelled by rule-based labellers. Utilising BERT as the model\nof choice, this study evaluates different binarisation methods for uncertainty\nlabels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in\nestimating predictive uncertainty. The results demonstrate good model\nperformance, but also a modest correlation between predictive and linguistic\nuncertainty, highlighting the challenges in aligning machine uncertainty with\nhuman interpretation nuances. Our findings suggest that while Bayesian\napproximations provide valuable uncertainty estimates, further refinement is\nnecessary to fully capture and utilise the subtleties of human uncertainty in\nclinical applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04590", "pdf": "https://arxiv.org/pdf/2505.04590", "abs": "https://arxiv.org/abs/2505.04590", "authors": ["Alexandre Binninger", "Ruben Wiersma", "Philipp Herholz", "Olga Sorkine-Hornung"], "title": "TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral Grids for Gradient-Based Mesh Optimization", "categories": ["cs.GR", "cs.CV", "I.3.5"], "comment": "ACM Trans. Graph. 44, 4. SIGGRAPH 2025. 19 pages, 21 figures", "summary": "We introduce TetWeave, a novel isosurface representation for gradient-based\nmesh optimization that jointly optimizes the placement of a tetrahedral grid\nused for Marching Tetrahedra and a novel directional signed distance at each\npoint. TetWeave constructs tetrahedral grids on-the-fly via Delaunay\ntriangulation, enabling increased flexibility compared to predefined grids. The\nextracted meshes are guaranteed to be watertight, two-manifold and\nintersection-free. The flexibility of TetWeave enables a resampling strategy\nthat places new points where reconstruction error is high and allows to\nencourage mesh fairness without compromising on reconstruction error. This\nleads to high-quality, adaptive meshes that require minimal memory usage and\nfew parameters to optimize. Consequently, TetWeave exhibits near-linear memory\nscaling relative to the vertex count of the output mesh - a substantial\nimprovement over predefined grids. We demonstrate the applicability of TetWeave\nto a broad range of challenging tasks in computer graphics and vision, such as\nmulti-view 3D reconstruction, mesh compression and geometric texture\ngeneration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04622", "pdf": "https://arxiv.org/pdf/2505.04622", "abs": "https://arxiv.org/abs/2505.04622", "authors": ["Jingwen Ye", "Yuze He", "Yanning Zhou", "Yiqin Zhu", "Kaiwen Xiao", "Yong-Jin Liu", "Wei Yang", "Xiao Han"], "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025. 14 pages, 15 figures", "summary": "Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.03848", "pdf": "https://arxiv.org/pdf/2505.03848", "abs": "https://arxiv.org/abs/2505.03848", "authors": ["Janhavi Giri", "Attila Lengyel", "Don Kent", "Edward Kibardin"], "title": "Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "comment": "46 pages, 22 figures, 5 tables", "summary": "Semiconductor manufacturing generates vast amounts of image data, crucial for\ndefect identification and yield optimization, yet often exceeds manual\ninspection capabilities. Traditional clustering techniques struggle with\nhigh-dimensional, unlabeled data, limiting their effectiveness in capturing\nnuanced patterns. This paper introduces an advanced clustering framework that\nintegrates deep Topological Data Analysis (TDA) with self-supervised and\ntransfer learning techniques, offering a novel approach to unsupervised image\nclustering. TDA captures intrinsic topological features, while self-supervised\nlearning extracts meaningful representations from unlabeled data, reducing\nreliance on labeled datasets. Transfer learning enhances the framework's\nadaptability and scalability, allowing fine-tuning to new datasets without\nretraining from scratch. Validated on synthetic and open-source semiconductor\nimage datasets, the framework successfully identifies clusters aligned with\ndefect patterns and process variations. This study highlights the\ntransformative potential of combining TDA, self-supervised learning, and\ntransfer learning, providing a scalable solution for proactive process\nmonitoring and quality control in semiconductor manufacturing and other domains\nwith large-scale image datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04132", "pdf": "https://arxiv.org/pdf/2505.04132", "abs": "https://arxiv.org/abs/2505.04132", "authors": ["Mingruo Yuan", "Ben Kao", "Tien-Hsuan Wu", "Michael M. K. Cheung", "Henry W. H. Chan", "Anne S. Y. Cheung", "Felix W. H. Chan", "Yongxi Chen"], "title": "Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Access to legal information is fundamental to access to justice. Yet\naccessibility refers not only to making legal documents available to the\npublic, but also rendering legal information comprehensible to them. A vexing\nproblem in bringing legal information to the public is how to turn formal legal\ndocuments such as legislation and judgments, which are often highly technical,\nto easily navigable and comprehensible knowledge to those without legal\neducation. In this study, we formulate a three-step approach for bringing legal\nknowledge to laypersons, tackling the issues of navigability and\ncomprehensibility. First, we translate selected sections of the law into\nsnippets (called CLIC-pages), each being a small piece of article that focuses\non explaining certain technical legal concept in layperson's terms. Second, we\nconstruct a Legal Question Bank (LQB), which is a collection of legal questions\nwhose answers can be found in the CLIC-pages. Third, we design an interactive\nCLIC Recommender (CRec). Given a user's verbal description of a legal situation\nthat requires a legal solution, CRec interprets the user's input and shortlists\nquestions from the question bank that are most likely relevant to the given\nlegal situation and recommends their corresponding CLIC pages where relevant\nlegal knowledge can be found. In this paper we focus on the technical aspects\nof creating an LQB. We show how large-scale pre-trained language models, such\nas GPT-3, can be used to generate legal questions. We compare machine-generated\nquestions (MGQs) against human-composed questions (HCQs) and find that MGQs are\nmore scalable, cost-effective, and more diversified, while HCQs are more\nprecise. We also show a prototype of CRec and illustrate through an example how\nour 3-step approach effectively brings relevant legal knowledge to the public.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04393", "pdf": "https://arxiv.org/pdf/2505.04393", "abs": "https://arxiv.org/abs/2505.04393", "authors": ["David Exler", "Mark Schutera", "Markus Reischl", "Luca Rettenberger"], "title": "Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters", "categories": ["cs.CL"], "comment": null, "summary": "With the increasing prevalence of artificial intelligence, careful evaluation\nof inherent biases needs to be conducted to form the basis for alleviating the\neffects these predispositions can have on users. Large language models (LLMs)\nare predominantly used by many as a primary source of information for various\ntopics. LLMs frequently make factual errors, fabricate data (hallucinations),\nor present biases, exposing users to misinformation and influencing opinions.\nEducating users on their risks is key to responsible use, as bias, unlike\nhallucinations, cannot be caught through data verification. We quantify the\npolitical bias of popular LLMs in the context of the recent vote of the German\nBundestag using the score produced by the Wahl-O-Mat. This metric measures the\nalignment between an individual's political views and the positions of German\npolitical parties. We compare the models' alignment scores to identify factors\ninfluencing their political preferences. Doing so, we discover a bias toward\nleft-leaning parties, most dominant in larger LLMs. Also, we find that the\nlanguage we use to communicate with the models affects their political views.\nAdditionally, we analyze the influence of a model's origin and release date and\ncompare the results to the outcome of the recent vote of the Bundestag. Our\nresults imply that LLMs are prone to exhibiting political bias. Large\ncorporations with the necessary means to develop LLMs, thus, knowingly or\nunknowingly, have a responsibility to contain these biases, as they can\ninfluence each voter's decision-making process and inform public opinion in\ngeneral and at scale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04109", "pdf": "https://arxiv.org/pdf/2505.04109", "abs": "https://arxiv.org/abs/2505.04109", "authors": ["Mengya Liu", "Siyuan Li", "Ajad Chhatkuli", "Prune Truong", "Luc Van Gool", "Federico Tombari"], "title": "One2Any: One-Reference 6D Pose Estimation for Any Object", "categories": ["cs.CV"], "comment": "accepted by CVPR 2025", "summary": "6D object pose estimation remains challenging for many applications due to\ndependencies on complete 3D models, multi-view images, or training limited to\nspecific object categories. These requirements make generalization to novel\nobjects difficult for which neither 3D models nor multi-view images may be\navailable. To address this, we propose a novel method One2Any that estimates\nthe relative 6-degrees of freedom (DOF) object pose using only a single\nreference-single query RGB-D image, without prior knowledge of its 3D model,\nmulti-view data, or category constraints. We treat object pose estimation as an\nencoding-decoding process, first, we obtain a comprehensive Reference Object\nPose Embedding (ROPE) that encodes an object shape, orientation, and texture\nfrom a single reference view. Using this embedding, a U-Net-based pose decoding\nmodule produces Reference Object Coordinate (ROC) for new views, enabling fast\nand accurate pose estimation. This simple encoding-decoding framework allows\nour model to be trained on any pair-wise pose data, enabling large-scale\ntraining and demonstrating great scalability. Experiments on multiple benchmark\ndatasets demonstrate that our model generalizes well to novel objects,\nachieving state-of-the-art accuracy and robustness even rivaling methods that\nrequire multi-view or CAD inputs, at a fraction of compute.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04519", "pdf": "https://arxiv.org/pdf/2505.04519", "abs": "https://arxiv.org/abs/2505.04519", "authors": ["Yehui Tang", "Yichun Yin", "Yaoyuan Wang", "Hang Zhou", "Yu Pan", "Wei Guo", "Ziyang Zhang", "Miao Rang", "Fangcheng Liu", "Naifu Zhang", "Binghan Li", "Yonghan Dong", "Xiaojun Meng", "Yasheng Wang", "Dong Li", "Yin Li", "Dandan Tu", "Can Chen", "Youliang Yan", "Fisher Yu", "Ruiming Tang", "Yunhe Wang", "Botian Huang", "Bo Wang", "Boxiao Liu", "Changzheng Zhang", "Da Kuang", "Fei Liu", "Gang Huang", "Jiansheng Wei", "Jiarui Qin", "Jie Ran", "Jinpeng Li", "Jun Zhao", "Liang Dai", "Lin Li", "Liqun Deng", "Peifeng Qin", "Pengyuan Zeng", "Qiang Gu", "Shaohua Tang", "Shengjun Cheng", "Tao Gao", "Tao Yu", "Tianshu Li", "Tianyu Bi", "Wei He", "Weikai Mao", "Wenyong Huang", "Wulong Liu", "Xiabing Li", "Xianzhi Yu", "Xueyu Wu", "Xu He", "Yangkai Du", "Yan Xu", "Ye Tian", "Yimeng Wu", "Yongbing Huang", "Yong Tian", "Yong Zhu", "Yue Li", "Yufei Wang", "Yuhang Gai", "Yujun Li", "Yu Luo", "Yunsheng Ni", "Yusen Sun", "Zelin Chen", "Zhe Liu", "Zhicheng Liu", "Zhipeng Tu", "Zilin Ding", "Zongyuan Zhan"], "title": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs", "categories": ["cs.CL"], "comment": null, "summary": "Sparse large language models (LLMs) with Mixture of Experts (MoE) and close\nto a trillion parameters are dominating the realm of most capable language\nmodels. However, the massive model scale poses significant challenges for the\nunderlying software and hardware systems. In this paper, we aim to uncover a\nrecipe to harness such scale on Ascend NPUs. The key goals are better usage of\nthe computing resources under the dynamic sparse model structures and\nmaterializing the expected performance gain on the actual hardware. To select\nmodel configurations suitable for Ascend NPUs without repeatedly running the\nexpensive experiments, we leverage simulation to compare the trade-off of\nvarious model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM\nwith 718 billion parameters, and we conducted experiments on the model to\nverify the simulation results. On the system side, we dig into Expert\nParallelism to optimize the communication between NPU devices to reduce the\nsynchronization overhead. We also optimize the memory efficiency within the\ndevices to further reduce the parameter and activation management overhead. In\nthe end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with\nperformance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and\ndemonstrate that the Ascend system is capable of harnessing all the training\nstages of the state-of-the-art language models. Extensive experiments indicate\nthat our recipe can lead to efficient training of large-scale sparse language\nmodels with MoE. We also study the behaviors of such models for future\nreference.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04531", "pdf": "https://arxiv.org/pdf/2505.04531", "abs": "https://arxiv.org/abs/2505.04531", "authors": ["Josh McGiff", "Nikola S. Nikolov"], "title": "Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review", "categories": ["cs.CL", "cs.AI"], "comment": "This work is currently under review. Please do not cite without\n  permission", "summary": "Generative language modelling has surged in popularity with the emergence of\nservices such as ChatGPT and Google Gemini. While these models have\ndemonstrated transformative potential in productivity and communication, they\noverwhelmingly cater to high-resource languages like English. This has\namplified concerns over linguistic inequality in natural language processing\n(NLP). This paper presents the first systematic review focused specifically on\nstrategies to address data scarcity in generative language modelling for\nlow-resource languages (LRL). Drawing from 54 studies, we identify, categorise\nand evaluate technical approaches, including monolingual data augmentation,\nback-translation, multilingual training, and prompt engineering, across\ngenerative tasks. We also analyse trends in architecture choices, language\nfamily representation, and evaluation methods. Our findings highlight a strong\nreliance on transformer-based models, a concentration on a small subset of\nLRLs, and a lack of consistent evaluation across studies. We conclude with\nrecommendations for extending these methods to a wider range of LRLs and\noutline open challenges in building equitable generative language systems.\nUltimately, this review aims to support researchers and developers in building\ninclusive AI tools for underrepresented languages, a necessary step toward\nempowering LRL speakers and the preservation of linguistic diversity in a world\nincreasingly shaped by large-scale language technologies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04150", "pdf": "https://arxiv.org/pdf/2505.04150", "abs": "https://arxiv.org/abs/2505.04150", "authors": ["Yu Yamaoka or Weng Ian Chan", "Shigeto Seno", "Soichiro Fukada", "Hideo Matsuda"], "title": "Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages", "categories": ["cs.CV", "cs.LG"], "comment": "MICCAI2024 workshop ADSMI in Morocco (oral) [Peer-reviewed]", "summary": "Evaluating the regeneration process of damaged muscle tissue is a fundamental\nanalysis in muscle research to measure experimental effect sizes and uncover\nmechanisms behind muscle weakness due to aging and disease. The conventional\napproach to assessing muscle tissue regeneration involves whole-slide imaging\nand expert visual inspection of the recovery stages based on the morphological\ninformation of cells and fibers. There is a need to replace these tasks with\nautomated methods incorporating machine learning techniques to ensure a\nquantitative and objective analysis. Given the limited availability of fully\nlabeled data, a possible approach is Learning from Label Proportions (LLP), a\nweakly supervised learning method using class label proportions. However,\ncurrent LLP methods have two limitations: (1) they cannot adapt the feature\nextractor for muscle tissues, and (2) they treat the classes representing\nrecovery stages and cell morphological changes as nominal, resulting in the\nloss of ordinal information. To address these issues, we propose Ordinal Scale\nLearning from Similarity Proportion (OSLSP), which uses a similarity proportion\nloss derived from two bag combinations. OSLSP can update the feature extractor\nby using class proportion attention to the ordinal scale of the class. Our\nmodel with OSLSP outperforms large-scale pre-trained and fine-tuning models in\nclassification tasks of skeletal muscle recovery stages.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.03799", "pdf": "https://arxiv.org/pdf/2505.03799", "abs": "https://arxiv.org/abs/2505.03799", "authors": ["Hyun Lee", "Chris Yi", "Maminur Islam", "B. D. S. Aritra"], "title": "Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "To be published in International Joint Conference on Neural Networks\n  (IJCNN), 2025", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in various\nnatural language processing tasks; however, their application to graph-related\nproblems remains limited, primarily due to scalability constraints and the\nabsence of dedicated mechanisms for processing graph structures. Existing\napproaches predominantly integrate LLMs with Graph Neural Networks (GNNs),\nusing GNNs as feature encoders or auxiliary components. However, directly\nencoding graph structures within LLMs has been underexplored, particularly in\nthe context of large-scale graphs where token limitations hinder effective\nrepresentation. To address these challenges, we propose SDM-InstructGLM, a\nnovel instruction-tuned Graph Language Model (InstructGLM) framework that\nenhances scalability and efficiency without relying on GNNs. Our method\nintroduces a similarity-degree-based biased random walk mechanism, which\nselectively samples and encodes graph information based on node-feature\nsimilarity and degree centrality, ensuring an adaptive and structured\nrepresentation within the LLM. This approach significantly improves token\nefficiency, mitigates information loss due to random sampling, and enhances\nperformance on graph-based tasks such as node classification and link\nprediction. Furthermore, our results demonstrate the feasibility of LLM-only\ngraph processing, enabling scalable and interpretable Graph Language Models\n(GLMs) optimized through instruction-based fine-tuning. This work paves the way\nfor GNN-free approaches to graph learning, leveraging LLMs as standalone graph\nreasoning models. Our source code is available on GitHub.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.03814", "pdf": "https://arxiv.org/pdf/2505.03814", "abs": "https://arxiv.org/abs/2505.03814", "authors": ["Ganghua Wang", "Zhaorun Chen", "Bo Li", "Haifeng Xu"], "title": "Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "As foundation models continue to scale, the size of trained models grows\nexponentially, presenting significant challenges for their evaluation. Current\nevaluation practices involve curating increasingly large datasets to assess the\nperformance of large language models (LLMs). However, there is a lack of\nsystematic analysis and guidance on determining the sufficiency of test data or\nselecting informative samples for evaluation. This paper introduces a\ncertifiable and cost-efficient evaluation framework for LLMs. Our framework\nadapts to different evaluation objectives and outputs confidence intervals that\ncontain true values with high probability. We use ``test sample complexity'' to\nquantify the number of test points needed for a certifiable evaluation and\nderive tight bounds on test sample complexity. Based on the developed theory,\nwe develop a partition-based algorithm, named Cer-Eval, that adaptively selects\ntest points to minimize the cost of LLM evaluation. Real-world experiments\ndemonstrate that Cer-Eval can save 20% to 40% test points across various\nbenchmarks, while maintaining an estimation error level comparable to the\ncurrent evaluation process and providing a 95% confidence guarantee.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.03961", "pdf": "https://arxiv.org/pdf/2505.03961", "abs": "https://arxiv.org/abs/2505.03961", "authors": ["Gerrit GroÃmann", "Larisa Ivanova", "Sai Leela Poduru", "Mohaddeseh Tabrizian", "Islam Mesabah", "David A. Selby", "Sebastian J. Vollmer"], "title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete", "categories": ["cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; I.6; J.4"], "comment": "16 pages, 8 figures. Code available at\n  https://github.com/storyagents25/story-agents", "summary": "According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.03997", "pdf": "https://arxiv.org/pdf/2505.03997", "abs": "https://arxiv.org/abs/2505.03997", "authors": ["Prudhviraj Naidu", "Zixian Wang", "Leon Bergen", "Ramamohan Paturi"], "title": "Quiet Feature Learning in Algorithmic Tasks", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We train Transformer-based language models on ten foundational algorithmic\ntasks and observe pronounced phase transitions in their loss curves that\ndeviate from established power-law scaling trends. Over large ranges of\ncompute, the validation loss barely improves, then abruptly decreases. Probing\nthe models' internal representations reveals the learning of quiet features\nduring the stagnant phase, followed by sudden acquisition of loud features that\ncoincide with the sharp drop in loss. Our ablation experiments show that\ndisrupting a single learned feature can dramatically degrade performance,\nproviding evidence of their causal role in task performance. These findings\nchallenge the prevailing assumption that next-token predictive loss reliably\ntracks incremental progress; instead, key internal features may be developing\nbelow the surface until they coalesce, triggering a rapid performance gain.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04457", "pdf": "https://arxiv.org/pdf/2505.04457", "abs": "https://arxiv.org/abs/2505.04457", "authors": ["Shigeki Karita", "Yuma Koizumi", "Heiga Zen", "Haruko Ishikawa", "Robin Scheibler", "Michiel Bacchiani"], "title": "Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Training data cleaning is a new application for generative model-based speech\nrestoration (SR). This paper introduces Miipher-2, an SR model designed for\nmillion-hour scale data, for training data cleaning for large-scale generative\nmodels like large language models. Key challenges addressed include\ngeneralization to unseen languages, operation without explicit conditioning\n(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a\nfrozen, pre-trained Universal Speech Model (USM), supporting over 300\nlanguages, as a robust, conditioning-free feature extractor. To optimize\nefficiency and minimize memory, Miipher-2 incorporates parallel adapters for\npredicting clean USM features from noisy inputs and employs the WaneFit neural\nvocoder for waveform synthesis. These components were trained on 3,000 hours of\nmulti-lingual, studio-quality recordings with augmented degradations, while USM\nparameters remained fixed. Experimental results demonstrate Miipher-2's\nsuperior or comparable performance to conventional SR models in\nword-error-rate, speaker similarity, and both objective and subjective sound\nquality scores across all tested languages. Miipher-2 operates efficiently on\nconsumer-grade accelerators, achieving a real-time factor of 0.0078, enabling\nthe processing of a million-hour speech dataset in approximately three days\nusing only 100 such accelerators.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04376", "pdf": "https://arxiv.org/pdf/2505.04376", "abs": "https://arxiv.org/abs/2505.04376", "authors": ["Zili Zhang", "Ziting Wen", "Yiheng Qiang", "Hongzhou Dong", "Wenle Dong", "Xinyang Li", "Xiaofan Wang", "Xiaoqiang Ren"], "title": "Label-efficient Single Photon Images Classification via Active Learning", "categories": ["cs.CV"], "comment": null, "summary": "Single-photon LiDAR achieves high-precision 3D imaging in extreme\nenvironments through quantum-level photon detection technology. Current\nresearch primarily focuses on reconstructing 3D scenes from sparse photon\nevents, whereas the semantic interpretation of single-photon images remains\nunderexplored, due to high annotation costs and inefficient labeling\nstrategies. This paper presents the first active learning framework for\nsingle-photon image classification. The core contribution is an imaging\ncondition-aware sampling strategy that integrates synthetic augmentation to\nmodel variability across imaging conditions. By identifying samples where the\nmodel is both uncertain and sensitive to these conditions, the proposed method\nselectively annotates only the most informative examples. Experiments on both\nsynthetic and real-world datasets show that our approach outperforms all\nbaselines and achieves high classification accuracy with significantly fewer\nlabeled samples. Specifically, our approach achieves 97% accuracy on synthetic\nsingle-photon data using only 1.5% labeled samples. On real-world data, we\nmaintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher\nthan the best-performing baseline. This illustrates that active learning\nenables the same level of classification performance on single-photon images as\non classical images, opening doors to large-scale integration of single-photon\ndata in real-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "accuracy"], "score": 2}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04380", "pdf": "https://arxiv.org/pdf/2505.04380", "abs": "https://arxiv.org/abs/2505.04380", "authors": ["Jinhai Xiang", "Shuai Guo", "Qianru Han", "Dantong Shi", "Xinwei He", "Xiang Bai"], "title": "Tetrahedron-Net for Medical Image Registration", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Medical image registration plays a vital role in medical image processing.\nExtracting expressive representations for medical images is crucial for\nimproving the registration quality. One common practice for this end is\nconstructing a convolutional backbone to enable interactions with skip\nconnections among feature extraction layers. The de facto structure, U-Net-like\nnetworks, has attempted to design skip connections such as nested or full-scale\nones to connect one single encoder and one single decoder to improve its\nrepresentation capacity. Despite being effective, it still does not fully\nexplore interactions with a single encoder and decoder architectures. In this\npaper, we embrace this observation and introduce a simple yet effective\nalternative strategy to enhance the representations for registrations by\nappending one additional decoder. The new decoder is designed to interact with\nboth the original encoder and decoder. In this way, it not only reuses feature\npresentation from corresponding layers in the encoder but also interacts with\nthe original decoder to corporately give more accurate registration results.\nThe new architecture is concise yet generalized, with only one encoder and two\ndecoders forming a ``Tetrahedron'' structure, thereby dubbed Tetrahedron-Net.\nThree instantiations of Tetrahedron-Net are further constructed regarding the\ndifferent structures of the appended decoder. Our extensive experiments prove\nthat superior performance can be obtained on several representative benchmarks\nof medical image registration. Finally, such a ``Tetrahedron'' design can also\nbe easily integrated into popular U-Net-like architectures including\nVoxelMorph, ViT-V-Net, and TransMorph, leading to consistent performance gains.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04394", "pdf": "https://arxiv.org/pdf/2505.04394", "abs": "https://arxiv.org/abs/2505.04394", "authors": ["Young-Hu Park", "Rae-Hong Park", "Hyung-Min Park"], "title": "SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin Transformer", "categories": ["cs.CV", "eess.AS"], "comment": null, "summary": "This paper presents an efficient visual speech encoder for lip reading. While\nmost recent lip reading studies have been based on the ResNet architecture and\nhave achieved significant success, they are not sufficiently suitable for\nefficiently capturing lip reading features due to high computational complexity\nin modeling spatio-temporal information. Additionally, using a complex visual\nmodel not only increases the complexity of lip reading models but also induces\ndelays in the overall network for multi-modal studies (e.g., audio-visual\nspeech recognition, speech enhancement, and speech separation). To overcome the\nlimitations of Convolutional Neural Network (CNN)-based models, we apply the\nhierarchical structure and window self-attention of the Swin Transformer to lip\nreading. We configure a new lightweight scale of the Swin Transformer suitable\nfor processing lip reading data and present the SwinLip visual speech encoder,\nwhich efficiently reduces computational load by integrating modified\nConvolution-augmented Transformer (Conformer) temporal embeddings with\nconventional spatial embeddings in the hierarchical structure. Through\nextensive experiments, we have validated that our SwinLip successfully improves\nthe performance and inference speed of the lip reading network when applied to\nvarious backbones for word and sentence recognition, reducing computational\nload. In particular, our SwinLip demonstrated robust performance in both\nEnglish LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art\nperformance on the Mandarin LRW-1000 dataset with less computation compared to\nthe existing state-of-the-art model.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04488", "pdf": "https://arxiv.org/pdf/2505.04488", "abs": "https://arxiv.org/abs/2505.04488", "authors": ["Ziyi Zhang", "Zhen Sun", "Zongmin Zhang", "Zifan Peng", "Yuemeng Zhao", "Zichun Wang", "Zeren Luo", "Ruiting Zuo", "Xinlei He"], "title": "\"I Can See Forever!\": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM"], "comment": "12 pages, 6 figures", "summary": "The visually impaired population, especially the severely visually impaired,\nis currently large in scale, and daily activities pose significant challenges\nfor them. Although many studies use large language and vision-language models\nto assist the blind, most focus on static content and fail to meet real-time\nperception needs in dynamic and complex environments, such as daily activities.\nTo provide them with more effective intelligent assistance, it is imperative to\nincorporate advanced visual understanding technologies. Although real-time\nvision and speech interaction VideoLLMs demonstrate strong real-time visual\nunderstanding, no prior work has systematically evaluated their effectiveness\nin assisting visually impaired individuals. In this work, we conduct the first\nsuch evaluation. First, we construct a benchmark dataset (VisAssistDaily),\ncovering three categories of assistive tasks for visually impaired individuals:\nBasic Skills, Home Life Tasks, and Social Life Tasks. The results show that\nGPT-4o achieves the highest task success rate. Next, we conduct a user study to\nevaluate the models in both closed-world and open-world scenarios, further\nexploring the practical challenges of applying VideoLLMs in assistive contexts.\nOne key issue we identify is the difficulty current models face in perceiving\npotential hazards in dynamic environments. To address this, we build an\nenvironment-awareness dataset named SafeVid and introduce a polling mechanism\nthat enables the model to proactively detect environmental risks. We hope this\nwork provides valuable insights and inspiration for future research in this\nfield.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04586", "pdf": "https://arxiv.org/pdf/2505.04586", "abs": "https://arxiv.org/abs/2505.04586", "authors": ["Yuning Du", "Jingshuai Liu", "Rohan Dharmakumar", "Sotirios A. Tsaftaris"], "title": "Active Sampling for MRI-based Sequential Decision Making", "categories": ["cs.CV", "cs.LG"], "comment": "Under Review", "summary": "Despite the superior diagnostic capability of Magnetic Resonance Imaging\n(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and\ncomplexity. To enable such a future by reducing the magnetic field strength,\none key approach will be to improve sampling strategies. Previous work has\nshown that it is possible to make diagnostic decisions directly from k-space\nwith fewer samples. Such work shows that single diagnostic decisions can be\nmade, but if we aspire to see MRI as a true PoC, multiple and sequential\ndecisions are necessary while minimizing the number of samples acquired. We\npresent a novel multi-objective reinforcement learning framework enabling\ncomprehensive, sequential, diagnostic evaluation from undersampled k-space\ndata. Our approach during inference actively adapts to sequential decisions to\noptimally sample. To achieve this, we introduce a training methodology that\nidentifies the samples that contribute the best to each diagnostic objective\nusing a step-wise weighting reward function. We evaluate our approach in two\nsequential knee pathology assessment tasks: ACL sprain detection and cartilage\nthickness loss assessment. Our framework achieves diagnostic performance\ncompetitive with various policy-based benchmarks on disease detection, severity\nquantification, and overall sequential diagnosis, while substantially saving\nk-space samples. Our approach paves the way for the future of MRI as a\ncomprehensive and affordable PoC device. Our code is publicly available at\nhttps://github.com/vios-s/MRI_Sequential_Active_Sampling", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04612", "pdf": "https://arxiv.org/pdf/2505.04612", "abs": "https://arxiv.org/abs/2505.04612", "authors": ["Jiahao Li", "Haochen Wang", "Muhammad Zubair Irshad", "Igor Vasiljevic", "Matthew R. Walter", "Vitor Campagnolo Guizilini", "Greg Shakhnarovich"], "title": "FastMap: Revisiting Dense and Scalable Structure from Motion", "categories": ["cs.CV"], "comment": "Project webpage: https://jiahao.ai/fastmap", "summary": "We propose FastMap, a new global structure from motion method focused on\nspeed and simplicity. Previous methods like COLMAP and GLOMAP are able to\nestimate high-precision camera poses, but suffer from poor scalability when the\nnumber of matched keypoint pairs becomes large. We identify two key factors\nleading to this problem: poor parallelization and computationally expensive\noptimization steps. To overcome these issues, we design an SfM framework that\nrelies entirely on GPU-friendly operations, making it easily parallelizable.\nMoreover, each optimization step runs in time linear to the number of image\npairs, independent of keypoint pairs or 3D points. Through extensive\nexperiments, we show that FastMap is one to two orders of magnitude faster than\nCOLMAP and GLOMAP on large-scale scenes with comparable pose accuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04616", "pdf": "https://arxiv.org/pdf/2505.04616", "abs": "https://arxiv.org/abs/2505.04616", "authors": ["Feng Liu", "Nicholas Chimitt", "Lanqing Guo", "Jitesh Jain", "Aditya Kane", "Minchul Kim", "Wes Robbins", "Yiyang Su", "Dingqiang Ye", "Xingguang Zhang", "Jie Zhu", "Siddharth Satyakam", "Christopher Perry", "Stanley H. Chan", "Arun Ross", "Humphrey Shi", "Zhangyang Wang", "Anil Jain", "Xiaoming Liu"], "title": "Person Recognition at Altitude and Range: Fusion of Face, Body Shape and Gait", "categories": ["cs.CV"], "comment": "18 pages, 12 figures", "summary": "We address the problem of whole-body person recognition in unconstrained\nenvironments. This problem arises in surveillance scenarios such as those in\nthe IARPA Biometric Recognition and Identification at Altitude and Range\n(BRIAR) program, where biometric data is captured at long standoff distances,\nelevated viewing angles, and under adverse atmospheric conditions (e.g.,\nturbulence and high wind velocity). To this end, we propose FarSight, a unified\nend-to-end system for person recognition that integrates complementary\nbiometric cues across face, gait, and body shape modalities. FarSight\nincorporates novel algorithms across four core modules: multi-subject detection\nand tracking, recognition-aware video restoration, modality-specific biometric\nfeature encoding, and quality-guided multi-modal fusion. These components are\ndesigned to work cohesively under degraded image conditions, large pose and\nscale variations, and cross-domain gaps. Extensive experiments on the BRIAR\ndataset, one of the most comprehensive benchmarks for long-range, multi-modal\nbiometric recognition, demonstrate the effectiveness of FarSight. Compared to\nour preliminary system, this system achieves a 34.1% absolute gain in 1:1\nverification accuracy (TAR@0.1% FAR), a 17.8% increase in closed-set\nidentification (Rank-20), and a 34.3% reduction in open-set identification\nerrors (FNIR@1% FPIR). Furthermore, FarSight was evaluated in the 2025 NIST RTE\nFace in Video Evaluation (FIVE), which conducts standardized face recognition\ntesting on the BRIAR dataset. These results establish FarSight as a\nstate-of-the-art solution for operational biometric recognition in challenging\nreal-world conditions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04620", "pdf": "https://arxiv.org/pdf/2505.04620", "abs": "https://arxiv.org/abs/2505.04620", "authors": ["Hao Fei", "Yuan Zhou", "Juncheng Li", "Xiangtai Li", "Qingshan Xu", "Bobo Li", "Shengqiong Wu", "Yaoting Wang", "Junbao Zhou", "Jiahao Meng", "Qingyu Shi", "Zhiyuan Zhou", "Liangtao Shi", "Minghe Gao", "Daoan Zhang", "Zhiqi Ge", "Weiming Wu", "Siliang Tang", "Kaihang Pan", "Yaobo Ye", "Haobo Yuan", "Tao Zhang", "Tianjie Ju", "Zixiang Meng", "Shilin Xu", "Liyu Jia", "Wentao Hu", "Meng Luo", "Jiebo Luo", "Tat-Seng Chua", "Shuicheng Yan", "Hanwang Zhang"], "title": "On Path to Multimodal Generalist: General-Level and General-Bench", "categories": ["cs.CV"], "comment": "ICML'25, 305 pages, 115 tables, 177 figures, project page:\n  https://generalist.top/", "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.03757", "pdf": "https://arxiv.org/pdf/2505.03757", "abs": "https://arxiv.org/abs/2505.03757", "authors": ["Vinicius Francisco Rofatto", "Luiz Felipe Rodrigues de Almeida", "Marcelo Tomio Matsuoka", "Ivandro Klein", "Mauricio Roberto Veronez", "Luiz Gonzaga Da Silveira Junior"], "title": "On the Residual-based Neural Network for Unmodeled Distortions in Coordinate Transformation", "categories": ["physics.geo-ph", "cs.CV", "cs.LG", "stat.AP", "stat.ML"], "comment": null, "summary": "Coordinate transformation models often fail to account for nonlinear and\nspatially dependent distortions, leading to significant residual errors in\ngeospatial applications. Here we propose a residual-based neural correction\nstrategy, in which a neural network learns to model only the systematic\ndistortions left by an initial geometric transformation. By focusing solely on\nresidual patterns, the proposed method reduces model complexity and improves\nperformance, particularly in scenarios with sparse or structured control point\nconfigurations. We evaluate the method using both simulated datasets with\nvarying distortion intensities and sampling strategies, as well as under the\nreal-world image georeferencing tasks. Compared with direct neural network\ncoordinate converter and classical transformation models, the residual-based\nneural correction delivers more accurate and stable results under challenging\nconditions, while maintaining comparable performance in ideal cases. These\nfindings demonstrate the effectiveness of residual modelling as a lightweight\nand robust alternative for improving coordinate transformation accuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.03788", "pdf": "https://arxiv.org/pdf/2505.03788", "abs": "https://arxiv.org/abs/2505.03788", "authors": ["Trilok Padhi", "Ramneet Kaur", "Adam D. Cobb", "Manoj Acharya", "Anirban Roy", "Colin Samplawski", "Brian Matejek", "Alexander M. Berenbeim", "Nathaniel D. Bastian", "Susmit Jha"], "title": "Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce a novel approach for calibrating uncertainty quantification (UQ)\ntailored for multi-modal large language models (LLMs). Existing\nstate-of-the-art UQ methods rely on consistency among multiple responses\ngenerated by the LLM on an input query under diverse settings. However, these\napproaches often report higher confidence in scenarios where the LLM is\nconsistently incorrect. This leads to a poorly calibrated confidence with\nrespect to accuracy. To address this, we leverage cross-modal consistency in\naddition to self-consistency to improve the calibration of the multi-modal\nmodels. Specifically, we ground the textual responses to the visual inputs. The\nconfidence from the grounding model is used to calibrate the overall\nconfidence. Given that using a grounding model adds its own uncertainty in the\npipeline, we apply temperature scaling - a widely accepted parametric\ncalibration technique - to calibrate the grounding model's confidence in the\naccuracy of generated responses. We evaluate the proposed approach across\nmultiple multi-modal tasks, such as medical question answering (Slake) and\nvisual question answering (VQAv2), considering multi-modal models such as\nLLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework\nachieves significantly improved calibration on both tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.03844", "pdf": "https://arxiv.org/pdf/2505.03844", "abs": "https://arxiv.org/abs/2505.03844", "authors": ["SolÃ¨ne DebuysÃ¨re", "Nicolas TrouvÃ©", "Nathan Letheule", "Olivier LÃ©vÃªque", "Elise Colin"], "title": "From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The availability of Synthetic Aperture Radar (SAR) satellite imagery has\nincreased considerably in recent years, with datasets commercially available.\nHowever, the acquisition of high-resolution SAR images in airborne\nconfigurations, remains costly and limited. Thus, the lack of open source,\nwell-labeled, or easily exploitable SAR text-image datasets is a barrier to the\nuse of existing foundation models in remote sensing applications. In this\ncontext, synthetic image generation is a promising solution to augment this\nscarce data, enabling a broader range of applications. Leveraging over 15 years\nof ONERA's extensive archival airborn data from acquisition campaigns, we\ncreated a comprehensive training dataset of 110 thousands SAR images to exploit\na 3.5 billion parameters pre-trained latent diffusion model. In this work, we\npresent a novel approach utilizing spatial conditioning techniques within a\nfoundation model to transform satellite SAR imagery into airborne SAR\nrepresentations. Additionally, we demonstrate that our pipeline is effective\nfor bridging the realism of simulated images generated by ONERA's physics-based\nsimulator EMPRISE. Our method explores a key application of AI in advancing SAR\nimaging technology. To the best of our knowledge, we are the first to introduce\nthis approach in the literature.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.03845", "pdf": "https://arxiv.org/pdf/2505.03845", "abs": "https://arxiv.org/abs/2505.03845", "authors": ["Ioannis Kyprakis", "Vasileios Skaramagkas", "Iro Boura", "Georgios Karamanis", "Dimitrios I. Fotiadis", "Zinovia Kefalopoulou", "Cleanthe Spanaki", "Manolis Tsiknakis"], "title": "A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with\nmotor and non-motor symptoms. Depressive symptoms are prevalent in PD,\naffecting up to 45% of patients. They are often underdiagnosed due to\noverlapping motor features, such as hypomimia. This study explores deep\nlearning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention\nlayers-to assess the presence and severity of depressive symptoms, as detected\nby the Geriatric Depression Scale (GDS), in PD patients through facial video\nanalysis. The same parameters were assessed in a secondary analysis taking into\naccount whether patients were one hour after (ON-medication state) or 12 hours\nwithout (OFF-medication state) dopaminergic medication. Using a dataset of\n1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest\nperformance, with up to 94% accuracy and 93.7% F1-score in binary\nclassification (presence of absence of depressive symptoms), and 87.1% accuracy\nwith an 85.4% F1-score in multiclass tasks (absence or mild or severe\ndepressive symptoms).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04590", "pdf": "https://arxiv.org/pdf/2505.04590", "abs": "https://arxiv.org/abs/2505.04590", "authors": ["Alexandre Binninger", "Ruben Wiersma", "Philipp Herholz", "Olga Sorkine-Hornung"], "title": "TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral Grids for Gradient-Based Mesh Optimization", "categories": ["cs.GR", "cs.CV", "I.3.5"], "comment": "ACM Trans. Graph. 44, 4. SIGGRAPH 2025. 19 pages, 21 figures", "summary": "We introduce TetWeave, a novel isosurface representation for gradient-based\nmesh optimization that jointly optimizes the placement of a tetrahedral grid\nused for Marching Tetrahedra and a novel directional signed distance at each\npoint. TetWeave constructs tetrahedral grids on-the-fly via Delaunay\ntriangulation, enabling increased flexibility compared to predefined grids. The\nextracted meshes are guaranteed to be watertight, two-manifold and\nintersection-free. The flexibility of TetWeave enables a resampling strategy\nthat places new points where reconstruction error is high and allows to\nencourage mesh fairness without compromising on reconstruction error. This\nleads to high-quality, adaptive meshes that require minimal memory usage and\nfew parameters to optimize. Consequently, TetWeave exhibits near-linear memory\nscaling relative to the vertex count of the output mesh - a substantial\nimprovement over predefined grids. We demonstrate the applicability of TetWeave\nto a broad range of challenging tasks in computer graphics and vision, such as\nmulti-view 3D reconstruction, mesh compression and geometric texture\ngeneration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
{"id": "2505.04622", "pdf": "https://arxiv.org/pdf/2505.04622", "abs": "https://arxiv.org/abs/2505.04622", "authors": ["Jingwen Ye", "Yuze He", "Yanning Zhou", "Yiqin Zhu", "Kaiwen Xiao", "Yong-Jin Liu", "Wei Yang", "Xiao Han"], "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025. 14 pages, 15 figures", "summary": "Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-08.jsonl"}
