{"id": "2505.04831", "pdf": "https://arxiv.org/pdf/2505.04831", "abs": "https://arxiv.org/abs/2505.04831", "authors": ["Nicholas Pfaff", "Hongkai Dai", "Sergey Zakharov", "Shun Iwase", "Russ Tedrake"], "title": "Steerable Scene Generation with Post Training and Inference-Time Search", "categories": ["cs.RO", "cs.GR", "cs.LG"], "comment": "Project website: https://steerable-scene-generation.github.io/", "summary": "Training robots in simulation requires diverse 3D scenes that reflect the\nspecific challenges of downstream tasks. However, scenes that satisfy strict\ntask requirements, such as high-clutter environments with plausible spatial\narrangement, are rare and costly to curate manually. Instead, we generate\nlarge-scale scene data using procedural models that approximate realistic\nenvironments for robotic manipulation, and adapt it to task-specific goals. We\ndo this by training a unified diffusion-based generative model that predicts\nwhich objects to place from a fixed asset library, along with their SE(3)\nposes. This model serves as a flexible scene prior that can be adapted using\nreinforcement learning-based post training, conditional generation, or\ninference-time search, steering generation toward downstream objectives even\nwhen they differ from the original data distribution. Our method enables\ngoal-directed scene synthesis that respects physical feasibility and scales\nacross scene types. We introduce a novel MCTS-based inference-time search\nstrategy for diffusion models, enforce feasibility via projection and\nsimulation, and release a dataset of over 44 million SE(3) scenes spanning five\ndiverse environments. Website with videos, code, data, and model weights:\nhttps://steerable-scene-generation.github.io/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scale", "MCTS"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05408", "pdf": "https://arxiv.org/pdf/2505.05408", "abs": "https://arxiv.org/abs/2505.05408", "authors": ["Zheng-Xin Yong", "M. Farid Adilazuarda", "Jonibek Mansurov", "Ruochen Zhang", "Niklas Muennighoff", "Carsten Eickhoff", "Genta Indra Winata", "Julia Kreutzer", "Stephen H. Bach", "Alham Fikri Aji"], "title": "Crosslingual Reasoning through Test-Time Scaling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "inference compute"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04638", "pdf": "https://arxiv.org/pdf/2505.04638", "abs": "https://arxiv.org/abs/2505.04638", "authors": ["Tianyu Liu", "Simeng Han", "Xiao Luo", "Hanchen Wang", "Pan Lu", "Biqing Zhu", "Yuge Wang", "Keyi Li", "Jiapeng Chen", "Rihao Qu", "Yufeng Liu", "Xinyue Cui", "Aviv Yaish", "Yuhang Chen", "Minsheng Hao", "Chuhan Li", "Kexing Li", "Arman Cohan", "Hua Xu", "Mark Gerstein", "James Zou", "Hongyu Zhao"], "title": "Towards Artificial Intelligence Research Assistant for Expert-Involved Learning", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "36 pages, 7 figures", "summary": "Large Language Models (LLMs) and Large Multi-Modal Models (LMMs) have emerged\nas transformative tools in scientific research, yet their reliability and\nspecific contributions to biomedical applications remain insufficiently\ncharacterized. In this study, we present \\textbf{AR}tificial\n\\textbf{I}ntelligence research assistant for \\textbf{E}xpert-involved\n\\textbf{L}earning (ARIEL), a multimodal dataset designed to benchmark and\nenhance two critical capabilities of LLMs and LMMs in biomedical research:\nsummarizing extensive scientific texts and interpreting complex biomedical\nfigures. To facilitate rigorous assessment, we create two open-source sets\ncomprising biomedical articles and figures with designed questions. We\nsystematically benchmark both open- and closed-source foundation models,\nincorporating expert-driven human evaluations conducted by doctoral-level\nexperts. Furthermore, we improve model performance through targeted prompt\nengineering and fine-tuning strategies for summarizing research papers, and\napply test-time computational scaling to enhance the reasoning capabilities of\nLMMs, achieving superior accuracy compared to human-expert corrections. We also\nexplore the potential of using LMM Agents to generate scientific hypotheses\nfrom diverse multimodal inputs. Overall, our results delineate clear strengths\nand highlight significant limitations of current foundation models, providing\nactionable insights and guiding future advancements in deploying large-scale\nlanguage and multi-modal models within biomedical research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04864", "pdf": "https://arxiv.org/pdf/2505.04864", "abs": "https://arxiv.org/abs/2505.04864", "authors": ["Kanggeon Lee", "Soochahn Lee", "Kyoung Mu Lee"], "title": "Auto-regressive transformation for image alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing methods for image alignment struggle in cases involving\nfeature-sparse regions, extreme scale and field-of-view differences, and large\ndeformations, often resulting in suboptimal accuracy. Robustness to these\nchallenges improves through iterative refinement of the transformation field\nwhile focusing on critical regions in multi-scale image representations. We\nthus propose Auto-Regressive Transformation (ART), a novel method that\niteratively estimates the coarse-to-fine transformations within an\nauto-regressive framework. Leveraging hierarchical multi-scale features, our\nnetwork refines the transformations using randomly sampled points at each\nscale. By incorporating guidance from the cross-attention layer, the model\nfocuses on critical regions, ensuring accurate alignment even in challenging,\nfeature-limited conditions. Extensive experiments across diverse datasets\ndemonstrate that ART significantly outperforms state-of-the-art methods,\nestablishing it as a powerful new method for precise image alignment with broad\napplicability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "iterative refinement"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04846", "pdf": "https://arxiv.org/pdf/2505.04846", "abs": "https://arxiv.org/abs/2505.04846", "authors": ["Ozan Gokdemir", "Carlo Siebenschuh", "Alexander Brace", "Azton Wells", "Brian Hsu", "Kyle Hippe", "Priyanka V. Setty", "Aswathy Ajith", "J. Gregory Pauloski", "Varuni Sastry", "Sam Foreman", "Huihuo Zheng", "Heng Ma", "Bharat Kale", "Nicholas Chia", "Thomas Gibbs", "Michael E. Papka", "Thomas Brettin", "Francis J. Alexander", "Anima Anandkumar", "Ian Foster", "Rick Stevens", "Venkatram Vishwanath", "Arvind Ramanathan"], "title": "HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights", "categories": ["cs.IR", "cs.CE", "cs.CL", "cs.DC", "cs.LG", "H.3.3; I.2.7"], "comment": "This paper has been accepted at the Platform for Advanced Scientific\n  Computing Conference (PASC 25), June 16-18, 2025, Brugg-Windisch, Switzerland", "summary": "The volume of scientific literature is growing exponentially, leading to\nunderutilized discoveries, duplicated efforts, and limited cross-disciplinary\ncollaboration. Retrieval Augmented Generation (RAG) offers a way to assist\nscientists by improving the factuality of Large Language Models (LLMs) in\nprocessing this influx of information. However, scaling RAG to handle millions\nof articles introduces significant challenges, including the high computational\ncosts associated with parsing documents and embedding scientific knowledge, as\nwell as the algorithmic complexity of aligning these representations with the\nnuanced semantics of scientific content. To address these issues, we introduce\nHiPerRAG, a RAG workflow powered by high performance computing (HPC) to index\nand retrieve knowledge from more than 3.6 million scientific articles. At its\ncore are Oreo, a high-throughput model for multimodal document parsing, and\nColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval\naccuracy by using contrastive learning and late-interaction techniques.\nHiPerRAG delivers robust performance on existing scientific question answering\nbenchmarks and two new benchmarks introduced in this work, achieving 90%\naccuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific models\nlike PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs\non the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million\ndocument-scale RAG workflows for unifying scientific knowledge and fostering\ninterdisciplinary innovation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05375", "pdf": "https://arxiv.org/pdf/2505.05375", "abs": "https://arxiv.org/abs/2505.05375", "authors": ["Kejie Zhao", "Wenjia Hua", "Aiersi Tuerhong", "Luziwei Leng", "Yuxin Ma", "Qinghua Guo"], "title": "Threshold Modulation for Online Test-Time Adaptation of Spiking Neural Networks", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "Accepted by IJCNN 2025. \\c{opyright} 2025 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, including reprinting/republishing this material for advertising or\n  promotional purposes, collecting new collected works for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05315", "pdf": "https://arxiv.org/pdf/2505.05315", "abs": "https://arxiv.org/abs/2505.05315", "authors": ["Yuhui Xu", "Hanze Dong", "Lei Wang", "Doyen Sahoo", "Junnan Li", "Caiming Xiong"], "title": "Scalable Chain of Thoughts via Elastic Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05472", "pdf": "https://arxiv.org/pdf/2505.05472", "abs": "https://arxiv.org/abs/2505.05472", "authors": ["Chao Liao", "Liyang Liu", "Xun Wang", "Zhengxiong Luo", "Xinyu Zhang", "Wenliang Zhao", "Jie Wu", "Liang Li", "Zhi Tian", "Weilin Huang"], "title": "Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation", "categories": ["cs.CV"], "comment": "Mogao Technical Report", "summary": "Recent progress in unified models for image understanding and generation has\nbeen impressive, yet most approaches remain limited to single-modal generation\nconditioned on multiple modalities. In this paper, we present Mogao, a unified\nframework that advances this paradigm by enabling interleaved multi-modal\ngeneration through a causal approach. Mogao integrates a set of key technical\nimprovements in architecture design, including a deep-fusion design, dual\nvision encoders, interleaved rotary position embeddings, and multi-modal\nclassifier-free guidance, which allow it to harness the strengths of both\nautoregressive models for text generation and diffusion models for high-quality\nimage synthesis. These practical improvements also make Mogao particularly\neffective to process interleaved sequences of text and images arbitrarily. To\nfurther unlock the potential of unified models, we introduce an efficient\ntraining strategy on a large-scale, in-house dataset specifically curated for\njoint text and image generation. Extensive experiments show that Mogao not only\nachieves state-of-the-art performance in multi-modal understanding and\ntext-to-image generation, but also excels in producing high-quality, coherent\ninterleaved outputs. Its emergent capabilities in zero-shot image editing and\ncompositional generation highlight Mogao as a practical omni-modal foundation\nmodel, paving the way for future development and scaling the unified\nmulti-modal systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04628", "pdf": "https://arxiv.org/pdf/2505.04628", "abs": "https://arxiv.org/abs/2505.04628", "authors": ["Yusen Wu", "Junwu Xiong", "Xiaotie Deng"], "title": "How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Expanding the application of large language models (LLMs) to societal life,\ninstead of primary function only as auxiliary assistants to communicate with\nonly one person at a time, necessitates LLMs' capabilities to independently\nplay roles in multi-user, multi-turn social agent tasks within complex social\nsettings. However, currently the capability has not been systematically\nmeasured with available benchmarks. To address this gap, we first introduce an\nagent task leveling framework grounded in sociological principles.\nConcurrently, we propose a novel benchmark, How Social Is It (we call it HSII\nbelow), designed to assess LLM's social capabilities in comprehensive social\nagents tasks and benchmark representative models. HSII comprises four stages:\nformat parsing, target selection, target switching conversation, and stable\nconversation, which collectively evaluate the communication and task completion\ncapabilities of LLMs within realistic social interaction scenarios dataset,\nHSII-Dataset. The dataset is derived step by step from news dataset. We perform\nan ablation study by doing clustering to the dataset. Additionally, we\ninvestigate the impact of chain of thought (COT) method on enhancing LLMs'\nsocial performance. Since COT cost more computation, we further introduce a new\nstatistical metric, COT-complexity, to quantify the efficiency of certain LLMs\nwith COTs for specific social tasks and strike a better trade-off between\nmeasurement of correctness and efficiency. Various results of our experiments\ndemonstrate that our benchmark is well-suited for evaluating social skills in\nLLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04740", "pdf": "https://arxiv.org/pdf/2505.04740", "abs": "https://arxiv.org/abs/2505.04740", "authors": ["Sainath Dey", "Mitul Goswami", "Jashika Sethi", "Prasant Kumar Pattnaik"], "title": "Hyb-KAN ViT: Hybrid Kolmogorov-Arnold Networks Augmented Vision Transformer", "categories": ["cs.CV"], "comment": null, "summary": "This study addresses the inherent limitations of Multi-Layer Perceptrons\n(MLPs) in Vision Transformers (ViTs) by introducing Hybrid Kolmogorov-Arnold\nNetwork (KAN)-ViT (Hyb-KAN ViT), a novel framework that integrates\nwavelet-based spectral decomposition and spline-optimized activation functions,\nprior work has failed to focus on the prebuilt modularity of the ViT\narchitecture and integration of edge detection capabilities of Wavelet\nfunctions. We propose two key modules: Efficient-KAN (Eff-KAN), which replaces\nMLP layers with spline functions and Wavelet-KAN (Wav-KAN), leveraging\northogonal wavelet transforms for multi-resolution feature extraction. These\nmodules are systematically integrated in ViT encoder layers and classification\nheads to enhance spatial-frequency modeling while mitigating computational\nbottlenecks. Experiments on ImageNet-1K (Image Recognition), COCO (Object\nDetection and Instance Segmentation), and ADE20K (Semantic Segmentation)\ndemonstrate state-of-the-art performance with Hyb-KAN ViT. Ablation studies\nvalidate the efficacy of wavelet-driven spectral priors in segmentation and\nspline-based efficiency in detection tasks. The framework establishes a new\nparadigm for balancing parameter efficiency and multi-scale representation in\nvision architectures.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04758", "pdf": "https://arxiv.org/pdf/2505.04758", "abs": "https://arxiv.org/abs/2505.04758", "authors": ["Songsong Duan", "Xi Yang", "Nannan Wang", "Xinbo Gao"], "title": "Lightweight RGB-D Salient Object Detection from a Speed-Accuracy Tradeoff Perspective", "categories": ["cs.CV"], "comment": "Accepted by TIP 2025", "summary": "Current RGB-D methods usually leverage large-scale backbones to improve\naccuracy but sacrifice efficiency. Meanwhile, several existing lightweight\nmethods are difficult to achieve high-precision performance. To balance the\nefficiency and performance, we propose a Speed-Accuracy Tradeoff Network\n(SATNet) for Lightweight RGB-D SOD from three fundamental perspectives: depth\nquality, modality fusion, and feature representation. Concerning depth quality,\nwe introduce the Depth Anything Model to generate high-quality depth maps,which\neffectively alleviates the multi-modal gaps in the current datasets. For\nmodality fusion, we propose a Decoupled Attention Module (DAM) to explore the\nconsistency within and between modalities. Here, the multi-modal features are\ndecoupled into dual-view feature vectors to project discriminable information\nof feature maps. For feature representation, we develop a Dual Information\nRepresentation Module (DIRM) with a bi-directional inverted framework to\nenlarge the limited feature space generated by the lightweight backbones. DIRM\nmodels texture features and saliency features to enrich feature space, and\nemploy two-way prediction heads to optimal its parameters through a\nbi-directional backpropagation. Finally, we design a Dual Feature Aggregation\nModule (DFAM) in the decoder to aggregate texture and saliency features.\nExtensive experiments on five public RGB-D SOD datasets indicate that the\nproposed SATNet excels state-of-the-art (SOTA) CNN-based heavyweight models and\nachieves a lightweight framework with 5.2 M parameters and 415 FPS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04645", "pdf": "https://arxiv.org/pdf/2505.04645", "abs": "https://arxiv.org/abs/2505.04645", "authors": ["Tejas Jade", "Alex Yartsev"], "title": "ChatGPT for automated grading of short answer questions in mechanical ventilation", "categories": ["cs.CL", "cs.LG", "stat.CO"], "comment": null, "summary": "Standardised tests using short answer questions (SAQs) are common in\npostgraduate education. Large language models (LLMs) simulate conversational\nlanguage and interpret unstructured free-text responses in ways aligning with\napplying SAQ grading rubrics, making them attractive for automated grading. We\nevaluated ChatGPT 4o to grade SAQs in a postgraduate medical setting using data\nfrom 215 students (557 short-answer responses) enrolled in an online course on\nmechanical ventilation (2020--2024). Deidentified responses to three case-based\nscenarios were presented to ChatGPT with a standardised grading prompt and\nrubric. Outputs were analysed using mixed-effects modelling, variance component\nanalysis, intraclass correlation coefficients (ICCs), Cohen's kappa, Kendall's\nW, and Bland--Altman statistics. ChatGPT awarded systematically lower marks\nthan human graders with a mean difference (bias) of -1.34 on a 10-point scale.\nICC values indicated poor individual-level agreement (ICC1 = 0.086), and\nCohen's kappa (-0.0786) suggested no meaningful agreement. Variance component\nanalysis showed minimal variability among the five ChatGPT sessions (G-value =\n0.87), indicating internal consistency but divergence from the human grader.\nThe poorest agreement was observed for evaluative and analytic items, whereas\nchecklist and prescriptive rubric items had less disagreement. We caution\nagainst the use of LLMs in grading postgraduate coursework. Over 60% of\nChatGPT-assigned grades differed from human grades by more than acceptable\nboundaries for high-stakes assessments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "agreement", "consistency", "kappa", "rubric"], "score": 5}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04649", "pdf": "https://arxiv.org/pdf/2505.04649", "abs": "https://arxiv.org/abs/2505.04649", "authors": ["Chengzhang Yu", "Yiming Zhang", "Zhixin Liu", "Zenghui Ding", "Yining Sun", "Zhanpeng Jin"], "title": "FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights", "categories": ["cs.CL"], "comment": "12 pages, 4 figures, 5 table", "summary": "The automation of scientific research through large language models (LLMs)\npresents significant opportunities but faces critical challenges in knowledge\nsynthesis and quality assurance. We introduce Feedback-Refined Agent\nMethodology (FRAME), a novel framework that enhances medical paper generation\nthrough iterative refinement and structured feedback. Our approach comprises\nthree key innovations: (1) A structured dataset construction method that\ndecomposes 4,287 medical papers into essential research components through\niterative refinement; (2) A tripartite architecture integrating Generator,\nEvaluator, and Reflector agents that progressively improve content quality\nthrough metric-driven feedback; and (3) A comprehensive evaluation framework\nthat combines statistical metrics with human-grounded benchmarks. Experimental\nresults demonstrate FRAME's effectiveness, achieving significant improvements\nover conventional approaches across multiple models (9.91% average gain with\nDeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation\ndimensions. Human evaluation confirms that FRAME-generated papers achieve\nquality comparable to human-authored works, with particular strength in\nsynthesizing future research directions. The results demonstrated our work\ncould efficiently assist medical research by building a robust foundation for\nautomated medical research paper generation while maintaining rigorous academic\nstandards.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04793", "pdf": "https://arxiv.org/pdf/2505.04793", "abs": "https://arxiv.org/abs/2505.04793", "authors": ["Kailash A. Hambarde", "Nzakiese Mbongo", "Pavan Kumar MP", "Satish Mekewad", "Carolina Fernandes", "Gökhan Silahtaroğlu", "Alice Nithya", "Pawan Wasnik", "MD. Rashidunnabi", "Pranita Samale", "Hugo Proença"], "title": "DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Person reidentification (ReID) technology has been considered to perform\nrelatively well under controlled, ground-level conditions, but it breaks down\nwhen deployed in challenging real-world settings. Evidently, this is due to\nextreme data variability factors such as resolution, viewpoint changes, scale\nvariations, occlusions, and appearance shifts from clothing or session drifts.\nMoreover, the publicly available data sets do not realistically incorporate\nsuch kinds and magnitudes of variability, which limits the progress of this\ntechnology. This paper introduces DetReIDX, a large-scale aerial-ground person\ndataset, that was explicitly designed as a stress test to ReID under real-world\nconditions. DetReIDX is a multi-session set that includes over 13 million\nbounding boxes from 509 identities, collected in seven university campuses from\nthree continents, with drone altitudes between 5.8 and 120 meters. More\nimportant, as a key novelty, DetReIDX subjects were recorded in (at least) two\nsessions on different days, with changes in clothing, daylight and location,\nmaking it suitable to actually evaluate long-term person ReID. Plus, data were\nannotated from 16 soft biometric attributes and multitask labels for detection,\ntracking, ReID, and action recognition. In order to provide empirical evidence\nof DetReIDX usefulness, we considered the specific tasks of human detection and\nReID, where SOTA methods catastrophically degrade performance (up to 80% in\ndetection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs\nconditions. The dataset, annotations, and official evaluation protocols are\npublicly available at https://www.it.ubi.pt/DetReIDX/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04654", "pdf": "https://arxiv.org/pdf/2505.04654", "abs": "https://arxiv.org/abs/2505.04654", "authors": ["Yehor Tereshchenko", "Mika Hämäläinen"], "title": "A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient", "categories": ["cs.CL"], "comment": null, "summary": "Artificial Intelligence (AI) and Large Language Models (LLMs) have rapidly\nevolved in recent years, showcasing remarkable capabilities in natural language\nunderstanding and generation. However, these advancements also raise critical\nethical questions regarding safety, potential misuse, discrimination and\noverall societal impact. This article provides a comparative analysis of the\nethical performance of various AI models, including the brand new\nDeepSeek-V3(R1 with reasoning and without), various GPT variants (4o, 3.5\nTurbo, 4 Turbo, o1/o3 mini) and Gemini (1.5 flash, 2.0 flash and 2.0 flash exp)\nand highlights the need for robust human oversight, especially in situations\nwith high stakes. Furthermore, we present a new metric for calculating harm in\nLLMs called Relative Danger Coefficient (RDC).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04877", "pdf": "https://arxiv.org/pdf/2505.04877", "abs": "https://arxiv.org/abs/2505.04877", "authors": ["Lianbo Ma", "Jianlun Ma", "Yuee Zhou", "Guoyang Xie", "Qiang He", "Zhichao Lu"], "title": "Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mixed Precision Quantization (MPQ) has become an essential technique for\noptimizing neural network by determining the optimal bitwidth per layer.\nExisting MPQ methods, however, face a major hurdle: they require a\ncomputationally expensive search for quantization policies on large-scale\ndatasets. To resolve this issue, we introduce a novel approach that first\nsearches for quantization policies on small datasets and then generalizes them\nto large-scale datasets. This approach simplifies the process, eliminating the\nneed for large-scale quantization fine-tuning and only necessitating model\nweight adjustment. Our method is characterized by three key techniques:\nsharpness-aware minimization for enhanced quantization generalization, implicit\ngradient direction alignment to handle gradient conflicts among different\noptimization objectives, and an adaptive perturbation radius to accelerate\noptimization. Both theoretical analysis and experimental results validate our\napproach. Using the CIFAR10 dataset (just 0.5\\% the size of ImageNet training\ndata) for MPQ policy search, we achieved equivalent accuracy on ImageNet with a\nsignificantly lower computational cost, while improving efficiency by up to\n150% over the baselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04915", "pdf": "https://arxiv.org/pdf/2505.04915", "abs": "https://arxiv.org/abs/2505.04915", "authors": ["Tong Wang", "Ting Liu", "Xiaochao Qu", "Chengjing Wu", "Luoqi Liu", "Xiaolin Hu"], "title": "GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Scene text editing, a subfield of image editing, requires modifying texts in\nimages while preserving style consistency and visual coherence with the\nsurrounding environment. While diffusion-based methods have shown promise in\ntext generation, they still struggle to produce high-quality results. These\nmethods often generate distorted or unrecognizable characters, particularly\nwhen dealing with complex characters like Chinese. In such systems, characters\nare composed of intricate stroke patterns and spatial relationships that must\nbe precisely maintained. We present GlyphMastero, a specialized glyph encoder\ndesigned to guide the latent diffusion model for generating texts with\nstroke-level precision. Our key insight is that existing methods, despite using\npretrained OCR models for feature extraction, fail to capture the hierarchical\nnature of text structures - from individual strokes to stroke-level\ninteractions to overall character-level structure. To address this, our glyph\nencoder explicitly models and captures the cross-level interactions between\nlocal-level individual characters and global-level text lines through our novel\nglyph attention module. Meanwhile, our model implements a feature pyramid\nnetwork to fuse the multi-scale OCR backbone features at the global-level.\nThrough these cross-level and multi-scale fusions, we obtain more detailed\nglyph-aware guidance, enabling precise control over the scene text generation\nprocess. Our method achieves an 18.02\\% improvement in sentence accuracy over\nthe state-of-the-art multi-lingual scene text editing baseline, while\nsimultaneously reducing the text-region Fr\\'echet inception distance by\n53.28\\%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04844", "pdf": "https://arxiv.org/pdf/2505.04844", "abs": "https://arxiv.org/abs/2505.04844", "authors": ["Alex Shan", "John Bauer", "Christopher D. Manning"], "title": "Osiris: A Lightweight Open-Source Hallucination Detection System", "categories": ["cs.CL"], "comment": "Stanford 191W", "summary": "Retrieval-Augmented Generation (RAG) systems have gained widespread adoption\nby application builders because they leverage sources of truth to enable Large\nLanguage Models (LLMs) to generate more factually sound responses. However,\nhallucinations, instances of LLM responses that are unfaithful to the provided\ncontext, often prevent these systems from being deployed in production\nenvironments. Current hallucination detection methods typically involve human\nevaluation or the use of closed-source models to review RAG system outputs for\nhallucinations. Both human evaluators and closed-source models suffer from\nscaling issues due to their high costs and slow inference speeds. In this work,\nwe introduce a perturbed multi-hop QA dataset with induced hallucinations. Via\nsupervised fine-tuning on our dataset, we achieve better recall with a 7B model\nthan GPT-4o on the RAGTruth hallucination detection benchmark and offer\ncompetitive performance on precision and accuracy, all while using a fraction\nof the parameters. Code is released at our repository.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04922", "pdf": "https://arxiv.org/pdf/2505.04922", "abs": "https://arxiv.org/abs/2505.04922", "authors": ["Xingzeng Lan", "Xing Duan", "Chen Chen", "Weiyu Lin", "Bo Wang"], "title": "Canny2Palm: Realistic and Controllable Palmprint Generation for Large-scale Pre-training", "categories": ["cs.CV"], "comment": null, "summary": "Palmprint recognition is a secure and privacy-friendly method of biometric\nidentification. One of the major challenges to improve palmprint recognition\naccuracy is the scarcity of palmprint data. Recently, a popular line of\nresearch revolves around the synthesis of virtual palmprints for large-scale\npre-training purposes. In this paper, we propose a novel synthesis method named\nCanny2Palm that extracts palm textures with Canny edge detector and uses them\nto condition a Pix2Pix network for realistic palmprint generation. By\nre-assembling palmprint textures from different identities, we are able to\ncreate new identities by seeding the generator with new assemblies. Canny2Palm\nnot only synthesizes realistic data following the distribution of real\npalmprints but also enables controllable diversity to generate large-scale new\nidentities. On open-set palmprint recognition benchmarks, models pre-trained\nwith Canny2Palm synthetic data outperform the state-of-the-art with up to 7.2%\nhigher identification accuracy. Moreover, the performance of models pre-trained\nwith Canny2Palm continues to improve given 10,000 synthetic IDs while those\nwith existing methods already saturate, demonstrating the potential of our\nmethod for large-scale pre-training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04941", "pdf": "https://arxiv.org/pdf/2505.04941", "abs": "https://arxiv.org/abs/2505.04941", "authors": ["Jiepan Li", "He Huang", "Yu Sheng", "Yujun Guo", "Wei He"], "title": "Building-Guided Pseudo-Label Learning for Cross-Modal Building Damage Mapping", "categories": ["cs.CV"], "comment": null, "summary": "Accurate building damage assessment using bi-temporal multi-modal remote\nsensing images is essential for effective disaster response and recovery\nplanning. This study proposes a novel Building-Guided Pseudo-Label Learning\nFramework to address the challenges of mapping building damage from\npre-disaster optical and post-disaster SAR images. First, we train a series of\nbuilding extraction models using pre-disaster optical images and building\nlabels. To enhance building segmentation, we employ multi-model fusion and\ntest-time augmentation strategies to generate pseudo-probabilities, followed by\na low-uncertainty pseudo-label training method for further refinement. Next, a\nchange detection model is trained on bi-temporal cross-modal images and damaged\nbuilding labels. To improve damage classification accuracy, we introduce a\nbuilding-guided low-uncertainty pseudo-label refinement strategy, which\nleverages building priors from the previous step to guide pseudo-label\ngeneration for damaged buildings, reducing uncertainty and enhancing\nreliability. Experimental results on the 2025 IEEE GRSS Data Fusion Contest\ndataset demonstrate the effectiveness of our approach, which achieved the\nhighest mIoU score (54.28%) and secured first place in the competition.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04946", "pdf": "https://arxiv.org/pdf/2505.04946", "abs": "https://arxiv.org/abs/2505.04946", "authors": ["Xuyang Guo", "Jiayan Huo", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang", "Jiale Zhao"], "title": "T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Thanks to recent advancements in scalable deep architectures and large-scale\npretraining, text-to-video generation has achieved unprecedented capabilities\nin producing high-fidelity, instruction-following content across a wide range\nof styles, enabling applications in advertising, entertainment, and education.\nHowever, these models' ability to render precise on-screen text, such as\ncaptions or mathematical formulas, remains largely untested, posing significant\nchallenges for applications requiring exact textual accuracy. In this work, we\nintroduce T2VTextBench, the first human-evaluation benchmark dedicated to\nevaluating on-screen text fidelity and temporal consistency in text-to-video\nmodels. Our suite of prompts integrates complex text strings with dynamic scene\nchanges, testing each model's ability to maintain detailed instructions across\nframes. We evaluate ten state-of-the-art systems, ranging from open-source\nsolutions to commercial offerings, and find that most struggle to generate\nlegible, consistent text. These results highlight a critical gap in current\nvideo generators and provide a clear direction for future research aimed at\nenhancing textual manipulation in video synthesis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05017", "pdf": "https://arxiv.org/pdf/2505.05017", "abs": "https://arxiv.org/abs/2505.05017", "authors": ["Yuntai Bao", "Xuhong Zhang", "Tianyu Du", "Xinkui Zhao", "Jiang Zong", "Hao Peng", "Jianwei Yin"], "title": "Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization", "categories": ["cs.CL"], "comment": "9 pages, accepted by IJCAI 2025", "summary": "Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to\ndownstream tasks. Since the majority of knowledge is acquired during\npre-training, attributing the predictions of fine-tuned LLMs to their\npre-training data may provide valuable insights. Influence functions have been\nproposed as a means to explain model predictions based on training data.\nHowever, existing approaches fail to compute ``multi-stage'' influence and lack\nscalability to billion-scale LLMs.\n  In this paper, we propose the multi-stage influence function to attribute the\ndownstream predictions of fine-tuned LLMs to pre-training data under the\nfull-parameter fine-tuning paradigm. To enhance the efficiency and practicality\nof our multi-stage influence function, we leverage Eigenvalue-corrected\nKronecker-Factored (EK-FAC) parameterization for efficient approximation.\nEmpirical results validate the superior scalability of EK-FAC approximation and\nthe effectiveness of our multi-stage influence function. Additionally, case\nstudies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power,\nwith exemplars illustrating insights provided by multi-stage influence\nestimates. Our code is public at\nhttps://github.com/colored-dye/multi_stage_influence_function.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04964", "pdf": "https://arxiv.org/pdf/2505.04964", "abs": "https://arxiv.org/abs/2505.04964", "authors": ["Yuto Nakamura", "Satoshi Kodera", "Haruki Settai", "Hiroki Shinohara", "Masatsugu Tamura", "Tomohiro Noguchi", "Tatsuki Furusawa", "Ryo Takizawa", "Tempei Kabayama", "Norihiko Takeda"], "title": "CAG-VLM: Fine-Tuning of a Large-Scale Model to Recognize Angiographic Images for Next-Generation Diagnostic Systems", "categories": ["cs.CV"], "comment": null, "summary": "Coronary angiography (CAG) is the gold-standard imaging modality for\nevaluating coronary artery disease, but its interpretation and subsequent\ntreatment planning rely heavily on expert cardiologists. To enable AI-based\ndecision support, we introduce a two-stage, physician-curated pipeline and a\nbilingual (Japanese/English) CAG image-report dataset. First, we sample 14,686\nframes from 539 exams and annotate them for key-frame detection and left/right\nlaterality; a ConvNeXt-Base CNN trained on this data achieves 0.96 F1 on\nlaterality classification, even on low-contrast frames. Second, we apply the\nCNN to 243 independent exams, extract 1,114 key frames, and pair each with its\npre-procedure report and expert-validated diagnostic and treatment summary,\nyielding a parallel corpus. We then fine-tune three open-source VLMs\n(PaliGemma2, Gemma3, and ConceptCLIP-enhanced Gemma3) via LoRA and evaluate\nthem using VLScore and cardiologist review. Although PaliGemma2 w/LoRA attains\nthe highest VLScore, Gemma3 w/LoRA achieves the top clinician rating (mean\n7.20/10); we designate this best-performing model as CAG-VLM. These results\ndemonstrate that specialized, fine-tuned VLMs can effectively assist\ncardiologists in generating clinical reports and treatment recommendations from\nCAG images.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05026", "pdf": "https://arxiv.org/pdf/2505.05026", "abs": "https://arxiv.org/abs/2505.05026", "authors": ["Jaehyun Jeon", "Janghan Yoon", "Minsoo Kim", "Sumin Shim", "Yejin Choi", "Hanbin Kim", "Youngjae Yu"], "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness", "categories": ["cs.CL", "cs.LG"], "comment": "31 pages, 17 figures", "summary": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05008", "pdf": "https://arxiv.org/pdf/2505.05008", "abs": "https://arxiv.org/abs/2505.05008", "authors": ["Xuesong Liu", "Tianyu Hao", "Emmett J. Ientilucci"], "title": "Adaptive Contextual Embedding for Robust Far-View Borehole Detection", "categories": ["cs.CV"], "comment": null, "summary": "In controlled blasting operations, accurately detecting densely distributed\ntiny boreholes from far-view imagery is critical for operational safety and\nefficiency. However, existing detection methods often struggle due to small\nobject scales, highly dense arrangements, and limited distinctive visual\nfeatures of boreholes. To address these challenges, we propose an adaptive\ndetection approach that builds upon existing architectures (e.g., YOLO) by\nexplicitly leveraging consistent embedding representations derived through\nexponential moving average (EMA)-based statistical updates.\n  Our method introduces three synergistic components: (1) adaptive augmentation\nutilizing dynamically updated image statistics to robustly handle illumination\nand texture variations; (2) embedding stabilization to ensure consistent and\nreliable feature extraction; and (3) contextual refinement leveraging spatial\ncontext for improved detection accuracy. The pervasive use of EMA in our method\nis particularly advantageous given the limited visual complexity and small\nscale of boreholes, allowing stable and robust representation learning even\nunder challenging visual conditions. Experiments on a challenging proprietary\nquarry-site dataset demonstrate substantial improvements over baseline\nYOLO-based architectures, highlighting our method's effectiveness in realistic\nand complex industrial scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "accuracy"], "score": 3}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05023", "pdf": "https://arxiv.org/pdf/2505.05023", "abs": "https://arxiv.org/abs/2505.05023", "authors": ["Jialei Chen", "Xu Zheng", "Dongyue Li", "Chong Yi", "Seigo Ito", "Danda Pani Paudel", "Luc Van Gool", "Hiroshi Murase", "Daisuke Deguchi"], "title": "Split Matching for Inductive Zero-shot Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot Semantic Segmentation (ZSS) aims to segment categories that are not\nannotated during training. While fine-tuning vision-language models has\nachieved promising results, these models often overfit to seen categories due\nto the lack of supervision for unseen classes. As an alternative to fully\nsupervised approaches, query-based segmentation has shown great latent in ZSS,\nas it enables object localization without relying on explicit labels. However,\nconventional Hungarian matching, a core component in query-based frameworks,\nneeds full supervision and often misclassifies unseen categories as background\nin the setting of ZSS. To address this issue, we propose Split Matching (SM), a\nnovel assignment strategy that decouples Hungarian matching into two\ncomponents: one for seen classes in annotated regions and another for latent\nclasses in unannotated regions (referred to as unseen candidates).\nSpecifically, we partition the queries into seen and candidate groups, enabling\neach to be optimized independently according to its available supervision. To\ndiscover unseen candidates, we cluster CLIP dense features to generate pseudo\nmasks and extract region-level embeddings using CLS tokens. Matching is then\nconducted separately for the two groups based on both class-level similarity\nand mask-level consistency. Additionally, we introduce a Multi-scale Feature\nEnhancement (MFE) module that refines decoder features through residual\nmulti-scale aggregation, improving the model's ability to capture spatial\ndetails across resolutions. SM is the first to introduce decoupled Hungarian\nmatching under the inductive ZSS setting, and achieves state-of-the-art\nperformance on two standard benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05043", "pdf": "https://arxiv.org/pdf/2505.05043", "abs": "https://arxiv.org/abs/2505.05043", "authors": ["Mani Kumar Tellamekala", "Shashank Jaiswal", "Thomas Smith", "Timur Alamev", "Gary McKeown", "Anthony Brown", "Michel Valstar"], "title": "xTrace: A Facial Expressive Behaviour Analysis Tool for Continuous Affect Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recognising expressive behaviours in face videos is a long-standing challenge\nin Affective Computing. Despite significant advancements in recent years, it\nstill remains a challenge to build a robust and reliable system for\nnaturalistic and in-the-wild facial expressive behaviour analysis in real time.\nThis paper addresses two key challenges in building such a system: (1). The\npaucity of large-scale labelled facial affect video datasets with extensive\ncoverage of the 2D emotion space, and (2). The difficulty of extracting facial\nvideo features that are discriminative, interpretable, robust, and\ncomputationally efficient. Toward addressing these challenges, we introduce\nxTrace, a robust tool for facial expressive behaviour analysis and predicting\ncontinuous values of dimensional emotions, namely valence and arousal, from\nin-the-wild face videos.\n  To address challenge (1), our affect recognition model is trained on the\nlargest facial affect video data set, containing ~450k videos that cover most\nemotion zones in the dimensional emotion space, making xTrace highly versatile\nin analysing a wide spectrum of naturalistic expressive behaviours. To address\nchallenge (2), xTrace uses facial affect descriptors that are not only\nexplainable, but can also achieve a high degree of accuracy and robustness with\nlow computational complexity. The key components of xTrace are benchmarked\nagainst three existing tools: MediaPipe, OpenFace, and Augsburg Affect Toolbox.\nOn an in-the-wild validation set composed of 50k videos, xTrace achieves 0.86\nmean CCC and 0.13 mean absolute error values. We present a detailed error\nanalysis of affect predictions from xTrace, illustrating (a). its ability to\nrecognise emotions with high accuracy across most bins in the 2D emotion space,\n(b). its robustness to non-frontal head pose angles, and (c). a strong\ncorrelation between its uncertainty estimates and its accuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05062", "pdf": "https://arxiv.org/pdf/2505.05062", "abs": "https://arxiv.org/abs/2505.05062", "authors": ["Enhao Zhang", "Chaohua Li", "Chuanxing Geng", "Songcan Chen"], "title": "ULFine: Unbiased Lightweight Fine-tuning for Foundation-Model-Assisted Long-Tailed Semi-Supervised Learning", "categories": ["cs.CV"], "comment": null, "summary": "Based on the success of large-scale visual foundation models like CLIP in\nvarious downstream tasks, this paper initially attempts to explore their impact\non Long-Tailed Semi-Supervised Learning (LTSSL) by employing the foundation\nmodel with three strategies: Linear Probing (LP), Lightweight Fine-Tuning\n(LFT), and Full Fine-Tuning (FFT). Our analysis presents the following\ninsights: i) Compared to LTSSL algorithms trained from scratch, FFT results in\na decline in model performance, whereas LP and LFT, although boosting overall\nmodel performance, exhibit negligible benefits to tail classes. ii) LP produces\nnumerous false pseudo-labels due to \\textit{underlearned} training data, while\nLFT can reduce the number of these false labels but becomes overconfident about\nthem owing to \\textit{biased fitting} training data. This exacerbates the\npseudo-labeled and classifier biases inherent in LTSSL, limiting performance\nimprovement in the tail classes. With these insights, we propose a Unbiased\nLightweight Fine-tuning strategy, \\textbf{ULFine}, which mitigates the\noverconfidence via confidence-aware adaptive fitting of textual prototypes and\ncounteracts the pseudo-labeled and classifier biases via complementary fusion\nof dual logits. Extensive experiments demonstrate that ULFine markedly\ndecreases training costs by over ten times and substantially increases\nprediction accuracies compared to state-of-the-art methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05410", "pdf": "https://arxiv.org/pdf/2505.05410", "abs": "https://arxiv.org/abs/2505.05410", "authors": ["Yanda Chen", "Joe Benton", "Ansh Radhakrishnan", "Jonathan Uesato", "Carson Denison", "John Schulman", "Arushi Somani", "Peter Hase", "Misha Wagner", "Fabien Roger", "Vlad Mikulik", "Samuel R. Bowman", "Jan Leike", "Jared Kaplan", "Ethan Perez"], "title": "Reasoning Models Don't Always Say What They Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought (CoT) offers a potential boon for AI safety as it allows\nmonitoring a model's CoT to try to understand its intentions and reasoning\nprocesses. However, the effectiveness of such monitoring hinges on CoTs\nfaithfully representing models' actual reasoning processes. We evaluate CoT\nfaithfulness of state-of-the-art reasoning models across 6 reasoning hints\npresented in the prompts and find: (1) for most settings and models tested,\nCoTs reveal their usage of hints in at least 1% of examples where they use the\nhint, but the reveal rate is often below 20%, (2) outcome-based reinforcement\nlearning initially improves faithfulness but plateaus without saturating, and\n(3) when reinforcement learning increases how frequently hints are used (reward\nhacking), the propensity to verbalize them does not increase, even without\ntraining against a CoT monitor. These results suggest that CoT monitoring is a\npromising way of noticing undesired behaviors during training and evaluations,\nbut that it is not sufficient to rule them out. They also suggest that in\nsettings like ours where CoT reasoning is not necessary, test-time monitoring\nof CoTs is unlikely to reliably catch rare and catastrophic unexpected\nbehaviors.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05445", "pdf": "https://arxiv.org/pdf/2505.05445", "abs": "https://arxiv.org/abs/2505.05445", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations", "categories": ["cs.CL"], "comment": "30 pages", "summary": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dialogue"], "score": 2}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05163", "pdf": "https://arxiv.org/pdf/2505.05163", "abs": "https://arxiv.org/abs/2505.05163", "authors": ["Aishwarya Venkataramanan", "Paul Bodesheim", "Joachim Denzler"], "title": "Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty Quantification with Gaussian Process Latent Variable Models", "categories": ["cs.CV", "cs.LG"], "comment": "UAI 2025, 22 pages", "summary": "Vision-Language Models (VLMs) learn joint representations by mapping images\nand text into a shared latent space. However, recent research highlights that\ndeterministic embeddings from standard VLMs often struggle to capture the\nuncertainties arising from the ambiguities in visual and textual descriptions\nand the multiple possible correspondences between images and texts. Existing\napproaches tackle this by learning probabilistic embeddings during VLM\ntraining, which demands large datasets and does not leverage the powerful\nrepresentations already learned by large-scale VLMs like CLIP. In this paper,\nwe propose GroVE, a post-hoc approach to obtaining probabilistic embeddings\nfrom frozen VLMs. GroVE builds on Gaussian Process Latent Variable Model\n(GPLVM) to learn a shared low-dimensional latent space where image and text\ninputs are mapped to a unified representation, optimized through single-modal\nembedding reconstruction and cross-modal alignment objectives. Once trained,\nthe Gaussian Process model generates uncertainty-aware probabilistic\nembeddings. Evaluation shows that GroVE achieves state-of-the-art uncertainty\ncalibration across multiple downstream tasks, including cross-modal retrieval,\nvisual question answering, and active learning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "question answering"], "score": 2}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04741", "pdf": "https://arxiv.org/pdf/2505.04741", "abs": "https://arxiv.org/abs/2505.04741", "authors": ["Kenneth Li", "Yida Chen", "Fernanda Viégas", "Martin Wattenberg"], "title": "When Bad Data Leads to Good Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "In large language model (LLM) pretraining, data quality is believed to\ndetermine model quality. In this paper, we re-examine the notion of \"quality\"\nfrom the perspective of pre- and post-training co-design. Specifically, we\nexplore the possibility that pre-training on more toxic data can lead to better\ncontrol in post-training, ultimately decreasing a model's output toxicity.\nFirst, we use a toy experiment to study how data composition affects the\ngeometry of features in the representation space. Next, through controlled\nexperiments with Olmo-1B models trained on varying ratios of clean and toxic\ndata, we find that the concept of toxicity enjoys a less entangled linear\nrepresentation as the proportion of toxic data increases. Furthermore, we show\nthat although toxic data increases the generational toxicity of the base model,\nit also makes the toxicity easier to remove. Evaluations on Toxigen and Real\nToxicity Prompts demonstrate that models trained on toxic data achieve a better\ntrade-off between reducing generational toxicity and preserving general\ncapabilities when detoxifying techniques such as inference-time intervention\n(ITI) are applied. Our findings suggest that, with post-training taken into\naccount, bad data may lead to good models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05307", "pdf": "https://arxiv.org/pdf/2505.05307", "abs": "https://arxiv.org/abs/2505.05307", "authors": ["Ciyu Ruan", "Ruishan Guo", "Zihang Gong", "Jingao Xu", "Wenhan Yang", "Xinlei Chen"], "title": "PRE-Mamba: A 4D State Space Model for Ultra-High-Frequent Event Camera Deraining", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras excel in high temporal resolution and dynamic range but suffer\nfrom dense noise in rainy conditions. Existing event deraining methods face\ntrade-offs between temporal precision, deraining effectiveness, and\ncomputational efficiency. In this paper, we propose PRE-Mamba, a novel\npoint-based event camera deraining framework that fully exploits the\nspatiotemporal characteristics of raw event and rain. Our framework introduces\na 4D event cloud representation that integrates dual temporal scales to\npreserve high temporal precision, a Spatio-Temporal Decoupling and Fusion\nmodule (STDF) that enhances deraining capability by enabling shallow decoupling\nand interaction of temporal and spatial information, and a Multi-Scale State\nSpace Model (MS3M) that captures deeper rain dynamics across dual-temporal and\nmulti-spatial scales with linear computational complexity. Enhanced by\nfrequency-domain regularization, PRE-Mamba achieves superior performance (0.95\nSR, 0.91 NR, and 0.4s/M events) with only 0.26M parameters on EventRain-27K, a\ncomprehensive dataset with labeled synthetic and real-world sequences.\nMoreover, our method generalizes well across varying rain intensities,\nviewpoints, and even snowy conditions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04946", "pdf": "https://arxiv.org/pdf/2505.04946", "abs": "https://arxiv.org/abs/2505.04946", "authors": ["Xuyang Guo", "Jiayan Huo", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang", "Jiale Zhao"], "title": "T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Thanks to recent advancements in scalable deep architectures and large-scale\npretraining, text-to-video generation has achieved unprecedented capabilities\nin producing high-fidelity, instruction-following content across a wide range\nof styles, enabling applications in advertising, entertainment, and education.\nHowever, these models' ability to render precise on-screen text, such as\ncaptions or mathematical formulas, remains largely untested, posing significant\nchallenges for applications requiring exact textual accuracy. In this work, we\nintroduce T2VTextBench, the first human-evaluation benchmark dedicated to\nevaluating on-screen text fidelity and temporal consistency in text-to-video\nmodels. Our suite of prompts integrates complex text strings with dynamic scene\nchanges, testing each model's ability to maintain detailed instructions across\nframes. We evaluate ten state-of-the-art systems, ranging from open-source\nsolutions to commercial offerings, and find that most struggle to generate\nlegible, consistent text. These results highlight a critical gap in current\nvideo generators and provide a clear direction for future research aimed at\nenhancing textual manipulation in video synthesis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05343", "pdf": "https://arxiv.org/pdf/2505.05343", "abs": "https://arxiv.org/abs/2505.05343", "authors": ["Sooyoung Park", "Arda Senocak", "Joon Son Chung"], "title": "Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound Source Localization", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "Journal Extension of WACV 2024 paper (arXiv:2311.04066). Code is\n  available at https://github.com/swimmiing/ACL-SSL", "summary": "Large-scale vision-language models demonstrate strong multimodal alignment\nand generalization across diverse tasks. Among them, CLIP stands out as one of\nthe most successful approaches. In this work, we extend the application of CLIP\nto sound source localization, proposing a self-supervised method operates\nwithout explicit text input. We introduce a framework that maps audios into\ntokens compatible with CLIP's text encoder, producing audio-driven embeddings.\nThese embeddings are used to generate sounding region masks, from which visual\nfeatures are extracted and aligned with the audio embeddings through a\ncontrastive audio-visual correspondence objective. Our findings show that\nalignment knowledge of pre-trained multimodal foundation model enables our\nmethod to generate more complete and compact localization for sounding objects.\nWe further propose an LLM-guided extension that distills object-aware\naudio-visual scene understanding into the model during training to enhance\nalignment. Extensive experiments across five diverse tasks demonstrate that our\nmethod, in all variants, outperforms state-of-the-art approaches and achieves\nstrong generalization in zero-shot settings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05367", "pdf": "https://arxiv.org/pdf/2505.05367", "abs": "https://arxiv.org/abs/2505.05367", "authors": ["Jie Deng", "Danfeng Hong", "Chenyu Li", "Naoto Yokoya"], "title": "Joint Super-Resolution and Segmentation for 1-m Impervious Surface Area Mapping in China's Yangtze River Economic Belt", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "We propose a novel joint framework by integrating super-resolution and\nsegmentation, called JointSeg, which enables the generation of 1-meter ISA maps\ndirectly from freely available Sentinel-2 imagery. JointSeg was trained on\nmultimodal cross-resolution inputs, offering a scalable and affordable\nalternative to traditional approaches. This synergistic design enables gradual\nresolution enhancement from 10m to 1m while preserving fine-grained spatial\ntextures, and ensures high classification fidelity through effective\ncross-scale feature fusion. This method has been successfully applied to the\nYangtze River Economic Belt (YREB), a region characterized by complex\nurban-rural patterns and diverse topography. As a result, a comprehensive ISA\nmapping product for 2021, referred to as ISA-1, was generated, covering an area\nof over 2.2 million square kilometers. Quantitative comparisons against the 10m\nESA WorldCover and other benchmark products reveal that ISA-1 achieves an\nF1-score of 85.71%, outperforming bilinear-interpolation-based segmentation by\n9.5%, and surpassing other ISA datasets by 21.43%-61.07%. In densely urbanized\nareas (e.g., Suzhou, Nanjing), ISA-1 reduces ISA overestimation through\nimproved discrimination of green spaces and water bodies. Conversely, in\nmountainous regions (e.g., Ganzi, Zhaotong), it identifies significantly more\nISA due to its enhanced ability to detect fragmented anthropogenic features\nsuch as rural roads and sparse settlements, demonstrating its robustness across\ndiverse landscapes. Moreover, we present biennial ISA maps from 2017 to 2023,\ncapturing spatiotemporal urbanization dynamics across representative cities.\nThe results highlight distinct regional growth patterns: rapid expansion in\nupstream cities, moderate growth in midstream regions, and saturation in\ndownstream metropolitan areas.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05145", "pdf": "https://arxiv.org/pdf/2505.05145", "abs": "https://arxiv.org/abs/2505.05145", "authors": ["Xinyan Hu", "Kayo Yin", "Michael I. Jordan", "Jacob Steinhardt", "Lijie Chen"], "title": "Understanding In-context Learning of Addition via Activation Subspaces", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "16 pages", "summary": "To perform in-context learning, language models must extract signals from\nindividual few-shot examples, aggregate these into a learned prediction rule,\nand then apply this rule to new examples. How is this implemented in the\nforward pass of modern transformer models? To study this, we consider a\nstructured family of few-shot learning tasks for which the true prediction rule\nis to add an integer $k$ to the input. We find that Llama-3-8B attains high\naccuracy on this task for a range of $k$, and localize its few-shot ability to\njust three attention heads via a novel optimization approach. We further show\nthe extracted signals lie in a six-dimensional subspace, where four of the\ndimensions track the unit digit and the other two dimensions track overall\nmagnitude. We finally examine how these heads extract information from\nindividual few-shot examples, identifying a self-correction mechanism in which\nmistakes from earlier examples are suppressed by later examples. Our results\ndemonstrate how tracking low-dimensional subspaces across a forward pass can\nprovide insight into fine-grained computational structures.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05391", "pdf": "https://arxiv.org/pdf/2505.05391", "abs": "https://arxiv.org/abs/2505.05391", "authors": ["Ciyu Ruan", "Zihang Gong", "Ruishan Guo", "Jingao Xu", "Xinlei Chen"], "title": "EDmamba: A Simple yet Effective Event Denoising Method with State Space Model", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras excel in high-speed vision due to their high temporal\nresolution, high dynamic range, and low power consumption. However, as dynamic\nvision sensors, their output is inherently noisy, making efficient denoising\nessential to preserve their ultra-low latency and real-time processing\ncapabilities. Existing event denoising methods struggle with a critical\ndilemma: computationally intensive approaches compromise the sensor's\nhigh-speed advantage, while lightweight methods often lack robustness across\nvarying noise levels. To address this, we propose a novel event denoising\nframework based on State Space Models (SSMs). Our approach represents events as\n4D event clouds and includes a Coarse Feature Extraction (CFE) module that\nextracts embedding features from both geometric and polarity-aware subspaces.\nThe model is further composed of two essential components: A Spatial Mamba\n(S-SSM) that models local geometric structures and a Temporal Mamba (T-SSM)\nthat captures global temporal dynamics, efficiently propagating spatiotemporal\nfeatures across events. Experiments demonstrate that our method achieves\nstate-of-the-art accuracy and efficiency, with 88.89K parameters, 0.0685s per\n100K events inference time, and a 0.982 accuracy score, outperforming\nTransformer-based methods by 2.08% in denoising accuracy and 36X faster.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05397", "pdf": "https://arxiv.org/pdf/2505.05397", "abs": "https://arxiv.org/abs/2505.05397", "authors": ["Zhang Zhang", "Chao Sun", "Chao Yue", "Da Wen", "Tianze Wang", "Jianghao Leng"], "title": "PillarMamba: Learning Local-Global Context for Roadside Point Cloud via Hybrid State Space Model", "categories": ["cs.CV"], "comment": null, "summary": "Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything\n(V2X) tasks, roadside perception has received increasing attention in recent\nyears, as it can extend the perception range of connected vehicles and improve\ntraffic safety. However, roadside point cloud oriented 3D object detection has\nnot been effectively explored. To some extent, the key to the performance of a\npoint cloud detector lies in the receptive field of the network and the ability\nto effectively utilize the scene context. The recent emergence of Mamba, based\non State Space Model (SSM), has shaken up the traditional convolution and\ntransformers that have long been the foundational building blocks, due to its\nefficient global receptive field. In this work, we introduce Mamba to\npillar-based roadside point cloud perception and propose a framework based on\nCross-stage State-space Group (CSG), called PillarMamba. It enhances the\nexpressiveness of the network and achieves efficient computation through\ncross-stage feature fusion. However, due to the limitations of scan directions,\nstate space model faces local connection disrupted and historical relationship\nforgotten. To address this, we propose the Hybrid State-space Block (HSB) to\nobtain the local-global context of roadside point cloud. Specifically, it\nenhances neighborhood connections through local convolution and preserves\nhistorical memory through residual attention. The proposed method outperforms\nthe state-of-the-art methods on the popular large scale roadside benchmark:\nDAIR-V2X-I. The code will be released soon.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05467", "pdf": "https://arxiv.org/pdf/2505.05467", "abs": "https://arxiv.org/abs/2505.05467", "authors": ["Haibo Wang", "Bo Feng", "Zhengfeng Lai", "Mingze Xu", "Shiyu Li", "Weifeng Ge", "Afshin Dehghan", "Meng Cao", "Ping Huang"], "title": "StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05467", "pdf": "https://arxiv.org/pdf/2505.05467", "abs": "https://arxiv.org/abs/2505.05467", "authors": ["Haibo Wang", "Bo Feng", "Zhengfeng Lai", "Mingze Xu", "Shiyu Li", "Weifeng Ge", "Afshin Dehghan", "Meng Cao", "Ping Huang"], "title": "StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05469", "pdf": "https://arxiv.org/pdf/2505.05469", "abs": "https://arxiv.org/abs/2505.05469", "authors": ["Ava Pun", "Kangle Deng", "Ruixuan Liu", "Deva Ramanan", "Changliu Liu", "Jun-Yan Zhu"], "title": "Generating Physically Stable and Buildable LEGO Designs from Text", "categories": ["cs.CV"], "comment": "Project page: https://avalovelace1.github.io/LegoGPT/", "summary": "We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.04913", "pdf": "https://arxiv.org/pdf/2505.04913", "abs": "https://arxiv.org/abs/2505.04913", "authors": ["Gugeong Sung"], "title": "Advanced 3D Imaging Approach to TSV/TGV Metrology and Inspection Using Only Optical Microscopy", "categories": ["eess.IV", "cs.CV", "physics.optics"], "comment": "6 pages, 6 figures, Submitted to arXiv for preprint", "summary": "This paper introduces an innovative approach to silicon and glass via\ninspection, which combines hybrid field microscopy with photometric stereo.\nConventional optical microscopy techniques are generally limited to superficial\ninspections and struggle to effectively visualize the internal structures of\nsilicon and glass vias. By utilizing various lighting conditions for 3D\nreconstruction, the proposed method surpasses these limitations. By integrating\nphotometric stereo to the traditional optical microscopy, the proposed method\nnot only enhances the capability to detect micro-scale defects but also\nprovides a detailed visualization of depth and edge abnormality, which are\ntypically not visible with conventional optical microscopy inspection. The\nexperimental results demonstrated that the proposed method effectively captures\nintricate surface details and internal structures. Quantitative comparisons\nbetween the reconstructed models and actual measurements present the capability\nof the proposed method to significantly improve silicon and glass via\ninspection process. As a result, the proposed method achieves enhanced\ncost-effectiveness while maintaining high accuracy and repeatability,\nsuggesting substantial advancements in silicon and glass via inspection\ntechniques", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05041", "pdf": "https://arxiv.org/pdf/2505.05041", "abs": "https://arxiv.org/abs/2505.05041", "authors": ["Chenxi Zhao", "Jianqiang Li", "Qing Zhao", "Jing Bai", "Susana Boluda", "Benoit Delatour", "Lev Stimmer", "Daniel Racoceanu", "Gabriel Jimenez", "Guanghui Fu"], "title": "ADNP-15: An Open-Source Histopathological Dataset for Neuritic Plaque Segmentation in Human Brain Whole Slide Images with Frequency Domain Image Enhancement for Stain Normalization", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Alzheimer's Disease (AD) is a neurodegenerative disorder characterized by\namyloid-beta plaques and tau neurofibrillary tangles, which serve as key\nhistopathological features. The identification and segmentation of these\nlesions are crucial for understanding AD progression but remain challenging due\nto the lack of large-scale annotated datasets and the impact of staining\nvariations on automated image analysis. Deep learning has emerged as a powerful\ntool for pathology image segmentation; however, model performance is\nsignificantly influenced by variations in staining characteristics,\nnecessitating effective stain normalization and enhancement techniques. In this\nstudy, we address these challenges by introducing an open-source dataset\n(ADNP-15) of neuritic plaques (i.e., amyloid deposits combined with a crown of\ndystrophic tau-positive neurites) in human brain whole slide images. We\nestablish a comprehensive benchmark by evaluating five widely adopted deep\nlearning models across four stain normalization techniques, providing deeper\ninsights into their influence on neuritic plaque segmentation. Additionally, we\npropose a novel image enhancement method that improves segmentation accuracy,\nparticularly in complex tissue structures, by enhancing structural details and\nmitigating staining inconsistencies. Our experimental results demonstrate that\nthis enhancement strategy significantly boosts model generalization and\nsegmentation accuracy. All datasets and code are open-source, ensuring\ntransparency and reproducibility while enabling further advancements in the\nfield.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05076", "pdf": "https://arxiv.org/pdf/2505.05076", "abs": "https://arxiv.org/abs/2505.05076", "authors": ["Hyunho Song", "Dongjae Lee", "Seunghun Oh", "Minwoo Jung", "Ayoung Kim"], "title": "The City that Never Settles: Simulation-based LiDAR Dataset for Long-Term Place Recognition Under Extreme Structural Changes", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Large-scale construction and demolition significantly challenge long-term\nplace recognition (PR) by drastically reshaping urban and suburban\nenvironments. Existing datasets predominantly reflect limited or indoor-focused\nchanges, failing to adequately represent extensive outdoor transformations. To\nbridge this gap, we introduce the City that Never Settles (CNS) dataset, a\nsimulation-based dataset created using the CARLA simulator, capturing major\nstructural changes-such as building construction and demolition-across diverse\nmaps and sequences. Additionally, we propose TCR_sym, a symmetric version of\nthe original TCR metric, enabling consistent measurement of structural changes\nirrespective of source-target ordering. Quantitative comparisons demonstrate\nthat CNS encompasses more extensive transformations than current real-world\nbenchmarks. Evaluations of state-of-the-art LiDAR-based PR methods on CNS\nreveal substantial performance degradation, underscoring the need for robust\nalgorithms capable of handling significant environmental changes. Our dataset\nis available at https://github.com/Hyunho111/CNS_dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05137", "pdf": "https://arxiv.org/pdf/2505.05137", "abs": "https://arxiv.org/abs/2505.05137", "authors": ["Yi Chen"], "title": "Research on Anomaly Detection Methods Based on Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": "6 pages, 3 table", "summary": "Anomaly detection is a fundamental task in machine learning and data mining,\nwith significant applications in cybersecurity, industrial fault diagnosis, and\nclinical disease monitoring. Traditional methods, such as statistical modeling\nand machine learning-based approaches, often face challenges in handling\ncomplex, high-dimensional data distributions. In this study, we explore the\npotential of diffusion models for anomaly detection, proposing a novel\nframework that leverages the strengths of diffusion probabilistic models (DPMs)\nto effectively identify anomalies in both image and audio data. The proposed\nmethod models the distribution of normal data through a diffusion process and\nreconstructs input data via reverse diffusion, using a combination of\nreconstruction errors and semantic discrepancies as anomaly indicators. To\nenhance the framework's performance, we introduce multi-scale feature\nextraction, attention mechanisms, and wavelet-domain representations, enabling\nthe model to capture fine-grained structures and global dependencies in the\ndata. Extensive experiments on benchmark datasets, including MVTec AD and\nUrbanSound8K, demonstrate that our method outperforms state-of-the-art anomaly\ndetection techniques, achieving superior accuracy and robustness across diverse\ndata modalities. This research highlights the effectiveness of diffusion models\nin anomaly detection and provides a robust and efficient solution for\nreal-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05208", "pdf": "https://arxiv.org/pdf/2505.05208", "abs": "https://arxiv.org/abs/2505.05208", "authors": ["Muhammad Irfan", "Anum Nawaz", "Riku Klen", "Abdulhamit Subasi", "Tomi Westerlund", "Wei Chen"], "title": "Improved Brain Tumor Detection in MRI: Fuzzy Sigmoid Convolution in Deep Learning", "categories": ["eess.IV", "cs.CV"], "comment": "IEEE IJCNN 2025 has accepted the paper", "summary": "Early detection and accurate diagnosis are essential to improving patient\noutcomes. The use of convolutional neural networks (CNNs) for tumor detection\nhas shown promise, but existing models often suffer from overparameterization,\nwhich limits their performance gains. In this study, fuzzy sigmoid convolution\n(FSC) is introduced along with two additional modules: top-of-the-funnel and\nmiddle-of-the-funnel. The proposed methodology significantly reduces the number\nof trainable parameters without compromising classification accuracy. A novel\nconvolutional operator is central to this approach, effectively dilating the\nreceptive field while preserving input data integrity. This enables efficient\nfeature map reduction and enhances the model's tumor detection capability. In\nthe FSC-based model, fuzzy sigmoid activation functions are incorporated within\nconvolutional layers to improve feature extraction and classification. The\ninclusion of fuzzy logic into the architecture improves its adaptability and\nrobustness. Extensive experiments on three benchmark datasets demonstrate the\nsuperior performance and efficiency of the proposed model. The FSC-based\narchitecture achieved classification accuracies of 99.17%, 99.75%, and 99.89%\non three different datasets. The model employs 100 times fewer parameters than\nlarge-scale transfer learning architectures, highlighting its computational\nefficiency and suitability for detecting brain tumors early. This research\noffers lightweight, high-performance deep-learning models for medical imaging\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-11.jsonl"}
{"id": "2505.05291", "pdf": "https://arxiv.org/pdf/2505.05291", "abs": "https://arxiv.org/abs/2505.05291", "authors": ["Benjamin A. Cohen", "Jonathan Fhima", "Meishar Meisel", "Baskin Meital", "Luis Filipe Nakayama", "Eran Berkowitz", "Joachim A. Behar"], "title": "Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection", "categories": ["eess.IV", "cs.AI", "cs.CV", "q-bio.TO"], "comment": "10 pages, 3 figures", "summary": "Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to\nlearn robust representations from large-scale natural image datasets, enhancing\ntheir generalization across domains. In retinal imaging, foundation models\npretrained on either natural or ophthalmic data have shown promise, but the\nbenefits of in-domain pretraining remain uncertain. To investigate this, we\nbenchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets\ntotaling 70,000 expert-annotated images for the task of moderate-to-late\nage-related macular degeneration (AMD) identification. Our results show that\niBOT pretrained on natural images achieves the highest out-of-distribution\ngeneralization, with AUROCs of 0.80-0.97, outperforming domain-specific models,\nwhich achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,\nwhich achieved AUROCs of 0.68-0.91. These findings highlight the value of\nfoundation models in improving AMD identification and challenge the assumption\nthat in-domain pretraining is necessary. Furthermore, we release BRAMD, an\nopen-access dataset (n=587) of DFIs with AMD labels from Brazil.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-11.jsonl"}
