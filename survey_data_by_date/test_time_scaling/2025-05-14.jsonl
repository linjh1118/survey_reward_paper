{"id": "2505.08303", "pdf": "https://arxiv.org/pdf/2505.08303", "abs": "https://arxiv.org/abs/2505.08303", "authors": ["Ziyu Zhou", "Yihang Wu", "Jingyuan Yang", "Zhan Xiao", "Rongjun Li"], "title": "Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow", "categories": ["cs.CL"], "comment": null, "summary": "Black-Box prompt optimization methods have emerged as a promising strategy\nfor refining input prompts to better align large language models (LLMs),\nthereby enhancing their task performance. Although these methods have\ndemonstrated encouraging results, most studies and experiments have primarily\nfocused on smaller-scale models (e.g., 7B, 14B) or earlier versions (e.g.,\nGPT-3.5) of LLMs. As the scale of LLMs continues to increase, such as with\nDeepSeek V3 (671B), it remains an open question whether these black-box\noptimization techniques will continue to yield significant performance\nimprovements for models of such scale. In response to this, we select three\nwell-known black-box optimization methods and evaluate them on large-scale LLMs\n(DeepSeek V3 and Gemini 2.0 Flash) across four NLU and NLG datasets. The\nresults show that these black-box prompt optimization methods offer only\nlimited improvements on these large-scale LLMs. Furthermore, we hypothesize\nthat the scale of the model is the primary factor contributing to the limited\nbenefits observed. To explore this hypothesis, we conducted experiments on LLMs\nof varying sizes (Qwen 2.5 series, ranging from 7B to 72B) and observed an\ninverse scaling law, wherein the effectiveness of black-box optimization\nmethods diminished as the model size increased.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale", "scaling law"], "score": 3}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08783", "pdf": "https://arxiv.org/pdf/2505.08783", "abs": "https://arxiv.org/abs/2505.08783", "authors": ["Shanda Li", "Tanya Marwah", "Junhong Shen", "Weiwei Sun", "Andrej Risteski", "Yiming Yang", "Ameet Talwalkar"], "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NA", "math.NA"], "comment": null, "summary": "Partial differential equations (PDEs) are fundamental to modeling physical\nsystems, yet solving them remains a complex challenge. Traditional numerical\nsolvers rely on expert knowledge to implement and are computationally\nexpensive, while neural-network-based solvers require large training datasets\nand often lack interpretability. In this work, we frame PDE solving as a code\ngeneration task and introduce CodePDE, the first inference framework for\ngenerating PDE solvers using large language models (LLMs). Leveraging advanced\ninference-time algorithms and scaling strategies, CodePDE unlocks critical\ncapacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and\ntest-time scaling -- all without task-specific tuning. CodePDE achieves\nsuperhuman performance across a range of representative PDE problems. We also\npresent a systematic empirical analysis of LLM generated solvers, analyzing\ntheir accuracy, efficiency, and numerical scheme choices. Our findings\nhighlight the promise and the current limitations of LLMs in PDE solving,\noffering a new perspective on solver design and opportunities for future model\ndevelopment. Our code is available at https://github.com/LithiumDA/CodePDE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time", "scaling"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08084", "pdf": "https://arxiv.org/pdf/2505.08084", "abs": "https://arxiv.org/abs/2505.08084", "authors": ["Yu Cheng", "Arushi Goel", "Hakan Bilen"], "title": "Visually Interpretable Subtask Reasoning for Visual Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "Answering complex visual questions like `Which red furniture can be used for\nsitting?' requires multi-step reasoning, including object recognition,\nattribute filtering, and relational understanding. Recent work improves\ninterpretability in multimodal large language models (MLLMs) by decomposing\ntasks into sub-task programs, but these methods are computationally expensive\nand less accurate due to poor adaptation to target data. To address this, we\nintroduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a\nsubtask-driven training framework that enhances both interpretability and\nreasoning by generating textual and visual explanations within MLLMs. Instead\nof relying on external models, VISTAR fine-tunes MLLMs to produce structured\nSubtask-of-Thought rationales (step-by-step reasoning sequences). Experiments\non two benchmarks show that VISTAR consistently improves reasoning accuracy\nwhile maintaining interpretability. Our code and dataset will be available at\nhttps://github.com/ChengJade/VISTAR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning", "reasoning model"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08695", "pdf": "https://arxiv.org/pdf/2505.08695", "abs": "https://arxiv.org/abs/2505.08695", "authors": ["Zhanjie Zhang", "Quanwei Zhang", "Junsheng Luan", "Mengyuan Yang", "Yun Wang", "Lei Zhao"], "title": "SPAST: Arbitrary Style Transfer with Style Priors via Pre-trained Large-scale Model", "categories": ["cs.CV"], "comment": "Accepted by Neural Networks", "summary": "Given an arbitrary content and style image, arbitrary style transfer aims to\nrender a new stylized\n  image which preserves the content image's structure and possesses the style\nimage's style. Existing\n  arbitrary style transfer methods are based on either small models or\npre-trained large-scale models.\n  The small model-based methods fail to generate high-quality stylized images,\nbringing artifacts and\n  disharmonious patterns. The pre-trained large-scale model-based methods can\ngenerate high-quality\n  stylized images but struggle to preserve the content structure and cost long\ninference time. To this\n  end, we propose a new framework, called SPAST, to generate high-quality\nstylized images with\n  less inference time. Specifically, we design a novel Local-global Window Size\nStylization Module\n  (LGWSSM)tofuse style features into content features. Besides, we introduce a\nnovel style prior loss,\n  which can dig out the style priors from a pre-trained large-scale model into\nthe SPAST and motivate\n  the SPAST to generate high-quality stylized images with short inference\ntime.We conduct abundant\n  experiments to verify that our proposed method can generate high-quality\nstylized images and less\n  inference time compared with the SOTA arbitrary style transfer methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07984", "pdf": "https://arxiv.org/pdf/2505.07984", "abs": "https://arxiv.org/abs/2505.07984", "authors": ["Aybora Koksal", "A. Aydin Alatan"], "title": "MilChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Remote Sensing", "categories": ["cs.CV"], "comment": "Submitted to JSTARS on April 2, 2025. Code and dataset will be\n  available upon acceptance", "summary": "Remarkable capabilities in understanding and generating text-image content\nhave been demonstrated by recent advancements in multimodal large language\nmodels (MLLMs). However, their effectiveness in specialized\ndomains-particularly those requiring resource-efficient and domain-specific\nadaptations-has remained limited. In this work, a lightweight multimodal\nlanguage model termed MilChat is introduced, specifically adapted to analyze\nremote sensing imagery in secluded areas, including challenging missile launch\nsites. A new dataset, MilData, was compiled by verifying hundreds of aerial\nimages through expert review, and subtle military installations were\nhighlighted via detailed captions. Supervised fine-tuning on a 2B-parameter\nopen-source MLLM with chain-of-thought (CoT) reasoning annotations was\nperformed, enabling more accurate and interpretable explanations. Additionally,\nGroup Relative Policy Optimization (GRPO) was leveraged to enhance the model's\nability to detect critical domain-specific cues-such as defensive layouts and\nkey military structures-while minimizing false positives on civilian scenes.\nThrough empirical evaluations, it has been shown that MilChat significantly\noutperforms both larger, general-purpose multimodal models and existing remote\nsensing-adapted approaches on open-ended captioning and classification metrics.\nOver 80% recall and 98% precision were achieved on the newly proposed MilData\nbenchmark, underscoring the potency of targeted fine-tuning and reinforcement\nlearning in specialized real-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08013", "pdf": "https://arxiv.org/pdf/2505.08013", "abs": "https://arxiv.org/abs/2505.08013", "authors": ["Gonglin Chen", "Tianwen Fu", "Haiwei Chen", "Wenbin Teng", "Hanyuan Xiao", "Yajie Zhao"], "title": "RDD: Robust Feature Detector and Descriptor using Deformable Transformer", "categories": ["cs.CV"], "comment": null, "summary": "As a core step in structure-from-motion and SLAM, robust feature detection\nand description under challenging scenarios such as significant viewpoint\nchanges remain unresolved despite their ubiquity. While recent works have\nidentified the importance of local features in modeling geometric\ntransformations, these methods fail to learn the visual cues present in\nlong-range relationships. We present Robust Deformable Detector (RDD), a novel\nand robust keypoint detector/descriptor leveraging the deformable transformer,\nwhich captures global context and geometric invariance through deformable\nself-attention mechanisms. Specifically, we observed that deformable attention\nfocuses on key locations, effectively reducing the search space complexity and\nmodeling the geometric invariance. Furthermore, we collected an Air-to-Ground\ndataset for training in addition to the standard MegaDepth dataset. Our\nproposed method outperforms all state-of-the-art keypoint detection/description\nmethods in sparse matching tasks and is also capable of semi-dense matching. To\nensure comprehensive evaluation, we introduce two challenging benchmarks: one\nemphasizing large viewpoint and scale variations, and the other being an\nAir-to-Ground benchmark -- an evaluation setting that has recently gaining\npopularity for 3D reconstruction across different altitudes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08101", "pdf": "https://arxiv.org/pdf/2505.08101", "abs": "https://arxiv.org/abs/2505.08101", "authors": ["Luu Tung Hai", "Thinh D. Le", "Zhicheng Ding", "Qing Tian", "Truong-Son Hy"], "title": "Topology-Guided Knowledge Distillation for Efficient Point Cloud Processing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Point cloud processing has gained significant attention due to its critical\nrole in applications such as autonomous driving and 3D object recognition.\nHowever, deploying high-performance models like Point Transformer V3 in\nresource-constrained environments remains challenging due to their high\ncomputational and memory demands. This work introduces a novel distillation\nframework that leverages topology-aware representations and gradient-guided\nknowledge distillation to effectively transfer knowledge from a high-capacity\nteacher to a lightweight student model. Our approach captures the underlying\ngeometric structures of point clouds while selectively guiding the student\nmodel's learning process through gradient-based feature alignment. Experimental\nresults in the Nuscenes, SemanticKITTI, and Waymo datasets demonstrate that the\nproposed method achieves competitive performance, with an approximately 16x\nreduction in model size and a nearly 1.9x decrease in inference time compared\nto its teacher model. Notably, on NuScenes, our method achieves\nstate-of-the-art performance among knowledge distillation techniques trained\nsolely on LiDAR data, surpassing prior knowledge distillation baselines in\nsegmentation performance. Our implementation is available publicly at:\n  https://github.com/HySonLab/PointDistill", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07858", "pdf": "https://arxiv.org/pdf/2505.07858", "abs": "https://arxiv.org/abs/2505.07858", "authors": ["Siyuan Yan", "Mo Zhu", "Guo-qing Jiang", "Jianfei Wang", "Jiaxing Chen", "Wentai Zhang", "Xiang Liao", "Xiao Cui", "Chen Zhang", "Zhuoran Song", "Ran Zhu"], "title": "Scaling Laws for Speculative Decoding", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures", "summary": "The escalating demand for efficient decoding in large language models (LLMs)\nis particularly critical for reasoning-intensive architectures like OpenAI-o3\nand DeepSeek-R1, which depend on extended chain-of-thought reasoning. This\nstudy investigates speculative decoding techniques through dense LLM\narchitectures to establish foundational insights for accelerating reasoning\ntasks. While speculative decoding methods leveraging parallel\ndraft-verification cycles have emerged as promising acceleration techniques,\nthe scaling laws governing decoding efficiency remain under-explored compared\nto conventional backbone LLMs developed through Pretraining->SFT->RLHF training\nparadigms. In this work, we discover Log-linear Scaling Laws (Theorem 1.1, 1.2\nand 1.3) governing draft model acceptance rate (or decoding speed) across three\ndimensions: pretraining token volume, draft model capacity, and decoding batch\nsize. Building on these laws, we achieve Scylla, which coordinates\nmulti-dimensional scaling for popular LLMs (Llama2/3, Qwen2.5). Empirical\nvalidation shows Scylla achieves 1.5-2.2 higher acceptance rate than EAGLE2 and\n0.3 higher than EAGLE3 at temperature T = 0, with peak performance gains on\nsummarization and QA tasks (Figure 2). Industrial inference engine deployments\ndemonstrate 2X decoding throughput improvements over EAGLE2 (Table 5),\nvalidating the transformative potential of systematic scaling for efficient LLM\ninference. Code will be released later.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization", "multi-dimensional"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08124", "pdf": "https://arxiv.org/pdf/2505.08124", "abs": "https://arxiv.org/abs/2505.08124", "authors": ["Laszlo Szilagyi", "Francis Engelmann", "Jeannette Bohg"], "title": "SLAG: Scalable Language-Augmented Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Language-augmented scene representations hold great promise for large-scale\nrobotics applications such as search-and-rescue, smart cities, and mining. Many\nof these scenarios are time-sensitive, requiring rapid scene encoding while\nalso being data-intensive, necessitating scalable solutions. Deploying these\nrepresentations on robots with limited computational resources further adds to\nthe challenge. To address this, we introduce SLAG, a multi-GPU framework for\nlanguage-augmented Gaussian splatting that enhances the speed and scalability\nof embedding large scenes. Our method integrates 2D visual-language model\nfeatures into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG\neliminates the need for a loss function to compute per-Gaussian language\nembeddings. Instead, it derives embeddings from 3D Gaussian scene parameters\nvia a normalized weighted average, enabling highly parallelized scene encoding.\nAdditionally, we introduce a vector database for efficient embedding storage\nand retrieval. Our experiments show that SLAG achieves an 18 times speedup in\nembedding computation on a 16-GPU setup compared to OpenGaussian, while\npreserving embedding quality on the ScanNet and LERF datasets. For more\ndetails, visit our project website: https://slag-project.github.io/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07862", "pdf": "https://arxiv.org/pdf/2505.07862", "abs": "https://arxiv.org/abs/2505.07862", "authors": ["Andrew Kiruluta", "Eric Lundy", "Priscilla Burity"], "title": "Graph Laplacian Wavelet Transformer via Learnable Spectral Decomposition", "categories": ["cs.CL"], "comment": null, "summary": "Existing sequence to sequence models for structured language tasks rely\nheavily on the dot product self attention mechanism, which incurs quadratic\ncomplexity in both computation and memory for input length N. We introduce the\nGraph Wavelet Transformer (GWT), a novel architecture that replaces this\nbottleneck with a learnable, multi scale wavelet transform defined over an\nexplicit graph Laplacian derived from syntactic or semantic parses. Our\nanalysis shows that multi scale spectral decomposition offers an interpretable,\nefficient, and expressive alternative to quadratic self attention for graph\nstructured sequence modeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08173", "pdf": "https://arxiv.org/pdf/2505.08173", "abs": "https://arxiv.org/abs/2505.08173", "authors": ["Xiaoshuo Yan", "Zhaochuan Li", "Lei Meng", "Zhuang Qi", "Wei Wu", "Zixuan Li", "Xiangxu Meng"], "title": "Empowering Vision Transformers with Multi-Scale Causal Intervention for Long-Tailed Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Causal inference has emerged as a promising approach to mitigate long-tail\nclassification by handling the biases introduced by class imbalance. However,\nalong with the change of advanced backbone models from Convolutional Neural\nNetworks (CNNs) to Visual Transformers (ViT), existing causal models may not\nachieve an expected performance gain. This paper investigates the influence of\nexisting causal models on CNNs and ViT variants, highlighting that ViT's global\nfeature representation makes it hard for causal methods to model associations\nbetween fine-grained features and predictions, which leads to difficulties in\nclassifying tail classes with similar visual appearance. To address these\nissues, this paper proposes TSCNet, a two-stage causal modeling method to\ndiscover fine-grained causal associations through multi-scale causal\ninterventions. Specifically, in the hierarchical causal representation learning\nstage (HCRL), it decouples the background and objects, applying backdoor\ninterventions at both the patch and feature level to prevent model from using\nclass-irrelevant areas to infer labels which enhances fine-grained causal\nrepresentation. In the counterfactual logits bias calibration stage (CLBC), it\nrefines the optimization of model's decision boundary by adaptive constructing\ncounterfactual balanced data distribution to remove the spurious associations\nin the logits caused by data distribution. Extensive experiments conducted on\nvarious long-tail benchmarks demonstrate that the proposed TSCNet can eliminate\nmultiple biases introduced by data imbalance, which outperforms existing\nmethods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08231", "pdf": "https://arxiv.org/pdf/2505.08231", "abs": "https://arxiv.org/abs/2505.08231", "authors": ["Yu Zhang", "Fengyuan Liu", "Juan Lyu", "Yi Wei", "Changdong Yu"], "title": "HMPNet: A Feature Aggregation Architecture for Maritime Object Detection from a Shipborne Perspective", "categories": ["cs.CV"], "comment": "This paper has been accepted to ICME 2025", "summary": "In the realm of intelligent maritime navigation, object detection from a\nshipborne perspective is paramount. Despite the criticality, the paucity of\nmaritime-specific data impedes the deployment of sophisticated visual\nperception techniques, akin to those utilized in autonomous vehicular systems,\nwithin the maritime context. To bridge this gap, we introduce Navigation12, a\nnovel dataset annotated for 12 object categories under diverse maritime\nenvironments and weather conditions. Based upon this dataset, we propose\nHMPNet, a lightweight architecture tailored for shipborne object detection.\nHMPNet incorporates a hierarchical dynamic modulation backbone to bolster\nfeature aggregation and expression, complemented by a matrix cascading\npoly-scale neck and a polymerization weight sharing detector, facilitating\nefficient multi-scale feature aggregation. Empirical evaluations indicate that\nHMPNet surpasses current state-of-the-art methods in terms of both accuracy and\ncomputational efficiency, realizing a 3.3% improvement in mean Average\nPrecision over YOLOv11n, the prevailing model, and reducing parameters by 23%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07889", "pdf": "https://arxiv.org/pdf/2505.07889", "abs": "https://arxiv.org/abs/2505.07889", "authors": ["Yuyang Liu", "Liuzhenghao Lv", "Xiancheng Zhang", "Li Yuan", "Yonghong Tian"], "title": "BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Biological protocols are fundamental to reproducible and safe life science\nresearch. While LLMs excel on general tasks, their systematic evaluation on\nthese highly specialized, accuracy-critical, and inherently procedural texts\nremains limited. In this work, we present BioProBench, the first large-scale,\nintegrated multi-task benchmark for biological protocol understanding and\nreasoning. While limited benchmarks have touched upon specific aspects like\nprotocol QA, BioProBench provides a comprehensive suite of five core tasks:\nProtocol Question Answering, Step Ordering, Error Correction, Protocol\nGeneration, and Protocol Reasoning, enabling a holistic evaluation of LLMs on\nprocedural biological texts. Built upon 27K original protocols, it yields\nnearly 556K high-quality structured instances. We evaluate 12 mainstream\nopen/closed-source LLMs on BioProBench. Experimental results reveal that while\ntop models preform well on surface understanding tasks, struggle significantly\nwith deep reasoning and structured generation tasks like ordering and\ngeneration. Furthermore, model comparisons reveal diverse performance: certain\nopen-source models approach closed-source levels on some tasks, yet\nbio-specific small models lag behind general LLMs, indicating limitations on\ncomplex procedural content. Overall, our findings underscore that procedural\nreasoning within biological protocols represents a significant challenge for\ncurrent LLMs. BioProBench serves as a standardized framework to diagnose these\nspecific limitations and guide the development of AI systems better equipped\nfor safely automating complex scientific procedures. The code and data are\navailable at: https://github.com/YuyangSunshine/bioprotocolbench and\nhttps://huggingface.co/datasets/GreatCaptainNemo/BioProBench.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "question answering"], "score": 5}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08233", "pdf": "https://arxiv.org/pdf/2505.08233", "abs": "https://arxiv.org/abs/2505.08233", "authors": ["Santhoshkumar Peddi", "Soham Bandyopadhyay", "Debasis Samanta"], "title": "G-MSGINet: A Grouped Multi-Scale Graph-Involution Network for Contactless Fingerprint Recognition", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents G-MSGINet, a unified and efficient framework for robust\ncontactless fingerprint recognition that jointly performs minutiae localization\nand identity embedding directly from raw input images. Existing approaches rely\non multi-branch architectures, orientation labels, or complex preprocessing\nsteps, which limit scalability and generalization across real-world acquisition\nscenarios. In contrast, the proposed architecture introduces the GMSGI layer, a\nnovel computational module that integrates grouped pixel-level involution,\ndynamic multi-scale kernel generation, and graph-based relational modelling\ninto a single processing unit. Stacked GMSGI layers progressively refine both\nlocal minutiae-sensitive features and global topological representations\nthrough end-to-end optimization. The architecture eliminates explicit\norientation supervision and adapts graph connectivity directly from learned\nkernel descriptors, thereby capturing meaningful structural relationships among\nfingerprint regions without fixed heuristics. Extensive experiments on three\nbenchmark datasets, namely PolyU, CFPose, and Benchmark 2D/3D, demonstrate that\nG-MSGINet consistently achieves minutiae F1-scores in the range of\n$0.83\\pm0.02$ and Rank-1 identification accuracies between 97.0% and 99.1%,\nwhile maintaining an Equal Error Rate (EER) as low as 0.5%. These results\ncorrespond to improvements of up to 4.8% in F1-score and 1.4% in Rank-1\naccuracy when compared to prior methods, using only 0.38 million parameters and\n6.63 giga floating-point operations, which represents up to ten times fewer\nparameters than competitive baselines. This highlights the scalability and\neffectiveness of G-MSGINet in real-world contactless biometric recognition\nscenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08266", "pdf": "https://arxiv.org/pdf/2505.08266", "abs": "https://arxiv.org/abs/2505.08266", "authors": ["Yanbin Wei", "Xuehao Wang", "Zhan Zhuang", "Yang Chen", "Shuhao Chen", "Yulong Zhang", "Yu Zhang", "James Kwok"], "title": "Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Message-passing graph neural networks (MPNNs) and structural features (SFs)\nare cornerstones for the link prediction task. However, as a common and\nintuitive mode of understanding, the potential of visual perception has been\noverlooked in the MPNN community. For the first time, we equip MPNNs with\nvision structural awareness by proposing an effective framework called Graph\nVision Network (GVN), along with a more efficient variant (E-GVN). Extensive\nempirical results demonstrate that with the proposed frameworks, GVN\nconsistently benefits from the vision enhancement across seven link prediction\ndatasets, including challenging large-scale graphs. Such improvements are\ncompatible with existing state-of-the-art (SOTA) methods and GVNs achieve new\nSOTA results, thereby underscoring a promising novel direction for link\nprediction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08273", "pdf": "https://arxiv.org/pdf/2505.08273", "abs": "https://arxiv.org/abs/2505.08273", "authors": ["Nibir Chandra Mandal", "Oishee Bintey Hoque", "Abhijin Adiga", "Samarth Swarup", "Mandy Wilson", "Lu Feng", "Yangfeng Ji", "Miaomiao Zhang", "Geoffrey Fox", "Madhav Marathe"], "title": "IrrMap: A Large-Scale Comprehensive Dataset for Irrigation Method Mapping", "categories": ["cs.CV"], "comment": null, "summary": "We introduce IrrMap, the first large-scale dataset (1.1 million patches) for\nirrigation method mapping across regions. IrrMap consists of multi-resolution\nsatellite imagery from LandSat and Sentinel, along with key auxiliary data such\nas crop type, land use, and vegetation indices. The dataset spans 1,687,899\nfarms and 14,117,330 acres across multiple western U.S. states from 2013 to\n2023, providing a rich and diverse foundation for irrigation analysis and\nensuring geospatial alignment and quality control. The dataset is ML-ready,\nwith standardized 224x224 GeoTIFF patches, the multiple input modalities,\ncarefully chosen train-test-split data, and accompanying dataloaders for\nseamless deep learning model training andbenchmarking in irrigation mapping.\nThe dataset is also accompanied by a complete pipeline for dataset generation,\nenabling researchers to extend IrrMap to new regions for irrigation data\ncollection or adapt it with minimal effort for other similar applications in\nagricultural and geospatial analysis. We also analyze the irrigation method\ndistribution across crop groups, spatial irrigation patterns (using Shannon\ndiversity indices), and irrigated area variations for both LandSat and\nSentinel, providing insights into regional and resolution-based differences. To\npromote further exploration, we openly release IrrMap, along with the derived\ndatasets, benchmark models, and pipeline code, through a GitHub repository:\nhttps://github.com/Nibir088/IrrMap and Data repository:\nhttps://huggingface.co/Nibir/IrrMap, providing comprehensive documentation and\nimplementation details.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08302", "pdf": "https://arxiv.org/pdf/2505.08302", "abs": "https://arxiv.org/abs/2505.08302", "authors": ["Oishee Bintey Hoque", "Nibir Chandra Mandal", "Abhijin Adiga", "Samarth Swarup", "Sayjro Kossi Nouwakpo", "Amanda Wilson", "Madhav Marathe"], "title": "Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing", "categories": ["cs.CV"], "comment": "Full version of the paper will be appearing at the Proceedings of the\n  Thirty-Third International Joint Conference on Artificial Intelligence\n  (IJCAI-25), Special Track on AI for Good", "summary": "Accurate mapping of irrigation methods is crucial for sustainable\nagricultural practices and food systems. However, existing models that rely\nsolely on spectral features from satellite imagery are ineffective due to the\ncomplexity of agricultural landscapes and limited training data, making this a\nchallenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a\nnovel Swin-Transformer based approach that uses (i) a specialized projection\nmatrix to encode crop to irrigation probability, (ii) a spatial attention map\nto identify agricultural lands from non-agricultural lands, (iii)\nbi-directional cross-attention to focus complementary information from\ndifferent modalities, and (iv) a weighted ensemble for combining predictions\nfrom images and crop information. Our experimentation on five states in the US\nshows up to 22.9\\% (IoU) improvement over baseline with a 71.4% (IoU)\nimprovement for hard-to-classify drip irrigation. In addition, we propose a\ntwo-phase transfer learning approach to enhance cross-state irrigation mapping,\nachieving a 51% IoU boost in a state with limited labeled data. The ability to\nachieve baseline performance with only 40% of the training data highlights its\nefficiency, reducing the dependency on extensive manual labeling efforts and\nmaking large-scale, automated irrigation mapping more feasible and\ncost-effective.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08349", "pdf": "https://arxiv.org/pdf/2505.08349", "abs": "https://arxiv.org/abs/2505.08349", "authors": ["Ruixiao Shi", "Fu Feng", "Yucheng Xie", "Jing Wang", "Xin Geng"], "title": "FAD: Frequency Adaptation and Diversion for Cross-domain Few-shot Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cross-domain few-shot learning (CD-FSL) requires models to generalize from\nlimited labeled samples under significant distribution shifts. While recent\nmethods enhance adaptability through lightweight task-specific modules, they\noperate solely in the spatial domain and overlook frequency-specific variations\nthat are often critical for robust transfer. We observe that spatially similar\nimages across domains can differ substantially in their spectral\nrepresentations, with low and high frequencies capturing complementary semantic\ninformation at coarse and fine levels. This indicates that uniform spatial\nadaptation may overlook these spectral distinctions, thus constraining\ngeneralization. To address this, we introduce Frequency Adaptation and\nDiversion (FAD), a frequency-aware framework that explicitly models and\nmodulates spectral components. At its core is the Frequency Diversion Adapter,\nwhich transforms intermediate features into the frequency domain using the\ndiscrete Fourier transform (DFT), partitions them into low, mid, and\nhigh-frequency bands via radial masks, and reconstructs each band using inverse\nDFT (IDFT). Each frequency band is then adapted using a dedicated convolutional\nbranch with a kernel size tailored to its spectral scale, enabling targeted and\ndisentangled adaptation across frequencies. Extensive experiments on the\nMeta-Dataset benchmark demonstrate that FAD consistently outperforms\nstate-of-the-art methods on both seen and unseen domains, validating the\nutility of frequency-domain representations and band-wise adaptation for\nimproving generalization in CD-FSL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08423", "pdf": "https://arxiv.org/pdf/2505.08423", "abs": "https://arxiv.org/abs/2505.08423", "authors": ["Sadaf Gulshad", "Abdullah Aldahlawi Thakaa"], "title": "DArFace: Deformation Aware Robustness for Low Quality Face Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Facial recognition systems have achieved remarkable success by leveraging\ndeep neural networks, advanced loss functions, and large-scale datasets.\nHowever, their performance often deteriorates in real-world scenarios involving\nlow-quality facial images. Such degradations, common in surveillance footage or\nstandoff imaging include low resolution, motion blur, and various distortions,\nresulting in a substantial domain gap from the high-quality data typically used\nduring training. While existing approaches attempt to address robustness by\nmodifying network architectures or modeling global spatial transformations,\nthey frequently overlook local, non-rigid deformations that are inherently\npresent in real-world settings. In this work, we introduce DArFace, a\nDeformation-Aware robust Face recognition framework that enhances robustness to\nsuch degradations without requiring paired high- and low-quality training\nsamples. Our method adversarially integrates both global transformations (e.g.,\nrotation, translation) and local elastic deformations during training to\nsimulate realistic low-quality conditions. Moreover, we introduce a contrastive\nobjective to enforce identity consistency across different deformed views.\nExtensive evaluations on low-quality benchmarks including TinyFace, IJB-B, and\nIJB-C demonstrate that DArFace surpasses state-of-the-art methods, with\nsignificant gains attributed to the inclusion of local deformation modeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08261", "pdf": "https://arxiv.org/pdf/2505.08261", "abs": "https://arxiv.org/abs/2505.08261", "authors": ["Rishabh Agrawal", "Himanshu Kumar"], "title": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08437", "pdf": "https://arxiv.org/pdf/2505.08437", "abs": "https://arxiv.org/abs/2505.08437", "authors": ["Wenkui Yang", "Zhida Zhang", "Xiaoqiang Zhou", "Junxian Duan", "Jie Cao"], "title": "TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human Body Forgery Detection", "categories": ["cs.CV"], "comment": "Accepted to PRCV 2024", "summary": "The emergence and popularity of facial deepfake methods spur the vigorous\ndevelopment of deepfake datasets and facial forgery detection, which to some\nextent alleviates the security concerns about facial-related artificial\nintelligence technologies. However, when it comes to human body forgery, there\nhas been a persistent lack of datasets and detection methods, due to the later\ninception and complexity of human body generation methods. To mitigate this\nissue, we introduce TikTok-DeepFake (TT-DF), a novel large-scale\ndiffusion-based dataset containing 6,120 forged videos with 1,378,857 synthetic\nframes, specifically tailored for body forgery detection. TT-DF offers a wide\nvariety of forgery methods, involving multiple advanced human image animation\nmodels utilized for manipulation, two generative configurations based on the\ndisentanglement of identity and pose information, as well as different\ncompressed versions. The aim is to simulate any potential unseen forged data in\nthe wild as comprehensively as possible, and we also furnish a benchmark on\nTT-DF. Additionally, we propose an adapted body forgery detection model,\nTemporal Optical Flow Network (TOF-Net), which exploits the spatiotemporal\ninconsistencies and optical flow distribution differences between natural data\nand forged data. Our experiments demonstrate that TOF-Net achieves favorable\nperformance on TT-DF, outperforming current state-of-the-art extendable facial\nforgery detection models. For our TT-DF dataset, please refer to\nhttps://github.com/HashTAG00002/TT-DF.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08311", "pdf": "https://arxiv.org/pdf/2505.08311", "abs": "https://arxiv.org/abs/2505.08311", "authors": ["Yunjie Ji", "Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Han Zhao", "Xiangang Li"], "title": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale", "categories": ["cs.CL"], "comment": null, "summary": "We present AM-Thinking-v1, a 32B dense language model that advances the\nfrontier of reasoning, embodying the collaborative spirit of open-source\ninnovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts\n(MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves\nimpressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on\nLiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities\namong open-source models of similar scale.\n  Built entirely from the open-source Qwen2.5-32B base model and publicly\navailable queries, AM-Thinking-v1 leverages a meticulously crafted\npost-training pipeline - combining supervised fine-tuning and reinforcement\nlearning - to deliver exceptional reasoning capabilities. This work\ndemonstrates that the open-source community can achieve high performance at the\n32B scale, a practical sweet spot for deployment and fine-tuning. By striking a\nbalance between top-tier performance and real-world usability, we hope\nAM-Thinking-v1 inspires further collaborative efforts to harness mid-scale\nmodels, pushing reasoning boundaries while keeping accessibility at the core of\ninnovation. We have open-sourced our model on\n\\href{https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging Face}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08435", "pdf": "https://arxiv.org/pdf/2505.08435", "abs": "https://arxiv.org/abs/2505.08435", "authors": ["Mehran Sarmadi", "Morteza Alikhani", "Erfan Zinvandi", "Zahra Pourbahman"], "title": "Hakim: Farsi Text Embedding Model", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advancements in text embedding have significantly improved natural\nlanguage understanding across many languages, yet Persian remains notably\nunderrepresented in large-scale embedding research. In this paper, we present\nHakim, a novel state-of-the-art Persian text embedding model that achieves a\n8.5% performance improvement over existing approaches on the FaMTEB benchmark,\noutperforming all previously developed Persian language models. As part of this\nwork, we introduce three new datasets - Corpesia, Pairsia-sup, and\nPairsia-unsup - to support supervised and unsupervised training scenarios.\nAdditionally, Hakim is designed for applications in chatbots and\nretrieval-augmented generation (RAG) systems, particularly addressing retrieval\ntasks that require incorporating message history within these systems. We also\npropose a new baseline model built on the BERT architecture. Our language model\nconsistently achieves higher accuracy across various Persian NLP tasks, while\nthe RetroMAE-based model proves particularly effective for textual information\nretrieval applications. Together, these contributions establish a new\nfoundation for advancing Persian language understanding.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08552", "pdf": "https://arxiv.org/pdf/2505.08552", "abs": "https://arxiv.org/abs/2505.08552", "authors": ["Haroon Wahab", "Hassan Ugail", "Irfan Mehmood"], "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent proliferation of generative AI tools for visual content\ncreation-particularly in the context of visual artworks-has raised serious\nconcerns about copyright infringement and forgery. The large-scale datasets\nused to train these models often contain a mixture of copyrighted and\nnon-copyrighted artworks. Given the tendency of generative models to memorize\ntraining patterns, they are susceptible to varying degrees of copyright\nviolation. Building on the recently proposed DeepfakeArt Challenge benchmark,\nthis work introduces DFA-CON, a contrastive learning framework designed to\ndetect copyright-infringing or forged AI-generated art. DFA-CON learns a\ndiscriminative representation space, posing affinity among original artworks\nand their forged counterparts within a contrastive learning framework. The\nmodel is trained across multiple attack types, including inpainting, style\ntransfer, adversarial perturbation, and cutmix. Evaluation results demonstrate\nrobust detection performance across most attack types, outperforming recent\npretrained foundation models. Code and model checkpoints will be released\npublicly upon acceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08585", "pdf": "https://arxiv.org/pdf/2505.08585", "abs": "https://arxiv.org/abs/2505.08585", "authors": ["Jorge Quesada", "Chen Zhou", "Prithwijit Chowdhury", "Mohammad Alotaibi", "Ahmad Mustafa", "Yusufjon Kumamnov", "Mohit Prabhushankar", "Ghassan AlRegib"], "title": "A Large-scale Benchmark on Geological Fault Delineation Models: Domain Shift, Training Dynamics, Generalizability, Evaluation and Inferential Behavior", "categories": ["cs.CV"], "comment": null, "summary": "Machine learning has taken a critical role in seismic interpretation\nworkflows, especially in fault delineation tasks. However, despite the recent\nproliferation of pretrained models and synthetic datasets, the field still\nlacks a systematic understanding of the generalizability limits of these models\nacross seismic data representing a variety of geologic, acquisition and\nprocessing settings. Distributional shifts between different data sources,\nlimitations in fine-tuning strategies and labeled data accessibility, and\ninconsistent evaluation protocols all represent major roadblocks in the\ndeployment of reliable and robust models in real-world exploration settings. In\nthis paper, we present the first large-scale benchmarking study explicitly\ndesigned to provide answers and guidelines for domain shift strategies in\nseismic interpretation. Our benchmark encompasses over $200$ models trained and\nevaluated on three heterogeneous datasets (synthetic and real data) including\nFaultSeg3D, CRACKS, and Thebe. We systematically assess pretraining,\nfine-tuning, and joint training strategies under varying degrees of domain\nshift. Our analysis highlights the fragility of current fine-tuning practices,\nthe emergence of catastrophic forgetting, and the challenges of interpreting\nperformance in a systematic manner. We establish a robust experimental baseline\nto provide insights into the tradeoffs inherent to current fault delineation\nworkflows, and shed light on directions for developing more generalizable,\ninterpretable and effective machine learning models for seismic interpretation.\nThe insights and analyses reported provide a set of guidelines on the\ndeployment of fault delineation models within seismic interpretation workflows.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08607", "pdf": "https://arxiv.org/pdf/2505.08607", "abs": "https://arxiv.org/abs/2505.08607", "authors": ["Yuran Wang", "Yingping Liang", "Ying Fu"], "title": "Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World", "categories": ["cs.CV"], "comment": null, "summary": "Stereo matching methods rely on dense pixel-wise ground truth labels, which\nare laborious to obtain, especially for real-world datasets. The scarcity of\nlabeled data and domain gaps between synthetic and real-world images also pose\nnotable challenges. In this paper, we propose a novel framework,\n\\textbf{BooSTer}, that leverages both vision foundation models and large-scale\nmixed image sources, including synthetic, real, and single-view images. First,\nto fully unleash the potential of large-scale single-view images, we design a\ndata generation strategy combining monocular depth estimation and diffusion\nmodels to generate dense stereo matching data from single-view images. Second,\nto tackle sparse labels in real-world datasets, we transfer knowledge from\nmonocular depth estimation models, using pseudo-mono depth labels and a dynamic\nscale- and shift-invariant loss for additional supervision. Furthermore, we\nincorporate vision foundation model as an encoder to extract robust and\ntransferable features, boosting accuracy and generalization. Extensive\nexperiments on benchmark datasets demonstrate the effectiveness of our\napproach, achieving significant improvements in accuracy over existing methods,\nparticularly in scenarios with limited labeled data and domain shifts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08651", "pdf": "https://arxiv.org/pdf/2505.08651", "abs": "https://arxiv.org/abs/2505.08651", "authors": ["Chen Wu", "Yin Song"], "title": "Scaling Context, Not Parameters: Training a Compact 7B Language Model for Efficient Long-Context Processing", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages, 6 figures, ACL 2025 (Industry Track)", "summary": "We present MegaBeam-Mistral-7B, a language model that supports 512K-token\ncontext length. Our work addresses practical limitations in long-context\ntraining, supporting real-world tasks such as compliance monitoring and\nverification. Evaluated on three long-context benchmarks, our 7B-parameter\nmodel demonstrates superior in-context learning performance on HELMET and\nrobust retrieval and tracing capability on RULER. It is currently the only open\nmodel to achieve competitive long-range reasoning on BABILong at 512K context\nlength without RAG or targeted fine-tuning. Released as fully open source under\nthe Apache 2.0 license, the model has been downloaded over 100,000 times on\nHugging Face. Model available at:\nhttps://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08751", "pdf": "https://arxiv.org/pdf/2505.08751", "abs": "https://arxiv.org/abs/2505.08751", "authors": ["Saurabh Dash", "Yiyang Nan", "John Dang", "Arash Ahmadian", "Shivalika Singh", "Madeline Smith", "Bharat Venkitesh", "Vlad Shmyhlo", "Viraat Aryabumi", "Walter Beller-Morales", "Jeremy Pekmez", "Jason Ozuzu", "Pierre Richemond", "Acyr Locatelli", "Nick Frosst", "Phil Blunsom", "Aidan Gomez", "Ivan Zhang", "Marzieh Fadaee", "Manoj Govindassamy", "Sudip Roy", "Matthias Gallé", "Beyza Ermis", "Ahmet Üstün", "Sara Hooker"], "title": "Aya Vision: Advancing the Frontier of Multilingual Multimodality", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Building multimodal language models is fundamentally challenging: it requires\naligning vision and language modalities, curating high-quality instruction\ndata, and avoiding the degradation of existing text-only capabilities once\nvision is introduced. These difficulties are further magnified in the\nmultilingual setting, where the need for multimodal data in different languages\nexacerbates existing data scarcity, machine translation often distorts meaning,\nand catastrophic forgetting is more pronounced. To address the aforementioned\nchallenges, we introduce novel techniques spanning both data and modeling.\nFirst, we develop a synthetic annotation framework that curates high-quality,\ndiverse multilingual multimodal instruction data, enabling Aya Vision models to\nproduce natural, human-preferred responses to multimodal inputs across many\nlanguages. Complementing this, we propose a cross-modal model merging technique\nthat mitigates catastrophic forgetting, effectively preserving text-only\ncapabilities while simultaneously enhancing multimodal generative performance.\nAya-Vision-8B achieves best-in-class performance compared to strong multimodal\nmodels such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger\nLlama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which\noutperforms models more than twice its size, such as Molmo-72B and\nLLaMA-3.2-90B-Vision. Our work advances multilingual progress on the\nmulti-modal frontier, and provides insights into techniques that effectively\nbend the need for compute while delivering extremely high performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08723", "pdf": "https://arxiv.org/pdf/2505.08723", "abs": "https://arxiv.org/abs/2505.08723", "authors": ["Xiaolei Qin", "Di Wang", "Jing Zhang", "Fengxiang Wang", "Xin Su", "Bo Du", "Liangpei Zhang"], "title": "TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series", "categories": ["cs.CV"], "comment": null, "summary": "Satellite image time series (SITS) provide continuous observations of the\nEarth's surface, making them essential for applications such as environmental\nmanagement and disaster assessment. However, existing spatiotemporal foundation\nmodels rely on plain vision transformers, which encode entire temporal\nsequences without explicitly capturing multiscale spatiotemporal relationships\nbetween land objects. This limitation hinders their effectiveness in downstream\ntasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision\ntransformer foundation model tailored for SITS analysis. At its core, we\nintroduce a spatiotemporal gyroscope attention mechanism that dynamically\ncaptures evolving multiscale patterns across both time and space. For\npre-training, we curate MillionST, a large-scale dataset of one million images\nfrom 100,000 geographic locations, each captured across 10 temporal phases over\nfive years, encompassing diverse geospatial changes and seasonal variations.\nLeveraging this dataset, we adapt masked image modeling to pre-train TiMo,\nenabling it to effectively learn and encode generalizable spatiotemporal\nrepresentations.Extensive experiments across multiple spatiotemporal\ntasks-including deforestation monitoring, land cover segmentation, crop type\nclassification, and flood detection-demonstrate TiMo's superiority over\nstate-of-the-art methods. Code, model, and dataset will be released at\nhttps://github.com/MiliLab/TiMo.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08725", "pdf": "https://arxiv.org/pdf/2505.08725", "abs": "https://arxiv.org/abs/2505.08725", "authors": ["Zongchuang Zhao", "Haoyu Fu", "Dingkang Liang", "Xin Zhou", "Dingyuan Zhang", "Hongwei Xie", "Bing Wang", "Xiang Bai"], "title": "Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving", "categories": ["cs.CV"], "comment": "The dataset and code will be released at\n  https://github.com/zc-zhao/DriveMonkey", "summary": "The Large Visual-Language Models (LVLMs) have significantly advanced image\nunderstanding. Their comprehension and reasoning capabilities enable promising\napplications in autonomous driving scenarios. However, existing research\ntypically focuses on front-view perspectives and partial objects within scenes,\nstruggling to achieve comprehensive scene understanding. Meanwhile, existing\nLVLMs suffer from the lack of mapping relationship between 2D and 3D and\ninsufficient integration of 3D object localization and instruction\nunderstanding. To tackle these limitations, we first introduce NuInteract, a\nlarge-scale dataset with over 1.5M multi-view image language pairs spanning\ndense scene captions and diverse interactive tasks. Furthermore, we propose\nDriveMonkey, a simple yet effective framework that seamlessly integrates LVLMs\nwith a spatial processor using a series of learnable queries. The spatial\nprocessor, designed as a plug-and-play component, can be initialized with\npre-trained 3D detectors to improve 3D perception. Our experiments show that\nDriveMonkey outperforms general LVLMs, especially achieving a 9.86% notable\nimprovement on the 3D visual grounding task. The dataset and code will be\nreleased at https://github.com/zc-zhao/DriveMonkey.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07865", "pdf": "https://arxiv.org/pdf/2505.07865", "abs": "https://arxiv.org/abs/2505.07865", "authors": ["Fan Zhang", "Tianyu Liu", "Zhihong Zhu", "Hao Wu", "Haixin Wang", "Donghao Zhou", "Yefeng Zheng", "Kun Wang", "Xian Wu", "Pheng-Ann Heng"], "title": "CellVerse: Do Large Language Models Really Understand Cell Biology?", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "q-bio.CB"], "comment": null, "summary": "Recent studies have demonstrated the feasibility of modeling single-cell data\nas natural languages and the potential of leveraging powerful large language\nmodels (LLMs) for understanding cell biology. However, a comprehensive\nevaluation of LLMs' performance on language-driven single-cell analysis tasks\nstill remains unexplored. Motivated by this challenge, we introduce CellVerse,\na unified language-centric question-answering benchmark that integrates four\ntypes of single-cell multi-omics data and encompasses three hierarchical levels\nof single-cell analysis tasks: cell type annotation (cell-level), drug response\nprediction (drug-level), and perturbation analysis (gene-level). Going beyond\nthis, we systematically evaluate the performance across 14 open-source and\nclosed-source LLMs ranging from 160M to 671B on CellVerse. Remarkably, the\nexperimental results reveal: (1) Existing specialist models (C2S-Pythia) fail\nto make reasonable decisions across all sub-tasks within CellVerse, while\ngeneralist models such as Qwen, Llama, GPT, and DeepSeek family models exhibit\npreliminary understanding capabilities within the realm of cell biology. (2)\nThe performance of current LLMs falls short of expectations and has substantial\nroom for improvement. Notably, in the widely studied drug response prediction\ntask, none of the evaluated LLMs demonstrate significant performance\nimprovement over random guessing. CellVerse offers the first large-scale\nempirical demonstration that significant challenges still remain in applying\nLLMs to cell biology. By introducing CellVerse, we lay the foundation for\nadvancing cell biology through natural languages and hope this paradigm could\nfacilitate next-generation single-cell analysis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "annotation"], "score": 3}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07840", "pdf": "https://arxiv.org/pdf/2505.07840", "abs": "https://arxiv.org/abs/2505.07840", "authors": ["Alavikunhu Panthakkan", "S M Anzar", "K. Sherin", "Saeed Al Mansoori", "Hussain Al-Ahmad"], "title": "Evaluation of UAV-Based RGB and Multispectral Vegetation Indices for Precision Agriculture in Palm Tree Cultivation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Precision farming relies on accurate vegetation monitoring to enhance crop\nproductivity and promote sustainable agricultural practices. This study\npresents a comprehensive evaluation of UAV-based imaging for vegetation health\nassessment in a palm tree cultivation region in Dubai. By comparing\nmultispectral and RGB image data, we demonstrate that RGBbased vegetation\nindices offer performance comparable to more expensive multispectral indices,\nproviding a cost-effective alternative for large-scale agricultural monitoring.\nUsing UAVs equipped with multispectral sensors, indices such as NDVI and SAVI\nwere computed to categorize vegetation into healthy, moderate, and stressed\nconditions. Simultaneously, RGB-based indices like VARI and MGRVI delivered\nsimilar results in vegetation classification and stress detection. Our findings\nhighlight the practical benefits of integrating RGB imagery into precision\nfarming, reducing operational costs while maintaining accuracy in plant health\nmonitoring. This research underscores the potential of UAVbased RGB imaging as\na powerful tool for precision agriculture, enabling broader adoption of\ndata-driven decision-making in crop management. By leveraging the strengths of\nboth multispectral and RGB imaging, this work advances the state of UAV\napplications in agriculture, paving the way for more efficient and scalable\nfarming solutions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07912", "pdf": "https://arxiv.org/pdf/2505.07912", "abs": "https://arxiv.org/abs/2505.07912", "authors": ["Tim Wittenborg", "Constantin Sebastian Tremel", "Niklas Stehr", "Oliver Karras", "Markus Stocker", "Sören Auer"], "title": "SciCom Wiki: Fact-Checking and FAIR Knowledge Distribution for Scientific Videos and Podcasts", "categories": ["cs.DL", "cs.CL", "cs.MM"], "comment": "18 pages, 10 figures, submitted to TPDL 2025", "summary": "Democratic societies need accessible, reliable information. Videos and\nPodcasts have established themselves as the medium of choice for civic\ndissemination, but also as carriers of misinformation. The emerging Science\nCommunication Knowledge Infrastructure (SciCom KI) curating non-textual media\nis still fragmented and not adequately equipped to scale against the content\nflood. Our work sets out to support the SciCom KI with a central, collaborative\nplatform, the SciCom Wiki, to facilitate FAIR (findable, accessible,\ninteroperable, reusable) media representation and the fact-checking of their\ncontent, particularly for videos and podcasts. Building an open-source service\nsystem centered around Wikibase, we survey requirements from 53 stakeholders,\nrefine these in 11 interviews, and evaluate our prototype based on these\nrequirements with another 14 participants. To address the most requested\nfeature, fact-checking, we developed a neurosymbolic computational\nfact-checking approach, converting heterogenous media into knowledge graphs.\nThis increases machine-readability and allows comparing statements against\nequally represented ground-truth. Our computational fact-checking tool was\niteratively evaluated through 10 expert interviews, a public user survey with\n43 participants verified the necessity and usability of our tool. Overall, our\nfindings identified several needs to systematically support the SciCom KI. The\nSciCom Wiki, as a FAIR digital library complementing our neurosymbolic\ncomputational fact-checking framework, was found suitable to address the raised\nrequirements. Further, we identified that the SciCom KI is severely\nunderdeveloped regarding FAIR knowledge and related systems facilitating its\ncollaborative creation and curation. Our system can provide a central knowledge\nnode, yet a collaborative effort is required to scale against the imminent\n(mis-)information flood.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07866", "pdf": "https://arxiv.org/pdf/2505.07866", "abs": "https://arxiv.org/abs/2505.07866", "authors": ["Abdullah", "Tao Huang", "Ickjai Lee", "Euijoon Ahn"], "title": "Computationally Efficient Diffusion Models in Medical Imaging: A Comprehensive Review", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "pages 36, 6 figures", "summary": "The diffusion model has recently emerged as a potent approach in computer\nvision, demonstrating remarkable performances in the field of generative\nartificial intelligence. Capable of producing high-quality synthetic images,\ndiffusion models have been successfully applied across a range of applications.\nHowever, a significant challenge remains with the high computational cost\nassociated with training and generating these models. This study focuses on the\nefficiency and inference time of diffusion-based generative models,\nhighlighting their applications in both natural and medical imaging. We present\nthe most recent advances in diffusion models by categorizing them into three\nkey models: the Denoising Diffusion Probabilistic Model (DDPM), the Latent\nDiffusion Model (LDM), and the Wavelet Diffusion Model (WDM). These models play\na crucial role in medical imaging, where producing fast, reliable, and\nhigh-quality medical images is essential for accurate analysis of abnormalities\nand disease diagnosis. We first investigate the general framework of DDPM, LDM,\nand WDM and discuss the computational complexity gap filled by these models in\nnatural and medical imaging. We then discuss the current limitations of these\nmodels as well as the opportunities and future research directions in medical\nimaging.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08148", "pdf": "https://arxiv.org/pdf/2505.08148", "abs": "https://arxiv.org/abs/2505.08148", "authors": ["Sunday Oyinlola Ogundoyin", "Muhammad Ikram", "Hassan Jameel Asghar", "Benjamin Zi Hao Zhao", "Dali Kaafar"], "title": "A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Millions of users leverage generative pretrained transformer (GPT)-based\nlanguage models developed by leading model providers for a wide range of tasks.\nTo support enhanced user interaction and customization, many platforms-such as\nOpenAI-now enable developers to create and publish tailored model instances,\nknown as custom GPTs, via dedicated repositories or application stores. These\ncustom GPTs empower users to browse and interact with specialized applications\ndesigned to meet specific needs. However, as custom GPTs see growing adoption,\nconcerns regarding their security vulnerabilities have intensified. Existing\nresearch on these vulnerabilities remains largely theoretical, often lacking\nempirical, large-scale, and statistically rigorous assessments of associated\nrisks.\n  In this study, we analyze 14,904 custom GPTs to assess their susceptibility\nto seven exploitable threats, such as roleplay-based attacks, system prompt\nleakage, phishing content generation, and malicious code synthesis, across\nvarious categories and popularity tiers within the OpenAI marketplace. We\nintroduce a multi-metric ranking system to examine the relationship between a\ncustom GPT's popularity and its associated security risks.\n  Our findings reveal that over 95% of custom GPTs lack adequate security\nprotections. The most prevalent vulnerabilities include roleplay-based\nvulnerabilities (96.51%), system prompt leakage (92.20%), and phishing\n(91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibit\ninherent security weaknesses, which are often inherited or amplified in custom\nGPTs. These results highlight the urgent need for enhanced security measures\nand stricter content moderation to ensure the safe deployment of GPT-based\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08163", "pdf": "https://arxiv.org/pdf/2505.08163", "abs": "https://arxiv.org/abs/2505.08163", "authors": ["Andrew Cart", "Shaohu Zhang", "Melanie Escue", "Xugui Zhou", "Haitao Zhao", "Prashanth BusiReddyGari", "Beiyu Lin", "Shuang Li"], "title": "Decoding Neighborhood Environments with Large Language Models", "categories": ["cs.AI", "cs.CV"], "comment": "8 pages", "summary": "Neighborhood environments include physical and environmental conditions such\nas housing quality, roads, and sidewalks, which significantly influence human\nhealth and well-being. Traditional methods for assessing these environments,\nincluding field surveys and geographic information systems (GIS), are\nresource-intensive and challenging to evaluate neighborhood environments at\nscale. Although machine learning offers potential for automated analysis, the\nlaborious process of labeling training data and the lack of accessible models\nhinder scalability. This study explores the feasibility of large language\nmodels (LLMs) such as ChatGPT and Gemini as tools for decoding neighborhood\nenvironments (e.g., sidewalk and powerline) at scale. We train a robust\nYOLOv11-based model, which achieves an average accuracy of 99.13% in detecting\nsix environmental indicators, including streetlight, sidewalk, powerline,\napartment, single-lane road, and multilane road. We then evaluate four LLMs,\nincluding ChatGPT, Gemini, Claude, and Grok, to assess their feasibility,\nrobustness, and limitations in identifying these indicators, with a focus on\nthe impact of prompting strategies and fine-tuning. We apply majority voting\nwith the top three LLMs to achieve over 88% accuracy, which demonstrates LLMs\ncould be a useful tool to decode the neighborhood environment without any\ntraining effort.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08638", "pdf": "https://arxiv.org/pdf/2505.08638", "abs": "https://arxiv.org/abs/2505.08638", "authors": ["Darshan Deshpande", "Varun Gangal", "Hersh Mehta", "Jitin Krishnan", "Anand Kannappan", "Rebecca Qian"], "title": "TRAIL: Trace Reasoning and Agentic Issue Localization", "categories": ["cs.AI", "cs.CL"], "comment": "Dataset link: https://huggingface.co/datasets/PatronusAI/TRAIL", "summary": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08727", "pdf": "https://arxiv.org/pdf/2505.08727", "abs": "https://arxiv.org/abs/2505.08727", "authors": ["Fangyuan Yu"], "title": "Memorization-Compression Cycles Improve Generalization", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "comment": "12 pages, 6 figures", "summary": "We prove theoretically that generalization improves not only through data\nscaling but also by compressing internal representations. To operationalize\nthis insight, we introduce the Information Bottleneck Language Modeling (IBLM)\nobjective, which reframes language modeling as a constrained optimization\nproblem: minimizing representation entropy subject to optimal prediction\nperformance. Empirically, we observe an emergent memorization-compression cycle\nduring LLM pretraining, evidenced by oscillation positive/negative gradient\nalignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of\nrepresentation entropy. This pattern closely mirrors the predictive-compressive\ntrade-off prescribed by IBLM and also parallels the biological alternation\nbetween awake learning and sleep consolidation. Motivated by this observation,\nwe propose Gated Phase Transition (GAPT), a training algorithm that adaptively\nswitches between memorization and compression phases. When applied to GPT-2\npretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves\ncross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining\ntask on arithmetic multiplication. In a setting designed to simulate\ncatastrophic forgetting, GAPT reduces interference by compressing and\nseparating representations, achieving a 97% improvement in separation -\nparalleling the functional role of sleep consolidation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08247", "pdf": "https://arxiv.org/pdf/2505.08247", "abs": "https://arxiv.org/abs/2505.08247", "authors": ["Midi Wan", "Pengfei Li", "Yizhuo Liang", "Di Wu", "Yushan Pan", "Guangzhen Zhu", "Hao Wang"], "title": "Skeleton-Guided Diffusion Model for Accurate Foot X-ray Synthesis in Hallux Valgus Diagnosis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image synthesis plays a crucial role in providing anatomically\naccurate images for diagnosis and treatment. Hallux valgus, which affects\napproximately 19% of the global population, requires frequent weight-bearing\nX-rays for assessment, placing additional strain on both patients and\nhealthcare providers. Existing X-ray models often struggle to balance image\nfidelity, skeletal consistency, and physical constraints, particularly in\ndiffusion-based methods that lack skeletal guidance. We propose the\nSkeletal-Constrained Conditional Diffusion Model (SCCDM) and introduce KCC, a\nfoot evaluation method utilizing skeletal landmarks. SCCDM incorporates\nmulti-scale feature extraction and attention mechanisms, improving the\nStructural Similarity Index (SSIM) by 5.72% (0.794) and Peak Signal-to-Noise\nRatio (PSNR) by 18.34% (21.40 dB). When combined with KCC, the model achieves\nan average score of 0.85, demonstrating strong clinical applicability. The code\nis available at https://github.com/midisec/SCCDM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08430", "pdf": "https://arxiv.org/pdf/2505.08430", "abs": "https://arxiv.org/abs/2505.08430", "authors": ["Lei Su"], "title": "GNCAF: A GNN-based Neighboring Context Aggregation Framework for Tertiary Lymphoid Structures Semantic Segmentation in WSI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Tertiary lymphoid structures (TLS) are organized clusters of immune cells,\nwhose maturity and area can be quantified in whole slide image (WSI) for\nvarious prognostic tasks. Existing methods for assessing these characteristics\ntypically rely on cell proxy tasks and require additional post-processing\nsteps. In this work, We focus on a novel task-TLS Semantic Segmentation\n(TLS-SS)-which segments both the regions and maturation stages of TLS in WSI in\nan end-to-end manner. Due to the extensive scale of WSI and patch-based\nsegmentation strategies, TLS-SS necessitates integrating from neighboring\npatches to guide target patch (target) segmentation. Previous techniques often\nemploy on multi-resolution approaches, constraining the capacity to leverage\nthe broader neighboring context while tend to preserve coarse-grained\ninformation. To address this, we propose a GNN-based Neighboring Context\nAggregation Framework (GNCAF), which progressively aggregates multi-hop\nneighboring context from the target and employs a self-attention mechanism to\nguide the segmentation of the target. GNCAF can be integrated with various\nsegmentation models to enhance their ability to perceive contextual information\noutside of the patch. We build two TLS-SS datasets, called TCGA-COAD and\nINHOUSE-PAAD, and make the former (comprising 225 WSIs and 5041 TLSs) publicly\navailable. Experiments on these datasets demonstrate the superiority of GNCAF,\nachieving a maximum of 22.08% and 26.57% improvement in mF1 and mIoU,\nrespectively. Additionally, we also validate the task scalability of GNCAF on\nsegmentation of lymph node metastases.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08693", "pdf": "https://arxiv.org/pdf/2505.08693", "abs": "https://arxiv.org/abs/2505.08693", "authors": ["Badhan Kumar Das", "Ajay Singh", "Gengyan Zhao", "Han Liu", "Thomas J. Re", "Dorin Comaniciu", "Eli Gibson", "Andreas Maier"], "title": "VIViT: Variable-Input Vision Transformer Framework for 3D MR Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "9 pages", "summary": "Self-supervised pretrain techniques have been widely used to improve the\ndownstream tasks' performance. However, real-world magnetic resonance (MR)\nstudies usually consist of different sets of contrasts due to different\nacquisition protocols, which poses challenges for the current deep learning\nmethods on large-scale pretrain and different downstream tasks with different\ninput requirements, since these methods typically require a fixed set of input\nmodalities or, contrasts. To address this challenge, we propose variable-input\nViT (VIViT), a transformer-based framework designed for self-supervised\npretraining and segmentation finetuning for variable contrasts in each study.\nWith this ability, our approach can maximize the data availability in pretrain,\nand can transfer the learned knowledge from pretrain to downstream tasks\ndespite variations in input requirements. We validate our method on brain\ninfarct and brain tumor segmentation, where our method outperforms current CNN\nand ViT-based models with a mean Dice score of 0.624 and 0.883 respectively.\nThese results highlight the efficacy of our design for better adaptability and\nperformance on tasks with real-world heterogeneous MR data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08751", "pdf": "https://arxiv.org/pdf/2505.08751", "abs": "https://arxiv.org/abs/2505.08751", "authors": ["Saurabh Dash", "Yiyang Nan", "John Dang", "Arash Ahmadian", "Shivalika Singh", "Madeline Smith", "Bharat Venkitesh", "Vlad Shmyhlo", "Viraat Aryabumi", "Walter Beller-Morales", "Jeremy Pekmez", "Jason Ozuzu", "Pierre Richemond", "Acyr Locatelli", "Nick Frosst", "Phil Blunsom", "Aidan Gomez", "Ivan Zhang", "Marzieh Fadaee", "Manoj Govindassamy", "Sudip Roy", "Matthias Gallé", "Beyza Ermis", "Ahmet Üstün", "Sara Hooker"], "title": "Aya Vision: Advancing the Frontier of Multilingual Multimodality", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Building multimodal language models is fundamentally challenging: it requires\naligning vision and language modalities, curating high-quality instruction\ndata, and avoiding the degradation of existing text-only capabilities once\nvision is introduced. These difficulties are further magnified in the\nmultilingual setting, where the need for multimodal data in different languages\nexacerbates existing data scarcity, machine translation often distorts meaning,\nand catastrophic forgetting is more pronounced. To address the aforementioned\nchallenges, we introduce novel techniques spanning both data and modeling.\nFirst, we develop a synthetic annotation framework that curates high-quality,\ndiverse multilingual multimodal instruction data, enabling Aya Vision models to\nproduce natural, human-preferred responses to multimodal inputs across many\nlanguages. Complementing this, we propose a cross-modal model merging technique\nthat mitigates catastrophic forgetting, effectively preserving text-only\ncapabilities while simultaneously enhancing multimodal generative performance.\nAya-Vision-8B achieves best-in-class performance compared to strong multimodal\nmodels such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger\nLlama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which\noutperforms models more than twice its size, such as Molmo-72B and\nLLaMA-3.2-90B-Vision. Our work advances multilingual progress on the\nmulti-modal frontier, and provides insights into techniques that effectively\nbend the need for compute while delivering extremely high performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08787", "pdf": "https://arxiv.org/pdf/2505.08787", "abs": "https://arxiv.org/abs/2505.08787", "authors": ["Hanjung Kim", "Jaehyun Kang", "Hyolim Kang", "Meedeum Cho", "Seon Joo Kim", "Youngwoon Lee"], "title": "UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations", "categories": ["cs.RO", "cs.CV"], "comment": "Project Page: https://kimhanjung.github.io/UniSkill/", "summary": "Mimicry is a fundamental learning mechanism in humans, enabling individuals\nto learn new tasks by observing and imitating experts. However, applying this\nability to robots presents significant challenges due to the inherent\ndifferences between human and robot embodiments in both their visual appearance\nand physical capabilities. While previous methods bridge this gap using\ncross-embodiment datasets with shared scenes and tasks, collecting such aligned\ndata between humans and robots at scale is not trivial. In this paper, we\npropose UniSkill, a novel framework that learns embodiment-agnostic skill\nrepresentations from large-scale cross-embodiment video data without any\nlabels, enabling skills extracted from human video prompts to effectively\ntransfer to robot policies trained only on robot data. Our experiments in both\nsimulation and real-world environments show that our cross-embodiment skills\nsuccessfully guide robots in selecting appropriate actions, even with unseen\nvideo prompts. The project website can be found at:\nhttps://kimhanjung.github.io/UniSkill.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
