{"id": "2505.13271", "pdf": "https://arxiv.org/pdf/2505.13271", "abs": "https://arxiv.org/abs/2505.13271", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning", "categories": ["cs.CL"], "comment": "11 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "self-correction"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13403", "pdf": "https://arxiv.org/pdf/2505.13403", "abs": "https://arxiv.org/abs/2505.13403", "authors": ["Renjie Pi", "Felix Bai", "Qibin Chen", "Simon Wang", "Jiulong Shan", "Kieran Liu", "Meng Cao"], "title": "MR. Judge: Multimodal Reasoner as a Judge", "categories": ["cs.CL"], "comment": null, "summary": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "reasoning model"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13308", "pdf": "https://arxiv.org/pdf/2505.13308", "abs": "https://arxiv.org/abs/2505.13308", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scaling law"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy gradient"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13438", "pdf": "https://arxiv.org/pdf/2505.13438", "abs": "https://arxiv.org/abs/2505.13438", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "test-time compute"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13427", "pdf": "https://arxiv.org/pdf/2505.13427", "abs": "https://arxiv.org/abs/2505.13427", "authors": ["Lingxiao Du", "Fanqing Meng", "Zongkai Liu", "Zhixiang Zhou", "Ping Luo", "Qiaosheng Zhang", "Wenqi Shao"], "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning", "monte carlo tree search", "MCTS"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "mathematical reasoning", "fine-grained"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11703", "pdf": "https://arxiv.org/pdf/2505.11703", "abs": "https://arxiv.org/abs/2505.11703", "authors": ["Jae Myung Kim", "Stephan Alaniz", "Cordelia Schmid", "Zeynep Akata"], "title": "LoFT: LoRA-fused Training Dataset Generation with Few-shot Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent advances in text-to-image generation, using synthetically\ngenerated data seldom brings a significant boost in performance for supervised\nlearning. Oftentimes, synthetic datasets do not faithfully recreate the data\ndistribution of real data, i.e., they lack the fidelity or diversity needed for\neffective downstream model training. While previous work has employed few-shot\nguidance to address this issue, existing methods still fail to capture and\ngenerate features unique to specific real images. In this paper, we introduce a\nnovel dataset generation framework named LoFT, LoRA-Fused Training-data\nGeneration with Few-shot Guidance. Our method fine-tunes LoRA weights on\nindividual real images and fuses them at inference time, producing synthetic\nimages that combine the features of real images for improved diversity and\nfidelity of generated data. We evaluate the synthetic data produced by LoFT on\n10 datasets, using 8 to 64 real images per class as guidance and scaling up to\n1000 images per class. Our experiments show that training on LoFT-generated\ndata consistently outperforms other synthetic dataset methods, significantly\nincreasing accuracy as the dataset size increases. Additionally, our analysis\ndemonstrates that LoFT generates datasets with high fidelity and sufficient\ndiversity, which contribute to the performance improvement. The code is\navailable at https://github.com/ExplainableML/LoFT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11820", "pdf": "https://arxiv.org/pdf/2505.11820", "abs": "https://arxiv.org/abs/2505.11820", "authors": ["Kaitao Song", "Xiaohua Wang", "Xu Tan", "Huiqiang Jiang", "Chengruidong Zhang", "Yongliang Shen", "Cen LU", "Zihao Li", "Zifan Song", "Caihua Shan", "Yansen Wang", "Kan Ren", "Xiaoqing Zheng", "Tao Qin", "Yuqing Yang", "Dongsheng Li", "Lili Qiu"], "title": "Chain-of-Model Learning for Language Model", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12050", "pdf": "https://arxiv.org/pdf/2505.12050", "abs": "https://arxiv.org/abs/2505.12050", "authors": ["Vinod Raman", "Hilal Asi", "Satyen Kale"], "title": "ABoN: Adaptive Best-of-N Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in test-time alignment methods, such as Best-of-N sampling,\noffer a simple and effective way to steer language models (LMs) toward\npreferred behaviors using reward models (RM). However, these approaches can be\ncomputationally expensive, especially when applied uniformly across prompts\nwithout accounting for differences in alignment difficulty. In this work, we\npropose a prompt-adaptive strategy for Best-of-N alignment that allocates\ninference-time compute more efficiently. Motivated by latency concerns, we\ndevelop a two-stage algorithm: an initial exploratory phase estimates the\nreward distribution for each prompt using a small exploration budget, and a\nsecond stage adaptively allocates the remaining budget using these estimates.\nOur method is simple, practical, and compatible with any LM/RM combination.\nEmpirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different\nbatches of prompts show that our adaptive strategy consistently outperforms the\nuniform allocation with the same inference budget. Moreover, our experiments\nshow that our adaptive strategy remains competitive against uniform allocations\nwith 20% larger inference budgets and even improves in performance as the batch\nsize grows.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12307", "pdf": "https://arxiv.org/pdf/2505.12307", "abs": "https://arxiv.org/abs/2505.12307", "authors": ["Maoyuan Ye", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?", "categories": ["cs.CV", "cs.CL"], "comment": "GitHub: \\url{https://github.com/MiliLab/LogicOCR}", "summary": "Recent advances in Large Multimodal Models (LMMs) have significantly improved\ntheir reasoning and Optical Character Recognition (OCR) capabilities. However,\ntheir performance on complex logical reasoning tasks involving text-rich images\nremains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark\ncomprising 1,100 multiple-choice questions designed to evaluate LMMs' logical\nreasoning abilities on text-rich images, while minimizing reliance on\ndomain-specific knowledge (e.g., mathematics). We construct LogicOCR by\ncurating a text corpus from the Chinese National Civil Servant Examination and\ndevelop a scalable, automated pipeline to convert it into multimodal samples.\nFirst, we design prompt templates to steer GPT-Image-1 to generate images with\ndiverse backgrounds, interleaved text-illustration layouts, and varied fonts,\nensuring contextual relevance and visual realism. Then, the generated images\nare manually verified, with low-quality examples discarded. We evaluate a range\nof representative open-source and proprietary LMMs under both Chain-of-Thought\n(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key\ninsights, such as the impact of test-time scaling, input modality differences,\nand sensitivity to visual-text orientation. Notably, LMMs still lag in\nmultimodal reasoning compared to text-only inputs, indicating that they have\nnot fully bridged visual reading with reasoning. We hope LogicOCR will serve as\na valuable resource for advancing multimodal reasoning research. The dataset is\navailable at https://github.com/MiliLab/LogicOCR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "multi-dimensional"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12476", "pdf": "https://arxiv.org/pdf/2505.12476", "abs": "https://arxiv.org/abs/2505.12476", "authors": ["Xiao Long", "Liansheng Zhuang", "Chen Shen", "Shaotian Yan", "Yifei Li", "Shafei Wang"], "title": "Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive\nperformance in Knowledge Graph Question Answering (KGQA) tasks, which aim to\nfind answers based on knowledge graphs (KGs) for natural language questions.\nExisting LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented\nGeneration (GraphRAG) paradigm, which first retrieves reasoning paths from the\nlarge KGs, and then generates the answers based on them. However, these methods\nemphasize the exploration of new optimal reasoning paths in KGs while ignoring\nthe exploitation of historical reasoning paths, which may lead to sub-optimal\nreasoning paths. Additionally, the complex semantics contained in questions may\nlead to the retrieval of inaccurate reasoning paths. To address these issues,\nthis paper proposes a novel and training-free framework for KGQA tasks called\nReward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original\nquestion into a series of simpler and well-defined sub-questions to handle the\ncomplex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided\nby a reward model is introduced to iteratively retrieve weighted reasoning\npaths as contextual knowledge. Finally, it stacks the weighted reasoning paths\naccording to their weights to generate the final answers. Extensive experiments\non four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves\n8.7\\% and 7.0\\% performance improvement over the state-of-the-art method on the\nGrailQA and the WebQSP respectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search", "MCTS"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12625", "pdf": "https://arxiv.org/pdf/2505.12625", "abs": "https://arxiv.org/abs/2505.12625", "authors": ["Ali Naseh", "Harsh Chaudhari", "Jaechul Roh", "Mingshi Wu", "Alina Oprea", "Amir Houmansadr"], "title": "R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "DeepSeek recently released R1, a high-performing large language model (LLM)\noptimized for reasoning tasks. Despite its efficient training pipeline, R1\nachieves competitive performance, even surpassing leading reasoning models like\nOpenAI's o1 on several benchmarks. However, emerging reports suggest that R1\nrefuses to answer certain prompts related to politically sensitive topics in\nChina. While existing LLMs often implement safeguards to avoid generating\nharmful or offensive outputs, R1 represents a notable shift - exhibiting\ncensorship-like behavior on politically charged queries. In this paper, we\ninvestigate this phenomenon by first introducing a large-scale set of heavily\ncurated prompts that get censored by R1, covering a range of politically\nsensitive topics, but are not censored by other models. We then conduct a\ncomprehensive analysis of R1's censorship patterns, examining their\nconsistency, triggers, and variations across topics, prompt phrasing, and\ncontext. Beyond English-language queries, we explore censorship behavior in\nother languages. We also investigate the transferability of censorship to\nmodels distilled from the R1 language model. Finally, we propose techniques for\nbypassing or removing this censorship. Our findings reveal possible additional\ncensorship integration likely shaped by design choices during training or\nalignment, raising concerns about transparency, bias, and governance in\nlanguage model deployment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "o1"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12768", "pdf": "https://arxiv.org/pdf/2505.12768", "abs": "https://arxiv.org/abs/2505.12768", "authors": ["Yaxun Dai", "Wenxuan Xie", "Xialie Zhuang", "Tianyu Yang", "Yiying Yang", "Haiqin Yang", "Yuhang Zhao", "Pingfu Chao", "Wenhao Jiang"], "title": "ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL", "categories": ["cs.CL"], "comment": null, "summary": "In Text-to-SQL, execution feedback is essential for guiding large language\nmodels (LLMs) to reason accurately and generate reliable SQL queries. However,\nexisting methods treat execution feedback solely as a post-hoc signal for\ncorrection or selection, failing to integrate it into the generation process.\nThis limitation hinders their ability to address reasoning errors as they\noccur, ultimately reducing query accuracy and robustness. To address this\nissue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement\nLearning), a framework for Text-to-SQL that enables models to interact with the\ndatabase during decoding and dynamically adjust their reasoning based on\nexecution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm\nthat interleaves intermediate SQL execution into reasoning paths, facilitating\ncontext-sensitive revisions. It achieves this through structured prompts with\nmarkup tags and a stepwise rollout strategy that integrates execution feedback\ninto each stage of generation. To supervise policy learning, we develop a\ncomposite reward function that includes an exploration reward, explicitly\nencouraging effective database interaction. Additionally, ReEx-SQL adopts a\ntree-based decoding strategy to support exploratory reasoning, enabling dynamic\nexpansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on\nSpider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning\nbaseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving\n85.2% on Spider-Realistic with leading performance. In addition, its\ntree-structured decoding improves efficiency and performance over linear\ndecoding, reducing inference time by 51.9% on the BIRD development set.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12864", "pdf": "https://arxiv.org/pdf/2505.12864", "abs": "https://arxiv.org/abs/2505.12864", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstr√ºwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "comment": null, "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12912", "pdf": "https://arxiv.org/pdf/2505.12912", "abs": "https://arxiv.org/abs/2505.12912", "authors": ["Kazuki Adachi", "Shin'ya Yamaguchi", "Tomoki Hamagami"], "title": "Uniformity First: Uniformity-aware Test-time Adaptation of Vision-language Models against Image Corruption", "categories": ["cs.CV"], "comment": "Code is available at https://github.com/kzkadc/uninfo", "summary": "Pre-trained vision-language models such as contrastive language-image\npre-training (CLIP) have demonstrated a remarkable generalizability, which has\nenabled a wide range of applications represented by zero-shot classification.\nHowever, vision-language models still suffer when they face datasets with large\ngaps from training ones, i.e., distribution shifts. We found that CLIP is\nespecially vulnerable to sensor degradation, a type of realistic distribution\nshift caused by sensor conditions such as weather, light, or noise. Collecting\na new dataset from a test distribution for fine-tuning highly costs since\nsensor degradation occurs unexpectedly and has a range of variety. Thus, we\ninvestigate test-time adaptation (TTA) of zero-shot classification, which\nenables on-the-fly adaptation to the test distribution with unlabeled test\ndata. Existing TTA methods for CLIP mainly focus on modifying image and text\nembeddings or predictions to address distribution shifts. Although these\nmethods can adapt to domain shifts, such as fine-grained labels spaces or\ndifferent renditions in input images, they fail to adapt to distribution shifts\ncaused by sensor degradation. We found that this is because image embeddings\nare \"corrupted\" in terms of uniformity, a measure related to the amount of\ninformation. To make models robust to sensor degradation, we propose a novel\nmethod called uniformity-aware information-balanced TTA (UnInfo). To address\nthe corruption of image embeddings, we introduce uniformity-aware confidence\nmaximization, information-aware loss balancing, and knowledge distillation from\nthe exponential moving average (EMA) teacher. Through experiments, we\ndemonstrate that our UnInfo improves accuracy under sensor degradation by\nretaining information in terms of uniformity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13233", "pdf": "https://arxiv.org/pdf/2505.13233", "abs": "https://arxiv.org/abs/2505.13233", "authors": ["Lincan Cai", "Jingxuan Kang", "Shuang Li", "Wenxuan Ma", "Binhui Xie", "Zhida Qin", "Jian Liang"], "title": "From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection", "categories": ["cs.CV"], "comment": null, "summary": "Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive\nzero-shot capabilities on downstream tasks. Prior research highlights the\ncrucial role of visual augmentation techniques, like random cropping, in\nalignment with fine-grained class descriptions generated by large language\nmodels (LLMs), significantly enhancing zero-shot performance by incorporating\nmulti-view information. However, the inherent randomness of these augmentations\ncan inevitably introduce background artifacts and cause models to overly focus\non local details, compromising global semantic understanding. To address these\nissues, we propose an \\textbf{A}ttention-\\textbf{B}ased \\textbf{S}election\n(\\textbf{ABS}) method from local details to global context, which applies\nattention-guided cropping in both raw images and feature space, supplement\nglobal semantic information through strategic feature selection. Additionally,\nwe introduce a soft matching technique to effectively filter LLM descriptions\nfor better alignment. \\textbf{ABS} achieves state-of-the-art performance on\nout-of-distribution generalization and zero-shot classification tasks. Notably,\n\\textbf{ABS} is training-free and even rivals few-shot and test-time adaptation\nmethods. Our code is available at\n\\href{https://github.com/BIT-DA/ABS}{\\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11875", "pdf": "https://arxiv.org/pdf/2505.11875", "abs": "https://arxiv.org/abs/2505.11875", "authors": ["Chi-Min Chan", "Chunpu Xu", "Jiaming Ji", "Zhen Ye", "Pengcheng Wen", "Chunyang Jiang", "Yaodong Yang", "Wei Xue", "Sirui Han", "Yike Guo"], "title": "J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge", "categories": ["cs.LG", "cs.CL"], "comment": "33 pages, 27 figures", "summary": "The current focus of AI research is shifting from emphasizing model training\ntowards enhancing evaluation quality, a transition that is crucial for driving\nfurther advancements in AI systems. Traditional evaluation methods typically\nrely on reward models assigning scalar preference scores to outputs. Although\neffective, such approaches lack interpretability, leaving users often uncertain\nabout why a reward model rates a particular response as high or low. The advent\nof LLM-as-a-Judge provides a more scalable and interpretable method of\nsupervision, offering insights into the decision-making process. Moreover, with\nthe emergence of large reasoning models, which consume more tokens for deeper\nthinking and answer refinement, scaling test-time computation in the\nLLM-as-a-Judge paradigm presents an avenue for further boosting performance and\nproviding more interpretability through reasoning traces. In this paper, we\nintroduce $\\textbf{J1-7B}$, which is first supervised fine-tuned on\nreflection-enhanced datasets collected via rejection-sampling and subsequently\ntrained using Reinforcement Learning (RL) with verifiable rewards. At inference\ntime, we apply Simple Test-Time Scaling (STTS) strategies for additional\nperformance improvement. Experimental results demonstrate that $\\textbf{J1-7B}$\nsurpasses the previous state-of-the-art LLM-as-a-Judge by $ \\textbf{4.8}$\\% and\nexhibits a $ \\textbf{5.1}$\\% stronger scaling trend under STTS. Additionally,\nwe present three key findings: (1) Existing LLM-as-a-Judge does not inherently\nexhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced\ndatasets continues to demonstrate similarly weak scaling behavior. (3)\nSignificant scaling trend emerges primarily during the RL phase, suggesting\nthat effective STTS capability is acquired predominantly through RL training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning", "preference"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11883", "pdf": "https://arxiv.org/pdf/2505.11883", "abs": "https://arxiv.org/abs/2505.11883", "authors": ["Zihuan Qiu", "Yi Xu", "Chiyuan He", "Fanman Meng", "Linfeng Xu", "Qingbo Wu", "Hongliang Li"], "title": "MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Continual model merging integrates independently fine-tuned models\nsequentially without access to original training data, providing a scalable and\nefficient solution to continual learning. However, current methods still face\ncritical challenges, notably parameter interference among tasks and limited\nadaptability to evolving test distributions. The former causes catastrophic\nforgetting of integrated tasks, while the latter hinders effective adaptation\nto new tasks. To address these, we propose MINGLE, a novel framework for\ntest-time continual model merging, which leverages test-time adaptation using a\nsmall set of unlabeled test samples from the current task to dynamically guide\nthe merging process. MINGLE employs a mixture-of-experts architecture composed\nof parameter-efficient, low-rank experts, enabling efficient adaptation and\nimproving robustness to distribution shifts. To mitigate catastrophic\nforgetting, we propose Null-Space Constrained Gating, which restricts gating\nupdates to subspaces orthogonal to prior task representations. This suppresses\nactivations on old task inputs and preserves model behavior on past tasks. To\nfurther balance stability and adaptability, we design an Adaptive Relaxation\nStrategy, which dynamically adjusts the constraint strength based on\ninterference signals captured during test-time adaptation. Extensive\nexperiments on standard continual merging benchmarks demonstrate that MINGLE\nachieves robust generalization, reduces forgetting significantly, and\nconsistently surpasses previous state-of-the-art methods by 7-9\\% on average\nacross diverse task orders.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12307", "pdf": "https://arxiv.org/pdf/2505.12307", "abs": "https://arxiv.org/abs/2505.12307", "authors": ["Maoyuan Ye", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?", "categories": ["cs.CV", "cs.CL"], "comment": "GitHub: \\url{https://github.com/MiliLab/LogicOCR}", "summary": "Recent advances in Large Multimodal Models (LMMs) have significantly improved\ntheir reasoning and Optical Character Recognition (OCR) capabilities. However,\ntheir performance on complex logical reasoning tasks involving text-rich images\nremains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark\ncomprising 1,100 multiple-choice questions designed to evaluate LMMs' logical\nreasoning abilities on text-rich images, while minimizing reliance on\ndomain-specific knowledge (e.g., mathematics). We construct LogicOCR by\ncurating a text corpus from the Chinese National Civil Servant Examination and\ndevelop a scalable, automated pipeline to convert it into multimodal samples.\nFirst, we design prompt templates to steer GPT-Image-1 to generate images with\ndiverse backgrounds, interleaved text-illustration layouts, and varied fonts,\nensuring contextual relevance and visual realism. Then, the generated images\nare manually verified, with low-quality examples discarded. We evaluate a range\nof representative open-source and proprietary LMMs under both Chain-of-Thought\n(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key\ninsights, such as the impact of test-time scaling, input modality differences,\nand sensitivity to visual-text orientation. Notably, LMMs still lag in\nmultimodal reasoning compared to text-only inputs, indicating that they have\nnot fully bridged visual reading with reasoning. We hope LogicOCR will serve as\na valuable resource for advancing multimodal reasoning research. The dataset is\navailable at https://github.com/MiliLab/LogicOCR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "multi-dimensional"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12891", "pdf": "https://arxiv.org/pdf/2505.12891", "abs": "https://arxiv.org/abs/2505.12891", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "categories": ["cs.AI", "cs.CL"], "comment": "First version. There are still some examples to be added into the\n  appendix", "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12992", "pdf": "https://arxiv.org/pdf/2505.12992", "abs": "https://arxiv.org/abs/2505.12992", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "title": "Fractured Chain-of-Thought Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227", "abs": "https://arxiv.org/abs/2505.13227", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227", "abs": "https://arxiv.org/abs/2505.13227", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11521", "pdf": "https://arxiv.org/pdf/2505.11521", "abs": "https://arxiv.org/abs/2505.11521", "authors": ["Wang Fang", "Shirin Rahimi", "Olivia Bennett", "Sophie Carter", "Mitra Hassani", "Xu Lan", "Omid Javadi", "Lucas Mitchell"], "title": "Improving Open-Set Semantic Segmentation in 3D Point Clouds by Conditional Channel Capacity Maximization: Preliminary Results", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Point-cloud semantic segmentation underpins a wide range of critical\napplications. Although recent deep architectures and large-scale datasets have\ndriven impressive closed-set performance, these models struggle to recognize or\nproperly segment objects outside their training classes. This gap has sparked\ninterest in Open-Set Semantic Segmentation (O3S), where models must both\ncorrectly label known categories and detect novel, unseen classes. In this\npaper, we propose a plug and play framework for O3S. By modeling the\nsegmentation pipeline as a conditional Markov chain, we derive a novel\nregularizer term dubbed Conditional Channel Capacity Maximization (3CM), that\nmaximizes the mutual information between features and predictions conditioned\non each class. When incorporated into standard loss functions, 3CM encourages\nthe encoder to retain richer, label-dependent features, thereby enhancing the\nnetwork's ability to distinguish and segment previously unseen categories.\nExperimental results demonstrate effectiveness of proposed method on detecting\nunseen objects. We further outline future directions for dynamic open-world\nadaptation and efficient information-theoretic estimation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11581", "pdf": "https://arxiv.org/pdf/2505.11581", "abs": "https://arxiv.org/abs/2505.11581", "authors": ["Akarsh Kumar", "Jeff Clune", "Joel Lehman", "Kenneth O. Stanley"], "title": "Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis", "categories": ["cs.CV", "cs.LG", "cs.NE"], "comment": "43 pages, 25 figures", "summary": "Much of the excitement in modern AI is driven by the observation that scaling\nup existing systems leads to better performance. But does better performance\nnecessarily imply better internal representations? While the representational\noptimist assumes it must, this position paper challenges that view. We compare\nneural networks evolved through an open-ended search process to networks\ntrained via conventional stochastic gradient descent (SGD) on the simple task\nof generating a single image. This minimal setup offers a unique advantage:\neach hidden neuron's full functional behavior can be easily visualized as an\nimage, thus revealing how the network's output behavior is internally\nconstructed neuron by neuron. The result is striking: while both networks\nproduce the same output behavior, their internal representations differ\ndramatically. The SGD-trained networks exhibit a form of disorganization that\nwe term fractured entangled representation (FER). Interestingly, the evolved\nnetworks largely lack FER, even approaching a unified factored representation\n(UFR). In large models, FER may be degrading core model capacities like\ngeneralization, creativity, and (continual) learning. Therefore, understanding\nand mitigating FER could be critical to the future of representation learning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11620", "pdf": "https://arxiv.org/pdf/2505.11620", "abs": "https://arxiv.org/abs/2505.11620", "authors": ["Aaron Wilhelm", "Nils Napp"], "title": "Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to ICRA 2025", "summary": "Ground texture localization using a downward-facing camera offers a low-cost,\nhigh-precision localization solution that is robust to dynamic environments and\nrequires no environmental modification. We present a significantly improved\nbag-of-words (BoW) image retrieval system for ground texture localization,\nachieving substantially higher accuracy for global localization and higher\nprecision and recall for loop closure detection in SLAM. Our approach leverages\nan approximate $k$-means (AKM) vocabulary with soft assignment, and exploits\nthe consistent orientation and constant scale constraints inherent to ground\ntexture localization. Identifying the different needs of global localization\nvs. loop closure detection for SLAM, we present both high-accuracy and\nhigh-speed versions of our algorithm. We test the effect of each of our\nproposed improvements through an ablation study and demonstrate our method's\neffectiveness for both global localization and loop closure detection. With\nnumerous ground texture localization systems already using BoW, our method can\nreadily replace other generic BoW systems in their pipeline and immediately\nimprove their results.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12373", "pdf": "https://arxiv.org/pdf/2505.12373", "abs": "https://arxiv.org/abs/2505.12373", "authors": ["Kapil Dev"], "title": "Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 8 figures, submitted to IEEE Transactions on Visualization\n  and Computer Graphics (TVCG)", "summary": "Human aesthetic preferences for 3D shapes are central to industrial design,\nvirtual reality, and consumer product development. However, most computational\nmodels of 3D aesthetics lack empirical grounding in large-scale human\njudgments, limiting their practical relevance. We present a large-scale study\nof human preferences. We collected 22,301 pairwise comparisons across five\nobject categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon\nMechanical Turk. Building on a previously published\ndataset~\\cite{dev2020learning}, we introduce new non-linear modeling and\ncross-category analysis to uncover the geometric drivers of aesthetic\npreference. We apply the Bradley-Terry model to infer latent aesthetic scores\nand use Random Forests with SHAP analysis to identify and interpret the most\ninfluential geometric features (e.g., symmetry, curvature, compactness). Our\ncross-category analysis reveals both universal principles and domain-specific\ntrends in aesthetic preferences. We focus on human interpretable geometric\nfeatures to ensure model transparency and actionable design insights, rather\nthan relying on black-box deep learning approaches. Our findings bridge\ncomputational aesthetics and cognitive science, providing practical guidance\nfor designers and a publicly available dataset to support reproducibility. This\nwork advances the understanding of 3D shape aesthetics through a human-centric,\ndata-driven framework.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "comparison", "pairwise"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11709", "pdf": "https://arxiv.org/pdf/2505.11709", "abs": "https://arxiv.org/abs/2505.11709", "authors": ["Ryan Hoque", "Peide Huang", "David J. Yoon", "Mouli Sivapurapu", "Jian Zhang"], "title": "EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Imitation learning for manipulation has a well-known data scarcity problem.\nUnlike natural language and 2D computer vision, there is no Internet-scale\ncorpus of data for dexterous manipulation. One appealing option is egocentric\nhuman video, a passively scalable data source. However, existing large-scale\ndatasets such as Ego4D do not have native hand pose annotations and do not\nfocus on object manipulation. To this end, we use Apple Vision Pro to collect\nEgoDex: the largest and most diverse dataset of dexterous human manipulation to\ndate. EgoDex has 829 hours of egocentric video with paired 3D hand and finger\ntracking data collected at the time of recording, where multiple calibrated\ncameras and on-device SLAM can be used to precisely track the pose of every\njoint of each hand. The dataset covers a wide range of diverse manipulation\nbehaviors with everyday household objects in 194 different tabletop tasks\nranging from tying shoelaces to folding laundry. Furthermore, we train and\nsystematically evaluate imitation learning policies for hand trajectory\nprediction on the dataset, introducing metrics and benchmarks for measuring\nprogress in this increasingly important area. By releasing this large-scale\ndataset, we hope to push the frontier of robotics, computer vision, and\nfoundation models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11720", "pdf": "https://arxiv.org/pdf/2505.11720", "abs": "https://arxiv.org/abs/2505.11720", "authors": ["Shijun Liang", "Ismail R. Alkhouri", "Siddhant Gautam", "Qing Qu", "Saiprasad Ravishankar"], "title": "UGoDIT: Unsupervised Group Deep Image Prior Via Transferable Weights", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Recent advances in data-centric deep generative models have led to\nsignificant progress in solving inverse imaging problems. However, these models\n(e.g., diffusion models (DMs)) typically require large amounts of fully sampled\n(clean) training data, which is often impractical in medical and scientific\nsettings such as dynamic imaging.\n  On the other hand, training-data-free approaches like the Deep Image Prior\n(DIP) do not require clean ground-truth images but suffer from noise\noverfitting and can be computationally expensive as the network parameters need\nto be optimized for each measurement set independently. Moreover, DIP-based\nmethods often overlook the potential of learning a prior using a small number\nof sub-sampled measurements (or degraded images) available during training. In\nthis paper, we propose UGoDIT, an Unsupervised Group DIP via Transferable\nweights, designed for the low-data regime where only a very small number, M, of\nsub-sampled measurement vectors are available during training. Our method\nlearns a set of transferable weights by optimizing a shared encoder and M\ndisentangled decoders. At test time, we reconstruct the unseen degraded image\nusing a DIP network, where part of the parameters are fixed to the learned\nweights, while the remaining are optimized to enforce measurement consistency.\nWe evaluate UGoDIT on both medical (multi-coil MRI) and natural (super\nresolution and non-linear deblurring) image recovery tasks under various\nsettings. Compared to recent standalone DIP methods, UGoDIT provides\naccelerated convergence and notable improvement in reconstruction quality.\nFurthermore, our method achieves performance competitive with SOTA DM-based and\nsupervised approaches, despite not requiring large amounts of clean training\ndata.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12734", "pdf": "https://arxiv.org/pdf/2505.12734", "abs": "https://arxiv.org/abs/2505.12734", "authors": ["Junbo Wang", "Haofeng Tan", "Bowen Liao", "Albert Jiang", "Teng Fei", "Qixing Huang", "Zhengzhong Tu", "Shan Ye", "Yuhao Kang"], "title": "SounDiT: Geo-Contextual Soundscape-to-Landscape Generation", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.HC", "eess.AS"], "comment": "14 pages, 5 figures", "summary": "We present a novel and practically significant problem-Geo-Contextual\nSoundscape-to-Landscape (GeoS2L) generation-which aims to synthesize\ngeographically realistic landscape images from environmental soundscapes. Prior\naudio-to-image generation methods typically rely on general-purpose datasets\nand overlook geographic and environmental contexts, resulting in unrealistic\nimages that are misaligned with real-world environmental settings. To address\nthis limitation, we introduce a novel geo-contextual computational framework\nthat explicitly integrates geographic knowledge into multimodal generative\nmodeling. We construct two large-scale geo-contextual multimodal datasets,\nSoundingSVI and SonicUrban, pairing diverse soundscapes with real-world\nlandscape images. We propose SounDiT, a novel Diffusion Transformer (DiT)-based\nmodel that incorporates geo-contextual scene conditioning to synthesize\ngeographically coherent landscape images. Furthermore, we propose a\npractically-informed geo-contextual evaluation framework, the Place Similarity\nScore (PSS), across element-, scene-, and human perception-levels to measure\nconsistency between input soundscapes and generated landscape images. Extensive\nexperiments demonstrate that SounDiT outperforms existing baselines in both\nvisual fidelity and geographic settings. Our work not only establishes\nfoundational benchmarks for GeoS2L generation but also highlights the\nimportance of incorporating geographic domain knowledge in advancing multimodal\ngenerative models, opening new directions at the intersection of generative AI,\ngeography, urban planning, and environmental sciences.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11804", "pdf": "https://arxiv.org/pdf/2505.11804", "abs": "https://arxiv.org/abs/2505.11804", "authors": ["Xi Wang", "Eric Nalisnick"], "title": "Are vision language models robust to uncertain inputs?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Robustness against uncertain and ambiguous inputs is a critical challenge for\ndeep learning models. While recent advancements in large scale vision language\nmodels (VLMs, e.g. GPT4o) might suggest that increasing model and training\ndataset size would mitigate this issue, our empirical evaluation shows a more\ncomplicated picture. Testing models using two classic uncertainty\nquantification tasks, anomaly detection and classification under inherently\nambiguous conditions, we find that newer and larger VLMs indeed exhibit\nimproved robustness compared to earlier models, but still suffer from a\ntendency to strictly follow instructions, often causing them to hallucinate\nconfident responses even when faced with unclear or anomalous inputs.\nRemarkably, for natural images such as ImageNet, this limitation can be\novercome without pipeline modifications: simply prompting models to abstain\nfrom uncertain predictions enables significant reliability gains, achieving\nnear-perfect robustness in several settings. However, for domain-specific tasks\nsuch as galaxy morphology classification, a lack of specialized knowledge\nprevents reliable uncertainty estimation. Finally, we propose a novel mechanism\nbased on caption diversity to reveal a model's internal uncertainty, enabling\npractitioners to predict when models will successfully abstain without relying\non labeled data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "reliability"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11739", "pdf": "https://arxiv.org/pdf/2505.11739", "abs": "https://arxiv.org/abs/2505.11739", "authors": ["Feijiang Han", "Xiaodong Yu", "Jianheng Tang", "Lyle Ungar"], "title": "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, training-free methods for improving large language models (LLMs)\nhave attracted growing interest, with token-level attention tuning emerging as\na promising and interpretable direction. However, existing methods typically\nrely on auxiliary mechanisms to identify important or irrelevant task-specific\ntokens, introducing potential bias and limiting applicability. In this paper,\nwe uncover a surprising and elegant alternative: the semantically empty initial\ntoken is a powerful and underexplored control point for optimizing model\nbehavior. Through theoretical analysis, we show that tuning the initial token's\nattention sharpens or flattens the attention distribution over subsequent\ntokens, and its role as an attention sink amplifies this effect. Empirically,\nwe find that: (1) tuning its attention improves LLM performance more\neffectively than tuning other task-specific tokens; (2) the effect follows a\nconsistent trend across layers, with earlier layers having greater impact, but\nvaries across attention heads, with different heads showing distinct\npreferences in how they attend to this token. Based on these findings, we\npropose ZeroTuning, a training-free approach that improves LLM performance by\napplying head-specific attention adjustments to this special token. Despite\ntuning only one token, ZeroTuning achieves higher performance on text\nclassification, multiple-choice, and multi-turn conversation tasks across\nmodels such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves\nLlama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its\nmulti-turn score from 7.804 to 7.966. The method is also robust to limited\nresources, few-shot settings, long contexts, quantization, decoding strategies,\nand prompt variations. Our work sheds light on a previously overlooked control\npoint in LLMs, offering new insights into both inference-time tuning and model\ninterpretability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11838", "pdf": "https://arxiv.org/pdf/2505.11838", "abs": "https://arxiv.org/abs/2505.11838", "authors": ["Yiqing Shen", "Chenjia Li", "Chenxiao Fan", "Mathias Unberath"], "title": "RVTBench: A Benchmark for Visual Reasoning Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Visual reasoning, the capability to interpret visual input in response to\nimplicit text query through multi-step reasoning, remains a challenge for deep\nlearning models due to the lack of relevant benchmarks. Previous work in visual\nreasoning has primarily focused on reasoning segmentation, where models aim to\nsegment objects based on implicit text queries. This paper introduces reasoning\nvisual tasks (RVTs), a unified formulation that extends beyond traditional\nvideo reasoning segmentation to a diverse family of visual language reasoning\nproblems, which can therefore accommodate multiple output formats including\nbounding boxes, natural language descriptions, and question-answer pairs.\nCorrespondingly, we identify the limitations in current benchmark construction\nmethods that rely solely on large language models (LLMs), which inadequately\ncapture complex spatial-temporal relationships and multi-step reasoning chains\nin video due to their reliance on token representation, resulting in benchmarks\nwith artificially limited reasoning complexity. To address this limitation, we\npropose a novel automated RVT benchmark construction pipeline that leverages\ndigital twin (DT) representations as structured intermediaries between\nperception and the generation of implicit text queries. Based on this method,\nwe construct RVTBench, a RVT benchmark containing 3,896 queries of over 1.2\nmillion tokens across four types of RVT (segmentation, grounding, VQA and\nsummary), three reasoning categories (semantic, spatial, and temporal), and\nfour increasing difficulty levels, derived from 200 video sequences. Finally,\nwe propose RVTagent, an agent framework for RVT that allows for zero-shot\ngeneralization across various types of RVT without task-specific fine-tuning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11852", "pdf": "https://arxiv.org/pdf/2505.11852", "abs": "https://arxiv.org/abs/2505.11852", "authors": ["Jingkun Yue", "Siqi Zhang", "Zinan Jia", "Huihuan Xu", "Zongbo Han", "Xiaohong Liu", "Guangyu Wang"], "title": "MedSG-Bench: A Benchmark for Medical Image Sequences Grounding", "categories": ["cs.CV"], "comment": null, "summary": "Visual grounding is essential for precise perception and reasoning in\nmultimodal large language models (MLLMs), especially in medical imaging\ndomains. While existing medical visual grounding benchmarks primarily focus on\nsingle-image scenarios, real-world clinical applications often involve\nsequential images, where accurate lesion localization across different\nmodalities and temporal tracking of disease progression (e.g., pre- vs.\npost-treatment comparison) require fine-grained cross-image semantic alignment\nand context-aware reasoning. To remedy the underrepresentation of image\nsequences in existing medical visual grounding benchmarks, we propose\nMedSG-Bench, the first benchmark tailored for Medical Image Sequences\nGrounding. It comprises eight VQA-style tasks, formulated into two paradigms of\nthe grounding tasks, including 1) Image Difference Grounding, which focuses on\ndetecting change regions across images, and 2) Image Consistency Grounding,\nwhich emphasizes detection of consistent or shared semantics across sequential\nimages. MedSG-Bench covers 76 public datasets, 10 medical imaging modalities,\nand a wide spectrum of anatomical structures and diseases, totaling 9,630\nquestion-answer pairs. We benchmark both general-purpose MLLMs (e.g.,\nQwen2.5-VL) and medical-domain specialized MLLMs (e.g., HuatuoGPT-vision),\nobserving that even the advanced models exhibit substantial limitations in\nmedical sequential grounding tasks. To advance this field, we construct\nMedSG-188K, a large-scale instruction-tuning dataset tailored for sequential\nvisual grounding, and further develop MedSeq-Grounder, an MLLM designed to\nfacilitate future research on fine-grained understanding across medical\nsequential images. The benchmark, dataset, and model are available at\nhttps://huggingface.co/MedSG-Bench", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency", "fine-grained"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11876", "pdf": "https://arxiv.org/pdf/2505.11876", "abs": "https://arxiv.org/abs/2505.11876", "authors": ["Yanbo Dai", "Zhenlan Ji", "Zongjie Li", "Shuai Wang"], "title": "NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Model editing techniques are essential for efficiently updating knowledge in\nlarge language models (LLMs). However, the effectiveness of existing approaches\ndegrades in massive editing scenarios, particularly when evaluated with\npractical metrics or in context-rich settings. We attribute these failures to\nembedding collisions among knowledge items, which undermine editing reliability\nat scale. To address this, we propose NAMET (Noise-aware Model Editing in\nTransformers), a simple yet effective method that introduces noise during\nmemory extraction via a one-line modification to MEMIT. Extensive experiments\nacross six LLMs and three datasets demonstrate that NAMET consistently\noutperforms existing methods when editing thousands of facts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11884", "pdf": "https://arxiv.org/pdf/2505.11884", "abs": "https://arxiv.org/abs/2505.11884", "authors": ["Zhongwen Li", "Zongwei Li", "Xiaoqi Li"], "title": "Facial Recognition Leveraging Generative Adversarial Networks", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Face recognition performance based on deep learning heavily relies on\nlarge-scale training data, which is often difficult to acquire in practical\napplications. To address this challenge, this paper proposes a GAN-based data\naugmentation method with three key contributions: (1) a residual-embedded\ngenerator to alleviate gradient vanishing/exploding problems, (2) an Inception\nResNet-V1 based FaceNet discriminator for improved adversarial training, and\n(3) an end-to-end framework that jointly optimizes data generation and\nrecognition performance. Experimental results demonstrate that our approach\nachieves stable training dynamics and significantly improves face recognition\naccuracy by 12.7% on the LFW benchmark compared to baseline methods, while\nmaintaining good generalization capability with limited training samples.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11924", "pdf": "https://arxiv.org/pdf/2505.11924", "abs": "https://arxiv.org/abs/2505.11924", "authors": ["Yu-Ting Lee", "Hui-Ying Shih", "Fu-Chieh Chang", "Pei-Yuan Wu"], "title": "An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We provide an explanation for the performance gains of intrinsic\nself-correction, a process where a language model iteratively refines its\noutputs without external feedback. More precisely, we investigate how prompting\ninduces interpretable changes in hidden states and thus affects the output\ndistributions. We hypothesize that each prompt-induced shift lies in a linear\nspan of some linear representation vectors, naturally separating tokens based\non individual concept alignment. Building around this idea, we give a\nmathematical formulation of self-correction and derive a concentration result\nfor output tokens based on alignment magnitudes. Our experiments on text\ndetoxification with zephyr-7b-sft reveal a substantial gap in the inner\nproducts of the prompt-induced shifts and the unembeddings of the top-100 most\ntoxic tokens vs. those of the unembeddings of the bottom-100 least toxic\ntokens, under toxic instructions. This suggests that self-correction prompts\nenhance a language model's capability of latent concept recognition. Our\nanalysis offers insights into the underlying mechanism of self-correction by\ncharacterizing how prompting works explainably. For reproducibility, our code\nis available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11935", "pdf": "https://arxiv.org/pdf/2505.11935", "abs": "https://arxiv.org/abs/2505.11935", "authors": ["Xuanle Zhao", "Xuexin Liu", "Haoyue Yang", "Xianzhen Luo", "Fanhu Zeng", "Jianling Li", "Qi Shi", "Chi Chen"], "title": "ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing", "categories": ["cs.CL"], "comment": "Accept by ACL2025 Findings, preprint version", "summary": "Although multimodal large language models (MLLMs) show promise in generating\nchart rendering code, chart editing presents a greater challenge. This\ndifficulty stems from its nature as a labor-intensive task for humans that also\ndemands MLLMs to integrate chart understanding, complex reasoning, and precise\nintent interpretation. While many MLLMs claim such editing capabilities,\ncurrent assessments typically rely on limited case studies rather than robust\nevaluation methodologies, highlighting the urgent need for a comprehensive\nevaluation framework. In this work, we propose ChartEdit, a new high-quality\nbenchmark designed for chart editing tasks. This benchmark comprises $1,405$\ndiverse editing instructions applied to $233$ real-world charts, with each\ninstruction-chart instance having been manually annotated and validated for\naccuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream\nMLLMs across two types of experiments, assessing them at both the code and\nchart levels. The results suggest that large-scale models can generate code to\nproduce images that partially match the reference images. However, their\nability to generate accurate edits according to the instructions remains\nlimited. The state-of-the-art (SOTA) model achieves a score of only $59.96$,\nhighlighting significant challenges in precise modification. In contrast,\nsmall-scale models, including chart-domain models, struggle both with following\nediting instructions and generating overall chart images, underscoring the need\nfor further development in this area. Code is available at\nhttps://github.com/xxlllz/ChartEdit.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11980", "pdf": "https://arxiv.org/pdf/2505.11980", "abs": "https://arxiv.org/abs/2505.11980", "authors": ["Yi Chen", "Mu-Young Son", "Chuanbo Hua", "Joo-Young Kim"], "title": "AoP-SAM: Automation of Prompts for Efficient Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at AAAI 2025", "summary": "The Segment Anything Model (SAM) is a powerful foundation model for image\nsegmentation, showing robust zero-shot generalization through prompt\nengineering. However, relying on manual prompts is impractical for real-world\napplications, particularly in scenarios where rapid prompt provision and\nresource efficiency are crucial. In this paper, we propose the Automation of\nPrompts for SAM (AoP-SAM), a novel approach that learns to generate essential\nprompts in optimal locations automatically. AoP-SAM enhances SAM's efficiency\nand usability by eliminating manual input, making it better suited for\nreal-world tasks. Our approach employs a lightweight yet efficient Prompt\nPredictor model that detects key entities across images and identifies the\noptimal regions for placing prompt candidates. This method leverages SAM's\nimage embeddings, preserving its zero-shot generalization capabilities without\nrequiring fine-tuning. Additionally, we introduce a test-time instance-level\nAdaptive Sampling and Filtering mechanism that generates prompts in a\ncoarse-to-fine manner. This notably enhances both prompt and mask generation\nefficiency by reducing computational overhead and minimizing redundant mask\nrefinements. Evaluations of three datasets demonstrate that AoP-SAM\nsubstantially improves both prompt generation efficiency and mask generation\naccuracy, making SAM more effective for automated segmentation tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11992", "pdf": "https://arxiv.org/pdf/2505.11992", "abs": "https://arxiv.org/abs/2505.11992", "authors": ["Songchun Zhang", "Huiyao Xu", "Sitong Guo", "Zhongwei Xie", "Pengwei Liu", "Hujun Bao", "Weiwei Xu", "Changqing Zou"], "title": "SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations", "categories": ["cs.CV"], "comment": "18 pages, 16 figures", "summary": "Novel view synthesis (NVS) boosts immersive experiences in computer vision\nand graphics. Existing techniques, though progressed, rely on dense multi-view\nobservations, restricting their application. This work takes on the challenge\nof reconstructing photorealistic 3D scenes from sparse or single-view inputs.\nWe introduce SpatialCrafter, a framework that leverages the rich knowledge in\nvideo diffusion models to generate plausible additional observations, thereby\nalleviating reconstruction ambiguity. Through a trainable camera encoder and an\nepipolar attention mechanism for explicit geometric constraints, we achieve\nprecise camera control and 3D consistency, further reinforced by a unified\nscale estimation strategy to handle scale discrepancies across datasets.\nFurthermore, by integrating monocular depth priors with semantic features in\nthe video latent space, our framework directly regresses 3D Gaussian primitives\nand efficiently processes long-sequence features using a hybrid network\nstructure. Extensive experiments show our method enhances sparse view\nreconstruction and restores the realistic appearance of 3D scenes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12082", "pdf": "https://arxiv.org/pdf/2505.12082", "abs": "https://arxiv.org/abs/2505.12082", "authors": ["Yunshui Li", "Yiyuan Ma", "Shen Yan", "Chaoyi Zhang", "Jing Liu", "Jianqiao Lu", "Ziwen Xu", "Mengzhao Chen", "Minrui Wang", "Shiyi Zhan", "Jin Ma", "Xunhao Lai", "Yao Luo", "Xingyan Bin", "Hongbin Ren", "Mingji Han", "Wenhao Hao", "Bairen Yi", "LingJun Liu", "Bole Ma", "Xiaoying Jia", "Zhou Xun", "Liang Xiang", "Yonghui Wu"], "title": "Model Merging in Pre-training of Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Model merging has emerged as a promising technique for enhancing large\nlanguage models, though its application in large-scale pre-training remains\nrelatively unexplored. In this paper, we present a comprehensive investigation\nof model merging techniques during the pre-training process. Through extensive\nexperiments with both dense and Mixture-of-Experts (MoE) architectures ranging\nfrom millions to over 100 billion parameters, we demonstrate that merging\ncheckpoints trained with constant learning rates not only achieves significant\nperformance improvements but also enables accurate prediction of annealing\nbehavior. These improvements lead to both more efficient model development and\nsignificantly lower training costs. Our detailed ablation studies on merging\nstrategies and hyperparameters provide new insights into the underlying\nmechanisms while uncovering novel applications. Through comprehensive\nexperimental analysis, we offer the open-source community practical\npre-training guidelines for effective model merging.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12053", "pdf": "https://arxiv.org/pdf/2505.12053", "abs": "https://arxiv.org/abs/2505.12053", "authors": ["Tianxiong Zhong", "Xingye Tian", "Boyuan Jiang", "Xuebo Wang", "Xin Tao", "Pengfei Wan", "Zhiwei Zhang"], "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 10 figures", "summary": "Modern video generation frameworks based on Latent Diffusion Models suffer\nfrom inefficiencies in tokenization due to the Frame-Proportional Information\nAssumption. Existing tokenizers provide fixed temporal compression rates,\ncausing the computational cost of the diffusion model to scale linearly with\nthe frame rate. The paper proposes the Duration-Proportional Information\nAssumption: the upper bound on the information capacity of a video is\nproportional to the duration rather than the number of frames. Based on this\ninsight, the paper introduces VFRTok, a Transformer-based video tokenizer, that\nenables variable frame rate encoding and decoding through asymmetric frame rate\ntraining between the encoder and decoder. Furthermore, the paper proposes\nPartial Rotary Position Embeddings (RoPE) to decouple position and content\nmodeling, which groups correlated patches into unified tokens. The Partial RoPE\neffectively improves content-awareness, enhancing the video generation\ncapability. Benefiting from the compact and continuous spatio-temporal\nrepresentation, VFRTok achieves competitive reconstruction quality and\nstate-of-the-art generation fidelity while using only 1/8 tokens compared to\nexisting tokenizers.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12066", "pdf": "https://arxiv.org/pdf/2505.12066", "abs": "https://arxiv.org/abs/2505.12066", "authors": ["Yijie Zheng", "Jinxuan Yang", "Yu Chen", "Yaxuan Wang", "Yihang Lu", "Guoqing Li"], "title": "Beluga Whale Detection from Satellite Imagery with Point Labels", "categories": ["cs.CV"], "comment": "Accepted for oral presentation at IGARSS 2025. Session at\n  https://www.2025.ieeeigarss.org/view_paper.php?PaperNum=2430&SessionID=1426", "summary": "Very high-resolution (VHR) satellite imagery has emerged as a powerful tool\nfor monitoring marine animals on a large scale. However, existing deep\nlearning-based whale detection methods usually require manually created,\nhigh-quality bounding box annotations, which are labor-intensive to produce.\nMoreover, existing studies often exclude ``uncertain whales'', individuals that\nhave ambiguous appearances in satellite imagery, limiting the applicability of\nthese models in real-world scenarios. To address these limitations, this study\nintroduces an automated pipeline for detecting beluga whales and harp seals in\nVHR satellite imagery. The pipeline leverages point annotations and the Segment\nAnything Model (SAM) to generate precise bounding box annotations, which are\nused to train YOLOv8 for multiclass detection of certain whales, uncertain\nwhales, and harp seals. Experimental results demonstrated that SAM-generated\nannotations significantly improved detection performance, achieving higher\n$\\text{F}_\\text{1}$-scores compared to traditional buffer-based annotations.\nYOLOv8 trained on SAM-labeled boxes achieved an overall\n$\\text{F}_\\text{1}$-score of 72.2% for whales overall and 70.3% for harp seals,\nwith superior performance in dense scenes. The proposed approach not only\nreduces the manual effort required for annotation but also enhances the\ndetection of uncertain whales, offering a more comprehensive solution for\nmarine animal monitoring. This method holds great potential for extending to\nother species, habitats, and remote sensing platforms, as well as for\nestimating whale biometrics, thereby advancing ecological monitoring and\nconservation efforts. The codes for our label and detection pipeline are\npublicly available at http://github.com/voyagerxvoyagerx/beluga-seeker .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12116", "pdf": "https://arxiv.org/pdf/2505.12116", "abs": "https://arxiv.org/abs/2505.12116", "authors": ["Fitsum Gaim", "Hoyun Song", "Huije Lee", "Changgeon Ko", "Eui Jun Hwang", "Jong C. Park"], "title": "A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Content moderation research has recently made significant advances, but still\nfails to serve the majority of the world's languages due to the lack of\nresources, leaving millions of vulnerable users to online hostility. This work\npresents a large-scale human-annotated multi-task benchmark dataset for abusive\nlanguage detection in Tigrinya social media with joint annotations for three\ntasks: abusiveness, sentiment, and topic classification. The dataset comprises\n13,717 YouTube comments annotated by nine native speakers, collected from 7,373\nvideos with a total of over 1.2 billion views across 51 channels. We developed\nan iterative term clustering approach for effective data selection. Recognizing\nthat around 64% of Tigrinya social media content uses Romanized\ntransliterations rather than native Ge'ez script, our dataset accommodates both\nwriting systems to reflect actual language use. We establish strong baselines\nacross the tasks in the benchmark, while leaving significant challenges for\nfuture contributions. Our experiments reveal that small, specialized multi-task\nmodels outperform the current frontier models in the low-resource setting,\nachieving up to 86% accuracy (+7 points) in abusiveness detection. We make the\nresources publicly available to promote research on online safety.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety", "accuracy"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12160", "pdf": "https://arxiv.org/pdf/2505.12160", "abs": "https://arxiv.org/abs/2505.12160", "authors": ["Darmawan Wicaksono", "Hasri Akbar Awal Rozaq", "Nevfel Boz"], "title": "Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse", "categories": ["cs.CL"], "comment": null, "summary": "Social media platforms like X (formerly Twitter) play a crucial role in\nshaping public discourse and societal norms. This study examines the term\nSessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise\nof anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and\nthe TREMO dataset, we developed an advanced Emotion Recognition Model (ERM)\ntailored for Turkish, achieving 92.62% accuracy in categorizing emotions such\nas happiness, fear, anger, sadness, disgust, and surprise. By applying this\nmodel to large-scale X data, the study uncovers emotional nuances in Turkish\ndiscourse, contributing to computational social science by advancing sentiment\nanalysis in underrepresented languages and enhancing our understanding of\nglobal digital discourse and the unique linguistic challenges of Turkish. The\nfindings underscore the transformative potential of localized NLP tools, with\nour ERM model offering practical applications for real-time sentiment analysis\nin Turkish-language contexts. By addressing critical areas, including\nmarketing, public relations, and crisis management, these models facilitate\nimproved decision-making through timely and accurate sentiment tracking. This\nhighlights the significance of advancing research that accounts for regional\nand linguistic nuances.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12098", "pdf": "https://arxiv.org/pdf/2505.12098", "abs": "https://arxiv.org/abs/2505.12098", "authors": ["Jiarui Wang", "Huiyu Duan", "Ziheng Jia", "Yu Zhao", "Woo Yi Yang", "Zicheng Zhang", "Zijian Chen", "Juntong Wang", "Yuke Xing", "Guangtao Zhai", "Xiongkuo Min"], "title": "LOVE: Benchmarking and Evaluating Text-to-Video Generation and Video-to-Text Interpretation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in large multimodal models (LMMs) have driven substantial\nprogress in both text-to-video (T2V) generation and video-to-text (V2T)\ninterpretation tasks. However, current AI-generated videos (AIGVs) still\nexhibit limitations in terms of perceptual quality and text-video alignment.\nTherefore, a reliable and scalable automatic model for AIGV evaluation is\ndesirable, which heavily relies on the scale and quality of human annotations.\nTo this end, we present AIGVE-60K, a comprehensive dataset and benchmark for\nAI-Generated Video Evaluation, which features (i) comprehensive tasks,\nencompassing 3,050 extensive prompts across 20 fine-grained task dimensions,\n(ii) the largest human annotations, including 120K mean-opinion scores (MOSs)\nand 60K question-answering (QA) pairs annotated on 58,500 videos generated from\n30 T2V models, and (iii) bidirectional benchmarking and evaluating for both T2V\ngeneration and V2T interpretation capabilities. Based on AIGVE-60K, we propose\nLOVE, a LMM-based metric for AIGV Evaluation from multiple dimensions including\nperceptual preference, text-video correspondence, and task-specific accuracy in\nterms of both instance level and model level. Comprehensive experiments\ndemonstrate that LOVE not only achieves state-of-the-art performance on the\nAIGVE-60K dataset, but also generalizes effectively to a wide range of other\nAIGV evaluation benchmarks. These findings highlight the significance of the\nAIGVE-60K dataset. Database and codes are anonymously available at\nhttps://github.com/IntMeGroup/LOVE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12196", "pdf": "https://arxiv.org/pdf/2505.12196", "abs": "https://arxiv.org/abs/2505.12196", "authors": ["Yi-Chien Lin", "Hongao Zhu", "William Schuler"], "title": "Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled", "categories": ["cs.CL"], "comment": null, "summary": "The impressive linguistic abilities of large language models (LLMs) have\nrecommended them as models of human sentence processing, with some conjecturing\na positive 'quality-power' relationship (Wilcox et al., 2023), in which\nlanguage models' (LMs') fit to psychometric data continues to improve as their\nability to predict words in context increases. This is important because it\nsuggests that elements of LLM architecture, such as veridical attention to\ncontext and a unique objective of predicting upcoming words, reflect the\narchitecture of the human sentence processing faculty, and that any\ninadequacies in predicting human reading time and brain imaging data may be\nattributed to insufficient model complexity, which recedes as larger models\nbecome available. Recent studies (Oh and Schuler, 2023) have shown this scaling\ninverts after a point, as LMs become excessively large and accurate, when word\nprediction probability (as information-theoretic surprisal) is used as a\npredictor. Other studies propose the use of entire vectors from differently\nsized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting\ndoubt on the value of surprisal as a predictor, but do not control for the\nlarger number of predictors in vectors from larger LMs. This study evaluates\nLLM scaling using entire LLM vectors, while controlling for the larger number\nof predictors in vectors from larger LLMs. Results show that inverse scaling\nobtains, suggesting that inadequacies in predicting human reading time and\nbrain imaging data may be due to substantial misalignment between LLMs and\nhuman sentence processing, which worsens as larger models are used.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12201", "pdf": "https://arxiv.org/pdf/2505.12201", "abs": "https://arxiv.org/abs/2505.12201", "authors": ["Xiyan Fu", "Wei Liu"], "title": "How Reliable is Multilingual LLM-as-a-Judge?", "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced\nlarge language models assess generation results in alignment with human\ninstructions. While these models serve as a promising alternative to human\nannotators, their reliability in multilingual evaluation remains uncertain. To\nbridge this gap, we conduct a comprehensive analysis of multilingual\nLLM-as-a-Judge. Specifically, we evaluate five models from different model\nfamilies across five diverse tasks involving 25 languages. Our findings reveal\nthat LLMs struggle to achieve consistent judgment results across languages,\nwith an average Fleiss' Kappa of approximately 0.3, and some models performing\neven worse. To investigate the cause of inconsistency, we analyze various\ninfluencing factors. We observe that consistency varies significantly across\nlanguages, with particularly poor performance in low-resource languages.\nAdditionally, we find that neither training on multilingual data nor increasing\nmodel scale directly improves judgment consistency. These findings suggest that\nLLMs are not yet reliable for evaluating multilingual predictions. We finally\npropose an ensemble strategy which improves the consistency of the multilingual\njudge in real-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "reliability", "kappa"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12218", "pdf": "https://arxiv.org/pdf/2505.12218", "abs": "https://arxiv.org/abs/2505.12218", "authors": ["Tong Bao", "Yi Zhao", "Jin Mao", "Chengzhi Zhang"], "title": "Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, have prompted academic\nconcerns about their impact on academic writing. Existing studies have\nprimarily examined LLM usage in academic writing through quantitative\napproaches, such as word frequency statistics and probability-based analyses.\nHowever, few have systematically examined the potential impact of LLMs on the\nlinguistic characteristics of academic writing. To address this gap, we\nconducted a large-scale analysis across 823,798 abstracts published in last\ndecade from arXiv dataset. Through the linguistic analysis of features such as\nthe frequency of LLM-preferred words, lexical complexity, syntactic complexity,\ncohesion, readability and sentiment, the results indicate a significant\nincrease in the proportion of LLM-preferred words in abstracts, revealing the\nwidespread influence of LLMs on academic writing. Additionally, we observed an\nincrease in lexical complexity and sentiment in the abstracts, but a decrease\nin syntactic complexity, suggesting that LLMs introduce more new vocabulary and\nsimplify sentence structure. However, the significant decrease in cohesion and\nreadability indicates that abstracts have fewer connecting words and are\nbecoming more difficult to read. Moreover, our analysis reveals that scholars\nwith weaker English proficiency were more likely to use the LLMs for academic\nwriting, and focused on improving the overall logic and fluency of the\nabstracts. Finally, at discipline level, we found that scholars in Computer\nScience showed more pronounced changes in writing style, while the changes in\nMathematics were minimal.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12200", "pdf": "https://arxiv.org/pdf/2505.12200", "abs": "https://arxiv.org/abs/2505.12200", "authors": ["Bohan Jia", "Wenxuan Huang", "Yuntian Tang", "Junbo Qiao", "Jincheng Liao", "Shaosheng Cao", "Fei Zhao", "Zhaopeng Feng", "Zhouhong Gu", "Zhenfei Yin", "Lei Bai", "Wanli Ouyang", "Lin Chen", "Fei Zhao", "Zihan Wang", "Yuan Xie", "Shaohui Lin"], "title": "CompBench: Benchmarking Complex Instruction-guided Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "While real-world applications increasingly demand intricate scene\nmanipulation, existing instruction-guided image editing benchmarks often\noversimplify task complexity and lack comprehensive, fine-grained instructions.\nTo bridge this gap, we introduce, a large-scale benchmark specifically designed\nfor complex instruction-guided image editing. CompBench features challenging\nediting scenarios that incorporate fine-grained instruction following, spatial\nand contextual reasoning, thereby enabling comprehensive evaluation of image\nediting models' precise manipulation capabilities. To construct CompBench, We\npropose an MLLM-human collaborative framework with tailored task pipelines.\nFurthermore, we propose an instruction decoupling strategy that disentangles\nediting intents into four key dimensions: location, appearance, dynamics, and\nobjects, ensuring closer alignment between instructions and complex editing\nrequirements. Extensive evaluations reveal that CompBench exposes fundamental\nlimitations of current image editing models and provides critical insights for\nthe development of next-generation instruction-guided image editing systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12238", "pdf": "https://arxiv.org/pdf/2505.12238", "abs": "https://arxiv.org/abs/2505.12238", "authors": ["Sriram Selvam", "Anneswa Ghosh"], "title": "PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The memorization of sensitive and personally identifiable information (PII)\nby large language models (LLMs) poses growing privacy risks as models scale and\nare increasingly deployed in real-world applications. Existing efforts to study\nsensitive and PII data memorization and develop mitigation strategies are\nhampered by the absence of comprehensive, realistic, and ethically sourced\ndatasets reflecting the diversity of sensitive information found on the web. We\nintroduce PANORAMA - Profile-based Assemblage for Naturalistic Online\nRepresentation and Attribute Memorization Analysis, a large-scale synthetic\ncorpus of 384,789 samples derived from 9,674 synthetic profiles designed to\nclosely emulate the distribution, variety, and context of PII and sensitive\ndata as it naturally occurs in online environments. Our data generation\npipeline begins with the construction of internally consistent, multi-attribute\nhuman profiles using constrained selection to reflect real-world demographics\nsuch as education, health attributes, financial status, etc. Using a\ncombination of zero-shot prompting and OpenAI o3-mini, we generate diverse\ncontent types - including wiki-style articles, social media posts, forum\ndiscussions, online reviews, comments, and marketplace listings - each\nembedding realistic, contextually appropriate PII and other sensitive\ninformation. We validate the utility of PANORAMA by fine-tuning the Mistral-7B\nmodel on 1x, 5x, 10x, and 25x data replication rates with a subset of data and\nmeasure PII memorization rates - revealing not only consistent increases with\nrepetition but also variation across content types, highlighting PANORAMA's\nability to model how memorization risks differ by context. Our dataset and code\nare publicly available, providing a much-needed resource for privacy risk\nassessment, model auditing, and the development of privacy-preserving LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12217", "pdf": "https://arxiv.org/pdf/2505.12217", "abs": "https://arxiv.org/abs/2505.12217", "authors": ["Aryan Das", "Tanishq Rachamalla", "Pravendra Singh", "Koushik Biswas", "Vinay Kumar Verma", "Swalpa Kumar Roy"], "title": "Hyperspectral Image Land Cover Captioning Dataset for Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce HyperCap, the first large-scale hyperspectral captioning dataset\ndesigned to enhance model performance and effectiveness in remote sensing\napplications. Unlike traditional hyperspectral imaging (HSI) datasets that\nfocus solely on classification tasks, HyperCap integrates spectral data with\npixel-wise textual annotations, enabling deeper semantic understanding of\nhyperspectral imagery. This dataset enhances model performance in tasks like\nclassification and feature extraction, providing a valuable resource for\nadvanced remote sensing applications. HyperCap is constructed from four\nbenchmark datasets and annotated through a hybrid approach combining automated\nand manual methods to ensure accuracy and consistency. Empirical evaluations\nusing state-of-the-art encoders and diverse fusion techniques demonstrate\nsignificant improvements in classification performance. These results\nunderscore the potential of vision-language learning in HSI and position\nHyperCap as a foundational dataset for future research in the field.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12235", "pdf": "https://arxiv.org/pdf/2505.12235", "abs": "https://arxiv.org/abs/2505.12235", "authors": ["Jia Li", "Nan Gao", "Huaibo Huang", "Ran He"], "title": "NOFT: Test-Time Noise Finetune via Information Bottleneck for Highly Correlated Asset Creation", "categories": ["cs.CV"], "comment": null, "summary": "The diffusion model has provided a strong tool for implementing text-to-image\n(T2I) and image-to-image (I2I) generation. Recently, topology and texture\ncontrol are popular explorations, e.g., ControlNet, IP-Adapter, Ctrl-X, and\nDSG. These methods explicitly consider high-fidelity controllable editing based\non external signals or diffusion feature manipulations. As for diversity, they\ndirectly choose different noise latents. However, the diffused noise is capable\nof implicitly representing the topological and textural manifold of the\ncorresponding image. Moreover, it's an effective workbench to conduct the\ntrade-off between content preservation and controllable variations. Previous\nT2I and I2I diffusion works do not explore the information within the\ncompressed contextual latent. In this paper, we first propose a plug-and-play\nnoise finetune NOFT module employed by Stable Diffusion to generate highly\ncorrelated and diverse images. We fine-tune seed noise or inverse noise through\nan optimal-transported (OT) information bottleneck (IB) with around only 14K\ntrainable parameters and 10 minutes of training. Our test-time NOFT is good at\nproducing high-fidelity image variations considering topology and texture\nalignments. Comprehensive experiments demonstrate that NOFT is a powerful\ngeneral reimagine approach to efficiently fine-tune the 2D/3D AIGC assets with\ntext or image guidance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12246", "pdf": "https://arxiv.org/pdf/2505.12246", "abs": "https://arxiv.org/abs/2505.12246", "authors": ["Muleilan Pei", "Jiayao Shan", "Peiliang Li", "Jieqi Shi", "Jing Huo", "Yang Gao", "Shaojie Shen"], "title": "SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted by IEEE Robotics and Automation Letters", "summary": "Online scene perception and topology reasoning are critical for autonomous\nvehicles to understand their driving environments, particularly for mapless\ndriving systems that endeavor to reduce reliance on costly High-Definition (HD)\nmaps. However, recent advances in online scene understanding still face\nlimitations, especially in long-range or occluded scenarios, due to the\ninherent constraints of onboard sensors. To address this challenge, we propose\na Standard-Definition (SD) Map Enhanced scene Perception and Topology reasoning\n(SEPT) framework, which explores how to effectively incorporate the SD map as\nprior knowledge into existing perception and reasoning pipelines. Specifically,\nwe introduce a novel hybrid feature fusion strategy that combines SD maps with\nBird's-Eye-View (BEV) features, considering both rasterized and vectorized\nrepresentations, while mitigating potential misalignment between SD maps and\nBEV feature spaces. Additionally, we leverage the SD map characteristics to\ndesign an auxiliary intersection-aware keypoint detection task, which further\nenhances the overall scene understanding performance. Experimental results on\nthe large-scale OpenLane-V2 dataset demonstrate that by effectively integrating\nSD map priors, our framework significantly improves both scene perception and\ntopology reasoning, outperforming existing methods by a substantial margin.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12254", "pdf": "https://arxiv.org/pdf/2505.12254", "abs": "https://arxiv.org/abs/2505.12254", "authors": ["Yiwei Ou", "Xiaobin Ren", "Ronggui Sun", "Guansong Gao", "Ziyi Jiang", "Kaiqi Zhao", "Manfredo Manfredini"], "title": "MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Existing visual place recognition (VPR) datasets predominantly rely on\nvehicle-mounted imagery, lack multimodal diversity and underrepresent dense,\nmixed-use street-level spaces, especially in non-Western urban contexts. To\naddress these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for\nstreet-level place recognition in complex, pedestrian-only environments. The\ndataset comprises 78,575 annotated images and 2,512 video clips captured across\n207 locations in a ~70,800 $\\mathrm{m}^2$ open-air commercial district in\nChengdu, China. Each image is labeled with precise GPS coordinates, timestamp,\nand textual metadata, and covers varied lighting conditions, viewpoints, and\ntimeframes. MMS-VPR follows a systematic and replicable data collection\nprotocol with minimal device requirements, lowering the barrier for scalable\ndataset creation. Importantly, the dataset forms an inherent spatial graph with\n125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place\nrecognition. We further define two application-specific subsets --\nDataset_Edges and Dataset_Points -- to support fine-grained and graph-based\nevaluation tasks. Extensive benchmarks using conventional VPR models, graph\nneural networks, and multimodal baselines show substantial improvements when\nleveraging multimodal and structural cues. MMS-VPR facilitates future research\nat the intersection of computer vision, geospatial understanding, and\nmultimodal reasoning. The dataset is publicly available at\nhttps://huggingface.co/datasets/Yiwei-Ou/MMS-VPR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12306", "pdf": "https://arxiv.org/pdf/2505.12306", "abs": "https://arxiv.org/abs/2505.12306", "authors": ["Yuwei Zhang", "Wenhao Yu", "Shangbin Feng", "Yifan Zhu", "Letian Peng", "Jayanth Srinivasa", "Gaowen Liu", "Jingbo Shang"], "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection", "categories": ["cs.CL"], "comment": "Dataset is available at\n  https://huggingface.co/datasets/YWZBrandon/wikidyk", "summary": "Despite significant advances in large language models (LLMs), their knowledge\nmemorization capabilities remain underexplored, due to the lack of standardized\nand high-quality test ground. In this paper, we introduce a novel, real-world\nand large-scale knowledge injection benchmark that evolves continuously over\ntime without requiring human intervention. Specifically, we propose WikiDYK,\nwhich leverages recently-added and human-written facts from Wikipedia's \"Did\nYou Know...\" entries. These entries are carefully selected by expert Wikipedia\neditors based on criteria such as verifiability and clarity. Each entry is\nconverted into multiple question-answer pairs spanning diverse task formats\nfrom easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290\nfacts and 77,180 questions, which is also seamlessly extensible with future\nupdates from Wikipedia editors. Extensive experiments using continued\npre-training reveal a surprising insight: despite their prevalence in modern\nLLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge\nmemorization capabilities compared to Bidirectional Language Models (BiLMs),\nexhibiting a 23% lower accuracy in terms of reliability. To compensate for the\nsmaller scales of current BiLMs, we introduce a modular collaborative framework\nutilizing ensembles of BiLMs as external knowledge repositories to integrate\nwith LLMs. Experiment shows that our framework further improves the reliability\naccuracy by up to 29.1%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "reliability", "accuracy", "criteria"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12274", "pdf": "https://arxiv.org/pdf/2505.12274", "abs": "https://arxiv.org/abs/2505.12274", "authors": ["Yixiao Chen", "Zhiyuan Ma", "Guoli Jia", "Che Jiang", "Jianjun Li", "Bowen Zhou"], "title": "Context-Aware Autoregressive Models for Multi-Conditional Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Autoregressive transformers have recently shown impressive image generation\nquality and efficiency on par with state-of-the-art diffusion models. Unlike\ndiffusion architectures, autoregressive models can naturally incorporate\narbitrary modalities into a single, unified token sequence--offering a concise\nsolution for multi-conditional image generation tasks. In this work, we propose\n$\\textbf{ContextAR}$, a flexible and effective framework for multi-conditional\nimage generation. ContextAR embeds diverse conditions (e.g., canny edges, depth\nmaps, poses) directly into the token sequence, preserving modality-specific\nsemantics. To maintain spatial alignment while enhancing discrimination among\ndifferent condition types, we introduce hybrid positional encodings that fuse\nRotary Position Embedding with Learnable Positional Embedding. We design\nConditional Context-aware Attention to reduces computational complexity while\npreserving effective intra-condition perception. Without any fine-tuning,\nContextAR supports arbitrary combinations of conditions during inference time.\nExperimental results demonstrate the powerful controllability and versatility\nof our approach, and show that the competitive perpormance than diffusion-based\nmulti-conditional control approaches the existing autoregressive baseline\nacross diverse multi-condition driven scenarios. Project page:\n$\\href{https://context-ar.github.io/}{https://context-ar.github.io/.}$", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12328", "pdf": "https://arxiv.org/pdf/2505.12328", "abs": "https://arxiv.org/abs/2505.12328", "authors": ["Xinye Li", "Mingqi Wan", "Dianbo Sui"], "title": "LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which\nevaluates large language models on producing fine-grained, controllable, and\ninterpretable reasoning processes. Systems must extract all problem conditions,\ndecompose a chain of thought into statement-evidence pairs, and verify the\nlogical validity of each pair. Leveraging only the off-the-shelf\nMeta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that\nfirst enumerates all conditions and then guides the model to label, cite, and\nadjudicate every reasoning step. A lightweight post-processor based on regular\nexpressions normalises spans and enforces the official JSON schema. Without\nfine-tuning, external retrieval, or ensembling, our method ranks 5th overall,\nachieving macro F1 scores on par with substantially more complex and\nresource-consuming pipelines. We conclude by analysing the strengths and\nlimitations of our approach and outlining directions for future research in\nstructural reasoning with LLMs. Our code is available at\nhttps://github.com/asdfo123/LLMSR-asdfo123.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12345", "pdf": "https://arxiv.org/pdf/2505.12345", "abs": "https://arxiv.org/abs/2505.12345", "authors": ["Qizhou Chen", "Dakan Wang", "Taolin Zhang", "Zaoming Yan", "Chengsong You", "Chengyu Wang", "Xiaofeng He"], "title": "UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Model editing aims to enhance the accuracy and reliability of large language\nmodels (LLMs) by efficiently adjusting their internal parameters. Currently,\nmost LLM editing datasets are confined to narrow knowledge domains and cover a\nlimited range of editing evaluation. They often overlook the broad scope of\nediting demands and the diversity of ripple effects resulting from edits. In\nthis context, we introduce UniEdit, a unified benchmark for LLM editing\ngrounded in open-domain knowledge. First, we construct editing samples by\nselecting entities from 25 common domains across five major categories,\nutilizing the extensive triple knowledge available in open-domain knowledge\ngraphs to ensure comprehensive coverage of the knowledge domains. To address\nthe issues of generality and locality in editing, we design an Neighborhood\nMulti-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given\nknowledge piece to entail comprehensive ripple effects to evaluate. Finally, we\nemploy proprietary LLMs to convert the sampled knowledge subgraphs into natural\nlanguage text, guaranteeing grammatical accuracy and syntactical diversity.\nExtensive statistical analysis confirms the scale, comprehensiveness, and\ndiversity of our UniEdit benchmark. We conduct comprehensive experiments across\nmultiple LLMs and editors, analyzing their performance to highlight strengths\nand weaknesses in editing across open knowledge domains and various evaluation\ncriteria, thereby offering valuable insights for future research endeavors.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "reliability", "accuracy", "criteria"], "score": 5}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12310", "pdf": "https://arxiv.org/pdf/2505.12310", "abs": "https://arxiv.org/abs/2505.12310", "authors": ["Shouyi Lu", "Huanyu Zhou", "Guirong Zhuo"], "title": "DNOI-4DRO: Deep 4D Radar Odometry with Differentiable Neural-Optimization Iterations", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "16 pages,10 figures", "summary": "A novel learning-optimization-combined 4D radar odometry model, named\nDNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates\ntraditional geometric optimization with end-to-end neural network training,\nleveraging an innovative differentiable neural-optimization iteration operator.\nIn this framework, point-wise motion flow is first estimated using a neural\nnetwork, followed by the construction of a cost function based on the\nrelationship between point motion and pose in 3D space. The radar pose is then\nrefined using Gauss-Newton updates. Additionally, we design a dual-stream 4D\nradar backbone that integrates multi-scale geometric features and\nclustering-based class-aware features to enhance the representation of sparse\n4D radar point clouds. Extensive experiments on the VoD and Snail-Radar\ndatasets demonstrate the superior performance of our model, which outperforms\nrecent classical and learning-based approaches. Notably, our method even\nachieves results comparable to A-LOAM with mapping optimization using LiDAR\npoint clouds as input. Our models and code will be publicly released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12335", "pdf": "https://arxiv.org/pdf/2505.12335", "abs": "https://arxiv.org/abs/2505.12335", "authors": ["Ziqiang Li", "Jiazhen Yan", "Ziwen He", "Kai Zeng", "Weiwei Jiang", "Lizhi Xiong", "Zhangjie Fu"], "title": "Is Artificial Intelligence Generated Image Detection a Solved Problem?", "categories": ["cs.CV", "cs.CR"], "comment": "Under Review", "summary": "The rapid advancement of generative models, such as GANs and Diffusion\nmodels, has enabled the creation of highly realistic synthetic images, raising\nserious concerns about misinformation, deepfakes, and copyright infringement.\nAlthough numerous Artificial Intelligence Generated Image (AIGI) detectors have\nbeen proposed, often reporting high accuracy, their effectiveness in real-world\nscenarios remains questionable. To bridge this gap, we introduce AIGIBench, a\ncomprehensive benchmark designed to rigorously evaluate the robustness and\ngeneralization capabilities of state-of-the-art AIGI detectors. AIGIBench\nsimulates real-world challenges through four core tasks: multi-source\ngeneralization, robustness to image degradation, sensitivity to data\naugmentation, and impact of test-time pre-processing. It includes 23 diverse\nfake image subsets that span both advanced and widely adopted image generation\ntechniques, along with real-world samples collected from social media and AI\nart platforms. Extensive experiments on 11 advanced detectors demonstrate that,\ndespite their high reported accuracy in controlled settings, these detectors\nsuffer significant performance drops on real-world data, limited benefits from\ncommon augmentations, and nuanced effects of pre-processing, highlighting the\nneed for more robust detection strategies. By providing a unified and realistic\nevaluation framework, AIGIBench offers valuable insights to guide future\nresearch toward dependable and generalizable AIGI detection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12392", "pdf": "https://arxiv.org/pdf/2505.12392", "abs": "https://arxiv.org/abs/2505.12392", "authors": ["Yang Hu", "Xingyu Zhang", "Xueji Fang", "Zhiyang Chen", "Xiao Wang", "Huatian Zhang", "Guojun Qi"], "title": "SLOT: Sample-specific Language Model Optimization at Test-time", "categories": ["cs.CL"], "comment": null, "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12339", "pdf": "https://arxiv.org/pdf/2505.12339", "abs": "https://arxiv.org/abs/2505.12339", "authors": ["Midou Guo", "Qilin Yin", "Wei Lu", "Xiangyang Luo"], "title": "Towards Open-world Generalized Deepfake Detection: General Feature Extraction via Unsupervised Domain Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the development of generative artificial intelligence, new forgery\nmethods are rapidly emerging. Social platforms are flooded with vast amounts of\nunlabeled synthetic data and authentic data, making it increasingly challenging\nto distinguish real from fake. Due to the lack of labels, existing supervised\ndetection methods struggle to effectively address the detection of unknown\ndeepfake methods. Moreover, in open world scenarios, the amount of unlabeled\ndata greatly exceeds that of labeled data. Therefore, we define a new deepfake\ndetection generalization task which focuses on how to achieve efficient\ndetection of large amounts of unlabeled data based on limited labeled data to\nsimulate a open world scenario. To solve the above mentioned task, we propose a\nnovel Open-World Deepfake Detection Generalization Enhancement Training\nStrategy (OWG-DS) to improve the generalization ability of existing methods.\nOur approach aims to transfer deepfake detection knowledge from a small amount\nof labeled source domain data to large-scale unlabeled target domain data.\nSpecifically, we introduce the Domain Distance Optimization (DDO) module to\nalign different domain features by optimizing both inter-domain and\nintra-domain distances. Additionally, the Similarity-based Class Boundary\nSeparation (SCBS) module is used to enhance the aggregation of similar samples\nto ensure clearer class boundaries, while an adversarial training mechanism is\nadopted to learn the domain-invariant features. Extensive experiments show that\nthe proposed deepfake detection generalization enhancement training strategy\nexcels in cross-method and cross-dataset scenarios, improving the model's\ngeneralization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363", "abs": "https://arxiv.org/abs/2505.12363", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12391", "pdf": "https://arxiv.org/pdf/2505.12391", "abs": "https://arxiv.org/abs/2505.12391", "authors": ["Zhengyang Lu", "Qian Xia", "Weifan Wang", "Feng Wang"], "title": "CLIP-aware Domain-Adaptive Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "This work introduces CLIP-aware Domain-Adaptive Super-Resolution (CDASR), a\nnovel framework that addresses the critical challenge of domain generalization\nin single image super-resolution. By leveraging the semantic capabilities of\nCLIP (Contrastive Language-Image Pre-training), CDASR achieves unprecedented\nperformance across diverse domains and extreme scaling factors. The proposed\nmethod integrates CLIP-guided feature alignment mechanism with a meta-learning\ninspired few-shot adaptation strategy, enabling efficient knowledge transfer\nand rapid adaptation to target domains. A custom domain-adaptive module\nprocesses CLIP features alongside super-resolution features through a\nmulti-stage transformation process, including CLIP feature processing, spatial\nfeature generation, and feature fusion. This intricate process ensures\neffective incorporation of semantic information into the super-resolution\npipeline. Additionally, CDASR employs a multi-component loss function that\ncombines pixel-wise reconstruction, perceptual similarity, and semantic\nconsistency. Extensive experiments on benchmark datasets demonstrate CDASR's\nsuperiority, particularly in challenging scenarios. On the Urban100 dataset at\n$\\times$8 scaling, CDASR achieves a significant PSNR gain of 0.15dB over\nexisting methods, with even larger improvements of up to 0.30dB observed at\n$\\times$16 scaling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12431", "pdf": "https://arxiv.org/pdf/2505.12431", "abs": "https://arxiv.org/abs/2505.12431", "authors": ["Yating Liu", "Yujie Zhang", "Qi Yang", "Yiling Xu", "Zhu Li", "Ye-Kui Wang"], "title": "DPCD: A Quality Assessment Database for Dynamic Point Clouds", "categories": ["cs.CV", "cs.DB"], "comment": null, "summary": "Recently, the advancements in Virtual/Augmented Reality (VR/AR) have driven\nthe demand for Dynamic Point Clouds (DPC). Unlike static point clouds, DPCs are\ncapable of capturing temporal changes within objects or scenes, offering a more\naccurate simulation of the real world. While significant progress has been made\nin the quality assessment research of static point cloud, little study has been\ndone on Dynamic Point Cloud Quality Assessment (DPCQA), which hinders the\ndevelopment of quality-oriented applications, such as interframe compression\nand transmission in practical scenarios. In this paper, we introduce a\nlarge-scale DPCQA database, named DPCD, which includes 15 reference DPCs and\n525 distorted DPCs from seven types of lossy compression and noise distortion.\nBy rendering these samples to Processed Video Sequences (PVS), a comprehensive\nsubjective experiment is conducted to obtain Mean Opinion Scores (MOS) from 21\nviewers for analysis. The characteristic of contents, impact of various\ndistortions, and accuracy of MOSs are presented to validate the heterogeneity\nand reliability of the proposed database. Furthermore, we evaluate the\nperformance of several objective metrics on DPCD. The experiment results show\nthat DPCQA is more challenge than that of static point cloud. The DPCD, which\nserves as a catalyst for new research endeavors on DPCQA, is publicly available\nat https://huggingface.co/datasets/Olivialyt/DPCD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12474", "pdf": "https://arxiv.org/pdf/2505.12474", "abs": "https://arxiv.org/abs/2505.12474", "authors": ["Weixiao Zhou", "Junnan Zhu", "Gengyao Li", "Xianfu Cheng", "Xinnian Liang", "Feifei Zhai", "Zhoujun Li"], "title": "What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization", "categories": ["cs.CL"], "comment": "Submitted to EMNLP 2025", "summary": "In this work, we investigate the performance of LLMs on a new task that\nrequires combining discussion with background knowledge for summarization. This\naims to address the limitation of outside observer confusion in existing\ndialogue summarization systems due to their reliance solely on discussion\ninformation. To achieve this, we model the task output as background and\nopinion summaries and define two standardized summarization patterns. To\nsupport assessment, we introduce the first benchmark comprising high-quality\nsamples consistently annotated by human experts and propose a novel\nhierarchical evaluation framework with fine-grained, interpretable metrics. We\nevaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our\nfindings reveal: (1) LLMs struggle with background summary retrieval,\ngeneration, and opinion summary integration. (2) Even top LLMs achieve less\nthan 69% average performance across both patterns. (3) Current LLMs lack\nadequate self-evaluation and self-correction capabilities for this task.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "summarization", "dialogue", "fine-grained"], "score": 5}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12434", "pdf": "https://arxiv.org/pdf/2505.12434", "abs": "https://arxiv.org/abs/2505.12434", "authors": ["Qi Wang", "Yanrui Yu", "Ye Yuan", "Rui Mao", "Tianfei Zhou"], "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning", "categories": ["cs.CV"], "comment": "Code: https://github.com/QiWang98/VideoRFT", "summary": "Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VIDEORFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a fully automatic CoT curation pipeline.\nFirst, we devise a cognitioninspired prompting strategy to elicit a reasoning\nLLM to generate preliminary CoTs based solely on rich, structured, and literal\nrepresentations of video content. Subsequently, these CoTs are revised by a\nvisual-language model conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strength the RL phase, we introduce a novel semantic-consistency reward\nthat explicitly promotes the alignment between textual reasoning with visual\nevidence. This reward encourages the model to produce coherent, context-aware\nreasoning outputs grounded in visual input. Extensive experiments show that\nVIDEORFT achieves state-of-the-art performance on six video reasoning\nbenchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12448", "pdf": "https://arxiv.org/pdf/2505.12448", "abs": "https://arxiv.org/abs/2505.12448", "authors": ["Yang Liu", "Ming Ma", "Xiaomin Yu", "Pengxiang Ding", "Han Zhao", "Mingyang Sun", "Siteng Huang", "Donglin Wang"], "title": "SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Despite impressive advancements in Visual-Language Models (VLMs) for\nmulti-modal tasks, their reliance on RGB inputs limits precise spatial\nunderstanding. Existing methods for integrating spatial cues, such as point\nclouds or depth, either require specialized sensors or fail to effectively\nexploit depth information for higher-order reasoning. To this end, we propose a\nnovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that\ntransforms raw depth data into structured, interpretable textual rationales.\nThese textual rationales serve as meaningful intermediate representations to\nsignificantly enhance spatial reasoning capabilities. Additionally, we leverage\nknowledge distillation to compress the generated rationales into compact latent\nembeddings, which facilitate resource-efficient and plug-and-play integration\ninto existing VLMs without retraining. To enable comprehensive evaluation, we\nintroduce a new dataset named SSR-CoT, a million-scale visual-language\nreasoning dataset enriched with intermediate spatial reasoning annotations, and\npresent SSRBench, a comprehensive multi-task benchmark. Extensive experiments\non multiple benchmarks demonstrate SSR substantially improves depth utilization\nand enhances spatial reasoning, thereby advancing VLMs toward more human-like\nmulti-modal understanding. Our project page is at\nhttps://yliu-cs.github.io/SSR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12531", "pdf": "https://arxiv.org/pdf/2505.12531", "abs": "https://arxiv.org/abs/2505.12531", "authors": ["Navid Madani", "Rohini Srihari"], "title": "ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) increasingly power mental-health chatbots, yet\nthe field still lacks a scalable, theory-grounded way to decide which model is\nmost effective to deploy. We present ESC-Judge, the first end-to-end evaluation\nframework that (i) grounds head-to-head comparisons of emotional-support LLMs\nin Clara Hill's established Exploration-Insight-Action counseling model,\nproviding a structured and interpretable view of performance, and (ii) fully\nautomates the evaluation pipeline at scale. ESC-Judge operates in three stages:\nfirst, it synthesizes realistic help-seeker roles by sampling empirically\nsalient attributes such as stressors, personality, and life history; second, it\nhas two candidate support agents conduct separate sessions with the same role,\nisolating model-specific strategies; and third, it asks a specialized judge LLM\nto express pairwise preferences across rubric-anchored skills that span the\nExploration, Insight, and Action spectrum. In our study, ESC-Judge matched\nPhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and\n86 percent of Action decisions, demonstrating human-level reliability at a\nfraction of the cost. All code, prompts, synthetic roles, transcripts, and\njudgment scripts are released to promote transparent progress in emotionally\nsupportive AI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "rubric"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12513", "pdf": "https://arxiv.org/pdf/2505.12513", "abs": "https://arxiv.org/abs/2505.12513", "authors": ["Yang Mu", "Zhitong Xiong", "Yi Wang", "Muhammad Shahzad", "Franz Essl", "Mark van Kleunen", "Xiao Xiang Zhu"], "title": "GlobalGeoTree: A Multi-Granular Vision-Language Dataset for Global Tree Species Classification", "categories": ["cs.CV"], "comment": null, "summary": "Global tree species mapping using remote sensing data is vital for\nbiodiversity monitoring, forest management, and ecological research. However,\nprogress in this field has been constrained by the scarcity of large-scale,\nlabeled datasets. To address this, we introduce GlobalGeoTree, a comprehensive\nglobal dataset for tree species classification. GlobalGeoTree comprises 6.3\nmillion geolocated tree occurrences, spanning 275 families, 2,734 genera, and\n21,001 species across the hierarchical taxonomic levels. Each sample is paired\nwith Sentinel-2 image time series and 27 auxiliary environmental variables,\nencompassing bioclimatic, geographic, and soil data. The dataset is partitioned\ninto GlobalGeoTree-6M for model pretraining and curated evaluation subsets,\nprimarily GlobalGeoTree-10kEval for zero-shot and few-shot benchmarking. To\ndemonstrate the utility of the dataset, we introduce a baseline model,\nGeoTreeCLIP, which leverages paired remote sensing data and taxonomic text\nlabels within a vision-language framework pretrained on GlobalGeoTree-6M.\nExperimental results show that GeoTreeCLIP achieves substantial improvements in\nzero- and few-shot classification on GlobalGeoTree-10kEval over existing\nadvanced models. By making the dataset, models, and code publicly available, we\naim to establish a benchmark to advance tree species classification and foster\ninnovation in biodiversity research and ecological applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12549", "pdf": "https://arxiv.org/pdf/2505.12549", "abs": "https://arxiv.org/abs/2505.12549", "authors": ["Dominic Maggio", "Hyungtae Lim", "Luca Carlone"], "title": "VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold", "categories": ["cs.CV"], "comment": null, "summary": "We present VGGT-SLAM, a dense RGB SLAM system constructed by incrementally\nand globally aligning submaps created from the feed-forward scene\nreconstruction approach VGGT using only uncalibrated monocular cameras. While\nrelated works align submaps using similarity transforms (i.e., translation,\nrotation, and scale), we show that such approaches are inadequate in the case\nof uncalibrated cameras. In particular, we revisit the idea of reconstruction\nambiguity, where given a set of uncalibrated cameras with no assumption on the\ncamera motion or scene structure, the scene can only be reconstructed up to a\n15-degrees-of-freedom projective transformation of the true geometry. This\ninspires us to recover a consistent scene reconstruction across submaps by\noptimizing over the SL(4) manifold, thus estimating 15-degrees-of-freedom\nhomography transforms between sequential submaps while accounting for potential\nloop closure constraints. As verified by extensive experiments, we demonstrate\nthat VGGT-SLAM achieves improved map quality using long video sequences that\nare infeasible for VGGT due to its high GPU requirements.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12589", "pdf": "https://arxiv.org/pdf/2505.12589", "abs": "https://arxiv.org/abs/2505.12589", "authors": ["Bo Liu", "Pengfei Qiao", "Minhan Ma", "Xuange Zhang", "Yinan Tang", "Peng Xu", "Kun Liu", "Tongtong Yuan"], "title": "SurveillanceVQA-589K: A Benchmark for Comprehensive Surveillance Video-Language Understanding with Large Models", "categories": ["cs.CV"], "comment": "The dataset and code are publicly available at:\n  https://huggingface.co/datasets/fei213/SurveillanceVQA-589K", "summary": "Understanding surveillance video content remains a critical yet underexplored\nchallenge in vision-language research, particularly due to its real-world\ncomplexity, irregular event dynamics, and safety-critical implications. In this\nwork, we introduce SurveillanceVQA-589K, the largest open-ended video question\nanswering benchmark tailored to the surveillance domain. The dataset comprises\n589,380 QA pairs spanning 12 cognitively diverse question types, including\ntemporal reasoning, causal inference, spatial understanding, and anomaly\ninterpretation, across both normal and abnormal video scenarios. To construct\nthe benchmark at scale, we design a hybrid annotation pipeline that combines\ntemporally aligned human-written captions with Large Vision-Language\nModel-assisted QA generation using prompt-based techniques. We also propose a\nmulti-dimensional evaluation protocol to assess contextual, temporal, and\ncausal comprehension. We evaluate eight LVLMs under this framework, revealing\nsignificant performance gaps, especially in causal and anomaly-related tasks,\nunderscoring the limitations of current models in real-world surveillance\ncontexts. Our benchmark provides a practical and comprehensive resource for\nadvancing video-language understanding in safety-critical applications such as\nintelligent monitoring, incident analysis, and autonomous decision-making.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "annotation", "safety", "multi-dimensional"], "score": 6}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12620", "pdf": "https://arxiv.org/pdf/2505.12620", "abs": "https://arxiv.org/abs/2505.12620", "authors": ["Haiquan Wen", "Yiwei He", "Zhenglin Huang", "Tianxiao Li", "Zihan YU", "Xingru Huang", "Lu Qi", "Baoyuan Wu", "Xiangtai Li", "Guangliang Cheng"], "title": "BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation", "categories": ["cs.CV"], "comment": null, "summary": "Advances in AI generative models facilitate super-realistic video synthesis,\namplifying misinformation risks via social media and eroding trust in digital\ncontent. Several research works have explored new deepfake detection methods on\nAI-generated images to alleviate these risks. However, with the fast\ndevelopment of video generation models, such as Sora and WanX, there is\ncurrently a lack of large-scale, high-quality AI-generated video datasets for\nforgery detection. In addition, existing detection approaches predominantly\ntreat the task as binary classification, lacking explainability in model\ndecision-making and failing to provide actionable insights or guidance for the\npublic. To address these challenges, we propose \\textbf{GenBuster-200K}, a\nlarge-scale AI-generated video dataset featuring 200K high-resolution video\nclips, diverse latest generative techniques, and real-world scenes. We further\nintroduce \\textbf{BusterX}, a novel AI-generated video detection and\nexplanation framework leveraging multimodal large language model (MLLM) and\nreinforcement learning for authenticity determination and explainable\nrationale. To our knowledge, GenBuster-200K is the {\\it \\textbf{first}}\nlarge-scale, high-quality AI-generated video dataset that incorporates the\nlatest generative techniques for real-world scenarios. BusterX is the {\\it\n\\textbf{first}} framework to integrate MLLM with reinforcement learning for\nexplainable AI-generated video detection. Extensive comparisons with\nstate-of-the-art methods and ablation studies validate the effectiveness and\ngeneralizability of BusterX. The code, models, and datasets will be released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12632", "pdf": "https://arxiv.org/pdf/2505.12632", "abs": "https://arxiv.org/abs/2505.12632", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12650", "pdf": "https://arxiv.org/pdf/2505.12650", "abs": "https://arxiv.org/abs/2505.12650", "authors": ["Yaotian Yang", "Yiwen Tang", "Yizhe Chen", "Xiao Chen", "Jiangjie Qiu", "Hao Xiong", "Haoyu Yin", "Zhiyao Luo", "Yifei Zhang", "Sijia Tao", "Wentao Li", "Qinghua Zhang", "Yuqiang Li", "Wanli Ouyang", "Bin Zhao", "Xiaonan Wang", "Fei Wei"], "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use", "categories": ["cs.CV", "cs.AI"], "comment": "The code and dataset are publicly available at\n  https://github.com/yyt-2378/AutoMat and\n  https://huggingface.co/datasets/yaotianvector/STEM2Mat", "summary": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12656", "pdf": "https://arxiv.org/pdf/2505.12656", "abs": "https://arxiv.org/abs/2505.12656", "authors": ["Yongchang Gao", "Meiling Jin", "Zhaofei Yu", "Tiejun Huang", "Guozhang Chen"], "title": "SPKLIP: Aligning Spike Video Streams with Natural Language", "categories": ["cs.CV"], "comment": null, "summary": "Spike cameras offer unique sensing capabilities but their sparse,\nasynchronous output challenges semantic understanding, especially for Spike\nVideo-Language Alignment (Spike-VLA) where models like CLIP underperform due to\nmodality mismatch. We introduce SPKLIP, the first architecture specifically for\nSpike-VLA. SPKLIP employs a hierarchical spike feature extractor that\nadaptively models multi-scale temporal dynamics in event streams, and uses\nspike-text contrastive learning to directly align spike video with language,\nenabling effective few-shot learning. A full-spiking visual encoder variant,\nintegrating SNN components into our pipeline, demonstrates enhanced energy\nefficiency. Experiments show state-of-the-art performance on benchmark spike\ndatasets and strong few-shot generalization on a newly contributed real-world\ndataset. SPKLIP's energy efficiency highlights its potential for neuromorphic\ndeployment, advancing event-based multimodal research. The source code and\ndataset are available at [link removed for anonymity].", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12792", "pdf": "https://arxiv.org/pdf/2505.12792", "abs": "https://arxiv.org/abs/2505.12792", "authors": ["Wenhao Zhu", "Yuhang Xie", "Guojie Song", "Xin Zhang"], "title": "EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs", "categories": ["cs.CL"], "comment": null, "summary": "The rapid evolution of large language models (LLMs) has revolutionized\nvarious fields, including the identification and discovery of human values\nwithin text data. While traditional NLP models, such as BERT, have been\nemployed for this task, their ability to represent textual data is\nsignificantly outperformed by emerging LLMs like GPTs. However, the performance\nof online LLMs often degrades when handling long contexts required for value\nidentification, which also incurs substantial computational costs. To address\nthese challenges, we propose EAVIT, an efficient and accurate framework for\nhuman value identification that combines the strengths of both locally\nfine-tunable and online black-box LLMs. Our framework employs a value detector\n- a small, local language model - to generate initial value estimations. These\nestimations are then used to construct concise input prompts for online LLMs,\nenabling accurate final value identification. To train the value detector, we\nintroduce explanation-based training and data generation techniques\nspecifically tailored for value identification, alongside sampling strategies\nto optimize the brevity of LLM input prompts. Our approach effectively reduces\nthe number of input tokens by up to 1/6 compared to directly querying online\nLLMs, while consistently outperforming traditional NLP methods and other\nLLM-based strategies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12677", "pdf": "https://arxiv.org/pdf/2505.12677", "abs": "https://arxiv.org/abs/2505.12677", "authors": ["Shristi Das Biswas", "Arani Roy", "Kaushik Roy"], "title": "CURE: Concept Unlearning via Orthogonal Representation Editing in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "As Text-to-Image models continue to evolve, so does the risk of generating\nunsafe, copyrighted, or privacy-violating content. Existing safety\ninterventions - ranging from training data curation and model fine-tuning to\ninference-time filtering and guidance - often suffer from incomplete concept\nremoval, susceptibility to jail-breaking, computational inefficiency, or\ncollateral damage to unrelated capabilities. In this paper, we introduce CURE,\na training-free concept unlearning framework that operates directly in the\nweight space of pre-trained diffusion models, enabling fast, interpretable, and\nhighly specific suppression of undesired concepts. At the core of our method is\nthe Spectral Eraser, a closed-form, orthogonal projection module that\nidentifies discriminative subspaces using Singular Value Decomposition over\ntoken embeddings associated with the concepts to forget and retain.\nIntuitively, the Spectral Eraser identifies and isolates features unique to the\nundesired concept while preserving safe attributes. This operator is then\napplied in a single step update to yield an edited model in which the target\nconcept is effectively unlearned - without retraining, supervision, or\niterative optimization. To balance the trade-off between filtering toxicity and\npreserving unrelated concepts, we further introduce an Expansion Mechanism for\nspectral regularization which selectively modulates singular vectors based on\ntheir relative significance to control the strength of forgetting. All the\nprocesses above are in closed-form, guaranteeing extremely efficient erasure in\nonly $2$ seconds. Benchmarking against prior approaches, CURE achieves a more\nefficient and thorough removal for targeted artistic styles, objects,\nidentities, or explicit content, with minor damage to original generation\nability and demonstrates enhanced robustness against red-teaming.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12808", "pdf": "https://arxiv.org/pdf/2505.12808", "abs": "https://arxiv.org/abs/2505.12808", "authors": ["Yanbin Yin", "Kun Zhou", "Zhen Wang", "Xiangdong Zhang", "Yifei Shao", "Shibo Hao", "Yi Gu", "Jieyuan Liu", "Somanshu Singla", "Tianyang Liu", "Eric P. Xing", "Zhengzhong Liu", "Haojian Jin", "Zhiting Hu"], "title": "Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "20 pages, ongoing work", "summary": "The recent explosion of large language models (LLMs), each with its own\ngeneral or specialized strengths, makes scalable, reliable benchmarking more\nurgent than ever. Standard practices nowadays face fundamental trade-offs:\nclosed-ended question-based benchmarks (eg MMLU) struggle with saturation as\nnewer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely\non costly and slow human judges. Recently, automated methods (eg\nLLM-as-a-judge) shed light on the scalability, but risk bias by relying on one\nor a few \"authority\" models. To tackle these issues, we propose Decentralized\nArena (dearena), a fully automated framework leveraging collective intelligence\nfrom all LLMs to evaluate each other. It mitigates single-model judge bias by\ndemocratic, pairwise evaluation, and remains efficient at scale through two key\ncomponents: (1) a coarse-to-fine ranking algorithm for fast incremental\ninsertion of new models with sub-quadratic complexity, and (2) an automatic\nquestion selection strategy for the construction of new evaluation dimensions.\nAcross extensive experiments across 66 LLMs, dearena attains up to 97%\ncorrelation with human judgements, while significantly reducing the cost. Our\ncode and data will be publicly released on\nhttps://github.com/maitrix-org/de-arena.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12685", "pdf": "https://arxiv.org/pdf/2505.12685", "abs": "https://arxiv.org/abs/2505.12685", "authors": ["Fei Xie", "Jiahao Nie", "Yujin Tang", "Wenkang Zhang", "Hongshen Zhao"], "title": "Mamba-Adaptor: State Space Model Adaptor for Visual Recognition", "categories": ["cs.CV"], "comment": "CVPR paper", "summary": "Recent State Space Models (SSM), especially Mamba, have demonstrated\nimpressive performance in visual modeling and possess superior model\nefficiency. However, the application of Mamba to visual tasks suffers inferior\nperformance due to three main constraints existing in the sequential model: 1)\nCasual computing is incapable of accessing global context; 2) Long-range\nforgetting when computing the current hidden states; 3) Weak spatial structural\nmodeling due to the transformed sequential input. To address these issues, we\ninvestigate a simple yet powerful vision task Adaptor for Mamba models, which\nconsists of two functional modules: Adaptor-T and Adaptor-S. When solving the\nhidden states for SSM, we apply a lightweight prediction module Adaptor-T to\nselect a set of learnable locations as memory augmentations to ease long-range\nforgetting issues. Moreover, we leverage Adapator-S, composed of multi-scale\ndilated convolutional kernels, to enhance the spatial modeling and introduce\nthe image inductive bias into the feature output. Both modules can enlarge the\ncontext modeling in casual computing, as the output is enhanced by the\ninaccessible features. We explore three usages of Mamba-Adaptor: A general\nvisual backbone for various vision tasks; A booster module to raise the\nperformance of pretrained backbones; A highly efficient fine-tuning module that\nadapts the base model for transfer learning tasks. Extensive experiments verify\nthe effectiveness of Mamba-Adaptor in three settings. Notably, our\nMamba-Adaptor achieves state-of the-art performance on the ImageNet and COCO\nbenchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12693", "pdf": "https://arxiv.org/pdf/2505.12693", "abs": "https://arxiv.org/abs/2505.12693", "authors": ["Luyao Lei", "Shuo Xu", "Yifan Bai", "Xing Wei"], "title": "TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D Semantic Occupancy", "categories": ["cs.CV"], "comment": null, "summary": "The performance of multi-modal 3D occupancy prediction is limited by\nineffective fusion, mainly due to geometry-semantics mismatch from fixed fusion\nstrategies and surface detail loss caused by sparse, noisy annotations. The\nmismatch stems from the heterogeneous scale and distribution of point cloud and\nimage features, leading to biased matching under fixed neighborhood fusion. To\naddress this, we propose a target-scale adaptive, bidirectional symmetric\nretrieval mechanism. It expands the neighborhood for large targets to enhance\ncontext awareness and shrinks it for small ones to improve efficiency and\nsuppress noise, enabling accurate cross-modal feature alignment. This mechanism\nexplicitly establishes spatial correspondences and improves fusion accuracy.\nFor surface detail loss, sparse labels provide limited supervision, resulting\nin poor predictions for small objects. We introduce an improved volume\nrendering pipeline based on 3D Gaussian Splatting, which takes fused features\nas input to render images, applies photometric consistency supervision, and\njointly optimizes 2D-3D consistency. This enhances surface detail\nreconstruction while suppressing noise propagation. In summary, we propose\nTACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancy\nprediction, enhanced by volume rendering supervision. Experiments on the\nnuScenes and SemanticKITTI benchmarks validate its effectiveness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12702", "pdf": "https://arxiv.org/pdf/2505.12702", "abs": "https://arxiv.org/abs/2505.12702", "authors": ["Tianming Liang", "Haichao Jiang", "Yuting Yang", "Chaolei Tan", "Shuai Li", "Wei-Shi Zheng", "Jian-Fang Hu"], "title": "Long-RVOS: A Comprehensive Benchmark for Long-term Referring Video Object Segmentation", "categories": ["cs.CV"], "comment": "Project Page: \\url{https://isee-laboratory.github.io/Long-RVOS}", "summary": "Referring video object segmentation (RVOS) aims to identify, track and\nsegment the objects in a video based on language descriptions, which has\nreceived great attention in recent years. However, existing datasets remain\nfocus on short video clips within several seconds, with salient objects visible\nin most frames. To advance the task towards more practical scenarios, we\nintroduce \\textbf{Long-RVOS}, a large-scale benchmark for long-term referring\nvideo object segmentation. Long-RVOS contains 2,000+ videos of an average\nduration exceeding 60 seconds, covering a variety of objects that undergo\nocclusion, disappearance-reappearance and shot changing. The objects are\nmanually annotated with three different types of descriptions to individually\nevaluate the understanding of static attributes, motion patterns and\nspatiotemporal relationships. Moreover, unlike previous benchmarks that rely\nsolely on the per-frame spatial evaluation, we introduce two new metrics to\nassess the temporal and spatiotemporal consistency. We benchmark 6\nstate-of-the-art methods on Long-RVOS. The results show that current approaches\nstruggle severely with the long-video challenges. To address this, we further\npropose ReferMo, a promising baseline method that integrates motion information\nto expand the temporal receptive field, and employs a local-to-global\narchitecture to capture both short-term dynamics and long-term dependencies.\nDespite simplicity, ReferMo achieves significant improvements over current\nmethods in long-term scenarios. We hope that Long-RVOS and our baseline can\ndrive future RVOS research towards tackling more realistic and long-form\nvideos.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12831", "pdf": "https://arxiv.org/pdf/2505.12831", "abs": "https://arxiv.org/abs/2505.12831", "authors": ["Zifeng Cheng", "Zhonghui Wang", "Yuchen Fu", "Zhiwei Jiang", "Yafeng Yin", "Cong Wang", "Qing Gu"], "title": "Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Extracting sentence embeddings from large language models (LLMs) is a\npractical direction, as it requires neither additional data nor fine-tuning.\nPrevious studies usually focus on prompt engineering to guide LLMs to encode\nthe core semantic information of the sentence into the embedding of the last\ntoken. However, the last token in these methods still encodes an excess of\nnon-essential information, such as stop words, limiting its encoding capacity.\nTo this end, we propose a Contrastive Prompting (CP) method that introduces an\nextra auxiliary prompt to elicit better sentence embedding. By contrasting with\nthe auxiliary prompt, CP can steer existing prompts to encode the core\nsemantics of the sentence, rather than non-essential information. CP is a\nplug-and-play inference-time intervention method that can be combined with\nvarious prompt-based methods. Extensive experiments on Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nmethod can improve the performance of existing prompt-based methods across\ndifferent LLMs. Our code will be released at https://github.com/zifengcheng/CP.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12835", "pdf": "https://arxiv.org/pdf/2505.12835", "abs": "https://arxiv.org/abs/2505.12835", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12742", "pdf": "https://arxiv.org/pdf/2505.12742", "abs": "https://arxiv.org/abs/2505.12742", "authors": ["Jinhua Zhang", "Wei Long", "Minghao Han", "Weiyi You", "Shuhang Gu"], "title": "MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian Conditioning", "categories": ["cs.CV"], "comment": null, "summary": "Essential to visual generation is efficient modeling of visual data priors.\nConventional next-token prediction methods define the process as learning the\nconditional probability distribution of successive tokens. Recently, next-scale\nprediction methods redefine the process to learn the distribution over\nmulti-scale representations, significantly reducing generation latency.\nHowever, these methods condition each scale on all previous scales and require\neach token to consider all preceding tokens, exhibiting scale and spatial\nredundancy. To better model the distribution by mitigating redundancy, we\npropose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive\nframework that introduces scale and spatial Markov assumptions to reduce the\ncomplexity of conditional probability modeling. Specifically, we introduce a\nscale-Markov trajectory that only takes as input the features of adjacent\npreceding scale for next-scale prediction, enabling the adoption of a parallel\ntraining strategy that significantly reduces GPU memory consumption.\nFurthermore, we propose spatial-Markov attention, which restricts the attention\nof each token to a localized neighborhood of size k at corresponding positions\non adjacent scales, rather than attending to every token across these scales,\nfor the pursuit of reduced modeling complexity. Building on these improvements,\nwe reduce the computational complexity of attention calculation from O(N^2) to\nO(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating\nthe need for KV cache during inference. Extensive experiments on ImageNet\ndemonstrate that MVAR achieves comparable or superior performance with both\nsmall model trained from scratch and large fine-tuned models, while reducing\nthe average GPU memory footprint by 3.0x.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12896", "pdf": "https://arxiv.org/pdf/2505.12896", "abs": "https://arxiv.org/abs/2505.12896", "authors": ["Chenxi Liu", "Yongqiang Chen", "Tongliang Liu", "James Cheng", "Bo Han", "Kun Zhang"], "title": "On the Thinking-Language Modeling Gap in Large Language Models", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Chenxi and Yongqiang contributed equally; project page:\n  https://causalcoat.github.io/lot.html", "summary": "System 2 reasoning is one of the defining characteristics of intelligence,\nwhich requires slow and logical thinking. Human conducts System 2 reasoning via\nthe language of thoughts that organizes the reasoning process as a causal\nsequence of mental language, or thoughts. Recently, it has been observed that\nSystem 2 reasoning can be elicited from Large Language Models (LLMs)\npre-trained on large-scale natural languages. However, in this work, we show\nthat there is a significant gap between the modeling of languages and thoughts.\nAs language is primarily a tool for humans to share knowledge and thinking,\nmodeling human language can easily absorb language biases into LLMs deviated\nfrom the chain of thoughts in minds. Furthermore, we show that the biases will\nmislead the eliciting of \"thoughts\" in LLMs to focus only on a biased part of\nthe premise. To this end, we propose a new prompt technique termed\nLanguage-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of\ndirectly eliciting the chain of thoughts from partial information, LoT\ninstructs LLMs to adjust the order and token used for the expressions of all\nthe relevant information. We show that the simple strategy significantly\nreduces the language modeling biases in LLMs and improves the performance of\nLLMs across a variety of reasoning tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12758", "pdf": "https://arxiv.org/pdf/2505.12758", "abs": "https://arxiv.org/abs/2505.12758", "authors": ["Matias Quintana", "Youlong Gu", "Xiucheng Liang", "Yujun Hou", "Koichi Ito", "Yihan Zhu", "Mahmoud Abdelrahman", "Filip Biljecki"], "title": "It's not you, it's me -- Global urban visual perception varies across demographics and personalities", "categories": ["cs.CV", "cs.LG"], "comment": "Under review", "summary": "Understanding people's preferences and needs is crucial for urban planning\ndecisions, yet current approaches often combine them from multi-cultural and\nmulti-city populations, obscuring important demographic differences and risking\namplifying biases. We conducted a large-scale urban visual perception survey of\nstreetscapes worldwide using street view imagery, examining how demographics --\nincluding gender, age, income, education, race and ethnicity, and, for the\nfirst time, personality traits -- shape perceptions among 1,000 participants,\nwith balanced demographics, from five countries and 45 nationalities. This\ndataset, introduced as Street Perception Evaluation Considering Socioeconomics\n(SPECS), exhibits statistically significant differences in perception scores in\nsix traditionally used indicators (safe, lively, wealthy, beautiful, boring,\nand depressing) and four new ones we propose (live nearby, walk, cycle, green)\namong demographics and personalities. We revealed that location-based\nsentiments are carried over in people's preferences when comparing urban\nstreetscapes with other cities. Further, we compared the perception scores\nbased on where participants and streetscapes are from. We found that an\noff-the-shelf machine learning model trained on an existing global perception\ndataset tends to overestimate positive indicators and underestimate negative\nones compared to human responses, suggesting that targeted intervention should\nconsider locals' perception. Our study aspires to rectify the myopic treatment\nof street perception, which rarely considers demographics or personality\ntraits.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12772", "pdf": "https://arxiv.org/pdf/2505.12772", "abs": "https://arxiv.org/abs/2505.12772", "authors": ["Junyi Hu", "Tian Bai", "Fengyi Wu", "Zhengming Peng", "Yi Zhang"], "title": "Pyramid Sparse Transformer: Enhancing Multi-Scale Feature Fusion with Dynamic Token Selection", "categories": ["cs.CV"], "comment": "13 pages, 5 figures", "summary": "Feature fusion is critical for high-performance vision models but often\nincurs prohibitive complexity. However, prevailing attention-based fusion\nmethods often involve significant computational complexity and implementation\nchallenges, limiting their efficiency in resource-constrained environments. To\naddress these issues, we introduce the Pyramid Sparse Transformer (PST), a\nlightweight, plug-and-play module that integrates coarse-to-fine token\nselection and shared attention parameters to reduce computation while\npreserving spatial detail. PST can be trained using only coarse attention and\nseamlessly activated at inference for further accuracy gains without\nretraining. When added to state-of-the-art real-time detection models, such as\nYOLOv11-N/S/M, PST yields mAP improvements of 0.9%, 0.5%, and 0.4% on MS COCO\nwith minimal latency impact. Likewise, embedding PST into ResNet-18/50/101 as\nbackbones, boosts ImageNet top-1 accuracy by 6.5%, 1.7%, and 1.0%,\nrespectively. These results demonstrate PST's effectiveness as a simple,\nhardware-friendly enhancement for both detection and classification tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12820", "pdf": "https://arxiv.org/pdf/2505.12820", "abs": "https://arxiv.org/abs/2505.12820", "authors": ["Hulin Li"], "title": "Rethinking Features-Fused-Pyramid-Neck for Object Detection", "categories": ["cs.CV"], "comment": "ECCV 2024", "summary": "Multi-head detectors typically employ a features-fused-pyramid-neck for\nmulti-scale detection and are widely adopted in the industry. However, this\napproach faces feature misalignment when representations from different\nhierarchical levels of the feature pyramid are forcibly fused point-to-point.\nTo address this issue, we designed an independent hierarchy pyramid (IHP)\narchitecture to evaluate the effectiveness of the features-unfused-pyramid-neck\nfor multi-head detectors. Subsequently, we introduced soft nearest neighbor\ninterpolation (SNI) with a weight downscaling factor to mitigate the impact of\nfeature fusion at different hierarchies while preserving key textures.\nFurthermore, we present a features adaptive selection method for down sampling\nin extended spatial windows (ESD) to retain spatial features and enhance\nlightweight convolutional techniques (GSConvE). These advancements culminate in\nour secondary features alignment solution (SA) for real-time detection,\nachieving state-of-the-art results on Pascal VOC and MS COCO. Code will be\nreleased at https://github.com/AlanLi1997/rethinking-fpn. This paper has been\naccepted by ECCV2024 and published on Springer Nature.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12996", "pdf": "https://arxiv.org/pdf/2505.12996", "abs": "https://arxiv.org/abs/2505.12996", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 2 figures", "summary": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12897", "pdf": "https://arxiv.org/pdf/2505.12897", "abs": "https://arxiv.org/abs/2505.12897", "authors": ["Piotr Borycki", "Magdalena Trƒôdowicz", "Szymon Janusz", "Jacek Tabor", "Przemys≈Çaw Spurek", "Arkadiusz Lewicki", "≈Åukasz Struski"], "title": "EPIC: Explanation of Pretrained Image Classification Networks via Prototype", "categories": ["cs.CV"], "comment": null, "summary": "Explainable AI (XAI) methods generally fall into two categories. Post-hoc\napproaches generate explanations for pre-trained models and are compatible with\nvarious neural network architectures. These methods often use feature\nimportance visualizations, such as saliency maps, to indicate which input\nregions influenced the model's prediction. Unfortunately, they typically offer\na coarse understanding of the model's decision-making process. In contrast,\nante-hoc (inherently explainable) methods rely on specially designed model\narchitectures trained from scratch. A notable subclass of these methods\nprovides explanations through prototypes, representative patches extracted from\nthe training data. However, prototype-based approaches have limitations: they\nrequire dedicated architectures, involve specialized training procedures, and\nperform well only on specific datasets. In this work, we propose EPIC\n(Explanation of Pretrained Image Classification), a novel approach that bridges\nthe gap between these two paradigms. Like post-hoc methods, EPIC operates on\npre-trained models without architectural modifications. Simultaneously, it\ndelivers intuitive, prototype-based explanations inspired by ante-hoc\ntechniques. To the best of our knowledge, EPIC is the first post-hoc method\ncapable of fully replicating the core explanatory power of inherently\ninterpretable models. We evaluate EPIC on benchmark datasets commonly used in\nprototype-based explanations, such as CUB-200-2011 and Stanford Cars, alongside\nlarge-scale datasets like ImageNet, typically employed by post-hoc methods.\nEPIC uses prototypes to explain model decisions, providing a flexible and\neasy-to-understand tool for creating clear, high-quality explanations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12908", "pdf": "https://arxiv.org/pdf/2505.12908", "abs": "https://arxiv.org/abs/2505.12908", "authors": ["Xiao Wang", "Yu Jin", "Lan Chen", "Bo Jiang", "Lin Zhu", "Yonghong Tian", "Jin Tang", "Bin Luo"], "title": "Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Event-based Vision Sensors (EVS) have demonstrated significant advantages\nover traditional RGB frame-based cameras in low-light conditions, high-speed\nmotion capture, and low latency. Consequently, object detection based on EVS\nhas attracted increasing attention from researchers. Current event stream\nobject detection algorithms are typically built upon Convolutional Neural\nNetworks (CNNs) or Transformers, which either capture limited local features\nusing convolutional filters or incur high computational costs due to the\nutilization of self-attention. Recently proposed vision heat conduction\nbackbone networks have shown a good balance between efficiency and accuracy;\nhowever, these models are not specifically designed for event stream data. They\nexhibit weak capability in modeling object contour information and fail to\nexploit the benefits of multi-scale features. To address these issues, this\npaper proposes a novel dynamic graph induced contour-aware heat conduction\nnetwork for event stream based object detection, termed CvHeat-DET. The\nproposed model effectively leverages the clear contour information inherent in\nevent streams to predict the thermal diffusivity coefficients within the heat\nconduction model, and integrates hierarchical structural graph features to\nenhance feature learning across multiple scales. Extensive experiments on three\nbenchmark datasets for event stream-based object detection fully validated the\neffectiveness of the proposed model. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenEvDET.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12998", "pdf": "https://arxiv.org/pdf/2505.12998", "abs": "https://arxiv.org/abs/2505.12998", "authors": ["Vinkle Srivastav", "Juliette Puel", "Jonathan Vappou", "Elijah Van Houten", "Paolo Cabras", "Nicolas Padoy"], "title": "A Skull-Adaptive Framework for AI-Based 3D Transcranial Focused Ultrasound Simulation", "categories": ["cs.CV"], "comment": "The project page is available at\n  https://github.com/CAMMA-public/TFUScapes", "summary": "Transcranial focused ultrasound (tFUS) is an emerging modality for\nnon-invasive brain stimulation and therapeutic intervention, offering\nmillimeter-scale spatial precision and the ability to target deep brain\nstructures. However, the heterogeneous and anisotropic nature of the human\nskull introduces significant distortions to the propagating ultrasound\nwavefront, which require time-consuming patient-specific planning and\ncorrections using numerical solvers for accurate targeting. To enable\ndata-driven approaches in this domain, we introduce TFUScapes, the first\nlarge-scale, high-resolution dataset of tFUS simulations through anatomically\nrealistic human skulls derived from T1-weighted MRI images. We have developed a\nscalable simulation engine pipeline using the k-Wave pseudo-spectral solver,\nwhere each simulation returns a steady-state pressure field generated by a\nfocused ultrasound transducer placed at realistic scalp locations. In addition\nto the dataset, we present DeepTFUS, a deep learning model that estimates\nnormalized pressure fields directly from input 3D CT volumes and transducer\nposition. The model extends a U-Net backbone with transducer-aware\nconditioning, incorporating Fourier-encoded position embeddings and MLP layers\nto create global transducer embeddings. These embeddings are fused with U-Net\nencoder features via feature-wise modulation, dynamic convolutions, and\ncross-attention mechanisms. The model is trained using a combination of\nspatially weighted and gradient-sensitive loss functions, enabling it to\napproximate high-fidelity wavefields. The TFUScapes dataset is publicly\nreleased to accelerate research at the intersection of computational acoustics,\nneurotechnology, and deep learning. The project page is available at\nhttps://github.com/CAMMA-public/TFUScapes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13023", "pdf": "https://arxiv.org/pdf/2505.13023", "abs": "https://arxiv.org/abs/2505.13023", "authors": ["Yimao Guo", "Zuomin Qu", "Wei Lu", "Xiangyang Luo"], "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13050", "pdf": "https://arxiv.org/pdf/2505.13050", "abs": "https://arxiv.org/abs/2505.13050", "authors": ["Beibei Lin", "Zifeng Yuan", "Tingting Chen"], "title": "RGB-to-Polarization Estimation: A New Task and Benchmark Study", "categories": ["cs.CV"], "comment": null, "summary": "Polarization images provide rich physical information that is fundamentally\nabsent from standard RGB images, benefiting a wide range of computer vision\napplications such as reflection separation and material classification.\nHowever, the acquisition of polarization images typically requires additional\noptical components, which increases both the cost and the complexity of the\napplications. To bridge this gap, we introduce a new task: RGB-to-polarization\nimage estimation, which aims to infer polarization information directly from\nRGB images. In this work, we establish the first comprehensive benchmark for\nthis task by leveraging existing polarization datasets and evaluating a diverse\nset of state-of-the-art deep learning models, including both\nrestoration-oriented and generative architectures. Through extensive\nquantitative and qualitative analysis, our benchmark not only establishes the\ncurrent performance ceiling of RGB-to-polarization estimation, but also\nsystematically reveals the respective strengths and limitations of different\nmodel families -- such as direct reconstruction versus generative synthesis,\nand task-specific training versus large-scale pre-training. In addition, we\nprovide some potential directions for future research on polarization\nestimation. This benchmark is intended to serve as a foundational resource to\nfacilitate the design and evaluation of future methods for polarization\nestimation from standard RGB inputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13156", "pdf": "https://arxiv.org/pdf/2505.13156", "abs": "https://arxiv.org/abs/2505.13156", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 4 figures, and 1 tables", "summary": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13171", "pdf": "https://arxiv.org/pdf/2505.13171", "abs": "https://arxiv.org/abs/2505.13171", "authors": ["Yixuan Xu", "Antoine Bosselut", "Imanol Schlag"], "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13173", "pdf": "https://arxiv.org/pdf/2505.13173", "abs": "https://arxiv.org/abs/2505.13173", "authors": ["V. S. D. S. Mahesh Akavarapu", "Hrishikesh Terdalkar", "Pramit Bhattacharyya", "Shubhangi Agarwal", "Vishakha Deulgaonkar", "Pralay Manna", "Chaitali Dangarikar", "Arnab Bhattacharya"], "title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs", "categories": ["cs.CL", "I.2.7"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13140", "pdf": "https://arxiv.org/pdf/2505.13140", "abs": "https://arxiv.org/abs/2505.13140", "authors": ["Takahiro Maeda", "Jinkun Cao", "Norimichi Ukita", "Kris Kitani"], "title": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow", "categories": ["cs.CV"], "comment": null, "summary": "Many density estimation techniques for 3D human motion prediction require a\nsignificant amount of inference time, often exceeding the duration of the\npredicted time horizon. To address the need for faster density estimation for\n3D human motion prediction, we introduce a novel flow-based method for human\nmotion prediction called CacheFlow. Unlike previous conditional generative\nmodels that suffer from time efficiency, CacheFlow takes advantage of an\nunconditional flow-based generative model that transforms a Gaussian mixture\ninto the density of future motions. The results of the computation of the\nflow-based generative model can be precomputed and cached. Then, for\nconditional prediction, we seek a mapping from historical trajectories to\nsamples in the Gaussian mixture. This mapping can be done by a much more\nlightweight model, thus saving significant computation overhead compared to a\ntypical conditional flow model. In such a two-stage fashion and by caching\nresults from the slow flow model computation, we build our CacheFlow without\nloss of prediction accuracy and model expressiveness. This inference process is\ncompleted in approximately one millisecond, making it 4 times faster than\nprevious VAE methods and 30 times faster than previous diffusion-based methods\non standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our\nmethod demonstrates improved density estimation accuracy and comparable\nprediction accuracy to a SOTA method on Human3.6M. Our code and models will be\npublicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13252", "pdf": "https://arxiv.org/pdf/2505.13252", "abs": "https://arxiv.org/abs/2505.13252", "authors": ["Rikhil Amonkar", "Ronan Le Bras", "Li Zhang"], "title": "Natural Language Planning via Coding and Inference Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Real-life textual planning tasks such as meeting scheduling have posed much\nchallenge to LLMs especially when the complexity is high. While previous work\nprimarily studied auto-regressive generation of plans with closed-source\nmodels, we systematically evaluate both closed- and open-source models,\nincluding those that scales output length with complexity during inference, in\ngenerating programs, which are executed to output the plan. We consider not\nonly standard Python code, but also the code to a constraint satisfaction\nproblem solver. Despite the algorithmic nature of the task, we show that\nprogramming often but not always outperforms planning. Our detailed error\nanalysis also indicates a lack of robustness and efficiency in the generated\ncode that hinders generalization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13201", "pdf": "https://arxiv.org/pdf/2505.13201", "abs": "https://arxiv.org/abs/2505.13201", "authors": ["Yuzhen Chen", "Hojun Son", "Arpan Kusari"], "title": "MatPredict: a dataset and benchmark for learning material properties of diverse indoor objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Determining material properties from camera images can expand the ability to\nidentify complex objects in indoor environments, which is valuable for consumer\nrobotics applications. To support this, we introduce MatPredict, a dataset that\ncombines the high-quality synthetic objects from Replica dataset with MatSynth\ndataset's material properties classes - to create objects with diverse material\nproperties. We select 3D meshes of specific foreground objects and render them\nwith different material properties. In total, we generate \\textbf{18} commonly\noccurring objects with \\textbf{14} different materials. We showcase how we\nprovide variability in terms of lighting and camera placement for these\nobjects. Next, we provide a benchmark for inferring material properties from\nvisual images using these perturbed models in the scene, discussing the\nspecific neural network models involved and their performance based on\ndifferent image comparison metrics. By accurately simulating light interactions\nwith different materials, we can enhance realism, which is crucial for training\nmodels effectively through large-scale simulations. This research aims to\nrevolutionize perception in consumer robotics. The dataset is provided\n\\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is\nprovided \\href{https://github.com/arpan-kusari/MatPredict}{here}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13211", "pdf": "https://arxiv.org/pdf/2505.13211", "abs": "https://arxiv.org/abs/2505.13211", "authors": ["Sand. ai", "Hansi Teng", "Hongyu Jia", "Lei Sun", "Lingzhi Li", "Maolin Li", "Mingqiu Tang", "Shuai Han", "Tianning Zhang", "W. Q. Zhang", "Weifeng Luo", "Xiaoyang Kang", "Yuchen Sun", "Yue Cao", "Yunpeng Huang", "Yutong Lin", "Yuxin Fang", "Zewei Tao", "Zheng Zhang", "Zhongshu Wang", "Zixun Liu", "Dai Shi", "Guoli Su", "Hanwen Sun", "Hong Pan", "Jie Wang", "Jiexin Sheng", "Min Cui", "Min Hu", "Ming Yan", "Shucheng Yin", "Siran Zhang", "Tingting Liu", "Xianping Yin", "Xiaoyu Yang", "Xin Song", "Xuan Hu", "Yankai Zhang", "Yuqiao Li"], "title": "MAGI-1: Autoregressive Video Generation at Scale", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13258", "pdf": "https://arxiv.org/pdf/2505.13258", "abs": "https://arxiv.org/abs/2505.13258", "authors": ["Jingyi Ren", "Yekun Xu", "Xiaolong Wang", "Weitao Li", "Weizhi Ma", "Yang Liu"], "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13235", "pdf": "https://arxiv.org/pdf/2505.13235", "abs": "https://arxiv.org/abs/2505.13235", "authors": ["Dang Hoai Nam", "Huynh Tong Dang Khoa", "Vo Nguyen Le Duy"], "title": "WriteViT: Handwritten Text Generation with Vision Transformer", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Humans can quickly generalize handwriting styles from a single example by\nintuitively separating content from style. Machines, however, struggle with\nthis task, especially in low-data settings, often missing subtle spatial and\nstylistic cues. Motivated by this gap, we introduce WriteViT, a one-shot\nhandwritten text synthesis framework that incorporates Vision Transformers\n(ViT), a family of models that have shown strong performance across various\ncomputer vision tasks. WriteViT integrates a ViT-based Writer Identifier for\nextracting style embeddings, a multi-scale generator built with Transformer\nencoder-decoder blocks enhanced by conditional positional encoding (CPE), and a\nlightweight ViT-based recognizer. While previous methods typically rely on CNNs\nor CRNNs, our design leverages transformers in key components to better capture\nboth fine-grained stroke details and higher-level style information. Although\nhandwritten text synthesis has been widely explored, its application to\nVietnamese -- a language rich in diacritics and complex typography -- remains\nlimited. Experiments on Vietnamese and English datasets demonstrate that\nWriteViT produces high-quality, style-consistent handwriting while maintaining\nstrong recognition performance in low-resource scenarios. These results\nhighlight the promise of transformer-based designs for multilingual handwriting\ngeneration and efficient style adaptation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13312", "pdf": "https://arxiv.org/pdf/2505.13312", "abs": "https://arxiv.org/abs/2505.13312", "authors": ["Zhijie Deng", "Chris Yuhao Liu", "Zirui Pang", "Xinlei He", "Lei Feng", "Qi Xuan", "Zhaowei Zhu", "Jiaheng Wei"], "title": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmemorizing vast amounts of knowledge across diverse domains. However, the\nability to selectively forget specific knowledge is critical for ensuring the\nsafety and compliance of deployed models. Existing unlearning efforts typically\nfine-tune the model with resources such as forget data, retain data, and a\ncalibration model. These additional gradient steps blur the decision boundary\nbetween forget and retain knowledge, making unlearning often at the expense of\noverall performance. To avoid the negative impact of fine-tuning, it would be\nbetter to unlearn solely at inference time by safely guarding the model against\ngenerating responses related to the forget target, without destroying the\nfluency of text generation. In this work, we propose Generation-time Unlearning\nvia Adaptive Restriction and Detection (GUARD), a framework that enables\ndynamic unlearning during LLM generation. Specifically, we first employ a\nprompt classifier to detect unlearning targets and extract the corresponding\nforbidden token. We then dynamically penalize and filter candidate tokens\nduring generation using a combination of token matching and semantic matching,\neffectively preventing the model from leaking the forgotten content.\nExperimental results on copyright content unlearning tasks over the Harry\nPotter dataset and the MUSE benchmark, as well as entity unlearning tasks on\nthe TOFU dataset, demonstrate that GUARD achieves strong forget quality across\nvarious tasks while causing almost no degradation to the LLM's general\ncapabilities, striking an excellent trade-off between forgetting and utility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13300", "pdf": "https://arxiv.org/pdf/2505.13300", "abs": "https://arxiv.org/abs/2505.13300", "authors": ["Zekai Li", "Xinhao Zhong", "Samir Khaki", "Zhiyuan Liang", "Yuhao Zhou", "Mingjia Shi", "Ziqiao Wang", "Xuanlei Zhao", "Wangbo Zhao", "Ziheng Qin", "Mengxuan Wu", "Pengfei Zhou", "Haonan Wang", "David Junhao Zhang", "Jia-Wei Liu", "Shaobo Wang", "Dai Liu", "Linfeng Zhang", "Guang Li", "Kun Wang", "Zheng Zhu", "Zhiheng Ma", "Joey Tianyi Zhou", "Jiancheng Lv", "Yaochu Jin", "Peihao Wang", "Kaipeng Zhang", "Lingjuan Lyu", "Yiran Huang", "Zeynep Akata", "Zhiwei Deng", "Xindi Wu", "George Cazenavette", "Yuzhang Shang", "Justin Cui", "Jindong Gu", "Qian Zheng", "Hao Ye", "Shuo Wang", "Xiaobo Wang", "Yan Yan", "Angela Yao", "Mike Zheng Shou", "Tianlong Chen", "Hakan Bilen", "Baharan Mirzasoleiman", "Manolis Kellis", "Konstantinos N. Plataniotis", "Zhangyang Wang", "Bo Zhao", "Yang You", "Kai Wang"], "title": "DD-Ranking: Rethinking the Evaluation of Dataset Distillation", "categories": ["cs.CV"], "comment": "20 pages, 4 figures", "summary": "In recent years, dataset distillation has provided a reliable solution for\ndata compression, where models trained on the resulting smaller synthetic\ndatasets achieve performance comparable to those trained on the original\ndatasets. To further improve the performance of synthetic datasets, various\ntraining pipelines and optimization objectives have been proposed, greatly\nadvancing the field of dataset distillation. Recent decoupled dataset\ndistillation methods introduce soft labels and stronger data augmentation\nduring the post-evaluation phase and scale dataset distillation up to larger\ndatasets (e.g., ImageNet-1K). However, this raises a question: Is accuracy\nstill a reliable metric to fairly evaluate dataset distillation methods? Our\nempirical findings suggest that the performance improvements of these methods\noften stem from additional techniques rather than the inherent quality of the\nimages themselves, with even randomly sampled images achieving superior\nresults. Such misaligned evaluation settings severely hinder the development of\nDD. Therefore, we propose DD-Ranking, a unified evaluation framework, along\nwith new general evaluation metrics to uncover the true performance\nimprovements achieved by different methods. By refocusing on the actual\ninformation enhancement of distilled datasets, DD-Ranking provides a more\ncomprehensive and fair evaluation standard for future research advancements.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13404", "pdf": "https://arxiv.org/pdf/2505.13404", "abs": "https://arxiv.org/abs/2505.13404", "authors": ["Nithin Rao Koluguri", "Monica Sekoyan", "George Zelenfroynd", "Sasha Meister", "Shuoyang Ding", "Sofia Kostandian", "He Huang", "Nikolay Karpov", "Jagadeesh Balam", "Vitaly Lavrukhin", "Yifan Peng", "Sara Papi", "Marco Gaido", "Alessio Brutti", "Boris Ginsburg"], "title": "Granary: Speech Recognition and Translation Dataset in 25 European Languages", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Multi-task and multilingual approaches benefit large models, yet speech\nprocessing for low-resource languages remains underexplored due to data\nscarcity. To address this, we present Granary, a large-scale collection of\nspeech datasets for recognition and translation across 25 European languages.\nThis is the first open-source effort at this scale for both transcription and\ntranslation. We enhance data quality using a pseudo-labeling pipeline with\nsegmentation, two-pass inference, hallucination filtering, and punctuation\nrestoration. We further generate translation pairs from pseudo-labeled\ntranscriptions using EuroLLM, followed by a data filtration pipeline. Designed\nfor efficiency, our pipeline processes vast amount of data within hours. We\nassess models trained on processed data by comparing their performance on\npreviously curated datasets for both high- and low-resource languages. Our\nfindings show that these models achieve similar performance using approx. 50%\nless data. Dataset will be made available at\nhttps://hf.co/datasets/nvidia/Granary", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13389", "pdf": "https://arxiv.org/pdf/2505.13389", "abs": "https://arxiv.org/abs/2505.13389", "authors": ["Peiyuan Zhang", "Haofeng Huang", "Yongqi Chen", "Will Lin", "Zhengzhong Liu", "Ion Stoica", "Eric P. Xing", "Hao Zhang"], "title": "Faster Video Diffusion with Trainable Sparse Attention", "categories": ["cs.CV"], "comment": null, "summary": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13417", "pdf": "https://arxiv.org/pdf/2505.13417", "abs": "https://arxiv.org/abs/2505.13417", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "title": "AdaptThink: Reasoning Models Can Learn When to Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13429", "pdf": "https://arxiv.org/pdf/2505.13429", "abs": "https://arxiv.org/abs/2505.13429", "authors": ["Cristobal Eyzaguirre", "Igor Vasiljevic", "Achal Dave", "Jiajun Wu", "Rares Andrei Ambrus", "Thomas Kollar", "Juan Carlos Niebles", "Pavel Tokmakov"], "title": "Understanding Complexity in VideoQA via Visual Program Generation", "categories": ["cs.CV"], "comment": null, "summary": "We propose a data-driven approach to analyzing query complexity in Video\nQuestion Answering (VideoQA). Previous efforts in benchmark design have relied\non human expertise to design challenging questions, yet we experimentally show\nthat humans struggle to predict which questions are difficult for machine\nlearning models. Our automatic approach leverages recent advances in code\ngeneration for visual question answering, using the complexity of generated\ncode as a proxy for question difficulty. We demonstrate that this measure\ncorrelates significantly better with model performance than human estimates. To\noperationalize this insight, we propose an algorithm for estimating question\ncomplexity from code. It identifies fine-grained primitives that correlate with\nthe hardest questions for any given set of models, making it easy to scale to\nnew approaches in the future. Finally, to further illustrate the utility of our\nmethod, we extend it to automatically generate complex questions, constructing\na new benchmark that is 1.9 times harder than the popular NExT-QA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering", "fine-grained"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13448", "pdf": "https://arxiv.org/pdf/2505.13448", "abs": "https://arxiv.org/abs/2505.13448", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13437", "pdf": "https://arxiv.org/pdf/2505.13437", "abs": "https://arxiv.org/abs/2505.13437", "authors": ["Dian Shao", "Mingfei Shi", "Shengda Xu", "Haodong Chen", "Yongle Huang", "Binglu Wang"], "title": "FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Despite significant advances in video generation, synthesizing physically\nplausible human actions remains a persistent challenge, particularly in\nmodeling fine-grained semantics and complex temporal dynamics. For instance,\ngenerating gymnastics routines such as \"switch leap with 0.5 turn\" poses\nsubstantial difficulties for current methods, often yielding unsatisfactory\nresults. To bridge this gap, we propose FinePhys, a Fine-grained human action\ngeneration framework that incorporates Physics to obtain effective skeletal\nguidance. Specifically, FinePhys first estimates 2D poses in an online manner\nand then performs 2D-to-3D dimension lifting via in-context learning. To\nmitigate the instability and limited interpretability of purely data-driven 3D\nposes, we further introduce a physics-based motion re-estimation module\ngoverned by Euler-Lagrange equations, calculating joint accelerations via\nbidirectional temporal updating. The physically predicted 3D poses are then\nfused with data-driven ones, offering multi-scale 2D heatmap guidance for the\ndiffusion process. Evaluated on three fine-grained action subsets from FineGym\n(FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms\ncompetitive baselines. Comprehensive qualitative results further demonstrate\nFinePhys's ability to generate more natural and plausible fine-grained human\nactions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained", "dimension"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13440", "pdf": "https://arxiv.org/pdf/2505.13440", "abs": "https://arxiv.org/abs/2505.13440", "authors": ["Ruoyu Wang", "Yi Ma", "Shenghua Gao"], "title": "Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos", "categories": ["cs.CV"], "comment": "13 pages, 4 figures", "summary": "Currently almost all state-of-the-art novel view synthesis and reconstruction\nmodels rely on calibrated cameras or additional geometric priors for training.\nThese prerequisites significantly limit their applicability to massive\nuncalibrated data. To alleviate this requirement and unlock the potential for\nself-supervised training on large-scale uncalibrated videos, we propose a novel\ntwo-stage strategy to train a view synthesis model from only raw video frames\nor multi-view images, without providing camera parameters or other priors. In\nthe first stage, we learn to reconstruct the scene implicitly in a latent space\nwithout relying on any explicit 3D representation. Specifically, we predict\nper-frame latent camera and scene context features, and employ a view synthesis\nmodel as a proxy for explicit rendering. This pretraining stage substantially\nreduces the optimization complexity and encourages the network to learn the\nunderlying 3D consistency in a self-supervised manner. The learned latent\ncamera and implicit scene representation have a large gap compared with the\nreal 3D world. To reduce this gap, we introduce the second stage training by\nexplicitly predicting 3D Gaussian primitives. We additionally apply explicit\nGaussian Splatting rendering loss and depth projection loss to align the\nlearned latent representations with physically grounded 3D geometry. In this\nway, Stage 1 provides a strong initialization and Stage 2 enforces 3D\nconsistency - the two stages are complementary and mutually beneficial.\nExtensive experiments demonstrate the effectiveness of our approach, achieving\nhigh-quality novel view synthesis and accurate camera pose estimation, compared\nto methods that employ supervision with calibration, pose, or depth\ninformation. The code is available at https://github.com/Dwawayu/Pensieve.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11614", "pdf": "https://arxiv.org/pdf/2505.11614", "abs": "https://arxiv.org/abs/2505.11614", "authors": ["Jian-Qiao Zhu", "Hanbo Xie", "Dilip Arumugam", "Robert C. Wilson", "Thomas L. Griffiths"], "title": "Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "A central goal of cognitive modeling is to develop models that not only\npredict human behavior but also provide insight into the underlying cognitive\nmechanisms. While neural network models trained on large-scale behavioral data\noften achieve strong predictive performance, they typically fall short in\noffering interpretable explanations of the cognitive processes they capture. In\nthis work, we explore the potential of pretrained large language models (LLMs)\nto serve as dual-purpose cognitive models--capable of both accurate prediction\nand interpretable explanation in natural language. Specifically, we employ\nreinforcement learning with outcome-based rewards to guide LLMs toward\ngenerating explicit reasoning traces for explaining human risky choices. Our\nfindings demonstrate that this approach produces high-quality explanations\nalongside strong quantitative predictions of human decisions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11731", "pdf": "https://arxiv.org/pdf/2505.11731", "abs": "https://arxiv.org/abs/2505.11731", "authors": ["Harshil Vejendla", "Haizhou Shi", "Yibin Wang", "Tunyu Zhang", "Huan Zhang", "Hao Wang"], "title": "Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; work in progress", "summary": "Recent advances in uncertainty estimation for Large Language Models (LLMs)\nduring downstream adaptation have addressed key challenges of reliability and\nsimplicity. However, existing Bayesian methods typically require multiple\nsampling iterations during inference, creating significant efficiency issues\nthat limit practical deployment. In this paper, we investigate the possibility\nof eliminating the need for test-time sampling for LLM uncertainty estimation.\nSpecifically, when given an off-the-shelf Bayesian LLM, we distill its aligned\nconfidence into a non-Bayesian student LLM by minimizing the divergence between\ntheir predictive distributions. Unlike typical calibration methods, our\ndistillation is carried out solely on the training dataset without the need of\nan additional validation dataset. This simple yet effective approach achieves\nN-times more efficient uncertainty estimation during testing, where N is the\nnumber of samples traditionally required by Bayesian LLMs. Our extensive\nexperiments demonstrate that uncertainty estimation capabilities on training\ndata can successfully generalize to unseen test data through our distillation\ntechnique, consistently producing results comparable to (or even better than)\nstate-of-the-art Bayesian LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11737", "pdf": "https://arxiv.org/pdf/2505.11737", "abs": "https://arxiv.org/abs/2505.11737", "authors": ["Tunyu Zhang", "Haizhou Shi", "Yibin Wang", "Hengyi Wang", "Xiaoxiao He", "Zhuowei Li", "Haoxian Chen", "Ligong Han", "Kai Xu", "Huan Zhang", "Dimitris Metaxas", "Hao Wang"], "title": "Token-Level Uncertainty Estimation for Large Language Model Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; Work in progress", "summary": "While Large Language Models (LLMs) have demonstrated impressive capabilities,\ntheir output quality remains inconsistent across various application scenarios,\nmaking it difficult to identify trustworthy responses, especially in complex\ntasks requiring multi-step reasoning. In this paper, we propose a token-level\nuncertainty estimation framework to enable LLMs to self-assess and self-improve\ntheir generation quality in mathematical reasoning. Specifically, we introduce\nlow-rank random weight perturbation to LLM decoding, generating predictive\ndistributions that we use to estimate token-level uncertainties. We then\naggregate these uncertainties to reflect semantic uncertainty of the generated\nsequences. Experiments on mathematical reasoning datasets of varying difficulty\ndemonstrate that our token-level uncertainty metrics strongly correlate with\nanswer correctness and model robustness. Additionally, we explore using\nuncertainty to directly enhance the model's reasoning performance through\nmultiple generations and the particle filtering algorithm. Our approach\nconsistently outperforms existing uncertainty estimation methods, establishing\neffective uncertainty estimation as a valuable tool for both evaluating and\nimproving reasoning generation in LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11756", "pdf": "https://arxiv.org/pdf/2505.11756", "abs": "https://arxiv.org/abs/2505.11756", "authors": ["David Chanin", "Tom√°≈° Dulka", "Adri√† Garriga-Alonso"], "title": "Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "It is assumed that sparse autoencoders (SAEs) decompose polysemantic\nactivations into interpretable linear directions, as long as the activations\nare composed of sparse linear combinations of underlying features. However, we\nfind that if an SAE is more narrow than the number of underlying \"true\nfeatures\" on which it is trained, and there is correlation between features,\nthe SAE will merge components of correlated features together, thus destroying\nmonosemanticity. In LLM SAEs, these two conditions are almost certainly true.\nThis phenomenon, which we call feature hedging, is caused by SAE reconstruction\nloss, and is more severe the narrower the SAE. In this work, we introduce the\nproblem of feature hedging and study it both theoretically in toy models and\nempirically in SAEs trained on LLMs. We suspect that feature hedging may be one\nof the core reasons that SAEs consistently underperform supervised baselines.\nFinally, we use our understanding of feature hedging to propose an improved\nvariant of matryoshka SAEs. Our work shows there remain fundamental issues with\nSAEs, but we are hopeful that that highlighting feature hedging will catalyze\nfuture advances that allow SAEs to achieve their full potential of interpreting\nLLMs at scale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11812", "pdf": "https://arxiv.org/pdf/2505.11812", "abs": "https://arxiv.org/abs/2505.11812", "authors": ["Yang Tan", "Wenrui Gou", "Bozitao Zhong", "Liang Hong", "Huiqun Yu", "Bingxin Zhou"], "title": "VenusX: Unlocking Fine-Grained Functional Understanding of Proteins", "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "comment": "29 pages, 3 figures, 17 tables", "summary": "Deep learning models have driven significant progress in predicting protein\nfunction and interactions at the protein level. While these advancements have\nbeen invaluable for many biological applications such as enzyme engineering and\nfunction annotation, a more detailed perspective is essential for understanding\nprotein functional mechanisms and evaluating the biological knowledge captured\nby models. To address this demand, we introduce VenusX, the first large-scale\nbenchmark for fine-grained functional annotation and function-based protein\npairing at the residue, fragment, and domain levels. VenusX comprises three\nmajor task categories across six types of annotations, including residue-level\nbinary classification, fragment-level multi-class classification, and pairwise\nfunctional similarity scoring for identifying critical active sites, binding\nsites, conserved sites, motifs, domains, and epitopes. The benchmark features\nover 878,000 samples curated from major open-source databases such as InterPro,\nBioLiP, and SAbDab. By providing mixed-family and cross-family splits at three\nsequence identity thresholds, our benchmark enables a comprehensive assessment\nof model performance on both in-distribution and out-of-distribution scenarios.\nFor baseline evaluation, we assess a diverse set of popular and open-source\nmodels, including pre-trained protein language models, sequence-structure\nhybrids, structure-based methods, and alignment-based techniques. Their\nperformance is reported across all benchmark datasets and evaluation settings\nusing multiple metrics, offering a thorough comparison and a strong foundation\nfor future research. Code and data are publicly available at\nhttps://github.com/ai4protein/VenusX.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "pairwise", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "annotation", "fine-grained"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11865", "pdf": "https://arxiv.org/pdf/2505.11865", "abs": "https://arxiv.org/abs/2505.11865", "authors": ["Teli Ma", "Jia Zheng", "Zifan Wang", "Ziyao Gao", "Jiaming Zhou", "Junwei Liang"], "title": "GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Learning manipulation skills from human demonstration videos offers a\npromising path toward generalizable and interpretable robotic\nintelligence-particularly through the lens of actionable affordances. However,\ntransferring such knowledge remains challenging due to: 1) a lack of\nlarge-scale datasets with precise affordance annotations, and 2) insufficient\nexploration of affordances in diverse manipulation contexts. To address these\ngaps, we introduce HOVA-500K, a large-scale, affordance-annotated dataset\ncomprising 500,000 images across 1,726 object categories and 675 actions. We\nalso release a standardized benchmarking suite for multi-modal affordance\nreasoning. Built upon HOVA-500K, we present GLOVER++, a global-to-local\naffordance training framework that effectively transfers actionable affordance\nknowledge from human demonstrations to downstream open-vocabulary reasoning\ntasks. GLOVER++ achieves state-of-the-art results on the HOVA-500K benchmark\nand demonstrates strong generalization across diverse downstream robotic\nmanipulation tasks. By explicitly modeling actionable affordances, GLOVER++\nfacilitates robust transfer across scenes, modalities, and tasks. We hope that\nHOVA-500K and the GLOVER++ framework will serve as valuable resources for\nbridging the gap between human demonstrations and robotic manipulation\ncapabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11979", "pdf": "https://arxiv.org/pdf/2505.11979", "abs": "https://arxiv.org/abs/2505.11979", "authors": ["Tarik Houichime", "Younes El Amrani"], "title": "Introduction to Analytical Software Engineering Design Paradigm", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MS", "cs.PL"], "comment": "The Conference's autorization to submit a preprint was granted", "summary": "As modern software systems expand in scale and complexity, the challenges\nassociated with their modeling and formulation grow increasingly intricate.\nTraditional approaches often fall short in effectively addressing these\ncomplexities, particularly in tasks such as design pattern detection for\nmaintenance and assessment, as well as code refactoring for optimization and\nlong-term sustainability. This growing inadequacy underscores the need for a\nparadigm shift in how such challenges are approached and resolved. This paper\npresents Analytical Software Engineering (ASE), a novel design paradigm aimed\nat balancing abstraction, tool accessibility, compatibility, and scalability.\nASE enables effective modeling and resolution of complex software engineering\nproblems. The paradigm is evaluated through two frameworks\nBehavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),\nboth developed in accordance with ASE principles. BSS offers a compact,\nlanguage-agnostic representation of codebases to facilitate precise design\npattern detection. ODR unifies artifact and solution representations to\noptimize code refactoring via heuristic algorithms while eliminating iterative\ncomputational overhead. By providing a structured approach to software design\nchallenges, ASE lays the groundwork for future research in encoding and\nanalyzing complex software metrics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12039", "pdf": "https://arxiv.org/pdf/2505.12039", "abs": "https://arxiv.org/abs/2505.12039", "authors": ["Renqi Chen", "Haoyang Su", "Shixiang Tang", "Zhenfei Yin", "Qi Wu", "Hui Li", "Ye Sun", "Nanqing Dong", "Wanli Ouyang", "Philip Torr"], "title": "AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research", "categories": ["cs.AI", "cs.CL", "physics.soc-ph"], "comment": null, "summary": "The Science of Science (SoS) explores the mechanisms underlying scientific\ndiscovery, and offers valuable insights for enhancing scientific efficiency and\nfostering innovation. Traditional approaches often rely on simplistic\nassumptions and basic statistical tools, such as linear regression and\nrule-based simulations, which struggle to capture the complexity and scale of\nmodern research ecosystems. The advent of artificial intelligence (AI) presents\na transformative opportunity for the next generation of SoS, enabling the\nautomation of large-scale pattern discovery and uncovering insights previously\nunattainable. This paper offers a forward-looking perspective on the\nintegration of Science of Science with AI for automated research pattern\ndiscovery and highlights key open challenges that could greatly benefit from\nAI. We outline the advantages of AI over traditional methods, discuss potential\nlimitations, and propose pathways to overcome them. Additionally, we present a\npreliminary multi-agent system as an illustrative example to simulate research\nsocieties, showcasing AI's ability to replicate real-world research patterns\nand accelerate progress in Science of Science research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11909", "pdf": "https://arxiv.org/pdf/2505.11909", "abs": "https://arxiv.org/abs/2505.11909", "authors": ["Pengfei Lyu", "Pak-Hei Yeung", "Xiaosheng Yu", "Jing Xia", "Jianning Chi", "Chengdong Wu", "Jagath C. Rajapakse"], "title": "Bridging the Inter-Domain Gap through Low-Level Features for Cross-Modal Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 2 figures", "summary": "This paper addresses the task of cross-modal medical image segmentation by\nexploring unsupervised domain adaptation (UDA) approaches. We propose a\nmodel-agnostic UDA framework, LowBridge, which builds on a simple observation\nthat cross-modal images share some similar low-level features (e.g., edges) as\nthey are depicting the same structures. Specifically, we first train a\ngenerative model to recover the source images from their edge features,\nfollowed by training a segmentation model on the generated source images,\nseparately. At test time, edge features from the target images are input to the\npretrained generative model to generate source-style target domain images,\nwhich are then segmented using the pretrained segmentation network. Despite its\nsimplicity, extensive experiments on various publicly available datasets\ndemonstrate that \\proposed achieves state-of-the-art performance, outperforming\neleven existing UDA approaches under different settings. Notably, further\nablation studies show that \\proposed is agnostic to different types of\ngenerative and segmentation models, suggesting its potential to be seamlessly\nplugged with the most advanced models to achieve even more outstanding results\nin the future. The code is available at https://github.com/JoshuaLPF/LowBridge.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12058", "pdf": "https://arxiv.org/pdf/2505.12058", "abs": "https://arxiv.org/abs/2505.12058", "authors": ["Vincent Koc"], "title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.6; H.2.8"], "comment": "28 pages, 7 figures, 3 tables. Includes expanded appendix & full\n  score matrices. Dataset & code: HF Hub + GitHub + Pypi links in abstract.\n  Core data and code Apache-2.0; synthetic packs eval-only", "summary": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual\nsmoke-test suite designed to give large-language-model (LLM) pipelines a\nunit-test style safety net dataset that runs in seconds with minimal cost. Born\nout of the tight feedback-loop demands building the Comet Opik\nprompt-optimization SDK, where waiting on heavyweight benchmarks breaks\ndeveloper flow. TQB++ couples a 52-item English gold set (less than 20 kB) with\na tiny synthetic-data generator pypi package built on provider-agnostic\nLiteLLM. The generator lets practitioners mint their own tiny packs in any\nlanguage, domain, or difficulty, while ten ready-made packs already cover\nArabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,\nSpanish, and Turkish. Every dataset ships with Croissant metadata and\nplug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so\nteams can drop deterministic micro-benchmarks directly into pull-request gates,\nprompt-engineering loops, and production dashboards without touching GPU\nbudgets. A complete TQB++ run adds only a few seconds to pipeline latency yet\nreliably flags prompt-template errors, tokenizer drift, and fine-tuning\nside-effects long before full-scale suites like MMLU or BIG-Bench would finish\nconfiguring. The entire framework is released to accelerate continuous,\nresource-efficient quality assurance across the generative-AI ecosystem.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12065", "pdf": "https://arxiv.org/pdf/2505.12065", "abs": "https://arxiv.org/abs/2505.12065", "authors": ["Tiannuo Yang", "Zebin Yao", "Bowen Jin", "Lixiao Cui", "Yusen Li", "Gang Wang", "Xiaoguang Liu"], "title": "Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM)-based search agents have shown remarkable\ncapabilities in solving complex tasks by dynamically decomposing problems and\naddressing them through interleaved reasoning and retrieval. However, this\ninterleaved paradigm introduces substantial efficiency bottlenecks. First, we\nobserve that both highly accurate and overly approximate retrieval methods\ndegrade system efficiency: exact search incurs significant retrieval overhead,\nwhile coarse retrieval requires additional reasoning steps during generation.\nSecond, we identify inefficiencies in system design, including improper\nscheduling and frequent retrieval stalls, which lead to cascading latency --\nwhere even minor delays in retrieval amplify end-to-end inference time. To\naddress these challenges, we introduce SearchAgent-X, a high-efficiency\ninference framework for LLM-based search agents. SearchAgent-X leverages\nhigh-recall approximate retrieval and incorporates two key techniques:\npriority-aware scheduling and non-stall retrieval. Extensive experiments\ndemonstrate that SearchAgent-X consistently outperforms state-of-the-art\nsystems such as vLLM and HNSW-based retrieval across diverse tasks, achieving\nup to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without\ncompromising generation quality. SearchAgent-X is available at\nhttps://github.com/tiannuo-yang/SearchAgent-X.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12061", "pdf": "https://arxiv.org/pdf/2505.12061", "abs": "https://arxiv.org/abs/2505.12061", "authors": ["Samuel T. M. Ball"], "title": "Bayesian Deep Learning Approaches for Uncertainty-Aware Retinal OCT Image Segmentation for Multiple Sclerosis", "categories": ["eess.IV", "cs.CV", "68U10, 92C55", "I.2.10; I.4.6; J.3"], "comment": null, "summary": "Optical Coherence Tomography (OCT) provides valuable insights in\nophthalmology, cardiology, and neurology due to high-resolution,\ncross-sectional images of the retina. One critical task for ophthalmologists\nusing OCT is delineation of retinal layers within scans. This process is\ntime-consuming and prone to human bias, affecting the accuracy and reliability\nof diagnoses. Previous efforts to automate delineation using deep learning face\nchallenges in uptake from clinicians and statisticians due to the absence of\nuncertainty estimation, leading to \"confidently wrong\" models via\nhallucinations. In this study, we address these challenges by applying Bayesian\nconvolutional neural networks (BCNNs) to segment an openly available OCT\nimaging dataset containing 35 human retina OCTs split between healthy controls\nand patients with multiple sclerosis. Our findings demonstrate that Bayesian\nmodels can be used to provide uncertainty maps of the segmentation, which can\nfurther be used to identify highly uncertain samples that exhibit recording\nartefacts such as noise or miscalibration at inference time. Our method also\nallows for uncertainty-estimation for important secondary measurements such as\nlayer thicknesses, that are medically relevant for patients. We show that these\nfeatures come in addition to greater performance compared to similar work over\nall delineations; with an overall Dice score of 95.65%. Our work brings greater\nclinical applicability, statistical robustness, and performance to retinal OCT\nsegmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12189", "pdf": "https://arxiv.org/pdf/2505.12189", "abs": "https://arxiv.org/abs/2505.12189", "authors": ["Marco Valentino", "Geonhee Kim", "Dhairya Dalal", "Zhixue Zhao", "Andr√© Freitas"], "title": "Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering", "categories": ["cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) frequently demonstrate reasoning limitations,\noften conflating content plausibility (i.e., material inference) with logical\nvalidity (i.e., formal inference). This can result in biased inferences, where\nplausible arguments are incorrectly deemed logically valid or vice versa.\nMitigating this limitation is critical, as it undermines the trustworthiness\nand generalizability of LLMs in applications that demand rigorous logical\nconsistency. This paper investigates the problem of mitigating content biases\non formal reasoning through activation steering. Specifically, we curate a\ncontrolled syllogistic reasoning dataset to disentangle formal validity from\ncontent plausibility. After localising the layers responsible for formal and\nmaterial inference, we investigate contrastive activation steering methods for\ntest-time interventions. An extensive empirical analysis on different LLMs\nreveals that contrastive steering consistently supports linear control over\ncontent biases. However, we observe that a static approach is insufficient for\nimproving all the tested models. We then leverage the possibility to control\ncontent effects by dynamically determining the value of the steering parameters\nvia fine-grained conditional methods. We found that conditional steering is\neffective on unresponsive models, achieving up to 15% absolute improvement in\nformal reasoning accuracy with a newly introduced kNN-based method (K-CAST).\nFinally, additional experiments reveal that steering for content effects is\nrobust to prompt variations, incurs minimal side effects on language modeling\ncapabilities, and can partially generalize to out-of-distribution reasoning\ntasks. Practically, this paper demonstrates that activation-level interventions\ncan offer a scalable strategy for enhancing the robustness of LLMs,\ncontributing towards more systematic and unbiased formal reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12260", "pdf": "https://arxiv.org/pdf/2505.12260", "abs": "https://arxiv.org/abs/2505.12260", "authors": ["Guangyuan Ma", "Yongliang Ma", "Xuanrui Gou", "Zhenpeng Su", "Ming Zhou", "Songlin Hu"], "title": "LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode\nqueries and documents into low-dimensional dense or high-dimensional sparse\nvectors. It retrieves documents relevant to search queries based on vector\nsimilarities. Documents are pre-encoded offline, while queries arrive in\nreal-time, necessitating an efficient online query encoder. Although LLMs\nsignificantly enhance retrieval capabilities, serving deeply parameterized LLMs\nslows down query inference throughput and increases demands for online\ndeployment resources. In this paper, we propose LightRetriever, a novel\nLLM-based hybrid retriever with extremely lightweight query encoders. Our\nmethod retains a full-sized LLM for document encoding, but reduces the workload\nof query encoding to no more than an embedding lookup. Compared to serving a\nfull-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for\nquery inference with GPU acceleration, and even a 20x speedup without GPU.\nExperiments on large-scale retrieval benchmarks demonstrate that our method\ngeneralizes well across diverse retrieval tasks, retaining an average of 95%\nfull-sized performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12120", "pdf": "https://arxiv.org/pdf/2505.12120", "abs": "https://arxiv.org/abs/2505.12120", "authors": ["Dmitry Nechaev", "Alexey Pchelnikov", "Ekaterina Ivanova"], "title": "HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for Computational Pathology", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Recent advancements in Digital Pathology (DP), particularly through\nartificial intelligence and Foundation Models, have underscored the importance\nof large-scale, diverse, and richly annotated datasets. Despite their critical\nrole, publicly available Whole Slide Image (WSI) datasets often lack sufficient\nscale, tissue diversity, and comprehensive clinical metadata, limiting the\nrobustness and generalizability of AI models. In response, we introduce the\nHISTAI dataset, a large, multimodal, open-access WSI collection comprising over\n60,000 slides from various tissue types. Each case in the HISTAI dataset is\naccompanied by extensive clinical metadata, including diagnosis, demographic\ninformation, detailed pathological annotations, and standardized diagnostic\ncoding. The dataset aims to fill gaps identified in existing resources,\npromoting innovation, reproducibility, and the development of clinically\nrelevant computational pathology solutions. The dataset can be accessed at\nhttps://github.com/HistAI/HISTAI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12203", "pdf": "https://arxiv.org/pdf/2505.12203", "abs": "https://arxiv.org/abs/2505.12203", "authors": ["Zhiting Zheng", "Shuqi Wu", "Wen Ding"], "title": "CTLformer: A Hybrid Denoising Model Combining Convolutional Layers and Self-Attention for Enhanced CT Image Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Low-dose CT (LDCT) images are often accompanied by significant noise, which\nnegatively impacts image quality and subsequent diagnostic accuracy. To address\nthe challenges of multi-scale feature fusion and diverse noise distribution\npatterns in LDCT denoising, this paper introduces an innovative model,\nCTLformer, which combines convolutional structures with transformer\narchitecture. Two key innovations are proposed: a multi-scale attention\nmechanism and a dynamic attention control mechanism. The multi-scale attention\nmechanism, implemented through the Token2Token mechanism and self-attention\ninteraction modules, effectively captures both fine details and global\nstructures at different scales, enhancing relevant features and suppressing\nnoise. The dynamic attention control mechanism adapts the attention\ndistribution based on the noise characteristics of the input image, focusing on\nhigh-noise regions while preserving details in low-noise areas, thereby\nenhancing robustness and improving denoising performance. Furthermore,\nCTLformer integrates convolutional layers for efficient feature extraction and\nuses overlapping inference to mitigate boundary artifacts, further\nstrengthening its denoising capability. Experimental results on the 2016\nNational Institutes of Health AAPM Mayo Clinic LDCT Challenge dataset\ndemonstrate that CTLformer significantly outperforms existing methods in both\ndenoising performance and model efficiency, greatly improving the quality of\nLDCT images. The proposed CTLformer not only provides an efficient solution for\nLDCT denoising but also shows broad potential in medical image analysis,\nespecially for clinical applications dealing with complex noise patterns.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12284", "pdf": "https://arxiv.org/pdf/2505.12284", "abs": "https://arxiv.org/abs/2505.12284", "authors": ["Danlong Yuan", "Tian Xie", "Shaohan Huang", "Zhuocheng Gong", "Huishuai Zhang", "Chong Luo", "Furu Wei", "Dongyan Zhao"], "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization", "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated\nremarkable performance on reasoning tasks but often incur a long reasoning path\nwith significant memory and time costs. Existing methods primarily aim to\nshorten reasoning paths by introducing additional training data and stages. In\nthis paper, we propose three critical reward designs integrated directly into\nthe reinforcement learning process of large reasoning models, which reduce the\nresponse length without extra training stages. Experiments on four settings\nshow that our method significantly decreases response length while maintaining\nor even improving performance. Specifically, in a logic reasoning setting, we\nachieve a 40% reduction in response length averaged by steps alongside a 14%\ngain in performance. For math problems, we reduce response length averaged by\nsteps by 33% while preserving performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12261", "pdf": "https://arxiv.org/pdf/2505.12261", "abs": "https://arxiv.org/abs/2505.12261", "authors": ["Hanchen Wang", "Yixuan Wu", "Yinan Feng", "Peng Jin", "Shihang Feng", "Yiming Mao", "James Wiskin", "Baris Turkbey", "Peter A. Pinto", "Bradford J. Wood", "Songting Luo", "Yinpeng Chen", "Emad Boctor", "Youzuo Lin"], "title": "OpenPros: A Large-Scale Dataset for Limited View Prostate Ultrasound Computed Tomography", "categories": ["physics.med-ph", "cs.CV"], "comment": null, "summary": "Prostate cancer is one of the most common and lethal cancers among men,\nmaking its early detection critically important. Although ultrasound imaging\noffers greater accessibility and cost-effectiveness compared to MRI,\ntraditional transrectal ultrasound methods suffer from low sensitivity,\nespecially in detecting anteriorly located tumors. Ultrasound computed\ntomography provides quantitative tissue characterization, but its clinical\nimplementation faces significant challenges, particularly under anatomically\nconstrained limited-angle acquisition conditions specific to prostate imaging.\nTo address these unmet needs, we introduce OpenPros, the first large-scale\nbenchmark dataset explicitly developed for limited-view prostate USCT. Our\ndataset includes over 280,000 paired samples of realistic 2D speed-of-sound\n(SOS) phantoms and corresponding ultrasound full-waveform data, generated from\nanatomically accurate 3D digital prostate models derived from real clinical\nMRI/CT scans and ex vivo ultrasound measurements, annotated by medical experts.\nSimulations are conducted under clinically realistic configurations using\nadvanced finite-difference time-domain and Runge-Kutta acoustic wave solvers,\nboth provided as open-source components. Through comprehensive baseline\nexperiments, we demonstrate that state-of-the-art deep learning methods surpass\ntraditional physics-based approaches in both inference efficiency and\nreconstruction accuracy. Nevertheless, current deep learning models still fall\nshort of delivering clinically acceptable high-resolution images with\nsufficient accuracy. By publicly releasing OpenPros, we aim to encourage the\ndevelopment of advanced machine learning algorithms capable of bridging this\nperformance gap and producing clinically usable, high-resolution, and highly\naccurate prostate ultrasound images. The dataset is publicly accessible at\nhttps://open-pros.github.io/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363", "abs": "https://arxiv.org/abs/2505.12363", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12457", "pdf": "https://arxiv.org/pdf/2505.12457", "abs": "https://arxiv.org/abs/2505.12457", "authors": ["Yang Zhao", "Kai Xiong", "Xiao Ding", "Li Du", "YangouOuyang", "Zhouhao Sun", "Jiannan Guan", "Wenbin Zhang", "Bin Liu", "Dong Hu", "Bing Qin", "Ting Liu"], "title": "UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Scaling RL for LLMs is computationally expensive, largely due to\nmulti-sampling for policy optimization and evaluation, making efficient data\nselection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,\nwe hypothesize LLMs learn best from data within their potential comprehension\nzone. Addressing the limitation of conventional, computationally intensive\nmulti-sampling methods for data assessment, we introduce UFO-RL. This novel\nframework uses a computationally efficient single-pass uncertainty estimation\nto identify informative data instances, achieving up to 185x faster data\nevaluation. UFO-RL leverages this metric to select data within the estimated\nZPD for training. Experiments show that training with just 10% of data selected\nby UFO-RL yields performance comparable to or surpassing full-data training,\nreducing overall training time by up to 16x while enhancing stability and\ngeneralization. UFO-RL offers a practical and highly efficient strategy for\nscaling RL fine-tuning of LLMs by focusing learning on valuable data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12629", "pdf": "https://arxiv.org/pdf/2505.12629", "abs": "https://arxiv.org/abs/2505.12629", "authors": ["Yuchang Sun", "Yanxi Chen", "Yaliang Li", "Bolin Ding"], "title": "Enhancing Latent Computation in Transformers with Latent Tokens", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Augmenting large language models (LLMs) with auxiliary tokens has emerged as\na promising strategy for enhancing model performance. In this work, we\nintroduce a lightweight method termed latent tokens; these are dummy tokens\nthat may be non-interpretable in natural language but steer the autoregressive\ndecoding process of a Transformer-based LLM via the attention mechanism. The\nproposed latent tokens can be seamlessly integrated with a pre-trained\nTransformer, trained in a parameter-efficient manner, and applied flexibly at\ninference time, while adding minimal complexity overhead to the existing\ninfrastructure of standard Transformers. We propose several hypotheses about\nthe underlying mechanisms of latent tokens and design synthetic tasks\naccordingly to verify them. Numerical results confirm that the proposed method\nnoticeably outperforms the baselines, particularly in the out-of-distribution\ngeneralization scenarios, highlighting its potential in improving the\nadaptability of LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12373", "pdf": "https://arxiv.org/pdf/2505.12373", "abs": "https://arxiv.org/abs/2505.12373", "authors": ["Kapil Dev"], "title": "Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 8 figures, submitted to IEEE Transactions on Visualization\n  and Computer Graphics (TVCG)", "summary": "Human aesthetic preferences for 3D shapes are central to industrial design,\nvirtual reality, and consumer product development. However, most computational\nmodels of 3D aesthetics lack empirical grounding in large-scale human\njudgments, limiting their practical relevance. We present a large-scale study\nof human preferences. We collected 22,301 pairwise comparisons across five\nobject categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon\nMechanical Turk. Building on a previously published\ndataset~\\cite{dev2020learning}, we introduce new non-linear modeling and\ncross-category analysis to uncover the geometric drivers of aesthetic\npreference. We apply the Bradley-Terry model to infer latent aesthetic scores\nand use Random Forests with SHAP analysis to identify and interpret the most\ninfluential geometric features (e.g., symmetry, curvature, compactness). Our\ncross-category analysis reveals both universal principles and domain-specific\ntrends in aesthetic preferences. We focus on human interpretable geometric\nfeatures to ensure model transparency and actionable design insights, rather\nthan relying on black-box deep learning approaches. Our findings bridge\ncomputational aesthetics and cognitive science, providing practical guidance\nfor designers and a publicly available dataset to support reproducibility. This\nwork advances the understanding of 3D shape aesthetics through a human-centric,\ndata-driven framework.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "comparison", "pairwise"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12632", "pdf": "https://arxiv.org/pdf/2505.12632", "abs": "https://arxiv.org/abs/2505.12632", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12886", "pdf": "https://arxiv.org/pdf/2505.12886", "abs": "https://arxiv.org/abs/2505.12886", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "25 pages", "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12835", "pdf": "https://arxiv.org/pdf/2505.12835", "abs": "https://arxiv.org/abs/2505.12835", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12863", "pdf": "https://arxiv.org/pdf/2505.12863", "abs": "https://arxiv.org/abs/2505.12863", "authors": ["Jongmin Jung", "Dongmin Kim", "Sihun Lee", "Seola Cho", "Hyungjoon Soh", "Irmak Bukey", "Chris Donahue", "Dasaem Jeong"], "title": "Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "comment": "Submitted to IEEE Transactions on Audio, Speech and Language\n  Processing (TASLPRO)", "summary": "Music exists in various modalities, such as score images, symbolic scores,\nMIDI, and audio. Translations between each modality are established as core\ntasks of music information retrieval, such as automatic music transcription\n(audio-to-MIDI) and optical music recognition (score image to symbolic score).\nHowever, most past work on multimodal translation trains specialized models on\nindividual translation tasks. In this paper, we propose a unified approach,\nwhere we train a general-purpose model on many translation tasks\nsimultaneously. Two key factors make this unified approach viable: a new\nlarge-scale dataset and the tokenization of each modality. Firstly, we propose\na new dataset that consists of more than 1,300 hours of paired audio-score\nimage data collected from YouTube videos, which is an order of magnitude larger\nthan any existing music modal translation datasets. Secondly, our unified\ntokenization framework discretizes score images, audio, MIDI, and MusicXML into\na sequence of tokens, enabling a single encoder-decoder Transformer to tackle\nmultiple cross-modal translation as one coherent sequence-to-sequence task.\nExperimental results confirm that our unified multitask model improves upon\nsingle-task baselines in several key areas, notably reducing the symbol error\nrate for optical music recognition from 24.58% to a state-of-the-art 13.67%,\nwhile similarly substantial improvements are observed across the other\ntranslation tasks. Notably, our approach achieves the first successful\nscore-image-conditioned audio generation, marking a significant breakthrough in\ncross-modal music generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13208", "pdf": "https://arxiv.org/pdf/2505.13208", "abs": "https://arxiv.org/abs/2505.13208", "authors": ["Colin Krawchuk", "Nikhil Khatri", "Neil John Ortega", "Dimitri Kartsaklis"], "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "categories": ["quant-ph", "cs.AI", "cs.CL"], "comment": null, "summary": "Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12887", "pdf": "https://arxiv.org/pdf/2505.12887", "abs": "https://arxiv.org/abs/2505.12887", "authors": ["Junzhi Ning", "Cheng Tang", "Kaijin Zhou", "Diping Song", "Lihao Liu", "Ming Hu", "Wei Li", "Yanzhou Su", "Tianbing Li", "Jiyao Liu", "Yejin", "Sheng Zhang", "Yuanfeng Ji", "Junjun He"], "title": "RetinaLogos: Fine-Grained Synthesis of High-Resolution Retinal Images Through Captions", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The scarcity of high-quality, labelled retinal imaging data, which presents a\nsignificant challenge in the development of machine learning models for\nophthalmology, hinders progress in the field. To synthesise Colour Fundus\nPhotographs (CFPs), existing methods primarily relying on predefined disease\nlabels face significant limitations. However, current methods remain limited,\nthus failing to generate images for broader categories with diverse and\nfine-grained anatomical structures. To overcome these challenges, we first\nintroduce an innovative pipeline that creates a large-scale, synthetic\nCaption-CFP dataset comprising 1.4 million entries, called RetinaLogos-1400k.\nSpecifically, RetinaLogos-1400k uses large language models (LLMs) to describe\nretinal conditions and key structures, such as optic disc configuration,\nvascular distribution, nerve fibre layers, and pathological features.\nFurthermore, based on this dataset, we employ a novel three-step training\nframework, called RetinaLogos, which enables fine-grained semantic control over\nretinal images and accurately captures different stages of disease progression,\nsubtle anatomical variations, and specific lesion types. Extensive experiments\ndemonstrate state-of-the-art performance across multiple datasets, with 62.07%\nof text-driven synthetic images indistinguishable from real ones by\nophthalmologists. Moreover, the synthetic data improves accuracy by 10%-25% in\ndiabetic retinopathy grading and glaucoma detection, thereby providing a\nscalable solution to augment ophthalmic datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12944", "pdf": "https://arxiv.org/pdf/2505.12944", "abs": "https://arxiv.org/abs/2505.12944", "authors": ["Jan Hagnberger", "Daniel Musekamp", "Mathias Niepert"], "title": "CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "physics.comp-ph"], "comment": null, "summary": "Solving time-dependent Partial Differential Equations (PDEs) using a densely\ndiscretized spatial domain is a fundamental problem in various scientific and\nengineering disciplines, including modeling climate phenomena and fluid\ndynamics. However, performing these computations directly in the physical space\noften incurs significant computational costs. To address this issue, several\nneural surrogate models have been developed that operate in a compressed latent\nspace to solve the PDE. While these approaches reduce computational complexity,\nthey often use Transformer-based attention mechanisms to handle irregularly\nsampled domains, resulting in increased memory consumption. In contrast,\nconvolutional neural networks allow memory-efficient encoding and decoding but\nare limited to regular discretizations. Motivated by these considerations, we\npropose CALM-PDE, a model class that efficiently solves arbitrarily discretized\nPDEs in a compressed latent space. We introduce a novel continuous\nconvolution-based encoder-decoder architecture that uses an\nepsilon-neighborhood-constrained kernel and learns to apply the convolution\noperator to adaptive and optimized query points. We demonstrate the\neffectiveness of CALM-PDE on a diverse set of PDEs with both regularly and\nirregularly sampled spatial domains. CALM-PDE is competitive with or\noutperforms existing baseline methods while offering significant improvements\nin memory and inference time efficiency compared to Transformer-based methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13380", "pdf": "https://arxiv.org/pdf/2505.13380", "abs": "https://arxiv.org/abs/2505.13380", "authors": ["Nam V. Nguyen", "Huy Nguyen", "Quang Pham", "Van Nguyen", "Savitha Ramasamy", "Nhat Ho"], "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition", "categories": ["cs.AI", "cs.CL"], "comment": "52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526", "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13081", "pdf": "https://arxiv.org/pdf/2505.13081", "abs": "https://arxiv.org/abs/2505.13081", "authors": ["Xiaoyu Yang", "Jie Lu", "En Yu"], "title": "Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": "17 pages, 5figures", "summary": "This paper uncovers a critical yet overlooked phenomenon in multi-modal large\nlanguage models (MLLMs): detrimental concept drift within chain-of-thought\n(CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where\nreasoning token distributions evolve unpredictably, thereby introducing\nsignificant biases in final predictions. To address this, we are pioneers in\nestablishing the theoretical bridge between concept drift theory and RFT\nprocesses by formalizing CoT's autoregressive token streams as non-stationary\ndistributions undergoing arbitrary temporal shifts. Leveraging this framework,\nwe propose a novel counterfact-aware RFT that systematically decouples\nbeneficial distribution adaptation from harmful concept drift through concept\ngraph-empowered LLM experts generating counterfactual reasoning trajectories.\nOur solution, Counterfactual Preference Optimization (CPO), enables stable RFT\nin non-stationary environments, particularly within the medical domain, through\ncustom-tuning of counterfactual-aware preference alignment. Extensive\nexperiments demonstrate our superior performance of robustness, generalization\nand coordination within RFT. Besides, we also contributed a large-scale dataset\nCXR-CounterFact (CCF), comprising 320,416 meticulously curated counterfactual\nreasoning trajectories derived from MIMIC-CXR. Our code and data are public.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13430", "pdf": "https://arxiv.org/pdf/2505.13430", "abs": "https://arxiv.org/abs/2505.13430", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13232", "pdf": "https://arxiv.org/pdf/2505.13232", "abs": "https://arxiv.org/abs/2505.13232", "authors": ["Younghyun Kim", "Jongheon Jeong", "Sangkyung Kwak", "Kyungmin Lee", "Juho Lee", "Jinwoo Shin"], "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions.We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features.Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13445", "pdf": "https://arxiv.org/pdf/2505.13445", "abs": "https://arxiv.org/abs/2505.13445", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "categories": ["cs.AI", "cs.CL"], "comment": "code available at https://github.com/xyliu-cs/RISE", "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-verification"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13414", "pdf": "https://arxiv.org/pdf/2505.13414", "abs": "https://arxiv.org/abs/2505.13414", "authors": ["Yaqian Chen", "Hanxue Gu", "Haoyu Dong", "Qihang Li", "Yuwen Chen", "Nicholas Konz", "Lin Li", "Maciej A. Mazurowski"], "title": "GuidedMorph: Two-Stage Deformable Registration for Breast MRI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurately registering breast MR images from different time points enables\nthe alignment of anatomical structures and tracking of tumor progression,\nsupporting more effective breast cancer detection, diagnosis, and treatment\nplanning. However, the complexity of dense tissue and its highly non-rigid\nnature pose challenges for conventional registration methods, which primarily\nfocus on aligning general structures while overlooking intricate internal\ndetails. To address this, we propose \\textbf{GuidedMorph}, a novel two-stage\nregistration framework designed to better align dense tissue. In addition to a\nsingle-scale network for global structure alignment, we introduce a framework\nthat utilizes dense tissue information to track breast movement. The learned\ntransformation fields are fused by introducing the Dual Spatial Transformer\nNetwork (DSTN), improving overall alignment accuracy. A novel warping method\nbased on the Euclidean distance transform (EDT) is also proposed to accurately\nwarp the registered dense tissue and breast masks, preserving fine structural\ndetails during deformation. The framework supports paradigms that require\nexternal segmentation models and with image data only. It also operates\neffectively with the VoxelMorph and TransMorph backbones, offering a versatile\nsolution for breast registration. We validate our method on ISPY2 and internal\ndataset, demonstrating superior performance in dense tissue, overall breast\nalignment, and breast structural similarity index measure (SSIM), with notable\nimprovements by over 13.01% in dense tissue Dice, 3.13% in breast Dice, and\n1.21% in breast SSIM compared to the best learning-based baseline.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13430", "pdf": "https://arxiv.org/pdf/2505.13430", "abs": "https://arxiv.org/abs/2505.13430", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
