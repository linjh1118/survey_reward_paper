{"id": "2505.15259", "pdf": "https://arxiv.org/pdf/2505.15259", "abs": "https://arxiv.org/abs/2505.15259", "authors": ["Hyunseok Lee", "Jeonghoon Kim", "Beomjun Kim", "Jihoon Tack", "Chansong Jo", "Jaehong Lee", "Cheonbok Park", "Sookyo In", "Jinwoo Shin", "Kang Min Yoo"], "title": "ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled\nautonomous agents to interact with computers via Graphical User Interfaces\n(GUIs), where accurately localizing the coordinates of interface elements\n(e.g., buttons) is often required for fine-grained actions. However, this\nremains significantly challenging, leading prior works to rely on large-scale\nweb datasets to improve the grounding accuracy. In this work, we propose\nReasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a\nnovel and effective framework for web grounding that enables MLLMs to learn\ndata efficiently through self-generated reasoning and spatial-aware criticism.\nMore specifically, ReGUIDE learns to (i) self-generate a language reasoning\nprocess for the localization via online reinforcement learning, and (ii)\ncriticize the prediction using spatial priors that enforce equivariance under\ninput transformations. At inference time, ReGUIDE further boosts performance\nthrough a test-time scaling strategy, which combines spatial search with\ncoordinate aggregation. Our experiments demonstrate that ReGUIDE significantly\nadvances web grounding performance across multiple benchmarks, outperforming\nbaselines with substantially fewer training data points (e.g., only 0.2%\nsamples compared to the best open-sourced baselines).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference time", "scaling", "scale"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15471", "pdf": "https://arxiv.org/pdf/2505.15471", "abs": "https://arxiv.org/abs/2505.15471", "authors": ["Yiyun Zhou", "Chang Yao", "Jingyuan Chen"], "title": "CoLA: Collaborative Low-Rank Adaptation", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025, Findings", "summary": "The scaling law of Large Language Models (LLMs) reveals a power-law\nrelationship, showing diminishing return on performance as model scale\nincreases. While training LLMs from scratch is resource-intensive, fine-tuning\na pre-trained model for specific tasks has become a practical alternative. Full\nfine-tuning (FFT) achieves strong performance; however, it is computationally\nexpensive and inefficient. Parameter-efficient fine-tuning (PEFT) methods, like\nLoRA, have been proposed to address these challenges by freezing the\npre-trained model and adding lightweight task-specific modules. LoRA, in\nparticular, has proven effective, but its application to multi-task scenarios\nis limited by interference between tasks. Recent approaches, such as\nMixture-of-Experts (MOE) and asymmetric LoRA, have aimed to mitigate these\nissues but still struggle with sample scarcity and noise interference due to\ntheir fixed structure. In response, we propose CoLA, a more flexible LoRA\narchitecture with an efficient initialization scheme, and introduces three\ncollaborative strategies to enhance performance by better utilizing the\nquantitative relationships between matrices $A$ and $B$. Our experiments\ndemonstrate the effectiveness and robustness of CoLA, outperforming existing\nPEFT methods, especially in low-sample scenarios. Our data and code are fully\npublicly available at https://github.com/zyy-2001/CoLA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale", "scaling law"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15508", "pdf": "https://arxiv.org/pdf/2505.15508", "abs": "https://arxiv.org/abs/2505.15508", "authors": ["Prasoon Bajpai", "Tanmoy Chakraborty"], "title": "Multilingual Test-Time Scaling via Initial Thought Transfer", "categories": ["cs.CL"], "comment": "14 pages, 9 figures, 5 Tables", "summary": "Test-time scaling has emerged as a widely adopted inference-time strategy for\nboosting reasoning performance. However, its effectiveness has been studied\nalmost exclusively in English, leaving its behavior in other languages largely\nunexplored. We present the first systematic study of test-time scaling in\nmultilingual settings, evaluating DeepSeek-R1-Distill-LLama-8B and\nDeepSeek-R1-Distill-Qwen-7B across both high- and low-resource Latin-script\nlanguages. Our findings reveal that the relative gains from test-time scaling\nvary significantly across languages. Additionally, models frequently switch to\nEnglish mid-reasoning, even when operating under strictly monolingual prompts.\nWe further show that low-resource languages not only produce initial reasoning\nthoughts that differ significantly from English but also have lower internal\nconsistency across generations in their early reasoning. Building on our\nfindings, we introduce MITT (Multilingual Initial Thought Transfer), an\nunsupervised and lightweight reasoning prefix-tuning approach that transfers\nhigh-resource reasoning prefixes to enhance test-time scaling across all\nlanguages, addressing inconsistencies in multilingual reasoning performance.\nMITT significantly boosts DeepSeek-R1-Distill-Qwen-7B's reasoning performance,\nespecially for underrepresented languages.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time", "scaling"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14763", "pdf": "https://arxiv.org/pdf/2505.14763", "abs": "https://arxiv.org/abs/2505.14763", "authors": ["Prabhu Prakash Kagitha", "Andrew Zhu", "Li Zhang"], "title": "Addressing the Challenges of Planning Language Generation", "categories": ["cs.CL"], "comment": null, "summary": "Using LLMs to generate formal planning languages such as PDDL that invokes\nsymbolic solvers to deterministically derive plans has been shown to outperform\ngenerating plans directly. While this success has been limited to\nclosed-sourced models or particular LLM pipelines, we design and evaluate 8\ndifferent PDDL generation pipelines with open-source models under 50 billion\nparameters previously shown to be incapable of this task. We find that\nintuitive approaches such as using a high-resource language wrapper or\nconstrained decoding with grammar decrease performance, yet inference-time\nscaling approaches such as revision with feedback from the solver and plan\nvalidator more than double the performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14887", "pdf": "https://arxiv.org/pdf/2505.14887", "abs": "https://arxiv.org/abs/2505.14887", "authors": ["Nathan Roll", "Calbert Graham", "Yuka Tatsumi", "Kim Tien Nguyen", "Meghan Sumner", "Dan Jurafsky"], "title": "In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties", "categories": ["cs.CL", "eess.AS"], "comment": "15 pages; 3 figures", "summary": "Human listeners readily adjust to unfamiliar speakers and language varieties\nthrough exposure, but do these adaptation benefits extend to state-of-the-art\nspoken language models? We introduce a scalable framework that allows for\nin-context learning (ICL) in Phi-4 Multimodal using interleaved task prompts\nand audio-text pairs, and find that as few as 12 example utterances (~50\nseconds) at inference time reduce word error rates by a relative 19.7% (1.2\npp.) on average across diverse English corpora. These improvements are most\npronounced in low-resource varieties, when the context and target speaker\nmatch, and when more examples are provided--though scaling our procedure yields\ndiminishing marginal returns to context length. Overall, we find that our novel\nICL adaptation scheme (1) reveals a similar performance profile to human\nlisteners, and (2) demonstrates consistent improvements to automatic speech\nrecognition (ASR) robustness across diverse speakers and language backgrounds.\nWhile adaptation succeeds broadly, significant gaps remain for certain\nvarieties, revealing where current models still fall short of human\nflexibility. We release our prompts and code on GitHub.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scaling"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15205", "pdf": "https://arxiv.org/pdf/2505.15205", "abs": "https://arxiv.org/abs/2505.15205", "authors": ["Hyogun Lee", "Haksub Kim", "Ig-Jae Kim", "Yonghun Choi"], "title": "Flashback: Memory-Driven Zero-shot, Real-time Video Anomaly Detection", "categories": ["cs.CV"], "comment": "12 pages, 5 figures", "summary": "Video Anomaly Detection (VAD) automatically identifies anomalous events from\nvideo, mitigating the need for human operators in large-scale surveillance\ndeployments. However, three fundamental obstacles hinder real-world adoption:\ndomain dependency and real-time constraints -- requiring near-instantaneous\nprocessing of incoming video. To this end, we propose Flashback, a zero-shot\nand real-time video anomaly detection paradigm. Inspired by the human cognitive\nmechanism of instantly judging anomalies and reasoning in current scenes based\non past experience, Flashback operates in two stages: Recall and Respond. In\nthe offline recall stage, an off-the-shelf LLM builds a pseudo-scene memory of\nboth normal and anomalous captions without any reliance on real anomaly data.\nIn the online respond stage, incoming video segments are embedded and matched\nagainst this memory via similarity search. By eliminating all LLM calls at\ninference time, Flashback delivers real-time VAD even on a consumer-grade GPU.\nOn two large datasets from real-world surveillance scenarios, UCF-Crime and\nXD-Violence, we achieve 87.3 AUC (+7.0 pp) and 75.1 AP (+13.1 pp),\nrespectively, outperforming prior zero-shot VAD methods by large margins.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15277", "pdf": "https://arxiv.org/pdf/2505.15277", "abs": "https://arxiv.org/abs/2505.15277", "authors": ["Hyungjoo Chae", "Sunghwan Kim", "Junhee Cho", "Seungone Kim", "Seungjun Moon", "Gyeom Hwangbo", "Dongha Lim", "Minjin Kim", "Yeonjun Hwang", "Minju Gwak", "Dongwook Choi", "Minseok Kang", "Gwanhoon Im", "ByeongUng Cho", "Hyojun Kim", "Jun Hee Han", "Taeyoon Kwon", "Minju Kim", "Beong-woo Kwak", "Dongjin Kang", "Jinyoung Yeo"], "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Web navigation is a unique domain that can automate many repetitive real-life\ntasks and is challenging as it requires long-horizon sequential decision making\nbeyond typical multimodal large language model (MLLM) tasks. Yet, specialized\nreward models for web navigation that can be utilized during both training and\ntest-time have been absent until now. Despite the importance of speed and\ncost-effectiveness, prior works have utilized MLLMs as reward models, which\nposes significant constraints for real-world deployment. To address this, in\nthis work, we propose the first process reward model (PRM) called Web-Shepherd\nwhich could assess web navigation trajectories in a step-level. To achieve\nthis, we first construct the WebPRM Collection, a large-scale dataset with 40K\nstep-level preference pairs and annotated checklists spanning diverse domains\nand difficulty levels. Next, we also introduce the WebRewardBench, the first\nmeta-evaluation benchmark for evaluating PRMs. In our experiments, we observe\nthat our Web-Shepherd achieves about 30 points better accuracy compared to\nusing GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by\nusing GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve\n10.9 points better performance, in 10 less cost compared to using GPT-4o-mini\nas the verifier. Our model, dataset, and code are publicly available at LINK.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15818", "pdf": "https://arxiv.org/pdf/2505.15818", "abs": "https://arxiv.org/abs/2505.15818", "authors": ["Yijie Zheng", "Weijie Wu", "Qingyun Li", "Xuehui Wang", "Xu Zhou", "Aiai Ren", "Jun Shen", "Long Zhao", "Guoqing Li", "Xue Yang"], "title": "InstructSAM: A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Language-Guided object recognition in remote sensing imagery is crucial for\nlarge-scale mapping and automated data annotation. However, existing\nopen-vocabulary and visual grounding methods rely on explicit category cues,\nlimiting their ability to handle complex or implicit queries that require\nadvanced reasoning. To address this issue, we introduce a new suite of tasks,\nincluding Instruction-Oriented Object Counting, Detection, and Segmentation\n(InstructCDS), covering open-vocabulary, open-ended, and open-subclass\nscenarios. We further present EarthInstruct, the first InstructCDS benchmark\nfor earth observation. It is constructed from two diverse remote sensing\ndatasets with varying spatial resolutions and annotation rules across 20\ncategories, necessitating models to interpret dataset-specific instructions.\nGiven the scarcity of semantically rich labeled data in remote sensing, we\npropose InstructSAM, a training-free framework for instruction-driven object\nrecognition. InstructSAM leverages large vision-language models to interpret\nuser instructions and estimate object counts, employs SAM2 for mask proposal,\nand formulates mask-label assignment as a binary integer programming problem.\nBy integrating semantic similarity with counting constraints, InstructSAM\nefficiently assigns categories to predicted masks without relying on confidence\nthresholds. Experiments demonstrate that InstructSAM matches or surpasses\nspecialized baselines across multiple tasks while maintaining near-constant\ninference time regardless of object count, reducing output tokens by 89% and\noverall runtime by over 32% compared to direct generation approaches. We\nbelieve the contributions of the proposed tasks, benchmark, and effective\napproach will advance future research in developing versatile object\nrecognition systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15270", "pdf": "https://arxiv.org/pdf/2505.15270", "abs": "https://arxiv.org/abs/2505.15270", "authors": ["Chenyu Zheng", "Xinyu Zhang", "Rongzhen Wang", "Wei Huang", "Zhi Tian", "Weilin Huang", "Jun Zhu", "Chongxuan Li"], "title": "Scaling Diffusion Transformers Efficiently via $μ$P", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "35 pages, 10 figures, 15 tables", "summary": "Diffusion Transformers have emerged as the foundation for vision generative\nmodels, but their scalability is limited by the high cost of hyperparameter\n(HP) tuning at large scales. Recently, Maximal Update Parametrization ($\\mu$P)\nwas proposed for vanilla Transformers, which enables stable HP transfer from\nsmall to large language models, and dramatically reduces tuning costs. However,\nit remains unclear whether $\\mu$P of vanilla Transformers extends to diffusion\nTransformers, which differ architecturally and objectively. In this work, we\ngeneralize standard $\\mu$P to diffusion Transformers and validate its\neffectiveness through large-scale experiments. First, we rigorously prove that\n$\\mu$P of mainstream diffusion Transformers, including DiT, U-ViT,\nPixArt-$\\alpha$, and MMDiT, aligns with that of the vanilla Transformer,\nenabling the direct application of existing $\\mu$P methodologies. Leveraging\nthis result, we systematically demonstrate that DiT-$\\mu$P enjoys robust HP\ntransferability. Notably, DiT-XL-2-$\\mu$P with transferred learning rate\nachieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we\nvalidate the effectiveness of $\\mu$P on text-to-image generation by scaling\nPixArt-$\\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,\nmodels under $\\mu$P outperform their respective baselines while requiring small\ntuning cost, only 5.5% of one training run for PixArt-$\\alpha$ and 3% of\nconsumption by human experts for MMDiT-18B. These results establish $\\mu$P as a\nprincipled and efficient framework for scaling diffusion Transformers.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15801", "pdf": "https://arxiv.org/pdf/2505.15801", "abs": "https://arxiv.org/abs/2505.15801", "authors": ["Yuchen Yan", "Jin Jiang", "Zhenbang Ren", "Yijun Li", "Xudong Cai", "Yang Liu", "Xin Xu", "Mengdi Zhang", "Jian Shao", "Yongliang Shen", "Jun Xiao", "Yueting Zhuang"], "title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Dataset: https://huggingface.co/datasets/ZJU-REAL/VerifyBench", "summary": "Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved\nremarkable performance in the domain of reasoning. A key component of their\ntraining is the incorporation of verifiable rewards within reinforcement\nlearning (RL). However, existing reward benchmarks do not evaluate\nreference-based reward systems, leaving researchers with limited understanding\nof the accuracy of verifiers used in RL. In this paper, we introduce two\nbenchmarks, VerifyBench and VerifyBench-Hard, designed to assess the\nperformance of reference-based reward systems. These benchmarks are constructed\nthrough meticulous data collection and curation, followed by careful human\nannotation to ensure high quality. Current models still show considerable room\nfor improvement on both VerifyBench and VerifyBench-Hard, especially\nsmaller-scale models. Furthermore, we conduct a thorough and comprehensive\nanalysis of evaluation results, offering insights for understanding and\ndeveloping reference-based reward systems. Our proposed benchmarks serve as\neffective tools for guiding the development of verifier accuracy and the\nreasoning capabilities of models trained via RL in reasoning tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "o1"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14705", "pdf": "https://arxiv.org/pdf/2505.14705", "abs": "https://arxiv.org/abs/2505.14705", "authors": ["Xin Zhang", "Ziruo Zhang", "Jiawei Du", "Zuozhu Liu", "Joey Tianyi Zhou"], "title": "Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal Dataset Distillation (MDD) seeks to condense large-scale\nimage-text datasets into compact surrogates while retaining their effectiveness\nfor cross-modal learning. Despite recent progress, existing MDD approaches\noften suffer from \\textit{\\textbf{Modality Collapse}}, characterized by\nover-concentrated intra-modal representations and enlarged distributional gap\nacross modalities. In this paper, at the first time, we identify this issue as\nstemming from a fundamental conflict between the over-compression behavior\ninherent in dataset distillation and the cross-modal supervision imposed by\ncontrastive objectives. To alleviate modality collapse, we introduce\n\\textbf{RepBlend}, a novel MDD framework that weakens overdominant cross-modal\nsupervision via representation blending, thereby significantly enhancing\nintra-modal diversity. Additionally, we observe that current MDD methods impose\nasymmetric supervision across modalities, resulting in biased optimization. To\naddress this, we propose symmetric projection trajectory matching, which\nsynchronizes the optimization dynamics using modality-specific projection\nheads, thereby promoting balanced supervision and enhancing cross-modal\nalignment. Experiments on Flickr-30K and MS-COCO show that RepBlend\nconsistently outperforms prior state-of-the-art MDD methods, achieving\nsignificant gains in retrieval performance (e.g., +9.4 IR@10, +6.3 TR@10 under\nthe 100-pair setting) and offering up to 6.7$\\times$ distillation speedup.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14810", "pdf": "https://arxiv.org/pdf/2505.14810", "abs": "https://arxiv.org/abs/2505.14810", "authors": ["Tingchen Fu", "Jiawei Gu", "Yafu Li", "Xiaoye Qu", "Yu Cheng"], "title": "Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction-following is essential for aligning large language models (LLMs)\nwith user intent. While recent reasoning-oriented models exhibit impressive\nperformance on complex mathematical problems, their ability to adhere to\nnatural language instructions remains underexplored. In this work, we introduce\nMathIF, a dedicated benchmark for evaluating instruction-following in\nmathematical reasoning tasks. Our empirical analysis reveals a consistent\ntension between scaling up reasoning capacity and maintaining controllability,\nas models that reason more effectively often struggle to comply with user\ndirectives. We find that models tuned on distilled long chains-of-thought or\ntrained with reasoning-oriented reinforcement learning often degrade in\ninstruction adherence, especially when generation length increases.\nFurthermore, we show that even simple interventions can partially recover\nobedience, though at the cost of reasoning performance. These findings\nhighlight a fundamental tension in current LLM training paradigms and motivate\nthe need for more instruction-aware reasoning models. We release the code and\ndata at https://github.com/TingchenFu/MathIF.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "mathematical reasoning"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14818", "pdf": "https://arxiv.org/pdf/2505.14818", "abs": "https://arxiv.org/abs/2505.14818", "authors": ["Leon Lin", "Jun Zheng", "Haidong Wang"], "title": "WebNovelBench: Placing LLM Novelists on the Web Novel Distribution", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Robustly evaluating the long-form storytelling capabilities of Large Language\nModels (LLMs) remains a significant challenge, as existing benchmarks often\nlack the necessary scale, diversity, or objective measures. To address this, we\nintroduce WebNovelBench, a novel benchmark specifically designed for evaluating\nlong-form novel generation. WebNovelBench leverages a large-scale dataset of\nover 4,000 Chinese web novels, framing evaluation as a synopsis-to-story\ngeneration task. We propose a multi-faceted framework encompassing eight\nnarrative quality dimensions, assessed automatically via an LLM-as-Judge\napproach. Scores are aggregated using Principal Component Analysis and mapped\nto a percentile rank against human-authored works. Our experiments demonstrate\nthat WebNovelBench effectively differentiates between human-written\nmasterpieces, popular web novels, and LLM-generated content. We provide a\ncomprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling\nabilities and offering insights for future development. This benchmark provides\na scalable, replicable, and data-driven methodology for assessing and advancing\nLLM-driven narrative generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14824", "pdf": "https://arxiv.org/pdf/2505.14824", "abs": "https://arxiv.org/abs/2505.14824", "authors": ["Yihong Liu", "Mingyang Wang", "Amir Hossein Kargaran", "Felicia Körner", "Ercong Nie", "Barbara Plank", "François Yvon", "Hinrich Schütze"], "title": "Tracing Multilingual Factual Knowledge Acquisition in Pretraining", "categories": ["cs.CL"], "comment": "preprint", "summary": "Large Language Models (LLMs) are capable of recalling multilingual factual\nknowledge present in their pretraining data. However, most studies evaluate\nonly the final model, leaving the development of factual recall and\ncrosslingual consistency throughout pretraining largely unexplored. In this\nwork, we trace how factual recall and crosslingual consistency evolve during\npretraining, focusing on OLMo-7B as a case study. We find that both accuracy\nand consistency improve over time for most languages. We show that this\nimprovement is primarily driven by the fact frequency in the pretraining\ncorpus: more frequent facts are more likely to be recalled correctly,\nregardless of language. Yet, some low-frequency facts in non-English languages\ncan still be correctly recalled. Our analysis reveals that these instances\nlargely benefit from crosslingual transfer of their English counterparts -- an\neffect that emerges predominantly in the early stages of pretraining. We\npinpoint two distinct pathways through which multilingual factual knowledge\nacquisition occurs: (1) frequency-driven learning, which is dominant and\nlanguage-agnostic, and (2) crosslingual transfer, which is limited in scale and\ntypically constrained to relation types involving named entities. We release\nour code and data to facilitate further research at\nhttps://github.com/cisnlp/multilingual-fact-tracing.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14719", "pdf": "https://arxiv.org/pdf/2505.14719", "abs": "https://arxiv.org/abs/2505.14719", "authors": ["Wei Hua", "Chenlin Zhou", "Jibin Wu", "Yansong Chua", "Yangyang Shu"], "title": "MSVIT: Improving Spiking Vision Transformer Using Multi-scale Attention Fusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The combination of Spiking Neural Networks(SNNs) with Vision Transformer\narchitectures has attracted significant attention due to the great potential\nfor energy-efficient and high-performance computing paradigms. However, a\nsubstantial performance gap still exists between SNN-based and ANN-based\ntransformer architectures. While existing methods propose spiking\nself-attention mechanisms that are successfully combined with SNNs, the overall\narchitectures proposed by these methods suffer from a bottleneck in effectively\nextracting features from different image scales. In this paper, we address this\nissue and propose MSVIT, a novel spike-driven Transformer architecture, which\nfirstly uses multi-scale spiking attention (MSSA) to enrich the capability of\nspiking attention blocks. We validate our approach across various main data\nsets. The experimental results show that MSVIT outperforms existing SNN-based\nmodels, positioning itself as a state-of-the-art solution among SNN-transformer\narchitectures. The codes are available at\nhttps://github.com/Nanhu-AI-Lab/MSViT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14729", "pdf": "https://arxiv.org/pdf/2505.14729", "abs": "https://arxiv.org/abs/2505.14729", "authors": ["Ram Mohan Rao Kadiyala", "Siddhant Gupta", "Jebish Purbey", "Srishti Yadav", "Alejandro Salamanca", "Desmond Elliott"], "title": "Uncovering Cultural Representation Disparities in Vision-Language Models", "categories": ["cs.CV"], "comment": "26 pages, 36 figures", "summary": "Vision-Language Models (VLMs) have demonstrated impressive capabilities\nacross a range of tasks, yet concerns about their potential biases exist. This\nwork investigates the extent to which prominent VLMs exhibit cultural biases by\nevaluating their performance on an image-based country identification task at a\ncountry level. Utilizing the geographically diverse Country211 dataset, we\nprobe several large vision language models (VLMs) under various prompting\nstrategies: open-ended questions, multiple-choice questions (MCQs) including\nchallenging setups like multilingual and adversarial settings. Our analysis\naims to uncover disparities in model accuracy across different countries and\nquestion formats, providing insights into how training data distribution and\nevaluation methodologies might influence cultural biases in VLMs. The findings\nhighlight significant variations in performance, suggesting that while VLMs\npossess considerable visual understanding, they inherit biases from their\npre-training data and scale that impact their ability to generalize uniformly\nacross diverse global contexts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14848", "pdf": "https://arxiv.org/pdf/2505.14848", "abs": "https://arxiv.org/abs/2505.14848", "authors": ["Xi Wang", "Jiaqian Hu", "Safinah Ali"], "title": "MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation", "categories": ["cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "We present MAATS, a Multi Agent Automated Translation System that leverages\nthe Multidimensional Quality Metrics (MQM) framework as a fine-grained signal\nfor error detection and refinement. MAATS employs multiple specialized AI\nagents, each focused on a distinct MQM category (e.g., Accuracy, Fluency,\nStyle, Terminology), followed by a synthesis agent that integrates the\nannotations to iteratively refine translations. This design contrasts with\nconventional single-agent methods that rely on self-correction.\n  Evaluated across diverse language pairs and Large Language Models (LLMs),\nMAATS outperforms zero-shot and single-agent baselines with statistically\nsignificant gains in both automatic metrics and human assessments. It excels\nparticularly in semantic accuracy, locale adaptation, and linguistically\ndistant language pairs. Qualitative analysis highlights its strengths in\nmulti-layered error diagnosis, omission detection across perspectives, and\ncontext-aware refinement. By aligning modular agent roles with interpretable\nMQM dimensions, MAATS narrows the gap between black-box LLMs and human\ntranslation workflows, shifting focus from surface fluency to deeper semantic\nand contextual fidelity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14852", "pdf": "https://arxiv.org/pdf/2505.14852", "abs": "https://arxiv.org/abs/2505.14852", "authors": ["Drishya Karki", "Michiel Kamphuis", "Angelecia Frey"], "title": "EasyMath: A 0-shot Math Benchmark for SLMs", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.6; I.2.7"], "comment": "17 pages, 9 figures, 8 tables", "summary": "EasyMath is a compact benchmark for practical math reasoning in small\nlanguage models. It covers thirteen categories, from basic arithmetic and order\nof operations to word problems, algebraic expressions, edge cases, and omits\nspecialist topics. We tested 23 models (14M to 4B parameters) using exact,\nnumerical, and symbolic checks on free-form answers in a zero-shot setting.\nAccuracy rises with size and training, chain-of-thought adds modest gains, and\nconsistency improves at scale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14931", "pdf": "https://arxiv.org/pdf/2505.14931", "abs": "https://arxiv.org/abs/2505.14931", "authors": ["Rama Alyoubi", "Taif Alharbi", "Albatul Alghamdi", "Yara Alshehri", "Elham Alghamdi"], "title": "Colors Matter: AI-Driven Exploration of Human Feature Colors", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This study presents a robust framework that leverages advanced imaging\ntechniques and machine learning for feature extraction and classification of\nkey human attributes-namely skin tone, hair color, iris color, and vein-based\nundertones. The system employs a multi-stage pipeline involving face detection,\nregion segmentation, and dominant color extraction to isolate and analyze these\nfeatures. Techniques such as X-means clustering, alongside perceptually uniform\ndistance metrics like Delta E (CIEDE2000), are applied within both LAB and HSV\ncolor spaces to enhance the accuracy of color differentiation. For\nclassification, the dominant tones of the skin, hair, and iris are extracted\nand matched to a custom tone scale, while vein analysis from wrist images\nenables undertone classification into \"Warm\" or \"Cool\" based on LAB\ndifferences. Each module uses targeted segmentation and color space\ntransformations to ensure perceptual precision. The system achieves up to 80%\naccuracy in tone classification using the Delta E-HSV method with Gaussian\nblur, demonstrating reliable performance across varied lighting and image\nconditions. This work highlights the potential of AI-powered color analysis and\nfeature extraction for delivering inclusive, precise, and nuanced\nclassification, supporting applications in beauty technology, digital\npersonalization, and visual analytics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15077", "pdf": "https://arxiv.org/pdf/2505.15077", "abs": "https://arxiv.org/abs/2505.15077", "authors": ["Alessandro dos Santos Ferreira", "Ana Paula Marques Ramos", "José Marcato Junior", "Wesley Nunes Gonçalves"], "title": "Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07 (Primary), 68U10, 68T45 (Secondary)", "I.4.8; I.2.10; I.5.4"], "comment": "18 pages, 13 figures", "summary": "Urban forests play a key role in enhancing environmental quality and\nsupporting biodiversity in cities. Mapping and monitoring these green spaces\nare crucial for urban planning and conservation, yet accurately detecting trees\nis challenging due to complex landscapes and the variability in image\nresolution caused by different satellite sensors or UAV flight altitudes. While\ndeep learning architectures have shown promise in addressing these challenges,\ntheir effectiveness remains strongly dependent on the availability of large and\nmanually labeled datasets, which are often expensive and difficult to obtain in\nsufficient quantity. In this work, we propose a novel pipeline that integrates\ndomain adaptation with GANs and Diffusion models to enhance the quality of\nlow-resolution aerial images. Our proposed pipeline enhances low-resolution\nimagery while preserving semantic content, enabling effective tree segmentation\nwithout requiring large volumes of manually annotated data. Leveraging models\nsuch as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we\ngenerate realistic and structurally consistent synthetic samples that expand\nthe training dataset and unify scale across domains. This approach not only\nimproves the robustness of segmentation models across different acquisition\nconditions but also provides a scalable and replicable solution for remote\nsensing scenarios with scarce annotation resources. Experimental results\ndemonstrated an improvement of over 50% in IoU for low-resolution images,\nhighlighting the effectiveness of our method compared to traditional pipelines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14892", "pdf": "https://arxiv.org/pdf/2505.14892", "abs": "https://arxiv.org/abs/2505.14892", "authors": ["Jacob X Li", "Shreyas S Raman", "Jessica Wan", "Fahad Samman", "Jazlyn Lin"], "title": "Scaling Laws for State Dynamics in Large Language Models", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.1; I.2.4; I.5.4"], "comment": "16 pages; 23 figures", "summary": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninternal state tracking, yet their ability to model state transition dynamics\nremains poorly understood. We evaluate how well LLMs capture deterministic\nstate dynamics across 3 domains: Box Tracking, Abstract DFA Sequences, and\nComplex Text Games, each formalizable as a finite-state system. Across tasks,\nwe find that next-state prediction accuracy degrades with increasing\nstate-space size and sparse transitions. GPT-2 XL reaches about 70% accuracy in\nlow-complexity settings but drops below 30% when the number of boxes or states\nexceeds 5 or 10, respectively. In DFA tasks, Pythia-1B fails to exceed 50%\naccuracy when the number of states is > 10 and transitions are < 30. Through\nactivation patching, we identify attention heads responsible for propagating\nstate information: GPT-2 XL Layer 22 Head 20, and Pythia-1B Heads at Layers 10,\n11, 12, and 14. While these heads successfully move relevant state features,\naction information is not reliably routed to the final token, indicating weak\njoint state-action reasoning. Our results suggest that state tracking in LLMs\nemerges from distributed interactions of next-token heads rather than explicit\nsymbolic computation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15137", "pdf": "https://arxiv.org/pdf/2505.15137", "abs": "https://arxiv.org/abs/2505.15137", "authors": ["Seongmin Hwang", "Daeyoung Han", "Moongu Jeon"], "title": "Multispectral Detection Transformer with Infrared-Centric Sensor Fusion", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Multispectral object detection aims to leverage complementary information\nfrom visible (RGB) and infrared (IR) modalities to enable robust performance\nunder diverse environmental conditions. In this letter, we propose IC-Fusion, a\nmultispectral object detector that effectively fuses visible and infrared\nfeatures through a lightweight and modalityaware design. Motivated by wavelet\nanalysis and empirical observations, we find that IR images contain\nstructurally rich high-frequency information critical for object localization,\nwhile RGB images provide complementary semantic context. To exploit this, we\nadopt a compact RGB backbone and design a novel fusion module comprising a\nMulti-Scale Feature Distillation (MSFD) block to enhance RGB features and a\nthree-stage fusion block with Cross-Modal Channel Shuffle Gate (CCSG) and\nCross-Modal Large Kernel Gate (CLKG) to facilitate effective cross-modal\ninteraction. Experiments on the FLIR and LLVIP benchmarks demonstrate the\neffectiveness and efficiency of our IR-centric fusion strategy. Our code is\navailable at https://github.com/smin-hwang/IC-Fusion.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15145", "pdf": "https://arxiv.org/pdf/2505.15145", "abs": "https://arxiv.org/abs/2505.15145", "authors": ["Xinran Wang", "Songyu Xu", "Xiangxuan Shan", "Yuxuan Zhang", "Muxi Diao", "Xueyan Duan", "Yanhua Huang", "Kongming Liang", "Zhanyu Ma"], "title": "CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation", "categories": ["cs.CV"], "comment": "Under review", "summary": "Cinematography is a cornerstone of film production and appreciation, shaping\nmood, emotion, and narrative through visual elements such as camera movement,\nshot composition, and lighting. Despite recent progress in multimodal large\nlanguage models (MLLMs) and video generation models, the capacity of current\nmodels to grasp and reproduce cinematographic techniques remains largely\nuncharted, hindered by the scarcity of expert-annotated data. To bridge this\ngap, we present CineTechBench, a pioneering benchmark founded on precise,\nmanual annotation by seasoned cinematography experts across key cinematography\ndimensions. Our benchmark covers seven essential aspects-shot scale, shot\nangle, composition, camera movement, lighting, color, and focal length-and\nincludes over 600 annotated movie images and 120 movie clips with clear\ncinematographic techniques. For the understanding task, we design question\nanswer pairs and annotated descriptions to assess MLLMs' ability to interpret\nand explain cinematographic techniques. For the generation task, we assess\nadvanced video generation models on their capacity to reconstruct\ncinema-quality camera movements given conditions such as textual prompts or\nkeyframes. We conduct a large-scale evaluation on 15+ MLLMs and 5+ video\ngeneration models. Our results offer insights into the limitations of current\nmodels and future directions for cinematography understanding and generation in\nautomatically film production and appreciation. The code and benchmark can be\naccessed at https://github.com/PRIS-CV/CineTechBench.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "annotation"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14984", "pdf": "https://arxiv.org/pdf/2505.14984", "abs": "https://arxiv.org/abs/2505.14984", "authors": ["Adarsh Singh", "Kushal Raj Bhandari", "Jianxi Gao", "Soham Dan", "Vivek Gupta"], "title": "CRAFT: Training-Free Cascaded Retrieval for Tabular QA", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Table Question Answering (TQA) involves retrieving relevant tables from a\nlarge corpus to answer natural language queries. Traditional dense retrieval\nmodels, such as DTR and ColBERT, not only incur high computational costs for\nlarge-scale retrieval tasks but also require retraining or fine-tuning on new\ndatasets, limiting their adaptability to evolving domains and knowledge. In\nthis work, we propose $\\textbf{CRAFT}$, a cascaded retrieval approach that\nfirst uses a sparse retrieval model to filter a subset of candidate tables\nbefore applying more computationally expensive dense models and neural\nre-rankers. Our approach achieves better retrieval performance than\nstate-of-the-art (SOTA) sparse, dense, and hybrid retrievers. We further\nenhance table representations by generating table descriptions and titles using\nGemini Flash 1.5. End-to-end TQA results using various Large Language Models\n(LLMs) on NQ-Tables, a subset of the Natural Questions Dataset, demonstrate\n$\\textbf{CRAFT}$ effectiveness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15173", "pdf": "https://arxiv.org/pdf/2505.15173", "abs": "https://arxiv.org/abs/2505.15173", "authors": ["Zhipei Xu", "Xuanyu Zhang", "Xing Zhou", "Jian Zhang"], "title": "AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The rapid advancement of Artificial Intelligence Generated Content (AIGC)\ntechnologies, particularly in video generation, has led to unprecedented\ncreative capabilities but also increased threats to information integrity,\nidentity security, and public trust. Existing detection methods, while\neffective in general scenarios, lack robust solutions for human-centric videos,\nwhich pose greater risks due to their realism and potential for legal and\nethical misuse. Moreover, current detection approaches often suffer from poor\ngeneralization, limited scalability, and reliance on labor-intensive supervised\nfine-tuning. To address these challenges, we propose AvatarShield, the first\ninterpretable MLLM-based framework for detecting human-centric fake videos,\nenhanced via Group Relative Policy Optimization (GRPO). Through our carefully\ndesigned accuracy detection reward and temporal compensation reward, it\neffectively avoids the use of high-cost text annotation data, enabling precise\ntemporal modeling and forgery detection. Meanwhile, we design a dual-encoder\narchitecture, combining high-level semantic reasoning and low-level artifact\namplification to guide MLLMs in effective forgery detection. We further collect\nFakeHumanVid, a large-scale human-centric video benchmark that includes\nsynthesis methods guided by pose, audio, and text inputs, enabling rigorous\nevaluation of detection methods in real-world scenes. Extensive experiments\nshow that AvatarShield significantly outperforms existing approaches in both\nin-domain and cross-domain detection, setting a new standard for human-centric\nvideo forensics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "annotation", "accuracy"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14996", "pdf": "https://arxiv.org/pdf/2505.14996", "abs": "https://arxiv.org/abs/2505.14996", "authors": ["Zixuan Ke", "Austin Xu", "Yifei Ming", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "title": "Meta-Design Matters: A Self-Design Multi-Agent System", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Multi-agent systems (MAS) leveraging the impressive capabilities of Large\nLanguage Models (LLMs) hold significant potential for tackling complex tasks.\nHowever, most current MAS depend on manually designed agent roles and\ncommunication protocols. These manual designs often fail to align with the\nunderlying LLMs' strengths and struggle to adapt to novel tasks. Recent\nautomatic MAS approaches attempt to mitigate these limitations but typically\nnecessitate a validation-set for tuning and yield static MAS designs lacking\nadaptability during inference. We introduce SELF-MAS, the first\nself-supervised, inference-time only framework for automatic MAS design.\nSELF-MAS employs meta-level design to iteratively generate, evaluate, and\nrefine MAS configurations tailored to each problem instance, without requiring\na validation set. Critically, it enables dynamic agent composition and problem\ndecomposition through meta-feedback on solvability and completeness.\nExperiments across math, graduate-level QA, and software engineering\nbenchmarks, using both closed-source and open-source LLM back-bones of varying\nsizes, demonstrate that SELF-MAS outperforms both manual and automatic MAS\nbaselines, achieving a 7.44% average accuracy improvement over the next\nstrongest baseline while maintaining cost-efficiency. These findings underscore\nthe promise of meta-level self-supervised design for creating effective and\nadaptive MAS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15045", "pdf": "https://arxiv.org/pdf/2505.15045", "abs": "https://arxiv.org/abs/2505.15045", "authors": ["Siyue Zhang", "Yilun Zhao", "Liyuan Geng", "Arman Cohan", "Anh Tuan Luu", "Chen Zhao"], "title": "Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM)-based embedding models, benefiting from large\nscale pre-training and post-training, have begun to surpass BERT and T5-based\nmodels on general-purpose text embedding tasks such as document retrieval.\nHowever, a fundamental limitation of LLM embeddings lies in the unidirectional\nattention used during autoregressive pre-training, which misaligns with the\nbidirectional nature of text embedding tasks. To this end, We propose adopting\ndiffusion language models for text embeddings, motivated by their inherent\nbidirectional architecture and recent success in matching or surpassing LLMs\nespecially on reasoning tasks. We present the first systematic study of the\ndiffusion language embedding model, which outperforms the LLM-based embedding\nmodel by 20% on long-document retrieval, 8% on reasoning-intensive retrieval,\n2% on instruction-following retrieval, and achieve competitive performance on\ntraditional text embedding benchmarks. Our analysis verifies that bidirectional\nattention is crucial for encoding global context in long and complex text.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15046", "pdf": "https://arxiv.org/pdf/2505.15046", "abs": "https://arxiv.org/abs/2505.15046", "authors": ["Yifan Wu", "Lutao Yan", "Leixian Shen", "Yinan Mei", "Jiannan Wang", "Yuyu Luo"], "title": "ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The emergence of Multi-modal Large Language Models (MLLMs) presents new\nopportunities for chart understanding. However, due to the fine-grained nature\nof these tasks, applying MLLMs typically requires large, high-quality datasets\nfor task-specific fine-tuning, leading to high data collection and training\ncosts. To address this, we propose ChartCards, a unified chart-metadata\ngeneration framework for multi-task chart understanding. ChartCards\nsystematically synthesizes various chart information, including data tables,\nvisualization code, visual elements, and multi-dimensional semantic captions.\nBy structuring this information into organized metadata, ChartCards enables a\nsingle chart to support multiple downstream tasks, such as text-to-chart\nretrieval, chart summarization, chart-to-table conversion, chart description,\nand chart question answering. Using ChartCards, we further construct MetaChart,\na large-scale high-quality dataset containing 10,862 data tables, 85K charts,\nand 170 K high-quality chart captions. We validate the dataset through\nqualitative crowdsourcing evaluations and quantitative fine-tuning experiments\nacross various chart understanding tasks. Fine-tuning six different models on\nMetaChart resulted in an average performance improvement of 5% across all\ntasks. The most notable improvements are seen in text-to-chart retrieval and\nchart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements\nof 17% and 28%, respectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "summarization", "question answering", "multi-dimensional", "fine-grained"], "score": 5}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15232", "pdf": "https://arxiv.org/pdf/2505.15232", "abs": "https://arxiv.org/abs/2505.15232", "authors": ["Ting Huang", "Zeyu Zhang", "Ruicheng Zhang", "Yang Zhao"], "title": "DC-Scene: Data-Centric Learning for 3D Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "3D scene understanding plays a fundamental role in vision applications such\nas robotics, autonomous driving, and augmented reality. However, advancing\nlearning-based 3D scene understanding remains challenging due to two key\nlimitations: (1) the large scale and complexity of 3D scenes lead to higher\ncomputational costs and slower training compared to 2D counterparts; and (2)\nhigh-quality annotated 3D datasets are significantly scarcer than those\navailable for 2D vision. These challenges underscore the need for more\nefficient learning paradigms. In this work, we propose DC-Scene, a data-centric\nframework tailored for 3D scene understanding, which emphasizes enhancing data\nquality and training efficiency. Specifically, we introduce a CLIP-driven\ndual-indicator quality (DIQ) filter, combining vision-language alignment scores\nwith caption-loss perplexity, along with a curriculum scheduler that\nprogressively expands the training pool from the top 25% to 75% of\nscene-caption pairs. This strategy filters out noisy samples and significantly\nreduces dependence on large-scale labeled 3D data. Extensive experiments on\nScanRefer and Nr3D demonstrate that DC-Scene achieves state-of-the-art\nperformance (86.1 CIDEr with the top-75% subset vs. 85.4 with the full dataset)\nwhile reducing training cost by approximately two-thirds, confirming that a\ncompact set of high-quality samples can outperform exhaustive training. Code\nwill be available at https://github.com/AIGeeksGroup/DC-Scene.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15263", "pdf": "https://arxiv.org/pdf/2505.15263", "abs": "https://arxiv.org/abs/2505.15263", "authors": ["Om Khangaonkar", "Hamed Pirsiavash"], "title": "gen2seg: Generative Models Enable Generalizable Instance Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Website: https://reachomk.github.io/gen2seg/", "summary": "By pretraining to synthesize coherent images from perturbed inputs,\ngenerative models inherently learn to understand object boundaries and scene\ncompositions. How can we repurpose these generative representations for\ngeneral-purpose perceptual organization? We finetune Stable Diffusion and MAE\n(encoder+decoder) for category-agnostic instance segmentation using our\ninstance coloring loss exclusively on a narrow set of object types (indoor\nfurnishings and cars). Surprisingly, our models exhibit strong zero-shot\ngeneralization, accurately segmenting objects of types and styles unseen in\nfinetuning (and in many cases, MAE's ImageNet-1K pretraining too). Our\nbest-performing models closely approach the heavily supervised SAM when\nevaluated on unseen object types and styles, and outperform it when segmenting\nfine structures and ambiguous boundaries. In contrast, existing promptable\nsegmentation architectures or discriminatively pretrained models fail to\ngeneralize. This suggests that generative models learn an inherent grouping\nmechanism that transfers across categories and domains, even without\ninternet-scale pretraining. Code, pretrained models, and demos are available on\nour website.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15074", "pdf": "https://arxiv.org/pdf/2505.15074", "abs": "https://arxiv.org/abs/2505.15074", "authors": ["Yuhang Zhou", "Jing Zhu", "Shengyi Qian", "Zhuokai Zhao", "Xiyao Wang", "Xiaoyu Liu", "Ming Li", "Paiheng Xu", "Wei Ai", "Furong Huang"], "title": "DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 3 figures", "summary": "Large Language Models (LLMs) are increasingly aligned with human preferences\nthrough Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,\nGroup Relative Policy Optimization (GRPO) has gained attention for its\nsimplicity and strong performance, notably eliminating the need for a learned\nvalue function. However, GRPO implicitly assumes a balanced domain distribution\nand uniform semantic alignment across groups - assumptions that rarely hold in\nreal-world datasets. When applied to multi-domain, imbalanced data, GRPO\ndisproportionately optimizes for dominant domains, neglecting underrepresented\nones and resulting in poor generalization and fairness. We propose\nDomain-Informed Self-Consistency Policy Optimization (DISCO), a principled\nextension to GRPO that addresses inter-group imbalance with two key\ninnovations. Domain-aware reward scaling counteracts frequency bias by\nreweighting optimization based on domain prevalence. Difficulty-aware reward\nscaling leverages prompt-level self-consistency to identify and prioritize\nuncertain prompts that offer greater learning value. Together, these strategies\npromote more equitable and effective policy learning across domains. Extensive\nexperiments across multiple LLMs and skewed training distributions show that\nDISCO improves generalization, outperforms existing GRPO variants by 5% on\nQwen3 models, and sets new state-of-the-art results on multi-domain alignment\nbenchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "policy optimization", "alignment"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15267", "pdf": "https://arxiv.org/pdf/2505.15267", "abs": "https://arxiv.org/abs/2505.15267", "authors": ["Wenmin Li", "Shunsuke Sakai", "Tatsuhito Hasegawa"], "title": "Contrastive Learning-Enhanced Trajectory Matching for Small-Scale Dataset Distillation", "categories": ["cs.CV"], "comment": "Under review", "summary": "Deploying machine learning models in resource-constrained environments, such\nas edge devices or rapid prototyping scenarios, increasingly demands\ndistillation of large datasets into significantly smaller yet informative\nsynthetic datasets. Current dataset distillation techniques, particularly\nTrajectory Matching methods, optimize synthetic data so that the model's\ntraining trajectory on synthetic samples mirrors that on real data. While\ndemonstrating efficacy on medium-scale synthetic datasets, these methods fail\nto adequately preserve semantic richness under extreme sample scarcity. To\naddress this limitation, we propose a novel dataset distillation method\nintegrating contrastive learning during image synthesis. By explicitly\nmaximizing instance-level feature discrimination, our approach produces more\ninformative and diverse synthetic samples, even when dataset sizes are\nsignificantly constrained. Experimental results demonstrate that incorporating\ncontrastive learning substantially enhances the performance of models trained\non very small-scale synthetic datasets. This integration not only guides more\neffective feature representation but also significantly improves the visual\nfidelity of the synthesized images. Experimental results demonstrate that our\nmethod achieves notable performance improvements over existing distillation\ntechniques, especially in scenarios with extremely limited synthetic data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15090", "pdf": "https://arxiv.org/pdf/2505.15090", "abs": "https://arxiv.org/abs/2505.15090", "authors": ["Sona Elza Simon", "Preethi Jyothi"], "title": "DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "Effective cross-lingual transfer remains a critical challenge in scaling the\nbenefits of large language models from high-resource to low-resource languages.\nTowards this goal, prior studies have explored many approaches to combine task\nknowledge from task-specific data in a (high-resource) source language and\nlanguage knowledge from unlabeled text in a (low-resource) target language. One\nnotable approach proposed composable sparse fine-tuning (SFT) for cross-lingual\ntransfer that learns task-specific and language-specific sparse masks to select\na subset of the pretrained model's parameters that are further fine-tuned.\nThese sparse fine-tuned vectors (SFTs) are subsequently composed with the\npretrained model to facilitate zero-shot cross-lingual transfer to a task in a\ntarget language, using only task-specific data from a source language. These\nsparse masks for SFTs were identified using a simple magnitude-based pruning.\nIn our work, we introduce DeFT-X, a novel composable SFT approach that denoises\nthe weight matrices of a pretrained model before magnitude pruning using\nsingular value decomposition, thus yielding more robust SFTs. We evaluate\nDeFT-X on a diverse set of extremely low-resource languages for sentiment\nclassification (NusaX) and natural language inference (AmericasNLI) and\ndemonstrate that it performs at par or outperforms SFT and other prominent\ncross-lingual transfer baselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15272", "pdf": "https://arxiv.org/pdf/2505.15272", "abs": "https://arxiv.org/abs/2505.15272", "authors": ["Eduarda Caldeira", "Jan Niklas Kolf", "Naser Damer", "Fadi Boutros"], "title": "DiffProb: Data Pruning for Face Recognition", "categories": ["cs.CV"], "comment": "Accepted at IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG) 2025", "summary": "Face recognition models have made substantial progress due to advances in\ndeep learning and the availability of large-scale datasets. However, reliance\non massive annotated datasets introduces challenges related to training\ncomputational cost and data storage, as well as potential privacy concerns\nregarding managing large face datasets. This paper presents DiffProb, the first\ndata pruning approach for the application of face recognition. DiffProb\nassesses the prediction probabilities of training samples within each identity\nand prunes the ones with identical or close prediction probability values, as\nthey are likely reinforcing the same decision boundaries, and thus contribute\nminimally with new information. We further enhance this process with an\nauxiliary cleaning mechanism to eliminate mislabeled and label-flipped samples,\nboosting data quality with minimal loss. Extensive experiments on CASIA-WebFace\nwith different pruning ratios and multiple benchmarks, including LFW, CFP-FP,\nand IJB-C, demonstrate that DiffProb can prune up to 50% of the dataset while\nmaintaining or even, in some settings, improving the verification accuracies.\nAdditionally, we demonstrate DiffProb's robustness across different\narchitectures and loss functions. Our method significantly reduces training\ncost and data volume, enabling efficient face recognition training and reducing\nthe reliance on massive datasets and their demanding management.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15287", "pdf": "https://arxiv.org/pdf/2505.15287", "abs": "https://arxiv.org/abs/2505.15287", "authors": ["Yuchen Li", "Chaoran Feng", "Zhenyu Tang", "Kaiyuan Deng", "Wangbo Yu", "Yonghong Tian", "Li Yuan"], "title": "GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation", "categories": ["cs.CV"], "comment": "21 pages, 7 figures. More details at\n  http://intothemild.github.io/GS2E.github.io", "summary": "We introduce GS2E (Gaussian Splatting to Event), a large-scale synthetic\nevent dataset for high-fidelity event vision tasks, captured from real-world\nsparse multi-view RGB images. Existing event datasets are often synthesized\nfrom dense RGB videos, which typically lack viewpoint diversity and geometric\nconsistency, or depend on expensive, difficult-to-scale hardware setups. GS2E\novercomes these limitations by first reconstructing photorealistic static\nscenes using 3D Gaussian Splatting, and subsequently employing a novel,\nphysically-informed event simulation pipeline. This pipeline generally\nintegrates adaptive trajectory interpolation with physically-consistent event\ncontrast threshold modeling. Such an approach yields temporally dense and\ngeometrically consistent event streams under diverse motion and lighting\nconditions, while ensuring strong alignment with underlying scene structures.\nExperimental results on event-based 3D reconstruction demonstrate GS2E's\nsuperior generalization capabilities and its practical value as a benchmark for\nadvancing event vision research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15110", "pdf": "https://arxiv.org/pdf/2505.15110", "abs": "https://arxiv.org/abs/2505.15110", "authors": ["Xuanliang Zhang", "Dingzirui Wang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che"], "title": "RoT: Enhancing Table Reasoning with Iterative Row-Wise Traversals", "categories": ["cs.CL"], "comment": null, "summary": "The table reasoning task, crucial for efficient data acquisition, aims to\nanswer questions based on the given table. Recently, reasoning large language\nmodels (RLLMs) with Long Chain-of-Thought (Long CoT) significantly enhance\nreasoning capabilities, leading to brilliant performance on table reasoning.\nHowever, Long CoT suffers from high cost for training and exhibits low\nreliability due to table content hallucinations. Therefore, we propose\nRow-of-Thought (RoT), which performs iteratively row-wise table traversal,\nallowing for reasoning extension and reflection-based refinement at each\ntraversal. Scaling reasoning length by row-wise traversal and leveraging\nreflection capabilities of LLMs, RoT is training-free. The sequential traversal\nencourages greater attention to the table, thus reducing hallucinations.\nExperiments show that RoT, using non-reasoning models, outperforms RLLMs by an\naverage of 4.3%, and achieves state-of-the-art results on WikiTableQuestions\nand TableBench with comparable models, proving its effectiveness. Also, RoT\noutperforms Long CoT with fewer reasoning tokens, indicating higher efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15325", "pdf": "https://arxiv.org/pdf/2505.15325", "abs": "https://arxiv.org/abs/2505.15325", "authors": ["Mengqi Lei", "Yihong Wu", "Siqi Li", "Xinhu Zheng", "Juan Wang", "Yue Gao", "Shaoyi Du"], "title": "SoftHGNN: Soft Hypergraph Neural Networks for General Visual Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Visual recognition relies on understanding both the semantics of image tokens\nand the complex interactions among them. Mainstream self-attention methods,\nwhile effective at modeling global pair-wise relations, fail to capture\nhigh-order associations inherent in real-world scenes and often suffer from\nredundant computation. Hypergraphs extend conventional graphs by modeling\nhigh-order interactions and offer a promising framework for addressing these\nlimitations. However, existing hypergraph neural networks typically rely on\nstatic and hard hyperedge assignments, leading to excessive and redundant\nhyperedges with hard binary vertex memberships that overlook the continuity of\nvisual semantics. To overcome these issues, we present Soft Hypergraph Neural\nNetworks (SoftHGNNs), which extend the methodology of hypergraph computation,\nto make it truly efficient and versatile in visual recognition tasks. Our\nframework introduces the concept of soft hyperedges, where each vertex is\nassociated with hyperedges via continuous participation weights rather than\nhard binary assignments. This dynamic and differentiable association is\nachieved by using the learnable hyperedge prototype. Through similarity\nmeasurements between token features and the prototype, the model generates\nsemantically rich soft hyperedges. SoftHGNN then aggregates messages over soft\nhyperedges to capture high-order semantics. To further enhance efficiency when\nscaling up the number of soft hyperedges, we incorporate a sparse hyperedge\nselection mechanism that activates only the top-k important hyperedges, along\nwith a load-balancing regularizer to ensure balanced hyperedge utilization.\nExperimental results across three tasks on five datasets demonstrate that\nSoftHGNN efficiently captures high-order associations in visual scenes,\nachieving significant performance improvements.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15117", "pdf": "https://arxiv.org/pdf/2505.15117", "abs": "https://arxiv.org/abs/2505.15117", "authors": ["Bowen Jin", "Jinsung Yoon", "Priyanka Kargupta", "Sercan O. Arik", "Jiawei Han"], "title": "An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "22 pages", "summary": "Reinforcement learning (RL) has demonstrated strong potential in training\nlarge language models (LLMs) capable of complex reasoning for real-world\nproblem solving. More recently, RL has been leveraged to create sophisticated\nLLM-based search agents that adeptly combine reasoning with search engine use.\nWhile the use of RL for training search agents is promising, the optimal design\nof such agents remains not fully understood. In particular, key factors -- such\nas (1) reward formulation, (2) the choice and characteristics of the underlying\nLLM, and (3) the role of the search engine in the RL process -- require further\ninvestigation. In this work, we conduct comprehensive empirical studies to\nsystematically investigate these and offer actionable insights. We highlight\nseveral key findings: format rewards are effective in improving final\nperformance, whereas intermediate retrieval rewards have limited impact; the\nscale and initialization of the LLM (general-purpose vs. reasoning-specialized)\nsignificantly influence RL outcomes; and the choice of search engine plays a\ncritical role in shaping RL training dynamics and the robustness of the trained\nagent during inference. These establish important guidelines for successfully\nbuilding and deploying LLM-based search agents in real-world applications. Code\nis available at https://github.com/PeterGriffinJin/Search-R1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15196", "pdf": "https://arxiv.org/pdf/2505.15196", "abs": "https://arxiv.org/abs/2505.15196", "authors": ["Weiqi Wang", "Limeng Cui", "Xin Liu", "Sreyashi Nag", "Wenju Xu", "Chen Luo", "Sheikh Muhammad Sarwar", "Yang Li", "Hansu Gu", "Hui Liu", "Changlong Yu", "Jiaxin Bai", "Yifan Gao", "Haiyang Zhang", "Qi He", "Shuiwang Ji", "Yangqiu Song"], "title": "EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via Step-wise Intention-Driven Product Association", "categories": ["cs.CL"], "comment": "ACL2025", "summary": "Goal-oriented script planning, or the ability to devise coherent sequences of\nactions toward specific goals, is commonly employed by humans to plan for\ntypical activities. In e-commerce, customers increasingly seek LLM-based\nassistants to generate scripts and recommend products at each step, thereby\nfacilitating convenient and efficient shopping experiences. However, this\ncapability remains underexplored due to several challenges, including the\ninability of LLMs to simultaneously conduct script planning and product\nretrieval, difficulties in matching products caused by semantic discrepancies\nbetween planned actions and search queries, and a lack of methods and benchmark\ndata for evaluation. In this paper, we step forward by formally defining the\ntask of E-commerce Script Planning (EcomScript) as three sequential subtasks.\nWe propose a novel framework that enables the scalable generation of\nproduct-enriched scripts by associating products with each step based on the\nsemantic similarity between the actions and their purchase intentions. By\napplying our framework to real-world e-commerce data, we construct the very\nfirst large-scale EcomScript dataset, EcomScriptBench, which includes 605,229\nscripts sourced from 2.4 million products. Human annotations are then conducted\nto provide gold labels for a sampled subset, forming an evaluation benchmark.\nExtensive experiments reveal that current (L)LMs face significant challenges\nwith EcomScript tasks, even after fine-tuning, while injecting product purchase\nintentions improves their performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15367", "pdf": "https://arxiv.org/pdf/2505.15367", "abs": "https://arxiv.org/abs/2505.15367", "authors": ["Dasol Choi", "Seunghyun Lee", "Youngsook Song"], "title": "Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "13 pages", "summary": "Vision-Language Models (VLMs) have demonstrated impressive capabilities in\nunderstanding visual content, but their reliability in safety-critical contexts\nremains under-explored. We introduce VERI (Visual Emergency Recognition\nDataset), a carefully designed diagnostic benchmark of 200 images (100\ncontrastive pairs). Each emergency scene is matched with a visually similar but\nsafe counterpart through multi-stage human verification and iterative\nrefinement. Using a two-stage protocol - risk identification and emergency\nresponse - we evaluate 14 VLMs (2B-124B parameters) across medical emergencies,\naccidents, and natural disasters. Our analysis reveals a systematic\noverreaction problem: models excel at identifying real emergencies (70-100\npercent success rate) but suffer from an alarming rate of false alarms,\nmisidentifying 31-96 percent of safe situations as dangerous, with 10 scenarios\nfailed by all models regardless of scale. This \"better-safe-than-sorry\" bias\nmanifests primarily through contextual overinterpretation (88-93 percent of\nerrors), challenging VLMs' reliability for safety applications. These findings\nhighlight persistent limitations that are not resolved by increasing model\nscale, motivating targeted approaches for improving contextual safety\nassessment in visually misleading scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety", "reliability"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15379", "pdf": "https://arxiv.org/pdf/2505.15379", "abs": "https://arxiv.org/abs/2505.15379", "authors": ["Raphael Sulzer", "Liuyun Duan", "Nicolas Girard", "Florent Lafarge"], "title": "The P$^3$ dataset: Pixels, Points and Polygons for Multimodal Building Vectorization", "categories": ["cs.CV"], "comment": null, "summary": "We present the P$^3$ dataset, a large-scale multimodal benchmark for building\nvectorization, constructed from aerial LiDAR point clouds, high-resolution\naerial imagery, and vectorized 2D building outlines, collected across three\ncontinents. The dataset contains over 10 billion LiDAR points with\ndecimeter-level accuracy and RGB images at a ground sampling distance of 25\ncentimeter. While many existing datasets primarily focus on the image modality,\nP$^3$ offers a complementary perspective by also incorporating dense 3D\ninformation. We demonstrate that LiDAR point clouds serve as a robust modality\nfor predicting building polygons, both in hybrid and end-to-end learning\nframeworks. Moreover, fusing aerial LiDAR and imagery further improves accuracy\nand geometric quality of predicted polygons. The P$^3$ dataset is publicly\navailable, along with code and pretrained weights of three state-of-the-art\nmodels for building polygon prediction at\nhttps://github.com/raphaelsulzer/PixelsPointsPolygons .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15255", "pdf": "https://arxiv.org/pdf/2505.15255", "abs": "https://arxiv.org/abs/2505.15255", "authors": ["Yuansheng Gao", "Han Bao", "Tong Zhang", "Bin Li", "Zonghui Wang", "Wenzhi Chen"], "title": "MentalMAC: Enhancing Large Language Models for Detecting Mental Manipulation via Multi-Task Anti-Curriculum Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Mental manipulation is a subtle yet pervasive form of psychological abuse\nthat poses serious threats to mental health. Its covert nature and the\ncomplexity of manipulation strategies make it challenging to detect, even for\nstate-of-the-art large language models (LLMs). This concealment also hinders\nthe manual collection of large-scale, high-quality annotations essential for\ntraining effective models. Although recent efforts have sought to improve LLM's\nperformance on this task, progress remains limited due to the scarcity of\nreal-world annotated datasets. To address these challenges, we propose\nMentalMAC, a multi-task anti-curriculum distillation method that enhances LLMs'\nability to detect mental manipulation in multi-turn dialogue. Our approach\nincludes: (i) EvoSA, an unsupervised data expansion method based on\nevolutionary operations and speech act theory; (ii) teacher-model-generated\nmulti-task supervision; and (iii) progressive knowledge distillation from\ncomplex to simpler tasks. We then constructed the ReaMent dataset with 5,000\nreal-world dialogue samples, using a MentalMAC-distilled model to assist human\nannotation. Vast experiments demonstrate that our method significantly narrows\nthe gap between student and teacher models and outperforms competitive LLMs\nacross key evaluation metrics. All code, datasets, and checkpoints will be\nreleased upon paper acceptance. Warning: This paper contains content that may\nbe offensive to readers.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation", "dialogue"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15257", "pdf": "https://arxiv.org/pdf/2505.15257", "abs": "https://arxiv.org/abs/2505.15257", "authors": ["Weixiang Zhao", "Jiahe Guo", "Yang Deng", "Tongtong Wu", "Wenxuan Zhang", "Yulin Hu", "Xingyu Sui", "Yanyan Zhao", "Wanxiang Che", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "title": "When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners", "categories": ["cs.CL"], "comment": "26 pages, 13 figures", "summary": "Multilingual reasoning remains a significant challenge for large language\nmodels (LLMs), with performance disproportionately favoring high-resource\nlanguages. Drawing inspiration from cognitive neuroscience, which suggests that\nhuman reasoning functions largely independently of language processing, we\nhypothesize that LLMs similarly encode reasoning and language as separable\ncomponents that can be disentangled to enhance multilingual reasoning. To\nevaluate this, we perform a causal intervention by ablating language-specific\nrepresentations at inference time. Experiments on 10 open-source LLMs spanning\n11 typologically diverse languages show that this language-specific ablation\nconsistently boosts multilingual reasoning performance. Layer-wise analyses\nfurther confirm that language and reasoning representations can be effectively\ndecoupled throughout the model, yielding improved multilingual reasoning\ncapabilities, while preserving top-layer language features remains essential\nfor maintaining linguistic fidelity. Compared to post-training such as\nsupervised fine-tuning or reinforcement learning, our training-free ablation\nachieves comparable or superior results with minimal computational overhead.\nThese findings shed light on the internal mechanisms underlying multilingual\nreasoning in LLMs and suggest a lightweight and interpretable strategy for\nimproving cross-lingual generalization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15441", "pdf": "https://arxiv.org/pdf/2505.15441", "abs": "https://arxiv.org/abs/2505.15441", "authors": ["David Nordström", "Johan Edstedt", "Fredrik Kahl", "Georg Bökman"], "title": "Stronger ViTs With Octic Equivariance", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent efforts at scaling computer vision models have established Vision\nTransformers (ViTs) as the leading architecture. ViTs incorporate weight\nsharing over image patches as an important inductive bias. In this work, we\nshow that ViTs benefit from incorporating equivariance under the octic group,\ni.e., reflections and 90-degree rotations, as a further inductive bias. We\ndevelop new architectures, octic ViTs, that use octic-equivariant layers and\nput them to the test on both supervised and self-supervised learning. Through\nextensive experiments on DeiT-III and DINOv2 training on ImageNet-1K, we show\nthat octic ViTs yield more computationally efficient networks while also\nimproving performance. In particular, we achieve approximately 40% reduction in\nFLOPs for ViT-H while simultaneously improving both classification and\nsegmentation results.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15489", "pdf": "https://arxiv.org/pdf/2505.15489", "abs": "https://arxiv.org/abs/2505.15489", "authors": ["Jiaying Wu", "Fanxiao Li", "Min-Yen Kan", "Bryan Hooi"], "title": "Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "The real-world impact of misinformation stems from the underlying misleading\nnarratives that creators seek to convey. As such, interpreting misleading\ncreator intent is essential for multimodal misinformation detection (MMD)\nsystems aimed at effective information governance. In this paper, we introduce\nan automated framework that simulates real-world multimodal news creation by\nexplicitly modeling creator intent through two components: the desired\ninfluence and the execution plan. Using this framework, we construct\nDeceptionDecoded, a large-scale benchmark comprising 12,000 image-caption pairs\naligned with trustworthy reference articles. The dataset captures both\nmisleading and non-misleading intents and spans manipulations across visual and\ntextual modalities. We conduct a comprehensive evaluation of 14\nstate-of-the-art vision-language models (VLMs) on three intent-centric tasks:\n(1) misleading intent detection, (2) misleading source attribution, and (3)\ncreator desire inference. Despite recent advances, we observe that current VLMs\nfall short in recognizing misleading intent, often relying on spurious cues\nsuch as superficial cross-modal consistency, stylistic signals, and heuristic\nauthenticity hints. Our findings highlight the pressing need for intent-aware\nmodeling in MMD and open new directions for developing systems capable of\ndeeper reasoning about multimodal misinformation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "consistency"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15348", "pdf": "https://arxiv.org/pdf/2505.15348", "abs": "https://arxiv.org/abs/2505.15348", "authors": ["Enric Junqué de Fortuny"], "title": "The Super Emotion Dataset", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite the wide-scale usage and development of emotion classification\ndatasets in NLP, the field lacks a standardized, large-scale resource that\nfollows a psychologically grounded taxonomy. Existing datasets either use\ninconsistent emotion categories, suffer from limited sample size, or focus on\nspecific domains. The Super Emotion Dataset addresses this gap by harmonizing\ndiverse text sources into a unified framework based on Shaver's empirically\nvalidated emotion taxonomy, enabling more consistent cross-domain emotion\nrecognition research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15506", "pdf": "https://arxiv.org/pdf/2505.15506", "abs": "https://arxiv.org/abs/2505.15506", "authors": ["Debarshi Brahma", "Anuska Roy", "Soma Biswas"], "title": "Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts", "categories": ["cs.CV", "cs.LG"], "comment": "Published in TMLR (2025)", "summary": "Recently, Vision-Language foundation models like CLIP and ALIGN, which are\npre-trained on large-scale data have shown remarkable zero-shot generalization\nto diverse datasets with different classes and even domains. In this work, we\ntake a step further and analyze whether these models can be adapted to target\ndatasets having very different distributions and classes compared to what these\nmodels have been trained on, using only a few labeled examples from the target\ndataset. In such scenarios, finetuning large pretrained models is challenging\ndue to problems of overfitting as well as loss of generalization, and has not\nbeen well explored in prior literature. Since, the pre-training data of such\nmodels are unavailable, it is difficult to comprehend the performance on\nvarious downstream datasets. First, we try to answer the question: Given a\ntarget dataset with a few labelled examples, can we estimate whether further\nfine-tuning can enhance the performance compared to zero-shot evaluation? by\nanalyzing the common vision-language embedding space. Based on the analysis, we\npropose a novel prompt-tuning method, PromptMargin for adapting such\nlarge-scale VLMs directly on the few target samples. PromptMargin effectively\ntunes the text as well as visual prompts for this task, and has two main\nmodules: 1) Firstly, we use a selective augmentation strategy to complement the\nfew training samples in each task; 2) Additionally, to ensure robust training\nin the presence of unfamiliar class names, we increase the inter-class margin\nfor improved class discrimination using a novel Multimodal Margin Regularizer.\nExtensive experiments and analysis across fifteen target benchmark datasets,\nwith varying degrees of distribution shifts from natural images, shows the\neffectiveness of the proposed framework over the existing state-of-the-art\napproaches applied to this setting. github.com/debarshigit/PromptMargin.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15389", "pdf": "https://arxiv.org/pdf/2505.15389", "abs": "https://arxiv.org/abs/2505.15389", "authors": ["DongGeon Lee", "Joonwon Jang", "Jihae Jeong", "Hwanjo Yu"], "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study", "categories": ["cs.CL", "cs.CR", "cs.CV"], "comment": null, "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs show greater vulnerability to\nmeme-based harmful prompts than to synthetic or typographic images. Memes\nsignificantly increase harmful responses and decrease refusals compared to\ntext-only inputs. Though multi-turn interactions provide partial mitigation,\nelevated vulnerability persists. These results highlight the need for\necologically valid evaluations and stronger safety mechanisms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15392", "pdf": "https://arxiv.org/pdf/2505.15392", "abs": "https://arxiv.org/abs/2505.15392", "authors": ["Yiming Huang", "Biquan Bie", "Zuqiu Na", "Weilin Ruan", "Songxin Lei", "Yutao Yue", "Xinlei He"], "title": "An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism, and Potential Mitigations", "categories": ["cs.CL"], "comment": null, "summary": "The rise of Large Language Models (LLMs) like ChatGPT has advanced natural\nlanguage processing, yet concerns about cognitive biases are growing. In this\npaper, we investigate the anchoring effect, a cognitive bias where the mind\nrelies heavily on the first information as anchors to make affected judgments.\nWe explore whether LLMs are affected by anchoring, the underlying mechanisms,\nand potential mitigation strategies. To facilitate studies at scale on the\nanchoring effect, we introduce a new dataset, SynAnchors. Combining refined\nevaluation metrics, we benchmark current widely used LLMs. Our findings show\nthat LLMs' anchoring bias exists commonly with shallow-layer acting and is not\neliminated by conventional strategies, while reasoning can offer some\nmitigation. This recontextualization via cognitive psychology urges that LLM\nevaluations focus not on standard benchmarks or over-optimized robustness\ntests, but on cognitive-bias-aware trustworthy evaluation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15545", "pdf": "https://arxiv.org/pdf/2505.15545", "abs": "https://arxiv.org/abs/2505.15545", "authors": ["Andrew Caunes", "Thierry Chateau", "Vincent Fremont"], "title": "seg_3D_by_PC2D: Multi-View Projection for Domain Generalization and Adaptation in 3D Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic segmentation plays a pivotal role in autonomous driving and road\ninfrastructure analysis, yet state-of-the-art 3D models are prone to severe\ndomain shift when deployed across different datasets. We propose a novel\nmulti-view projection framework that excels in both domain generalization (DG)\nand unsupervised domain adaptation (UDA). Our approach first aligns Lidar scans\ninto coherent 3D scenes and renders them from multiple virtual camera poses to\ncreate a large-scale synthetic 2D dataset (PC2D). We then use it to train a 2D\nsegmentation model in-domain. During inference, the model processes hundreds of\nviews per scene; the resulting logits are back-projected to 3D with an\nocclusion-aware voting scheme to generate final point-wise labels. Our\nframework is modular and enables extensive exploration of key design\nparameters, such as view generation optimization (VGO), visualization modality\noptimization (MODO), and 2D model choice. We evaluate on the nuScenes and\nSemanticKITTI datasets under both the DG and UDA settings. We achieve\nstate-of-the-art results in UDA and close to state-of-the-art in DG, with\nparticularly large gains on large, static classes. Our code and dataset\ngeneration tools will be publicly available at\nhttps://github.com/andrewcaunes/ia4markings", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15564", "pdf": "https://arxiv.org/pdf/2505.15564", "abs": "https://arxiv.org/abs/2505.15564", "authors": ["Hossein Hassani", "Soodeh Nikan", "Abdallah Shami"], "title": "TinyDrive: Multiscale Visual Question Answering with Selective Token Routing for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Vision Language Models (VLMs) employed for visual question-answering (VQA) in\nautonomous driving often require substantial computational resources that pose\na challenge for their deployment in resource-constrained vehicles. To address\nthis challenge, we introduce TinyDrive, a lightweight yet effective VLM for\nmulti-view VQA in driving scenarios. Our model comprises two key components\nincluding a multiscale vision encoder and a dual-level prioritization mechanism\nfor tokens and sequences. The multiscale encoder facilitates the processing of\nmulti-view images at diverse resolutions through scale injection and\ncross-scale gating to generate enhanced visual representations. At the token\nlevel, we design a token routing mechanism that dynamically selects and process\nthe most informative tokens based on learned importance scores. At the sequence\nlevel, we propose integrating normalized loss, uncertainty estimates, and a\ndiversity metric to formulate sequence scores that rank and preserve samples\nwithin a sequence priority buffer. Samples with higher scores are more\nfrequently selected for training. TinyDrive is first evaluated on our\ncustom-curated VQA dataset, and it is subsequently tested on the public DriveLM\nbenchmark, where it achieves state-of-the-art language understanding\nperformance. Notably, it achieves relative improvements of 11.1% and 35.4% in\nBLEU-4 and METEOR scores, respectively, despite having a significantly smaller\nparameter count.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15581", "pdf": "https://arxiv.org/pdf/2505.15581", "abs": "https://arxiv.org/abs/2505.15581", "authors": ["Hua Li", "Shijie Lian", "Zhiyuan Li", "Runmin Cong", "Sam Kwong"], "title": "UWSAM: Segment Anything Model Guided Underwater Instance Segmentation and A Large-scale Benchmark Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With recent breakthroughs in large-scale modeling, the Segment Anything Model\n(SAM) has demonstrated significant potential in a variety of visual\napplications. However, due to the lack of underwater domain expertise, SAM and\nits variants face performance limitations in end-to-end underwater instance\nsegmentation tasks, while their higher computational requirements further\nhinder their application in underwater scenarios. To address this challenge, we\npropose a large-scale underwater instance segmentation dataset, UIIS10K, which\nincludes 10,048 images with pixel-level annotations for 10 categories. Then, we\nintroduce UWSAM, an efficient model designed for automatic and accurate\nsegmentation of underwater instances. UWSAM efficiently distills knowledge from\nthe SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the\nMask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective\nvisual representation learning. Furthermore, we design an End-to-end Underwater\nPrompt Generator (EUPG) for UWSAM, which automatically generates underwater\nprompts instead of explicitly providing foreground points or boxes as prompts,\nthus enabling the network to locate underwater instances accurately for\nefficient segmentation. Comprehensive experimental results show that our model\nis effective, achieving significant performance improvements over\nstate-of-the-art methods on multiple underwater instance datasets. Datasets and\ncodes are available at https://github.com/LiamLian0727/UIIS10K.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15592", "pdf": "https://arxiv.org/pdf/2505.15592", "abs": "https://arxiv.org/abs/2505.15592", "authors": ["Niccolo Avogaro", "Thomas Frick", "Yagmur G. Cinar", "Daniel Caraballo", "Cezary Skura", "Filip M. Janicki", "Piotr Kluska", "Brown Ebouky", "Nicola Farronato", "Florian Scheidegger", "Cristiano Malossi", "Konrad Schindler", "Andrea Bartezzaghi", "Roy Assaf", "Mattia Rigotti"], "title": "VP Lab: a PEFT-Enabled Visual Prompting Laboratory for Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale pretrained vision backbones have transformed computer vision by\nproviding powerful feature extractors that enable various downstream tasks,\nincluding training-free approaches like visual prompting for semantic\nsegmentation. Despite their success in generic scenarios, these models often\nfall short when applied to specialized technical domains where the visual\nfeatures differ significantly from their training distribution. To bridge this\ngap, we introduce VP Lab, a comprehensive iterative framework that enhances\nvisual prompting for robust segmentation model development. At the core of VP\nLab lies E-PEFT, a novel ensemble of parameter-efficient fine-tuning techniques\nspecifically designed to adapt our visual prompting pipeline to specific\ndomains in a manner that is both parameter- and data-efficient. Our approach\nnot only surpasses the state-of-the-art in parameter-efficient fine-tuning for\nthe Segment Anything Model (SAM), but also facilitates an interactive,\nnear-real-time loop, allowing users to observe progressively improving results\nas they experiment within the framework. By integrating E-PEFT with visual\nprompting, we demonstrate a remarkable 50\\% increase in semantic segmentation\nmIoU performance across various technical datasets using only 5 validated\nimages, establishing a new paradigm for fast, efficient, and interactive model\ndeployment in new, challenging domains. This work comes in the form of a\ndemonstration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15431", "pdf": "https://arxiv.org/pdf/2505.15431", "abs": "https://arxiv.org/abs/2505.15431", "authors": ["Ao Liu", "Botong Zhou", "Can Xu", "Chayse Zhou", "ChenChen Zhang", "Chengcheng Xu", "Chenhao Wang", "Decheng Wu", "Dengpeng Wu", "Dian Jiao", "Dong Du", "Dong Wang", "Feng Zhang", "Fengzong Lian", "Guanghui Xu", "Guanwei Zhang", "Hai Wang", "Haipeng Luo", "Han Hu", "Huilin Xu", "Jiajia Wu", "Jianchen Zhu", "Jianfeng Yan", "Jiaqi Zhu", "Jihong Zhang", "Jinbao Xue", "Jun Xia", "Junqiang Zheng", "Kai Liu", "Kai Zhang", "Kai Zheng", "Kejiao Li", "Keyao Wang", "Lan Jiang", "Lixin Liu", "Lulu Wu", "Mengyuan Huang", "Peijie Yu", "Peiqi Wang", "Qian Wang", "Qianbiao Xiang", "Qibin Liu", "Qingfeng Sun", "Richard Guo", "Ruobing Xie", "Saiyong Yang", "Shaohua Chen", "Shihui Hu", "Shuai Li", "Shuaipeng Li", "Shuang Chen", "Suncong Zheng", "Tao Yang", "Tian Zhang", "Tinghao Yu", "Weidong Han", "Weijie Liu", "Weijin Zhou", "Weikang Wang", "Wesleye Chen", "Xiao Feng", "Xiaoqin Ren", "Xingwu Sun", "Xiong Kuang", "Xuemeng Huang", "Xun Cao", "Yanfeng Chen", "Yang Du", "Yang Zhen", "Yangyu Tao", "Yaping Deng", "Yi Shen", "Yigeng Hong", "Yiqi Chen", "Yiqing Huang", "Yuchi Deng", "Yue Mao", "Yulong Wang", "Yuyuan Zeng", "Zenan Xu", "Zhanhui Kang", "Zhe Zhao", "ZhenXiang Yan", "Zheng Fang", "Zhichao Hu", "Zhongzhi Chen", "Zhuoyu Li", "Zongwei Li", "Alex Yan", "Ande Liang", "Baitong Liu", "Beiping Pan", "Bin Xing", "Binghong Wu", "Bingxin Qu", "Bolin Ni", "Boyu Wu", "Chen Li", "Cheng Jiang", "Cheng Zhang", "Chengjun Liu", "Chengxu Yang", "Chiyu Wang", "Chong Zha", "Daisy Yi", "Di Wang", "Fanyang Lu", "Fei Chen", "Feifei Liu", "Feng Zheng", "Guanghua Yu", "Guiyang Li", "Guohua Wang", "Haisheng Lin", "Han Liu", "Han Wang", "Hao Fei", "Hao Lu", "Haoqing Jiang", "Haoran Sun", "Haotian Zhu", "Huangjin Dai", "Huankui Chen", "Huawen Feng", "Huihui Cai", "Huxin Peng", "Jackson Lv", "Jiacheng Shi", "Jiahao Bu", "Jianbo Li", "Jianglu Hu", "Jiangtao Guan", "Jianing Xu", "Jianwei Cai", "Jiarong Zhang", "Jiawei Song", "Jie Jiang", "Jie Liu", "Jieneng Yang", "Jihong Zhang", "Jin lv", "Jing Zhao", "Jinjian Li", "Jinxing Liu", "Jun Zhao", "Juntao Guo", "Kai Wang", "Kan Wu", "Lei Fu", "Lei He", "Lei Wang", "Li Liu", "Liang Dong", "Liya Zhan", "Long Cheng", "Long Xu", "Mao Zheng", "Meng Liu", "Mengkang Hu", "Nanli Chen", "Peirui Chen", "Peng He", "Pengju Pan", "Pengzhi Wei", "Qi Yang", "Qi Yi", "Roberts Wang", "Rongpeng Chen", "Rui Sun", "Rui Yang", "Ruibin Chen", "Ruixu Zhou", "Shaofeng Zhang", "Sheng Zhang", "Shihao Xu", "Shuaishuai Chang", "Shulin Liu", "SiQi Wang", "Songjia Feng", "Songling Yuan", "Tao Zhang", "Tianjiao Lang", "Tongkai Li", "Wei Deng", "Wei Li", "Weichao Wang", "Weigang Zhang", "Weixuan Sun", "Wen Ouyang", "Wenxiang Jiao", "Wenzhi Sun", "Wenzhuo Jia", "Xiang Zhang", "Xiangyu He", "Xianshun Ren", "XiaoYing Zhu", "Xiaolong Guo", "Xiaoxue Li", "Xiaoyu Ma", "Xican Lu", "Xinhua Feng", "Xinting Huang", "Xinyu Guan", "Xirui Li", "Xu Zhang", "Xudong Gao", "Xun Luo", "Xuxiang Qi", "Yangkun Chen", "Yangyu Tao", "Yanling Xiao", "Yantao Mai", "Yanze Chen", "Yao Ding", "Yeting Yang", "YiFan Song", "Yifan Yang", "Yijiao Zhu", "Yinhe Wu", "Yixian Liu", "Yong Yang", "Yuanjun Cai", "Yuanlin Tu", "Yue Zhang", "Yufei Huang", "Yuhang Zhou", "Yuhao Jiang", "Yuhong Liu", "Yuhui Hu", "Yujin Lin", "Yun Yang", "Yunhao Wang", "Yusong Zhang", "Zekun Wu", "Zelong Zhang", "Zhan Yu", "Zhaoliang Yang", "Zhe Zhao", "Zheng Li", "Zhenyu Huang", "Zhiguang Liu", "Zhijiang Xu", "Zhiqing Kui", "Zhiyin Zeng", "Zhiyuan Xiong", "Zhuo Han", "Zifan Wu", "Zigang Geng", "Zilong Zhao", "Ziyan Tang", "Ziyuan Zhu", "Zonglei Zhu", "Zhijiang Xu"], "title": "Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15637", "pdf": "https://arxiv.org/pdf/2505.15637", "abs": "https://arxiv.org/abs/2505.15637", "authors": ["Pujun Xue", "Junyi Ge", "Xiaotong Jiang", "Siyang Song", "Zijian Wu", "Yupeng Huo", "Weicheng Xie", "Linlin Shen", "Xiaoqin Zhou", "Xiaofeng Liu", "Min Gu"], "title": "Oral Imaging for Malocclusion Issues Assessments: OMNI Dataset, Deep Learning Baselines and Benchmarking", "categories": ["cs.CV"], "comment": null, "summary": "Malocclusion is a major challenge in orthodontics, and its complex\npresentation and diverse clinical manifestations make accurate localization and\ndiagnosis particularly important. Currently, one of the major shortcomings\nfacing the field of dental image analysis is the lack of large-scale,\naccurately labeled datasets dedicated to malocclusion issues, which limits the\ndevelopment of automated diagnostics in the field of dentistry and leads to a\nlack of diagnostic accuracy and efficiency in clinical practice. Therefore, in\nthis study, we propose the Oral and Maxillofacial Natural Images (OMNI)\ndataset, a novel and comprehensive dental image dataset aimed at advancing the\nstudy of analyzing dental images for issues of malocclusion. Specifically, the\ndataset contains 4166 multi-view images with 384 participants in data\ncollection and annotated by professional dentists. In addition, we performed a\ncomprehensive validation of the created OMNI dataset, including three CNN-based\nmethods, two Transformer-based methods, and one GNN-based method, and conducted\nautomated diagnostic experiments for malocclusion issues. The experimental\nresults show that the OMNI dataset can facilitate the automated diagnosis\nresearch of malocclusion issues and provide a new benchmark for the research in\nthis field. Our OMNI dataset and baseline code are publicly available at\nhttps://github.com/RoundFaceJ/OMNI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15442", "pdf": "https://arxiv.org/pdf/2505.15442", "abs": "https://arxiv.org/abs/2505.15442", "authors": ["Suhas Kamasetty Ramesh", "Ayan Sengupta", "Tanmoy Chakraborty"], "title": "On the Generalization vs Fidelity Paradox in Knowledge Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge distillation (KD) is a key technique for compressing large language\nmodels into smaller ones while preserving performance. Despite the recent\ntraction of KD research, its effectiveness for smaller language models (LMs)\nand the mechanisms driving knowledge transfer remain underexplored. In this\nwork, we present the first large-scale empirical and statistical analysis of KD\nacross models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks\nin a zero-shot setting. Our findings reveal that KD can improve the average\nperformance of smaller models by up to $10\\%$, with a peak task specific gain\nof $22\\%$, while providing only marginal benefits ($\\sim 1.3\\%$) for larger\nmodels. Surprisingly, teacher performance has a minimal impact on student\noutcomes, while teacher task expertise impacts KD effectiveness. A correlation\nstudy indicates that smaller LMs benefit more from KD, whereas larger LMs show\ndiminished gains. Additionally, we uncover a misalignment between improvements\nin student performance and reasoning fidelity, suggesting that while KD\nenhances accuracy, it does not always maintain the structured decision-making\nprocesses of the teacher. Our ablation study further highlights the importance\nof teacher signals and logit smoothing in influencing students' performance\nafter distillation. Overall, our study offers a comprehensive empirical and\nstatistical assessment of KD, highlighting both its benefits and trade-offs\nwhen distilling knowledge from larger to smaller LMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15644", "pdf": "https://arxiv.org/pdf/2505.15644", "abs": "https://arxiv.org/abs/2505.15644", "authors": ["Zhen Sun", "Ziyi Zhang", "Zeren Luo", "Zeyang Sha", "Tianshuo Cong", "Zheng Li", "Shiwen Cui", "Weiqiang Wang", "Jiaheng Wei", "Xinlei He", "Qi Li", "Qian Wang"], "title": "FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "14pages,15 figures", "summary": "Fine-grained edited image detection of localized edits in images is crucial\nfor assessing content authenticity, especially given that modern diffusion\nmodels and image editing methods can produce highly realistic manipulations.\nHowever, this domain faces three challenges: (1) Binary classifiers yield only\na global real-or-fake label without providing localization; (2) Traditional\ncomputer vision methods often rely on costly pixel-level annotations; and (3)\nNo large-scale, high-quality dataset exists for modern image-editing detection\ntechniques. To address these gaps, we develop an automated data-generation\npipeline to create FragFake, the first dedicated benchmark dataset for edited\nimage detection, which includes high-quality images from diverse editing models\nand a wide variety of edited objects. Based on FragFake, we utilize Vision\nLanguage Models (VLMs) for the first time in the task of edited image\nclassification and edited region localization. Experimental results show that\nfine-tuned VLMs achieve higher average Object Precision across all datasets,\nsignificantly outperforming pretrained models. We further conduct ablation and\ntransferability analyses to evaluate the detectors across various\nconfigurations and editing scenarios. To the best of our knowledge, this work\nis the first to reformulate localized image edit detection as a vision-language\nunderstanding task, establishing a new paradigm for the field. We anticipate\nthat this work will establish a solid foundation to facilitate and inspire\nsubsequent research endeavors in the domain of multimodal content authenticity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15501", "pdf": "https://arxiv.org/pdf/2505.15501", "abs": "https://arxiv.org/abs/2505.15501", "authors": ["Federico Ranaldi", "Andrea Zugarini", "Leonardo Ranaldi", "Fabio Massimo Zanzotto"], "title": "Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce the concept of protoknowledge to formalize and measure how\nsequences of tokens encoding Knowledge Graphs are internalized during\npretraining and utilized at inference time by Large Language Models (LLMs).\nIndeed, LLMs have demonstrated the ability to memorize vast amounts of token\nsequences during pretraining, and a central open question is how they leverage\nthis memorization as reusable knowledge through generalization. We then\ncategorize protoknowledge into lexical, hierarchical, and topological forms,\nvarying on the type of knowledge that needs to be activated. We measure\nprotoknowledge through Knowledge Activation Tasks (KATs), analyzing its general\nproperties such as semantic bias. We then investigate the impact of\nprotoknowledge on Text-to-SPARQL performance by varying prompting strategies\ndepending on input conditions. To this end, we adopt a novel analysis framework\nthat assesses whether model predictions align with the successful activation of\nthe relevant protoknowledge for each query. This methodology provides a\npractical tool to explore Semantic-Level Data Contamination and serves as an\neffective strategy for Closed-Pretraining models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15556", "pdf": "https://arxiv.org/pdf/2505.15556", "abs": "https://arxiv.org/abs/2505.15556", "authors": ["Ana-Maria Bucur", "Marcos Zampieri", "Tharindu Ranasinghe", "Fabio Crestani"], "title": "A Survey on Multilingual Mental Disorders Detection from Social Media Data", "categories": ["cs.CL"], "comment": null, "summary": "The increasing prevalence of mental health disorders globally highlights the\nurgent need for effective digital screening methods that can be used in\nmultilingual contexts. Most existing studies, however, focus on English data,\noverlooking critical mental health signals that may be present in non-English\ntexts. To address this important gap, we present the first survey on the\ndetection of mental health disorders using multilingual social media data. We\ninvestigate the cultural nuances that influence online language patterns and\nself-disclosure behaviors, and how these factors can impact the performance of\nNLP tools. Additionally, we provide a comprehensive list of multilingual data\ncollections that can be used for developing NLP models for mental health\nscreening. Our findings can inform the design of effective multilingual mental\nhealth screening tools that can meet the needs of diverse populations,\nultimately improving mental health outcomes on a global scale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14717", "pdf": "https://arxiv.org/pdf/2505.14717", "abs": "https://arxiv.org/abs/2505.14717", "authors": ["Xigui Li", "Yuanye Zhou", "Feiyang Xiao", "Xin Guo", "Chen Jiang", "Tan Pan", "Xingmeng Zhang", "Cenyu Liu", "Zeyun Miao", "Jianchao Ge", "Xiansheng Wang", "Qimeng Wang", "Yichi Zhang", "Wenbo Zhang", "Fengping Zhu", "Limei Han", "Yuan Qi", "Chensen Lin", "Yuan Cheng"], "title": "Aneumo: A Large-Scale Multimodal Aneurysm Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Intracranial aneurysms (IAs) are serious cerebrovascular lesions found in\napproximately 5\\% of the general population. Their rupture may lead to high\nmortality. Current methods for assessing IA risk focus on morphological and\npatient-specific factors, but the hemodynamic influences on IA development and\nrupture remain unclear. While accurate for hemodynamic studies, conventional\ncomputational fluid dynamics (CFD) methods are computationally intensive,\nhindering their deployment in large-scale or real-time clinical applications.\nTo address this challenge, we curated a large-scale, high-fidelity aneurysm CFD\ndataset to facilitate the development of efficient machine learning algorithms\nfor such applications. Based on 427 real aneurysm geometries, we synthesized\n10,660 3D shapes via controlled deformation to simulate aneurysm evolution. The\nauthenticity of these synthetic shapes was confirmed by neurosurgeons. CFD\ncomputations were performed on each shape under eight steady-state mass flow\nconditions, generating a total of 85,280 blood flow dynamics data covering key\nparameters. Furthermore, the dataset includes segmentation masks, which can\nsupport tasks that use images, point clouds or other multimodal data as input.\nAdditionally, we introduced a benchmark for estimating flow parameters to\nassess current modeling methods. This dataset aims to advance aneurysm research\nand promote data-driven approaches in biofluids, biomedical engineering, and\nclinical risk assessment. The code and dataset are available at:\nhttps://github.com/Xigui-Li/Aneumo.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15234", "pdf": "https://arxiv.org/pdf/2505.15234", "abs": "https://arxiv.org/abs/2505.15234", "authors": ["Saqib Qamar", "Mohd Fazil", "Parvez Ahmad", "Ghulam Muhammad"], "title": "SAMA-UNet: Enhancing Medical Image Segmentation with Self-Adaptive Mamba-Like Attention and Causal-Resonance Learning", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Medical image segmentation plays an important role in various clinical\napplications, but existing models often struggle with the computational\ninefficiencies and challenges posed by complex medical data. State Space\nSequence Models (SSMs) have demonstrated promise in modeling long-range\ndependencies with linear computational complexity, yet their application in\nmedical image segmentation remains hindered by incompatibilities with image\ntokens and autoregressive assumptions. Moreover, it is difficult to achieve a\nbalance in capturing both local fine-grained information and global semantic\ndependencies. To address these challenges, we introduce SAMA-UNet, a novel\narchitecture for medical image segmentation. A key innovation is the\nSelf-Adaptive Mamba-like Aggregated Attention (SAMA) block, which integrates\ncontextual self-attention with dynamic weight modulation to prioritise the most\nrelevant features based on local and global contexts. This approach reduces\ncomputational complexity and improves the representation of complex image\nfeatures across multiple scales. We also suggest the Causal-Resonance\nMulti-Scale Module (CR-MSM), which enhances the flow of information between the\nencoder and decoder by using causal resonance learning. This mechanism allows\nthe model to automatically adjust feature resolution and causal dependencies\nacross scales, leading to better semantic alignment between the low-level and\nhigh-level features in U-shaped architectures. Experiments on MRI, CT, and\nendoscopy images show that SAMA-UNet performs better in segmentation accuracy\nthan current methods using CNN, Transformer, and Mamba. The implementation is\npublicly available at GitHub.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15235", "pdf": "https://arxiv.org/pdf/2505.15235", "abs": "https://arxiv.org/abs/2505.15235", "authors": ["Yifan Liu", "Wuyang Li", "Weihao Yu", "Chenxin Li", "Alexandre Alahi", "Max Meng", "Yixuan Yuan"], "title": "X-GRM: Large Gaussian Reconstruction Model for Sparse-view X-rays to Computed Tomography", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Computed Tomography serves as an indispensable tool in clinical workflows,\nproviding non-invasive visualization of internal anatomical structures.\nExisting CT reconstruction works are limited to small-capacity model\narchitecture, inflexible volume representation, and small-scale training data.\nIn this paper, we present X-GRM (X-ray Gaussian Reconstruction Model), a large\nfeedforward model for reconstructing 3D CT from sparse-view 2D X-ray\nprojections. X-GRM employs a scalable transformer-based architecture to encode\nan arbitrary number of sparse X-ray inputs, where tokens from different views\nare integrated efficiently. Then, tokens are decoded into a new volume\nrepresentation, named Voxel-based Gaussian Splatting (VoxGS), which enables\nefficient CT volume extraction and differentiable X-ray rendering. To support\nthe training of X-GRM, we collect ReconX-15K, a large-scale CT reconstruction\ndataset containing around 15,000 CT/X-ray pairs across diverse organs,\nincluding the chest, abdomen, pelvis, and tooth etc. This combination of a\nhigh-capacity model, flexible volume representation, and large-scale training\ndata empowers our model to produce high-quality reconstructions from various\ntesting inputs, including in-domain and out-domain X-ray projections. Project\nPage: https://github.com/CUHK-AIM-Group/X-GRM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15284", "pdf": "https://arxiv.org/pdf/2505.15284", "abs": "https://arxiv.org/abs/2505.15284", "authors": ["Kun Fang", "Qinghua Tao", "Mingzhen He", "Kexin Lv", "Runze Yang", "Haibo Hu", "Xiaolin Huang", "Jie Yang", "Longbin Cao"], "title": "Kernel PCA for Out-of-Distribution Detection: Non-Linear Kernel Selections and Approximations", "categories": ["cs.LG", "cs.CV"], "comment": "This study is an extension of its conference version published in\n  NeurIPS'24, see\n  https://proceedings.neurips.cc/paper_files/paper/2024/hash/f2543511e5f4d4764857f9ad833a977d-Abstract-Conference.html", "summary": "Out-of-Distribution (OoD) detection is vital for the reliability of deep\nneural networks, the key of which lies in effectively characterizing the\ndisparities between OoD and In-Distribution (InD) data. In this work, such\ndisparities are exploited through a fresh perspective of non-linear feature\nsubspace. That is, a discriminative non-linear subspace is learned from InD\nfeatures to capture representative patterns of InD, while informative patterns\nof OoD features cannot be well captured in such a subspace due to their\ndifferent distribution. Grounded on this perspective, we exploit the deviations\nof InD and OoD features in such a non-linear subspace for effective OoD\ndetection. To be specific, we leverage the framework of Kernel Principal\nComponent Analysis (KPCA) to attain the discriminative non-linear subspace and\ndeploy the reconstruction error on such subspace to distinguish InD and OoD\ndata. Two challenges emerge: (i) the learning of an effective non-linear\nsubspace, i.e., the selection of kernel function in KPCA, and (ii) the\ncomputation of the kernel matrix with large-scale InD data. For the former, we\nreveal two vital non-linear patterns that closely relate to the InD-OoD\ndisparity, leading to the establishment of a Cosine-Gaussian kernel for\nconstructing the subspace. For the latter, we introduce two techniques to\napproximate the Cosine-Gaussian kernel with significantly cheap computations.\nIn particular, our approximation is further tailored by incorporating the InD\ndata confidence, which is demonstrated to promote the learning of\ndiscriminative subspaces for OoD data. Our study presents new insights into the\nnon-linear feature subspace for OoD detection and contributes practical\nexplorations on the associated kernel design and efficient computations,\nyielding a KPCA detection method with distinctively improved efficacy and\nefficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15298", "pdf": "https://arxiv.org/pdf/2505.15298", "abs": "https://arxiv.org/abs/2505.15298", "authors": ["Kangan Qian", "Sicong Jiang", "Yang Zhong", "Ziang Luo", "Zilin Huang", "Tianze Zhu", "Kun Jiang", "Mengmeng Yang", "Zheng Fu", "Jinyu Miao", "Yining Shi", "He Zhe Lim", "Li Liu", "Tianbao Zhou", "Hongyi Wang", "Huang Yu", "Yifei Hu", "Guang Li", "Guang Chen", "Hao Ye", "Lijun Sun", "Diange Yang"], "title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "18 pages, 8 figures", "summary": "Vision-Language Models (VLMs) show promise for autonomous driving, yet their\nstruggle with hallucinations, inefficient reasoning, and limited real-world\nvalidation hinders accurate perception and robust step-by-step reasoning. To\novercome this, we introduce \\textbf{AgentThink}, a pioneering unified framework\nthat, for the first time, integrates Chain-of-Thought (CoT) reasoning with\ndynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's\ncore innovations include: \\textbf{(i) Structured Data Generation}, by\nestablishing an autonomous driving tool library to automatically construct\nstructured, self-verified reasoning data explicitly incorporating tool usage\nfor diverse driving scenarios; \\textbf{(ii) A Two-stage Training Pipeline},\nemploying Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO) to equip VLMs with the capability for autonomous tool invocation; and\n\\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel\nmulti-tool assessment protocol to rigorously evaluate the model's tool\ninvocation and utilization. Experiments on the DriveLMM-o1 benchmark\ndemonstrate AgentThink significantly boosts overall reasoning scores by\n\\textbf{53.91\\%} and enhances answer accuracy by \\textbf{33.54\\%}, while\nmarkedly improving reasoning quality and consistency. Furthermore, ablation\nstudies and robust zero-shot/few-shot generalization experiments across various\nbenchmarks underscore its powerful capabilities. These findings highlight a\npromising trajectory for developing trustworthy and tool-aware autonomous\ndriving models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15389", "pdf": "https://arxiv.org/pdf/2505.15389", "abs": "https://arxiv.org/abs/2505.15389", "authors": ["DongGeon Lee", "Joonwon Jang", "Jihae Jeong", "Hwanjo Yu"], "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study", "categories": ["cs.CL", "cs.CR", "cs.CV"], "comment": null, "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs show greater vulnerability to\nmeme-based harmful prompts than to synthetic or typographic images. Memes\nsignificantly increase harmful responses and decrease refusals compared to\ntext-only inputs. Though multi-turn interactions provide partial mitigation,\nelevated vulnerability persists. These results highlight the need for\necologically valid evaluations and stronger safety mechanisms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15805", "pdf": "https://arxiv.org/pdf/2505.15805", "abs": "https://arxiv.org/abs/2505.15805", "authors": ["Hwan Chang", "Yumin Kim", "Yonghyun Jun", "Hwanhee Lee"], "title": "Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety", "question answering"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15810", "pdf": "https://arxiv.org/pdf/2505.15810", "abs": "https://arxiv.org/abs/2505.15810", "authors": ["Yuqi Zhou", "Sunhao Dai", "Shuai Wang", "Kaiwen Zhou", "Qinqlin Jia", "Junxu"], "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,\ncoupling online Reinforcement Learning (RL) with explicit chain-of-thought\nreasoning prior to object grounding and thereby achieving substantial\nperformance gains. In this paper, we first conduct extensive analysis\nexperiments of three key components of that training pipeline: input design,\noutput evaluation, and policy update-each revealing distinct challenges arising\nfrom blindly applying general-purpose RL without adapting to GUI grounding\ntasks. Input design: Current templates encourage the model to generate\nchain-of-thought reasoning, but longer chains unexpectedly lead to worse\ngrounding performance. Output evaluation: Reward functions based on hit signals\nor box area allow models to exploit box size, leading to reward hacking and\npoor localization quality. Policy update: Online RL tends to overfit easy\nexamples due to biases in length and sample difficulty, leading to\nunder-optimization on harder cases. To address these issues, we propose three\ntargeted solutions. First, we adopt a Fast Thinking Template that encourages\ndirect answer generation, reducing excessive reasoning during training. Second,\nwe incorporate a box size constraint into the reward function to mitigate\nreward hacking. Third, we revise the RL objective by adjusting length\nnormalization and adding a difficulty-aware scaling factor, enabling better\noptimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with\nQwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on\nScreenSpot-Pro. This surpasses all prior models of similar size and even\noutperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI\nagent grounding. The project repository is available at\nhttps://github.com/Yuqi-Zhou/GUI-G1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning", "reward hacking"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15810", "pdf": "https://arxiv.org/pdf/2505.15810", "abs": "https://arxiv.org/abs/2505.15810", "authors": ["Yuqi Zhou", "Sunhao Dai", "Shuai Wang", "Kaiwen Zhou", "Qinqlin Jia", "Junxu"], "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,\ncoupling online Reinforcement Learning (RL) with explicit chain-of-thought\nreasoning prior to object grounding and thereby achieving substantial\nperformance gains. In this paper, we first conduct extensive analysis\nexperiments of three key components of that training pipeline: input design,\noutput evaluation, and policy update-each revealing distinct challenges arising\nfrom blindly applying general-purpose RL without adapting to GUI grounding\ntasks. Input design: Current templates encourage the model to generate\nchain-of-thought reasoning, but longer chains unexpectedly lead to worse\ngrounding performance. Output evaluation: Reward functions based on hit signals\nor box area allow models to exploit box size, leading to reward hacking and\npoor localization quality. Policy update: Online RL tends to overfit easy\nexamples due to biases in length and sample difficulty, leading to\nunder-optimization on harder cases. To address these issues, we propose three\ntargeted solutions. First, we adopt a Fast Thinking Template that encourages\ndirect answer generation, reducing excessive reasoning during training. Second,\nwe incorporate a box size constraint into the reward function to mitigate\nreward hacking. Third, we revise the RL objective by adjusting length\nnormalization and adding a difficulty-aware scaling factor, enabling better\noptimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with\nQwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on\nScreenSpot-Pro. This surpasses all prior models of similar size and even\noutperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI\nagent grounding. The project repository is available at\nhttps://github.com/Yuqi-Zhou/GUI-G1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning", "reward hacking"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15817", "pdf": "https://arxiv.org/pdf/2505.15817", "abs": "https://arxiv.org/abs/2505.15817", "authors": ["Tong Zheng", "Lichang Chen", "Simeng Han", "R. Thomas McCoy", "Heng Huang"], "title": "Learning to Reason via Mixture-of-Thought for Logical Reasoning", "categories": ["cs.CL"], "comment": "38 pages", "summary": "Human beings naturally utilize multiple reasoning modalities to learn and\nsolve logical problems, i.e., different representational formats such as\nnatural language, code, and symbolic logic. In contrast, most existing\nLLM-based approaches operate with a single reasoning modality during training,\ntypically natural language. Although some methods explored modality selection\nor augmentation at inference time, the training process remains modality-blind,\nlimiting synergy among modalities. To fill in this gap, we propose\nMixture-of-Thought (MoT), a framework that enables LLMs to reason across three\ncomplementary modalities: natural language, code, and a newly introduced\nsymbolic modality, truth-table, which systematically enumerates logical cases\nand partially mitigates key failure modes in natural language reasoning. MoT\nadopts a two-phase design: (1) self-evolving MoT training, which jointly learns\nfrom filtered, self-generated rationales across modalities; and (2) MoT\ninference, which fully leverages the synergy of three modalities to produce\nbetter predictions. Experiments on logical reasoning benchmarks including FOLIO\nand ProofWriter demonstrate that our MoT framework consistently and\nsignificantly outperforms strong LLM baselines with single-modality\nchain-of-thought approaches, achieving up to +11.7pp average accuracy gain.\nFurther analyses show that our MoT framework benefits both training and\ninference stages; that it is particularly effective on harder logical reasoning\nproblems; and that different modalities contribute complementary strengths,\nwith truth-table reasoning helping to overcome key bottlenecks in natural\nlanguage inference.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14999", "pdf": "https://arxiv.org/pdf/2505.14999", "abs": "https://arxiv.org/abs/2505.14999", "authors": ["Eric Hanchen Jiang", "Haozheng Luo", "Shengyuan Pang", "Xiaomin Li", "Zhenting Qi", "Hengli Li", "Cheng-Fu Yang", "Zongyu Lin", "Xinfeng Li", "Hao Xu", "Kai-Wei Chang", "Ying Nian Wu"], "title": "Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs), often requiring robust multi step logical consistency. While\nChain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee\ncorrectness, and improving reliability via extensive sampling is\ncomputationally costly. This paper introduces the Energy Outcome Reward Model\n(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy\nBased Models (EBMs) to simplify the training of reward models by learning to\nassign a scalar energy score to CoT solutions using only outcome labels,\nthereby avoiding detailed annotations. It achieves this by interpreting\ndiscriminator output logits as negative energies, effectively ranking\ncandidates where lower energy is assigned to solutions leading to correct final\noutcomes implicitly favoring coherent reasoning. On mathematical benchmarks\n(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with\nLlama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively\nleverages a given pool of candidate solutions to match or exceed the\nperformance of brute force sampling, thereby enhancing LLM reasoning outcome\nreliability through its streamlined post hoc verification process.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "ranking"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "reliability", "accuracy", "mathematical reasoning"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15034", "pdf": "https://arxiv.org/pdf/2505.15034", "abs": "https://arxiv.org/abs/2505.15034", "authors": ["Kaiwen Zha", "Zhengqi Gao", "Maohao Shen", "Zhang-Wei Hong", "Duane S. Boning", "Dina Katabi"], "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Tech report. The first two authors contributed equally", "summary": "Reinforcement learning (RL) has recently emerged as a compelling approach for\nenhancing the reasoning capabilities of large language models (LLMs), where an\nLLM generator serves as a policy guided by a verifier (reward model). However,\ncurrent RL post-training methods for LLMs typically use verifiers that are\nfixed (rule-based or frozen pretrained) or trained discriminatively via\nsupervised fine-tuning (SFT). Such designs are susceptible to reward hacking\nand generalize poorly beyond their training distributions. To overcome these\nlimitations, we propose Tango, a novel framework that uses RL to concurrently\ntrain both an LLM generator and a verifier in an interleaved manner. A central\ninnovation of Tango is its generative, process-level LLM verifier, which is\ntrained via RL and co-evolves with the generator. Importantly, the verifier is\ntrained solely based on outcome-level verification correctness rewards without\nrequiring explicit process-level annotations. This generative RL-trained\nverifier exhibits improved robustness and superior generalization compared to\ndeterministic or SFT-trained verifiers, fostering effective mutual\nreinforcement with the generator. Extensive experiments demonstrate that both\ncomponents of Tango achieve state-of-the-art results among 7B/8B-scale models:\nthe generator attains best-in-class performance across five competition-level\nmath benchmarks and four challenging out-of-domain reasoning tasks, while the\nverifier leads on the ProcessBench dataset. Remarkably, both components exhibit\nparticularly substantial improvements on the most difficult mathematical\nreasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning", "reward hacking"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15070", "pdf": "https://arxiv.org/pdf/2505.15070", "abs": "https://arxiv.org/abs/2505.15070", "authors": ["Aldo Porco", "Dhruv Mehra", "Igor Malioutov", "Karthik Radhakrishnan", "Moniba Keymanesh", "Daniel Preoţiuc-Pietro", "Sean MacAvaney", "Pengxiang Cheng"], "title": "An Alternative to FLOPS Regularization to Effectively Productionize SPLADE-Doc", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted as a short paper at SIGIR 2025", "summary": "Learned Sparse Retrieval (LSR) models encode text as weighted term vectors,\nwhich need to be sparse to leverage inverted index structures during retrieval.\nSPLADE, the most popular LSR model, uses FLOPS regularization to encourage\nvector sparsity during training. However, FLOPS regularization does not ensure\nsparsity among terms - only within a given query or document. Terms with very\nhigh Document Frequencies (DFs) substantially increase latency in production\nretrieval engines, such as Apache Solr, due to their lengthy posting lists. To\naddress the issue of high DFs, we present a new variant of FLOPS\nregularization: DF-FLOPS. This new regularization technique penalizes the usage\nof high-DF terms, thereby shortening posting lists and reducing retrieval\nlatency. Unlike other inference-time sparsification methods, such as stopword\nremoval, DF-FLOPS regularization allows for the selective inclusion of\nhigh-frequency terms in cases where the terms are truly salient. We find that\nDF-FLOPS successfully reduces the prevalence of high-DF terms and lowers\nretrieval latency (around 10x faster) in a production-grade engine while\nmaintaining effectiveness both in-domain (only a 2.2-point drop in MRR@10) and\ncross-domain (improved performance in 12 out of 13 tasks on which we tested).\nWith retrieval latencies on par with BM25, this work provides an important step\ntowards making LSR practical for deployment in production-grade search engines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15298", "pdf": "https://arxiv.org/pdf/2505.15298", "abs": "https://arxiv.org/abs/2505.15298", "authors": ["Kangan Qian", "Sicong Jiang", "Yang Zhong", "Ziang Luo", "Zilin Huang", "Tianze Zhu", "Kun Jiang", "Mengmeng Yang", "Zheng Fu", "Jinyu Miao", "Yining Shi", "He Zhe Lim", "Li Liu", "Tianbao Zhou", "Hongyi Wang", "Huang Yu", "Yifei Hu", "Guang Li", "Guang Chen", "Hao Ye", "Lijun Sun", "Diange Yang"], "title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "18 pages, 8 figures", "summary": "Vision-Language Models (VLMs) show promise for autonomous driving, yet their\nstruggle with hallucinations, inefficient reasoning, and limited real-world\nvalidation hinders accurate perception and robust step-by-step reasoning. To\novercome this, we introduce \\textbf{AgentThink}, a pioneering unified framework\nthat, for the first time, integrates Chain-of-Thought (CoT) reasoning with\ndynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's\ncore innovations include: \\textbf{(i) Structured Data Generation}, by\nestablishing an autonomous driving tool library to automatically construct\nstructured, self-verified reasoning data explicitly incorporating tool usage\nfor diverse driving scenarios; \\textbf{(ii) A Two-stage Training Pipeline},\nemploying Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO) to equip VLMs with the capability for autonomous tool invocation; and\n\\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel\nmulti-tool assessment protocol to rigorously evaluate the model's tool\ninvocation and utilization. Experiments on the DriveLMM-o1 benchmark\ndemonstrate AgentThink significantly boosts overall reasoning scores by\n\\textbf{53.91\\%} and enhances answer accuracy by \\textbf{33.54\\%}, while\nmarkedly improving reasoning quality and consistency. Furthermore, ablation\nstudies and robust zero-shot/few-shot generalization experiments across various\nbenchmarks underscore its powerful capabilities. These findings highlight a\npromising trajectory for developing trustworthy and tool-aware autonomous\ndriving models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15367", "pdf": "https://arxiv.org/pdf/2505.15367", "abs": "https://arxiv.org/abs/2505.15367", "authors": ["Dasol Choi", "Seunghyun Lee", "Youngsook Song"], "title": "Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "13 pages", "summary": "Vision-Language Models (VLMs) have demonstrated impressive capabilities in\nunderstanding visual content, but their reliability in safety-critical contexts\nremains under-explored. We introduce VERI (Visual Emergency Recognition\nDataset), a carefully designed diagnostic benchmark of 200 images (100\ncontrastive pairs). Each emergency scene is matched with a visually similar but\nsafe counterpart through multi-stage human verification and iterative\nrefinement. Using a two-stage protocol - risk identification and emergency\nresponse - we evaluate 14 VLMs (2B-124B parameters) across medical emergencies,\naccidents, and natural disasters. Our analysis reveals a systematic\noverreaction problem: models excel at identifying real emergencies (70-100\npercent success rate) but suffer from an alarming rate of false alarms,\nmisidentifying 31-96 percent of safe situations as dangerous, with 10 scenarios\nfailed by all models regardless of scale. This \"better-safe-than-sorry\" bias\nmanifests primarily through contextual overinterpretation (88-93 percent of\nerrors), challenging VLMs' reliability for safety applications. These findings\nhighlight persistent limitations that are not resolved by increasing model\nscale, motivating targeted approaches for improving contextual safety\nassessment in visually misleading scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety", "reliability"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15489", "pdf": "https://arxiv.org/pdf/2505.15489", "abs": "https://arxiv.org/abs/2505.15489", "authors": ["Jiaying Wu", "Fanxiao Li", "Min-Yen Kan", "Bryan Hooi"], "title": "Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "The real-world impact of misinformation stems from the underlying misleading\nnarratives that creators seek to convey. As such, interpreting misleading\ncreator intent is essential for multimodal misinformation detection (MMD)\nsystems aimed at effective information governance. In this paper, we introduce\nan automated framework that simulates real-world multimodal news creation by\nexplicitly modeling creator intent through two components: the desired\ninfluence and the execution plan. Using this framework, we construct\nDeceptionDecoded, a large-scale benchmark comprising 12,000 image-caption pairs\naligned with trustworthy reference articles. The dataset captures both\nmisleading and non-misleading intents and spans manipulations across visual and\ntextual modalities. We conduct a comprehensive evaluation of 14\nstate-of-the-art vision-language models (VLMs) on three intent-centric tasks:\n(1) misleading intent detection, (2) misleading source attribution, and (3)\ncreator desire inference. Despite recent advances, we observe that current VLMs\nfall short in recognizing misleading intent, often relying on spurious cues\nsuch as superficial cross-modal consistency, stylistic signals, and heuristic\nauthenticity hints. Our findings highlight the pressing need for intent-aware\nmodeling in MMD and open new directions for developing systems capable of\ndeeper reasoning about multimodal misinformation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "consistency"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15517", "pdf": "https://arxiv.org/pdf/2505.15517", "abs": "https://arxiv.org/abs/2505.15517", "authors": ["Kaiyuan Chen", "Shuangyu Xie", "Zehan Ma", "Ken Goldberg"], "title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) acquire real-world knowledge and general\nreasoning ability through Internet-scale image-text corpora. They can augment\nrobotic systems with scene understanding and task planning, and assist\nvisuomotor policies that are trained on robot trajectory data. We explore the\nreverse paradigm - using rich, real, multi-modal robot trajectory data to\nenhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual\nQuestion Answering (VQA) dataset generation framework for VLMs. Given a human\ntele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual\nand non-descriptive sensory modalities, such as end-effector pose, gripper\naperture, and force sensing. Based on these modalities, it segments the robot\ntrajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses\nscene and interaction understanding to identify 3D properties of the robot,\ntask goal, and the target object. The properties are used to generate\nrepresentative VQA queries - images with textural multiple-choice questions -\nbased on spatial, goal-conditioned, and interaction reasoning question\ntemplates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710\nquestions covering 463 distinct scenes and 3,396 robotic manipulation tasks\nfrom 176k real robot trajectories. Results suggest that Robo2VLM-1 can\nbenchmark and improve VLM capabilities in spatial and interaction reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15772", "pdf": "https://arxiv.org/pdf/2505.15772", "abs": "https://arxiv.org/abs/2505.15772", "authors": ["Cheng Yifan", "Zhang Ruoyi", "Shi Jiatong"], "title": "MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech Paralinguistic and Affect Labeling", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted by Interspeech", "summary": "Acquiring large-scale emotional speech data with strong consistency remains a\nchallenge for speech synthesis. This paper presents MIKU-PAL, a fully automated\nmultimodal pipeline for extracting high-consistency emotional speech from\nunlabeled video data. Leveraging face detection and tracking algorithms, we\ndeveloped an automatic emotion analysis system using a multimodal large\nlanguage model (MLLM). Our results demonstrate that MIKU-PAL can achieve\nhuman-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss\nkappa score) while being much cheaper and faster than human annotation. With\nthe high-quality, flexible, and consistent annotation from MIKU-PAL, we can\nannotate fine-grained speech emotion categories of up to 26 types, validated by\nhuman annotators with 83% rationality ratings. Based on our proposed system, we\nfurther released a fine-grained emotional speech dataset MIKU-EmoBench(131.2\nhours) as a new benchmark for emotional text-to-speech and visual voice\ncloning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "consistency", "kappa", "accuracy", "fine-grained"], "score": 7}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15784", "pdf": "https://arxiv.org/pdf/2505.15784", "abs": "https://arxiv.org/abs/2505.15784", "authors": ["Jun Wan", "Lingrui Mei"], "title": "Large Language Models as Computable Approximations to Solomonoff Induction", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Both authors contributed equally", "summary": "The rapid advancement of large language models (LLMs) calls for a rigorous\ntheoretical framework to explain their empirical success. While significant\nprogress has been made in understanding LLM behaviors, existing theoretical\nframeworks remain fragmented in explaining emergent phenomena through a unified\nmathematical lens. We establish the first formal connection between LLM\narchitectures and Algorithmic Information Theory (AIT) by proving two\nfundamental results: (1) the training process computationally approximates\nSolomonoff prior through loss minimization interpreted as program length\noptimization, and (2) next-token prediction implements approximate Solomonoff\ninduction. We leverage AIT to provide a unified theoretical explanation for\nin-context learning, few-shot learning, and scaling laws. Furthermore, our\ntheoretical insights lead to a principled method for few-shot example selection\nthat prioritizes samples where models exhibit lower predictive confidence. We\ndemonstrate through experiments on diverse text classification benchmarks that\nthis strategy yields significant performance improvements, particularly for\nsmaller model architectures, when compared to selecting high-confidence\nexamples. Our framework bridges the gap between theoretical foundations and\npractical LLM behaviors, providing both explanatory power and actionable\ninsights for future model development.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
