{"id": "2505.24872", "pdf": "https://arxiv.org/pdf/2505.24872", "abs": "https://arxiv.org/abs/2505.24872", "authors": ["Zilin Xiao", "Jaywon Koo", "Siru Ouyang", "Jefferson Hernandez", "Yu Meng", "Vicente Ordonez"], "title": "ProxyThinker: Test-Time Guidance through Small Visual Reasoners", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in reinforcement learning with verifiable rewards have\npushed the boundaries of the visual reasoning capabilities in large\nvision-language models (LVLMs). However, training LVLMs with reinforcement\nfine-tuning (RFT) is computationally expensive, posing a significant challenge\nto scaling model size. In this work, we propose ProxyThinker, an inference-time\ntechnique that enables large models to inherit the visual reasoning\ncapabilities from small, slow-thinking visual reasoners without any training.\nBy subtracting the output distributions of base models from those of RFT\nreasoners, ProxyThinker modifies the decoding dynamics and successfully elicits\nthe slow-thinking reasoning demonstrated by the emerged sophisticated behaviors\nsuch as self-verification and self-correction. ProxyThinker consistently boosts\nperformance on challenging visual benchmarks on spatial, mathematical, and\nmulti-disciplinary reasoning, enabling untuned base models to compete with the\nperformance of their full-scale RFT counterparts. Furthermore, our\nimplementation efficiently coordinates multiple language models with\nparallelism techniques and achieves up to 38 $\\times$ faster inference compared\nto previous decoding-time methods, paving the way for the practical deployment\nof ProxyThinker. Code is available at\nhttps://github.com/MrZilinXiao/ProxyThinker.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time", "scaling", "scale", "self-verification", "self-correction"], "score": 6}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24872", "pdf": "https://arxiv.org/pdf/2505.24872", "abs": "https://arxiv.org/abs/2505.24872", "authors": ["Zilin Xiao", "Jaywon Koo", "Siru Ouyang", "Jefferson Hernandez", "Yu Meng", "Vicente Ordonez"], "title": "ProxyThinker: Test-Time Guidance through Small Visual Reasoners", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in reinforcement learning with verifiable rewards have\npushed the boundaries of the visual reasoning capabilities in large\nvision-language models (LVLMs). However, training LVLMs with reinforcement\nfine-tuning (RFT) is computationally expensive, posing a significant challenge\nto scaling model size. In this work, we propose ProxyThinker, an inference-time\ntechnique that enables large models to inherit the visual reasoning\ncapabilities from small, slow-thinking visual reasoners without any training.\nBy subtracting the output distributions of base models from those of RFT\nreasoners, ProxyThinker modifies the decoding dynamics and successfully elicits\nthe slow-thinking reasoning demonstrated by the emerged sophisticated behaviors\nsuch as self-verification and self-correction. ProxyThinker consistently boosts\nperformance on challenging visual benchmarks on spatial, mathematical, and\nmulti-disciplinary reasoning, enabling untuned base models to compete with the\nperformance of their full-scale RFT counterparts. Furthermore, our\nimplementation efficiently coordinates multiple language models with\nparallelism techniques and achieves up to 38 $\\times$ faster inference compared\nto previous decoding-time methods, paving the way for the practical deployment\nof ProxyThinker. Code is available at\nhttps://github.com/MrZilinXiao/ProxyThinker.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time", "scaling", "scale", "self-verification", "self-correction"], "score": 6}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23884", "pdf": "https://arxiv.org/pdf/2505.23884", "abs": "https://arxiv.org/abs/2505.23884", "authors": ["Tianyuan Zhang", "Sai Bi", "Yicong Hong", "Kai Zhang", "Fujun Luan", "Songlin Yang", "Kalyan Sunkavalli", "William T. Freeman", "Hao Tan"], "title": "Test-Time Training Done Right", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "32 pages, 11 figures", "summary": "Test-Time Training (TTT) models context dependencies by adapting part of the\nmodel's weights (referred to as fast weights) during inference. This fast\nweight, akin to recurrent states in RNNs, stores temporary memories of past\ntokens in the current sequence. Existing TTT methods struggled to show\neffectiveness in handling long-context data, due to their inefficiency on\nmodern GPUs. The TTT layers in many of these approaches operate with extremely\nlow FLOPs utilization (often <5%) because they deliberately apply small online\nminibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,\na small minibatch implies fine-grained block-wise causal dependencies in the\ndata, unsuitable for data beyond 1D ordered sequences, like sets or\nN-dimensional grids such as images or videos. In contrast, we pursue the\nopposite direction by using an extremely large chunk update, ranging from 2K to\n1M tokens across tasks of varying modalities, which we refer to as Large Chunk\nTest-Time Training (LaCT). It improves hardware utilization by orders of\nmagnitude, and more importantly, facilitates scaling of nonlinear state size\n(up to 40% of model parameters), hence substantially improving state capacity,\nall without requiring cumbersome and error-prone kernel implementations. It\nalso allows easy integration of sophisticated optimizers, e.g. Muon for online\nupdates. We validate our approach across diverse modalities and tasks,\nincluding novel view synthesis with image set, language models, and\nauto-regressive video diffusion. Our approach can scale up to 14B-parameter AR\nvideo diffusion model on sequences up to 56K tokens. In our longest sequence\nexperiment, we perform novel view synthesis with 1 million context length. We\nhope this work will inspire and accelerate new research in the field of\nlong-context modeling and test-time training. Website:\nhttps://tianyuanzhang.com/projects/ttt-done-right", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale", "test-time training"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23884", "pdf": "https://arxiv.org/pdf/2505.23884", "abs": "https://arxiv.org/abs/2505.23884", "authors": ["Tianyuan Zhang", "Sai Bi", "Yicong Hong", "Kai Zhang", "Fujun Luan", "Songlin Yang", "Kalyan Sunkavalli", "William T. Freeman", "Hao Tan"], "title": "Test-Time Training Done Right", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "32 pages, 11 figures", "summary": "Test-Time Training (TTT) models context dependencies by adapting part of the\nmodel's weights (referred to as fast weights) during inference. This fast\nweight, akin to recurrent states in RNNs, stores temporary memories of past\ntokens in the current sequence. Existing TTT methods struggled to show\neffectiveness in handling long-context data, due to their inefficiency on\nmodern GPUs. The TTT layers in many of these approaches operate with extremely\nlow FLOPs utilization (often <5%) because they deliberately apply small online\nminibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,\na small minibatch implies fine-grained block-wise causal dependencies in the\ndata, unsuitable for data beyond 1D ordered sequences, like sets or\nN-dimensional grids such as images or videos. In contrast, we pursue the\nopposite direction by using an extremely large chunk update, ranging from 2K to\n1M tokens across tasks of varying modalities, which we refer to as Large Chunk\nTest-Time Training (LaCT). It improves hardware utilization by orders of\nmagnitude, and more importantly, facilitates scaling of nonlinear state size\n(up to 40% of model parameters), hence substantially improving state capacity,\nall without requiring cumbersome and error-prone kernel implementations. It\nalso allows easy integration of sophisticated optimizers, e.g. Muon for online\nupdates. We validate our approach across diverse modalities and tasks,\nincluding novel view synthesis with image set, language models, and\nauto-regressive video diffusion. Our approach can scale up to 14B-parameter AR\nvideo diffusion model on sequences up to 56K tokens. In our longest sequence\nexperiment, we perform novel view synthesis with 1 million context length. We\nhope this work will inspire and accelerate new research in the field of\nlong-context modeling and test-time training. Website:\nhttps://tianyuanzhang.com/projects/ttt-done-right", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale", "test-time training"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23926", "pdf": "https://arxiv.org/pdf/2505.23926", "abs": "https://arxiv.org/abs/2505.23926", "authors": ["Xuweiyi Chen", "Wentao Zhou", "Aruni RoyChowdhury", "Zezhou Cheng"], "title": "Point-MoE: Towards Cross-Domain Generalization in 3D Semantic Segmentation via Mixture-of-Experts", "categories": ["cs.CV"], "comment": "Project page: https://uva-computer-vision-lab.github.io/point-moe/", "summary": "While scaling laws have transformed natural language processing and computer\nvision, 3D point cloud understanding has yet to reach that stage. This can be\nattributed to both the comparatively smaller scale of 3D datasets, as well as\nthe disparate sources of the data itself. Point clouds are captured by diverse\nsensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor,\noutdoor), each introducing unique scanning patterns, sampling densities, and\nsemantic biases. Such domain heterogeneity poses a major barrier towards\ntraining unified models at scale, especially under the realistic constraint\nthat domain labels are typically inaccessible at inference time. In this work,\nwe propose Point-MoE, a Mixture-of-Experts architecture designed to enable\nlarge-scale, cross-domain generalization in 3D perception. We show that\nstandard point cloud backbones degrade significantly in performance when\ntrained on mixed-domain data, whereas Point-MoE with a simple top-k routing\nstrategy can automatically specialize experts, even without access to domain\nlabels. Our experiments demonstrate that Point-MoE not only outperforms strong\nmulti-domain baselines but also generalizes better to unseen domains. This work\nhighlights a scalable path forward for 3D understanding: letting the model\ndiscover structure in diverse 3D data, rather than imposing it via manual\ncuration or domain supervision.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scaling", "scale"], "score": 3}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23883", "pdf": "https://arxiv.org/pdf/2505.23883", "abs": "https://arxiv.org/abs/2505.23883", "authors": ["Jianyang Gu", "Samuel Stevens", "Elizabeth G Campolongo", "Matthew J Thompson", "Net Zhang", "Jiaman Wu", "Andrei Kopanev", "Zheda Mai", "Alexander E. White", "James Balhoff", "Wasila Dahdul", "Daniel Rubenstein", "Hilmar Lapp", "Tanya Berger-Wolf", "Wei-Lun Chao", "Yu Su"], "title": "BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project page: https://imageomics.github.io/bioclip-2/", "summary": "Foundation models trained at scale exhibit remarkable emergent behaviors,\nlearning new capabilities beyond their initial training objectives. We find\nsuch emergent behaviors in biological vision models via large-scale contrastive\nvision-language training. To achieve this, we first curate TreeOfLife-200M,\ncomprising 214 million images of living organisms, the largest and most diverse\nbiological organism image dataset to date. We then train BioCLIP 2 on\nTreeOfLife-200M to distinguish different species. Despite the narrow training\nobjective, BioCLIP 2 yields extraordinary accuracy when applied to various\nbiological visual tasks such as habitat classification and trait prediction. We\nidentify emergent properties in the learned embedding space of BioCLIP 2. At\nthe inter-species level, the embedding distribution of different species aligns\nclosely with functional and ecological meanings (e.g., beak sizes and\nhabitats). At the intra-species level, instead of being diminished, the\nintra-species variations (e.g., life stages and sexes) are preserved and better\nseparated in subspaces orthogonal to inter-species distinctions. We provide\nformal proof and analyses to explain why hierarchical supervision and\ncontrastive objectives encourage these emergent properties. Crucially, our\nresults reveal that these properties become increasingly significant with\nlarger-scale training data, leading to a biologically meaningful embedding\nspace.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23829", "pdf": "https://arxiv.org/pdf/2505.23829", "abs": "https://arxiv.org/abs/2505.23829", "authors": ["Xiaoqing Cheng", "Ruizhe Chen", "Hongying Zan", "Yuxiang Jia", "Min Peng"], "title": "BiasFilter: An Inference-Time Debiasing Framework for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Mitigating social bias in large language models (LLMs) has become an\nincreasingly important research objective. However, existing debiasing methods\noften incur high human and computational costs, exhibit limited effectiveness,\nand struggle to scale to larger models and open-ended generation tasks. To\naddress these limitations, this paper proposes BiasFilter, a model-agnostic,\ninference-time debiasing framework that integrates seamlessly with both\nopen-source and API-based LLMs. Instead of relying on retraining with balanced\ndata or modifying model parameters, BiasFilter enforces fairness by filtering\ngeneration outputs in real time. Specifically, it periodically evaluates\nintermediate outputs every few tokens, maintains an active set of candidate\ncontinuations, and incrementally completes generation by discarding low-reward\nsegments based on a fairness reward signal. To support this process, we\nconstruct a fairness preference dataset and train an implicit reward model to\nassess token-level fairness in generated responses. Extensive experiments\ndemonstrate that BiasFilter effectively mitigates social bias across a range of\nLLMs while preserving overall generation quality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24207", "pdf": "https://arxiv.org/pdf/2505.24207", "abs": "https://arxiv.org/abs/2505.24207", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu"], "title": "Boosting All-in-One Image Restoration via Self-Improved Privilege Learning", "categories": ["cs.CV"], "comment": null, "summary": "Unified image restoration models for diverse and mixed degradations often\nsuffer from unstable optimization dynamics and inter-task conflicts. This paper\nintroduces Self-Improved Privilege Learning (SIPL), a novel paradigm that\novercomes these limitations by innovatively extending the utility of privileged\ninformation (PI) beyond training into the inference stage. Unlike conventional\nPrivilege Learning, where ground-truth-derived guidance is typically discarded\nafter training, SIPL empowers the model to leverage its own preliminary outputs\nas pseudo-privileged signals for iterative self-refinement at test time.\nCentral to SIPL is Proxy Fusion, a lightweight module incorporating a learnable\nPrivileged Dictionary. During training, this dictionary distills essential\nhigh-frequency and structural priors from privileged feature representations.\nCritically, at inference, the same learned dictionary then interacts with\nfeatures derived from the model's initial restoration, facilitating a\nself-correction loop. SIPL can be seamlessly integrated into various backbone\narchitectures, offering substantial performance improvements with minimal\ncomputational overhead. Extensive experiments demonstrate that SIPL\nsignificantly advances the state-of-the-art on diverse all-in-one image\nrestoration benchmarks. For instance, when integrated with the PromptIR model,\nSIPL achieves remarkable PSNR improvements of +4.58 dB on composite degradation\ntasks and +1.28 dB on diverse five-task benchmarks, underscoring its\neffectiveness and broad applicability. Codes are available at our project page\nhttps://github.com/Aitical/SIPL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time", "self-correction"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23845", "pdf": "https://arxiv.org/pdf/2505.23845", "abs": "https://arxiv.org/abs/2505.23845", "authors": ["Jakub Podolak", "Rajeev Verma"], "title": "Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "We study the source of uncertainty in DeepSeek R1-32B by analyzing its\nself-reported verbal confidence on question answering (QA) tasks. In the\ndefault answer-then-confidence setting, the model is regularly over-confident,\nwhereas semantic entropy - obtained by sampling many responses - remains\nreliable. We hypothesize that this is because of semantic entropy's larger\ntest-time compute, which lets us explore the model's predictive distribution.\nWe show that granting DeepSeek the budget to explore its distribution by\nforcing a long chain-of-thought before the final answer greatly improves its\nverbal score effectiveness, even on simple fact-retrieval questions that\nnormally require no reasoning. Furthermore, a separate reader model that sees\nonly the chain can reconstruct very similar confidences, indicating the verbal\nscore might be merely a statistic of the alternatives surfaced during\nreasoning. Our analysis concludes that reliable uncertainty estimation requires\nexplicit exploration of the generative space, and self-reported confidence is\ntrustworthy only after such exploration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time compute"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23931", "pdf": "https://arxiv.org/pdf/2505.23931", "abs": "https://arxiv.org/abs/2505.23931", "authors": ["Daniel Wurgaft", "Ben Prystawski", "Kanishk Gandhi", "Cedegao E. Zhang", "Joshua B. Tenenbaum", "Noah D. Goodman"], "title": "Scaling up the think-aloud method", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 4 figures. Daniel Wurgaft and Ben Prystawski contributed\n  equally", "summary": "The think-aloud method, where participants voice their thoughts as they solve\na task, is a valuable source of rich data about human reasoning processes. Yet,\nit has declined in popularity in contemporary cognitive science, largely\nbecause labor-intensive transcription and annotation preclude large sample\nsizes. Here, we develop methods to automate the transcription and annotation of\nverbal reports of reasoning using natural language processing tools, allowing\nfor large-scale analysis of think-aloud data. In our study, 640 participants\nthought aloud while playing the Game of 24, a mathematical reasoning task. We\nautomatically transcribed the recordings and coded the transcripts as search\ngraphs, finding moderate inter-rater reliability with humans. We analyze these\ngraphs and characterize consistency and variation in human reasoning traces.\nOur work demonstrates the value of think-aloud data at scale and serves as a\nproof of concept for the automated analysis of verbal reports.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency", "reliability", "mathematical reasoning"], "score": 4}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24863", "pdf": "https://arxiv.org/pdf/2505.24863", "abs": "https://arxiv.org/abs/2505.24863", "authors": ["Junyu Zhang", "Runpei Dong", "Han Wang", "Xuying Ning", "Haoran Geng", "Peihao Li", "Xialin He", "Yutong Bai", "Jitendra Malik", "Saurabh Gupta", "Huan Zhang"], "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents AlphaOne ($\\alpha$1), a universal framework for\nmodulating reasoning progress in large reasoning models (LRMs) at test time.\n$\\alpha$1 first introduces $\\alpha$ moment, which represents the scaled\nthinking phase with a universal parameter $\\alpha$. Within this scaled\npre-$\\alpha$ moment phase, it dynamically schedules slow thinking transitions\nby modeling the insertion of reasoning transition tokens as a Bernoulli\nstochastic process. After the $\\alpha$ moment, $\\alpha$1 deterministically\nterminates slow thinking with the end-of-thinking token, thereby fostering fast\nreasoning and efficient answer generation. This approach unifies and\ngeneralizes existing monotonic scaling methods by enabling flexible and dense\nslow-to-fast reasoning modulation. Extensive empirical studies on various\nchallenging benchmarks across mathematical, coding, and scientific domains\ndemonstrate $\\alpha$1's superior reasoning capability and efficiency. Project\npage: https://alphaone-project.github.io/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time", "scaling"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23883", "pdf": "https://arxiv.org/pdf/2505.23883", "abs": "https://arxiv.org/abs/2505.23883", "authors": ["Jianyang Gu", "Samuel Stevens", "Elizabeth G Campolongo", "Matthew J Thompson", "Net Zhang", "Jiaman Wu", "Andrei Kopanev", "Zheda Mai", "Alexander E. White", "James Balhoff", "Wasila Dahdul", "Daniel Rubenstein", "Hilmar Lapp", "Tanya Berger-Wolf", "Wei-Lun Chao", "Yu Su"], "title": "BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project page: https://imageomics.github.io/bioclip-2/", "summary": "Foundation models trained at scale exhibit remarkable emergent behaviors,\nlearning new capabilities beyond their initial training objectives. We find\nsuch emergent behaviors in biological vision models via large-scale contrastive\nvision-language training. To achieve this, we first curate TreeOfLife-200M,\ncomprising 214 million images of living organisms, the largest and most diverse\nbiological organism image dataset to date. We then train BioCLIP 2 on\nTreeOfLife-200M to distinguish different species. Despite the narrow training\nobjective, BioCLIP 2 yields extraordinary accuracy when applied to various\nbiological visual tasks such as habitat classification and trait prediction. We\nidentify emergent properties in the learned embedding space of BioCLIP 2. At\nthe inter-species level, the embedding distribution of different species aligns\nclosely with functional and ecological meanings (e.g., beak sizes and\nhabitats). At the intra-species level, instead of being diminished, the\nintra-species variations (e.g., life stages and sexes) are preserved and better\nseparated in subspaces orthogonal to inter-species distinctions. We provide\nformal proof and analyses to explain why hierarchical supervision and\ncontrastive objectives encourage these emergent properties. Crucially, our\nresults reveal that these properties become increasingly significant with\nlarger-scale training data, leading to a biologically meaningful embedding\nspace.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23785", "pdf": "https://arxiv.org/pdf/2505.23785", "abs": "https://arxiv.org/abs/2505.23785", "authors": ["Cody Kommers", "Drew Hemment", "Maria Antoniak", "Joel Z. Leibo", "Hoyt Long", "Emily Robinson", "Adam Sobey"], "title": "Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Position paper", "summary": "This position paper argues that large language models (LLMs) can make\ncultural context, and therefore human meaning, legible at an unprecedented\nscale in AI-based sociotechnical systems. We argue that such systems have\npreviously been unable to represent human meaning because they rely on thin\ndescriptions: numerical representations that enforce standardization and\ntherefore strip human activity of the cultural context that gives it meaning.\nBy contrast, scholars in the humanities and qualitative social sciences have\ndeveloped frameworks for representing meaning through thick description: verbal\nrepresentations that accommodate heterogeneity and retain contextual\ninformation needed to represent human meaning. While these methods can\neffectively codify meaning, they are difficult to deploy at scale. However, the\nverbal capabilities of LLMs now provide a means of (at least partially)\nautomating the generation and processing of thick descriptions, potentially\novercoming this bottleneck. We argue that the problem of rendering human\nmeaning legible is not just about selecting better metrics, but about\ndeveloping new representational formats (based on thick description). We frame\nthis as a crucial direction for the application of generative AI and identify\nfive key challenges: preserving context, maintaining interpretive pluralism,\nintegrating perspectives based on lived experience and critical distance,\ndistinguishing qualitative content from quantitative magnitude, and\nacknowledging meaning as dynamic rather than static. Furthermore, we suggest\nthat thick description has the potential to serve as a unifying framework to\naddress a number of emerging concerns about the difficulties of representing\nculture in (or using) LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23788", "pdf": "https://arxiv.org/pdf/2505.23788", "abs": "https://arxiv.org/abs/2505.23788", "authors": ["Aakash Sen Sharma", "Debdeep Sanyal", "Priyansh Srivastava", "Sundar Atreya H.", "Shirish Karande", "Mohan Kankanhalli", "Murari Mandal"], "title": "Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework", "categories": ["cs.CL", "cs.AI"], "comment": "30 Pages", "summary": "Large language models (LLMs) commonly risk copyright infringement by\nreproducing protected content verbatim or with insufficient transformative\nmodifications, posing significant ethical, legal, and practical concerns.\nCurrent inference-time safeguards predominantly rely on restrictive\nrefusal-based filters, often compromising the practical utility of these\nmodels. To address this, we collaborated closely with intellectual property\nexperts to develop FUA-LLM (Fair Use Aligned Language Models), a\nlegally-grounded framework explicitly designed to align LLM outputs with\nfair-use doctrine. Central to our method is FairUseDB, a carefully constructed\ndataset containing 18,000 expert-validated examples covering nine realistic\ninfringement scenarios. Leveraging this dataset, we apply Direct Preference\nOptimization (DPO) to fine-tune open-source LLMs, encouraging them to produce\nlegally compliant and practically useful alternatives rather than resorting to\nblunt refusal. Recognizing the shortcomings of traditional evaluation metrics,\nwe propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic\nMean (CAH) to balance infringement risk against response utility. Extensive\nquantitative experiments coupled with expert evaluations confirm that FUA-LLM\nsubstantially reduces problematic outputs (up to 20\\%) compared to\nstate-of-the-art approaches, while preserving real-world usability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23789", "pdf": "https://arxiv.org/pdf/2505.23789", "abs": "https://arxiv.org/abs/2505.23789", "authors": ["Mingyu Huang", "Shasha Zhou", "Yuxuan Chen", "Ke Li"], "title": "Conversational Exploration of Literature Landscape with LitChat", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "We are living in an era of \"big literature\", where the volume of digital\nscientific publications is growing exponentially. While offering new\nopportunities, this also poses challenges for understanding literature\nlandscapes, as traditional manual reviewing is no longer feasible. Recent large\nlanguage models (LLMs) have shown strong capabilities for literature\ncomprehension, yet they are incapable of offering \"comprehensive, objective,\nopen and transparent\" views desired by systematic reviews due to their limited\ncontext windows and trust issues like hallucinations. Here we present LitChat,\nan end-to-end, interactive and conversational literature agent that augments\nLLM agents with data-driven discovery tools to facilitate literature\nexploration. LitChat automatically interprets user queries, retrieves relevant\nsources, constructs knowledge graphs, and employs diverse data-mining\ntechniques to generate evidence-based insights addressing user needs. We\nillustrate the effectiveness of LitChat via a case study on AI4Health,\nhighlighting its capacity to quickly navigate the users through large-scale\nliterature landscape with data-based evidence that is otherwise infeasible with\ntraditional means.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23922", "pdf": "https://arxiv.org/pdf/2505.23922", "abs": "https://arxiv.org/abs/2505.23922", "authors": ["David Ma", "Huaqing Yuan", "Xingjian Wang", "Qianbo Zang", "Tianci Liu", "Xinyang He", "Yanbin Wei", "Jiawei Guo", "Ni Jiahui", "Zhenzhu Yang", "Meng Cao", "Shanghaoran Quan", "Yizhi Li", "Wangchunshu Zhou", "Jiaheng Liu", "Wenhao Huang", "Ge Zhang", "Shiwen Ni", "Xiaojie Jin"], "title": "ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Although long-video understanding demands that models capture hierarchical\ntemporal information -- from clip (seconds) and shot (tens of seconds) to event\n(minutes) and story (hours) -- existing benchmarks either neglect this\nmulti-scale design or scatter scale-specific questions across different videos,\npreventing direct comparison of model performance across timescales on the same\ncontent. To address this, we introduce ScaleLong, the first benchmark to\ndisentangle these factors by embedding questions targeting four hierarchical\ntimescales -- clip (seconds), shot (tens of seconds), event (minutes), and\nstory (hours) -- all within the same video content. This within-content\nmulti-timescale questioning design enables direct comparison of model\nperformance across timescales on identical videos. ScaleLong features 269 long\nvideos (avg.\\ 86\\,min) from 5 main categories and 36 sub-categories, with 4--8\ncarefully designed questions, including at least one question for each\ntimescale. Evaluating 23 MLLMs reveals a U-shaped performance curve, with\nhigher accuracy at the shortest and longest timescales and a dip at\nintermediate levels. Furthermore, ablation studies show that increased visual\ntoken capacity consistently enhances reasoning across all timescales. ScaleLong\noffers a fine-grained, multi-timescale benchmark for advancing MLLM\ncapabilities in long-video understanding. The code and dataset are available\nhttps://github.com/multimodal-art-projection/ScaleLong.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23802", "pdf": "https://arxiv.org/pdf/2505.23802", "abs": "https://arxiv.org/abs/2505.23802", "authors": ["Suhana Bedi", "Hejie Cui", "Miguel Fuentes", "Alyssa Unell", "Michael Wornow", "Juan M. Banda", "Nikesh Kotecha", "Timothy Keyes", "Yifan Mai", "Mert Oez", "Hao Qiu", "Shrey Jain", "Leonardo Schettini", "Mehr Kashyap", "Jason Alan Fries", "Akshay Swaminathan", "Philip Chung", "Fateme Nateghi", "Asad Aali", "Ashwin Nayak", "Shivam Vedak", "Sneha S. Jain", "Birju Patel", "Oluseyi Fayanju", "Shreya Shah", "Ethan Goh", "Dong-han Yao", "Brian Soetikno", "Eduardo Reis", "Sergios Gatidis", "Vasu Divi", "Robson Capasso", "Rachna Saralkar", "Chia-Chun Chiang", "Jenelle Jindal", "Tho Pham", "Faraz Ghoddusi", "Steven Lin", "Albert S. Chiou", "Christy Hong", "Mohana Roy", "Michael F. Gensheimer", "Hinesh Patel", "Kevin Schulman", "Dev Dash", "Danton Char", "Lance Downing", "Francois Grolleau", "Kameron Black", "Bethel Mieso", "Aydin Zahedivash", "Wen-wai Yim", "Harshita Sharma", "Tony Lee", "Hannah Kirsch", "Jennifer Lee", "Nerissa Ambers", "Carlene Lugtu", "Aditya Sharma", "Bilal Mawji", "Alex Alekseyev", "Vicky Zhou", "Vikas Kakkar", "Jarrod Helzer", "Anurang Revri", "Yair Bannett", "Roxana Daneshjou", "Jonathan Chen", "Emily Alsentzer", "Keith Morse", "Nirmal Ravi", "Nima Aghaeepour", "Vanessa Kennedy", "Akshay Chaudhari", "Thomas Wang", "Sanmi Koyejo", "Matthew P. Lungren", "Eric Horvitz", "Percy Liang", "Mike Pfeffer", "Nigam H. Shah"], "title": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "agreement", "accuracy"], "score": 4}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23804", "pdf": "https://arxiv.org/pdf/2505.23804", "abs": "https://arxiv.org/abs/2505.23804", "authors": ["Terrance Liu", "Shuyi Wang", "Daniel Preotiuc-Pietro", "Yash Chandarana", "Chirag Gupta"], "title": "Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While large language models (LLMs) achieve strong performance on text-to-SQL\nparsing, they sometimes exhibit unexpected failures in which they are\nconfidently incorrect. Building trustworthy text-to-SQL systems thus requires\neliciting reliable uncertainty measures from the LLM. In this paper, we study\nthe problem of providing a calibrated confidence score that conveys the\nlikelihood of an output query being correct. Our work is the first to establish\na benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In\nparticular, we show that Platt scaling, a canonical method for calibration,\nprovides substantial improvements over directly using raw model output\nprobabilities as confidence scores. Furthermore, we propose a method for\ntext-to-SQL calibration that leverages the structured nature of SQL queries to\nprovide more granular signals of correctness, named \"sub-clause frequency\"\n(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the\ncanonical Platt scaling technique, we combine individual SCF scores into an\noverall accurate and calibrated score. Empirical evaluation on two popular\ntext-to-SQL datasets shows that our approach of combining MPS and SCF yields\nfurther improvements in calibration and the related task of error detection\nover traditional Platt scaling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23977", "pdf": "https://arxiv.org/pdf/2505.23977", "abs": "https://arxiv.org/abs/2505.23977", "authors": ["Yichen Feng", "Zhangchen Xu", "Fengqing Jiang", "Yuetai Li", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "title": "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page at https://visualsphinx.github.io/", "summary": "Vision language models (VLMs) are expected to perform effective multimodal\nreasoning and make logically coherent decisions, which is critical to tasks\nsuch as diagram understanding and spatial problem solving. However, current VLM\nreasoning lacks large-scale and well-structured training datasets. To bridge\nthis gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic\nvisual logical reasoning training data. To tackle the challenge of image\nsynthesis with grounding answers, we propose a rule-to-image synthesis\npipeline, which extracts and expands puzzle rules from seed questions and\ngenerates the code of grounding synthesis image synthesis for puzzle sample\nassembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx\nbenefit from logical coherence and readability of our dataset and exhibit\nimproved performance on logical reasoning tasks. The enhanced reasoning\ncapabilities developed from VisualSphinx also benefit other reasoning tasks\nsuch as algebraic reasoning, arithmetic reasoning and geometry reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23980", "pdf": "https://arxiv.org/pdf/2505.23980", "abs": "https://arxiv.org/abs/2505.23980", "authors": ["Bayu Adhi Tama", "Mansa Krishna", "Homayra Alam", "Mostafa Cham", "Omar Faruque", "Gong Cheng", "Jianwu Wang", "Mathieu Morlighem", "Vandana Janeja"], "title": "DeepTopoNet: A Framework for Subglacial Topography Estimation on the Greenland Ice Sheets", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "Submitted to SIGSPATIAL 2025", "summary": "Understanding Greenland's subglacial topography is critical for projecting\nthe future mass loss of the ice sheet and its contribution to global sea-level\nrise. However, the complex and sparse nature of observational data,\nparticularly information about the bed topography under the ice sheet,\nsignificantly increases the uncertainty in model projections. Bed topography is\ntraditionally measured by airborne ice-penetrating radar that measures the ice\nthickness directly underneath the aircraft, leaving data gap of tens of\nkilometers in between flight lines. This study introduces a deep learning\nframework, which we call as DeepTopoNet, that integrates radar-derived ice\nthickness observations and BedMachine Greenland data through a novel dynamic\nloss-balancing mechanism. Among all efforts to reconstruct bed topography,\nBedMachine has emerged as one of the most widely used datasets, combining mass\nconservation principles and ice thickness measurements to generate\nhigh-resolution bed elevation estimates. The proposed loss function adaptively\nadjusts the weighting between radar and BedMachine data, ensuring robustness in\nareas with limited radar coverage while leveraging the high spatial resolution\nof BedMachine predictions i.e. bed estimates. Our approach incorporates\ngradient-based and trend surface features to enhance model performance and\nutilizes a CNN architecture designed for subgrid-scale predictions. By\nsystematically testing on the Upernavik Isstr{\\o}m) region, the model achieves\nhigh accuracy, outperforming baseline methods in reconstructing subglacial\nterrain. This work demonstrates the potential of deep learning in bridging\nobservational gaps, providing a scalable and efficient solution to inferring\nsubglacial topography.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23807", "pdf": "https://arxiv.org/pdf/2505.23807", "abs": "https://arxiv.org/abs/2505.23807", "authors": ["Yuli Chen", "Bo Cheng", "Jiale Han", "Yingying Zhang", "Yingting Li", "Shuhao Zhang"], "title": "DLP: Dynamic Layerwise Pruning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23811", "pdf": "https://arxiv.org/pdf/2505.23811", "abs": "https://arxiv.org/abs/2505.23811", "authors": ["Hadi Askari", "Shivanshu Gupta", "Fei Wang", "Anshuman Chhabra", "Muhao Chen"], "title": "LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Pretrained Large Language Models (LLMs) achieve strong performance across a\nwide range of tasks, yet exhibit substantial variability in the various layers'\ntraining quality with respect to specific downstream applications, limiting\ntheir downstream performance.It is therefore critical to estimate layer-wise\ntraining quality in a manner that accounts for both model architecture and\ntraining data. However, existing approaches predominantly rely on model-centric\nheuristics (such as spectral statistics, outlier detection, or uniform\nallocation) while overlooking the influence of data. To address these\nlimitations, we propose LayerIF, a data-driven framework that leverages\nInfluence Functions to quantify the training quality of individual layers in a\nprincipled and task-sensitive manner. By isolating each layer's gradients and\nmeasuring the sensitivity of the validation loss to training examples by\ncomputing layer-wise influences, we derive data-driven estimates of layer\nimportance. Notably, our method produces task-specific layer importance\nestimates for the same LLM, revealing how layers specialize for different\ntest-time evaluation tasks. We demonstrate the utility of our scores by\nleveraging them for two downstream applications: (a) expert allocation in\nLoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM\npruning. Experiments across multiple LLM architectures demonstrate that our\nmodel-agnostic, influence-guided allocation leads to consistent gains in task\nperformance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23815", "pdf": "https://arxiv.org/pdf/2505.23815", "abs": "https://arxiv.org/abs/2505.23815", "authors": ["StÃ©phane Aroca-Ouellette", "Natalie Mackraz", "Barry-John Theobald", "Katherine Metcalf"], "title": "Aligning LLMs by Predicting Preferences from User Writing Samples", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025. 32 pages total: 9 main, 2 references, 21\n  appendix. arXiv admin note: substantial text overlap with arXiv:2410.06273", "summary": "Accommodating human preferences is essential for creating aligned LLM agents\nthat deliver personalized and effective interactions. Recent work has shown the\npotential for LLMs acting as writing agents to infer a description of user\npreferences. Agent alignment then comes from conditioning on the inferred\npreference description. However, existing methods often produce generic\npreference descriptions that fail to capture the unique and individualized\nnature of human preferences. This paper introduces PROSE, a method designed to\nenhance the precision of preference descriptions inferred from user writing\nsamples. PROSE incorporates two key elements: (1) iterative refinement of\ninferred preferences, and (2) verification of inferred preferences across\nmultiple user writing samples. We evaluate PROSE with several LLMs (i.e.,\nQwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an\nemail writing task. We find that PROSE more accurately infers nuanced human\npreferences, improving the quality of the writing agent's generations over\nCIPHER (a state-of-the-art method for inferring preferences) by 33\\%. Lastly,\nwe demonstrate that ICL and PROSE are complementary methods, and combining them\nprovides up to a 9\\% improvement over ICL alone.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23832", "pdf": "https://arxiv.org/pdf/2505.23832", "abs": "https://arxiv.org/abs/2505.23832", "authors": ["Chaeeun Kim", "Jinu Lee", "Wonseok Hwang"], "title": "LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation", "categories": ["cs.CL", "cs.IR"], "comment": "Under review", "summary": "Legal Case Retrieval (LCR), which retrieves relevant cases from a query case,\nis a fundamental task for legal professionals in research and decision-making.\nHowever, existing studies on LCR face two major limitations. First, they are\nevaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and\nuse a narrow range of criminal query types, which cannot sufficiently reflect\nthe complexity of real-world legal retrieval scenarios. Second, their reliance\non embedding-based or lexical matching methods often results in limited\nrepresentations and legally irrelevant matches. To address these issues, we\npresent: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering\n411 diverse crime types in queries over 1.2M legal cases; and (2)\nLegalSearchLM, a retrieval model that performs legal element reasoning over the\nquery case and directly generates content grounded in the target cases through\nconstrained decoding. Experimental results show that LegalSearchLM outperforms\nbaselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It\nalso demonstrates strong generalization to out-of-domain cases, outperforming\nnaive generative models trained on in-domain data by 15%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23840", "pdf": "https://arxiv.org/pdf/2505.23840", "abs": "https://arxiv.org/abs/2505.23840", "authors": ["Jiseung Hong", "Grace Byun", "Seungone Kim", "Kai Shu"], "title": "Measuring Sycophancy of Language Models in Multi-turn Dialogues", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are expected to provide helpful and harmless\nresponses, yet they often exhibit sycophancy--conforming to user beliefs\nregardless of factual accuracy or ethical soundness. Prior research on\nsycophancy has primarily focused on single-turn factual correctness,\noverlooking the dynamics of real-world interactions. In this work, we introduce\nSYCON Bench, a novel benchmark for evaluating sycophantic behavior in\nmulti-turn, free-form conversational settings. Our benchmark measures how\nquickly a model conforms to the user (Turn of Flip) and how frequently it\nshifts its stance under sustained user pressure (Number of Flip). Applying\nSYCON Bench to 17 LLMs across three real-world scenarios, we find that\nsycophancy remains a prevalent failure mode. Our analysis shows that alignment\ntuning amplifies sycophantic behavior, whereas model scaling and reasoning\noptimization strengthen the model's ability to resist undesirable user views.\nReasoning models generally outperform instruction-tuned models but often fail\nwhen they over-index on logical exposition instead of directly addressing the\nuser's underlying beliefs. Finally, we evaluate four additional prompting\nstrategies and demonstrate that adopting a third-person perspective reduces\nsycophancy by up to 63.8% in debate scenario. We release our code and data at\nhttps://github.com/JiseungHong/SYCON-Bench.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23842", "pdf": "https://arxiv.org/pdf/2505.23842", "abs": "https://arxiv.org/abs/2505.23842", "authors": ["Zikun Ye", "Hema Yoganarasimhan"], "title": "Document Valuation in LLM Summaries: A Cluster Shapley Approach", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in systems that retrieve\nand summarize content from multiple sources, such as search engines and AI\nassistants. While these models enhance user experience by generating coherent\nsummaries, they obscure the contributions of original content creators, raising\nconcerns about credit attribution and compensation. We address the challenge of\nvaluing individual documents used in LLM-generated summaries. We propose using\nShapley values, a game-theoretic method that allocates credit based on each\ndocument's marginal contribution. Although theoretically appealing, Shapley\nvalues are expensive to compute at scale. We therefore propose Cluster Shapley,\nan efficient approximation algorithm that leverages semantic similarity between\ndocuments. By clustering documents using LLM-based embeddings and computing\nShapley values at the cluster level, our method significantly reduces\ncomputation while maintaining attribution quality. We demonstrate our approach\nto a summarization task using Amazon product reviews. Cluster Shapley\nsignificantly reduces computational complexity while maintaining high accuracy,\noutperforming baseline methods such as Monte Carlo sampling and Kernel SHAP\nwith a better efficient frontier. Our approach is agnostic to the exact LLM\nused, the summarization process used, and the evaluation procedure, which makes\nit broadly applicable to a variety of summarization settings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "summarization"], "score": 3}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24238", "pdf": "https://arxiv.org/pdf/2505.24238", "abs": "https://arxiv.org/abs/2505.24238", "authors": ["Bowen Dong", "Minheng Ni", "Zitong Huang", "Guanglei Yang", "Wangmeng Zuo", "Lei Zhang"], "title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal hallucination in multimodal large language models (MLLMs)\nrestricts the correctness of MLLMs. However, multimodal hallucinations are\nmulti-sourced and arise from diverse causes. Existing benchmarks fail to\nadequately distinguish between perception-induced hallucinations and\nreasoning-induced hallucinations. This failure constitutes a significant issue\nand hinders the diagnosis of multimodal reasoning failures within MLLMs. To\naddress this, we propose the {\\dataset} benchmark, which isolates reasoning\nhallucinations by constructing questions where input images are correctly\nperceived by MLLMs yet reasoning errors persist. {\\dataset} introduces\nmulti-granular evaluation metrics: accuracy, factuality, and LLMs hallucination\nscore for hallucination quantification. Our analysis reveals that (1) the model\nscale, data scale, and training stages significantly affect the degree of\nlogical, fabrication, and factual hallucinations; (2) current MLLMs show no\neffective improvement on spatial hallucinations caused by misinterpreted\nspatial relationships, indicating their limited visual reasoning capabilities;\nand (3) question types correlate with distinct hallucination patterns,\nhighlighting targeted challenges and potential mitigation strategies. To\naddress these challenges, we propose {\\method}, a method that combines\ncurriculum reinforcement fine-tuning to encourage models to generate\nlogic-consistent reasoning chains by stepwise reducing learning difficulty, and\ncollaborative hint inference to reduce reasoning complexity. {\\method}\nestablishes a baseline on {\\dataset}, and reduces the logical hallucinations in\noriginal base models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "factuality", "accuracy"], "score": 5}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24247", "pdf": "https://arxiv.org/pdf/2505.24247", "abs": "https://arxiv.org/abs/2505.24247", "authors": ["Minchul Kim", "Anil Jain", "Xiaoming Liu"], "title": "50 Years of Automated Face Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Over the past 50 years, automated face recognition has evolved from\nrudimentary, handcrafted systems into sophisticated deep learning models that\nrival and often surpass human performance. This paper chronicles the history\nand technological progression of FR, from early geometric and statistical\nmethods to modern deep neural architectures leveraging massive real and\nAI-generated datasets. We examine key innovations that have shaped the field,\nincluding developments in dataset, loss function, neural network design and\nfeature fusion. We also analyze how the scale and diversity of training data\ninfluence model generalization, drawing connections between dataset growth and\nbenchmark improvements. Recent advances have achieved remarkable milestones:\nstate-of-the-art face verification systems now report False Negative\nIdentification Rates of 0.13% against a 12.4 million gallery in NIST FRVT\nevaluations for 1:N visa-to-border matching. While recent advances have enabled\nremarkable accuracy in high- and low-quality face scenarios, numerous\nchallenges persist. While remarkable progress has been achieved, several open\nresearch problems remain. We outline critical challenges and promising\ndirections for future face recognition research, including scalability,\nmulti-modal fusion, synthetic identity generation, and explainable systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23854", "pdf": "https://arxiv.org/pdf/2505.23854", "abs": "https://arxiv.org/abs/2505.23854", "authors": ["Linwei Tao", "Yi-Fan Yeh", "Minjing Dong", "Tao Huang", "Philip Torr", "Chang Xu"], "title": "Revisiting Uncertainty Estimation and Calibration of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\napplications, robust uncertainty estimation is essential for ensuring the safe\nand trustworthy deployment of LLMs. We present the most comprehensive study to\ndate of uncertainty estimation in LLMs, evaluating 80 models spanning open- and\nclosed-source families, dense and Mixture-of-Experts (MoE) architectures,\nreasoning and non-reasoning modes, quantization variants and parameter scales\nfrom 0.6B to 671B. Focusing on three representative black-box single-pass\nmethods, including token probability-based uncertainty (TPU), numerical verbal\nuncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically\nevaluate uncertainty calibration and selective classification using the\nchallenging MMLU-Pro benchmark, which covers both reasoning-intensive and\nknowledge-based tasks. Our results show that LVU consistently outperforms TPU\nand NVU, offering stronger calibration and discrimination while being more\ninterpretable. We also find that high accuracy does not imply reliable\nuncertainty, and that model scale, post-training, reasoning ability and\nquantization all influence estimation performance. Notably, LLMs exhibit better\nuncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good\ncalibration does not necessarily translate to effective error ranking. These\nfindings highlight the need for multi-perspective evaluation and position LVU\nas a practical tool for improving the reliability of LLMs in real-world\nsettings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23911", "pdf": "https://arxiv.org/pdf/2505.23911", "abs": "https://arxiv.org/abs/2505.23911", "authors": ["Pavel Tikhonov", "Ivan Oseledets", "Elena Tutubalina"], "title": "One Task Vector is not Enough: A Large-Scale Study for In-Context Learning", "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to adapt to\nnew tasks using few examples, with task vectors - specific hidden state\nactivations - hypothesized to encode task information. Existing studies are\nlimited by small-scale benchmarks, restricting comprehensive analysis. We\nintroduce QuiteAFew, a novel dataset of 3,096 diverse few-shot tasks, each with\n30 input-output pairs derived from the Alpaca dataset. Experiments with\nLlama-3-8B on QuiteAFew reveal: (1) task vector performance peaks at an\nintermediate layer (e.g., 15th), (2) effectiveness varies significantly by task\ntype, and (3) complex tasks rely on multiple, subtask-specific vectors rather\nthan a single vector, suggesting distributed task knowledge representation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "Alpaca"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24315", "pdf": "https://arxiv.org/pdf/2505.24315", "abs": "https://arxiv.org/abs/2505.24315", "authors": ["Jinlu Zhang", "Yixin Chen", "Zan Wang", "Jie Yang", "Yizhou Wang", "Siyuan Huang"], "title": "InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Recent advances in 3D human-aware generation have made significant progress.\nHowever, existing methods still struggle with generating novel Human Object\nInteraction (HOI) from text, particularly for open-set objects. We identify\nthree main challenges of this task: precise human-object relation reasoning,\naffordance parsing for any object, and detailed human interaction pose\nsynthesis aligning description and object geometry. In this work, we propose a\nnovel zero-shot 3D HOI generation framework without training on specific\ndatasets, leveraging the knowledge from large-scale pre-trained models.\nSpecifically, the human-object relations are inferred from large language\nmodels (LLMs) to initialize object properties and guide the optimization\nprocess. Then we utilize a pre-trained 2D image diffusion model to parse unseen\nobjects and extract contact points, avoiding the limitations imposed by\nexisting 3D asset knowledge. The initial human pose is generated by sampling\nmultiple hypotheses through multi-view SDS based on the input text and object\ngeometry. Finally, we introduce a detailed optimization to generate\nfine-grained, precise, and natural interaction, enforcing realistic 3D contact\nbetween the 3D object and the involved body parts, including hands in grasping.\nThis is achieved by distilling human-level feedback from LLMs to capture\ndetailed human-object relations from the text instruction. Extensive\nexperiments validate the effectiveness of our approach compared to prior works,\nparticularly in terms of the fine-grained nature of interactions and the\nability to handle open-set 3D objects.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23923", "pdf": "https://arxiv.org/pdf/2505.23923", "abs": "https://arxiv.org/abs/2505.23923", "authors": ["Feiteng Fang", "Ting-En Lin", "Yuchuan Wu", "Xiong Liu", "Xiang Huang", "Dingwei Chen", "Jing Ye", "Haonan Zhang", "Liang Zhu", "Hamid Alinejad-Rokny", "Min Yang", "Fei Huang", "Yongbin Li"], "title": "ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic\nand engaging human-computer interactions. However, traditional reward models\noften struggle with scalability and adapting to subjective conversational\npreferences. We propose ChARM, a Character-based Act-adaptive Reward Model,\naddressing these challenges through two innovations: (1) an act-adaptive margin\nthat significantly enhances learning efficiency and generalizability, and (2) a\nself-evolution mechanism leveraging large-scale unlabeled data to improve\ntraining coverage. Additionally, we introduce RoleplayPref, the first\nlarge-scale preference dataset specifically for RPLAs, featuring 1,108\ncharacters, 13 subcategories, and 16,888 bilingual dialogues, alongside\nRoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13%\nimprovement over the conventional Bradley-Terry model in preference rankings.\nFurthermore, applying ChARM-generated rewards to preference learning techniques\n(e.g., direct preference optimization) achieves state-of-the-art results on\nCharacterEval and RoleplayEval. Code and dataset are available at\nhttps://github.com/calubkk/ChARM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward modeling", "preference learning", "preference", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "preference dataset"], "score": 4}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24334", "pdf": "https://arxiv.org/pdf/2505.24334", "abs": "https://arxiv.org/abs/2505.24334", "authors": ["Uzair Khan", "Franco Fummi", "Luigi Capogrosso"], "title": "KairosAD: A SAM-Based Model for Industrial Anomaly Detection on Embedded Devices", "categories": ["cs.CV"], "comment": "Accepted at the 23rd International Conference on Image Analysis and\n  Processing (ICIAP 2025)", "summary": "In the era of intelligent manufacturing, anomaly detection has become\nessential for maintaining quality control on modern production lines. However,\nwhile many existing models show promising performance, they are often too\nlarge, computationally demanding, and impractical to deploy on\nresource-constrained embedded devices that can be easily installed on the\nproduction lines of Small and Medium Enterprises (SMEs). To bridge this gap, we\npresent KairosAD, a novel supervised approach that uses the power of the Mobile\nSegment Anything Model (MobileSAM) for image-based anomaly detection. KairosAD\nhas been evaluated on the two well-known industrial anomaly detection datasets,\ni.e., MVTec-AD and ViSA. The results show that KairosAD requires 78% fewer\nparameters and boasts a 4x faster inference time compared to the leading\nstate-of-the-art model, while maintaining comparable AUROC performance. We\ndeployed KairosAD on two embedded devices, the NVIDIA Jetson NX, and the NVIDIA\nJetson AGX. Finally, KairosAD was successfully installed and tested on the real\nproduction line of the Industrial Computer Engineering Laboratory (ICE Lab) at\nthe University of Verona. The code is available at\nhttps://github.com/intelligolabs/KairosAD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23932", "pdf": "https://arxiv.org/pdf/2505.23932", "abs": "https://arxiv.org/abs/2505.23932", "authors": ["Wendong Xu", "Jing Xiong", "Chenyang Zhao", "Qiujiang Chen", "Haoran Wang", "Hui Shen", "Zhongwei Wan", "Jianbo Dai", "Taiqiang Wu", "He Xiao", "Chaofan Tao", "Z. Morley Mao", "Ying Sheng", "Zhijiang Guo", "Hongxia Yang", "Bei Yu", "Lingpeng Kong", "Quanquan Gu", "Ngai Wong"], "title": "SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving", "categories": ["cs.CL"], "comment": null, "summary": "We present SwingArena, a competitive evaluation framework for Large Language\nModels (LLMs) that closely mirrors real-world software development workflows.\nUnlike traditional static benchmarks, SwingArena models the collaborative\nprocess of software iteration by pairing LLMs as submitters, who generate\npatches, and reviewers, who create test cases and verify the patches through\ncontinuous integration (CI) pipelines. To support these interactive\nevaluations, we introduce a retrieval-augmented code generation (RACG) module\nthat efficiently handles long-context challenges by providing syntactically and\nsemantically relevant code snippets from large codebases, supporting multiple\nprogramming languages (C++, Python, Rust, and Go). This enables the framework\nto scale across diverse tasks and contexts while respecting token limitations.\nOur experiments, using over 400 high-quality real-world GitHub issues selected\nfrom a pool of 2,300 issues, show that models like GPT-4o excel at aggressive\npatch generation, whereas DeepSeek and Gemini prioritize correctness in CI\nvalidation. SwingArena presents a scalable and extensible methodology for\nevaluating LLMs in realistic, CI-driven software development settings. More\ndetails are available on our project page: swing-bench.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "code generation"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24346", "pdf": "https://arxiv.org/pdf/2505.24346", "abs": "https://arxiv.org/abs/2505.24346", "authors": ["Ziyi Wang", "Zhi Gao", "Boxuan Yu", "Zirui Dai", "Yuxiang Song", "Qingyuan Lu", "Jin Chen", "Xinxiao Wu"], "title": "VUDG: A Dataset for Video Understanding Domain Generalization", "categories": ["cs.CV"], "comment": null, "summary": "Video understanding has made remarkable progress in recent years, largely\ndriven by advances in deep models and the availability of large-scale annotated\ndatasets. However, existing works typically ignore the inherent domain shifts\nencountered in real-world video applications, leaving domain generalization\n(DG) in video understanding underexplored. Hence, we propose Video\nUnderstanding Domain Generalization (VUDG), a novel dataset designed\nspecifically for evaluating the DG performance in video understanding. VUDG\ncontains videos from 11 distinct domains that cover three types of domain\nshifts, and maintains semantic similarity across different domains to ensure\nfair and meaningful evaluation. We propose a multi-expert progressive\nannotation framework to annotate each video with both multiple-choice and\nopen-ended question-answer pairs. Extensive experiments on 9 representative\nlarge video-language models (LVLMs) and several traditional video question\nanswering methods show that most models (including state-of-the-art LVLMs)\nsuffer performance degradation under domain shifts. These results highlight the\nchallenges posed by VUDG and the difference in the robustness of current models\nto data distribution shifts. We believe VUDG provides a valuable resource for\nprompting future research in domain generalization video understanding.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation"], "score": 3}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24009", "pdf": "https://arxiv.org/pdf/2505.24009", "abs": "https://arxiv.org/abs/2505.24009", "authors": ["Hidetaka Kamigaito", "Ying Zhang", "Jingun Kwon", "Katsuhiko Hayashi", "Manabu Okumura", "Taro Watanabe"], "title": "Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Transformers deliver outstanding performance across a wide range of tasks and\nare now a dominant backbone architecture for large language models (LLMs).\nTheir task-solving performance is improved by increasing parameter size, as\nshown in the recent studies on parameter scaling laws. Although recent\nmechanistic-interpretability studies have deepened our understanding of the\ninternal behavior of Transformers by analyzing their residual stream, the\nrelationship between these internal mechanisms and the parameter scaling laws\nremains unclear. To bridge this gap, we focus on layers and their size, which\nmainly decide the parameter size of Transformers. For this purpose, we first\ntheoretically investigate the layers within the residual stream through a\nbias-diversity decomposition. The decomposition separates (i) bias, the error\nof each layer's output from the ground truth, and (ii) diversity, which\nindicates how much the outputs of each layer differ from each other. Analyzing\nTransformers under this theory reveals that performance improves when\nindividual layers make predictions close to the correct answer and remain\nmutually diverse. We show that diversity becomes especially critical when\nindividual layers' outputs are far from the ground truth. Finally, we introduce\nan information-theoretic diversity and show our main findings that adding\nlayers enhances performance only when those layers behave differently, i.e.,\nare diverse. We also reveal the performance gains from increasing the number of\nlayers exhibit submodularity: marginal improvements diminish as additional\nlayers increase, mirroring the logarithmic convergence predicted by the\nparameter scaling laws. Experiments on multiple semantic-understanding tasks\nwith various LLMs empirically confirm the theoretical properties derived in\nthis study.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24372", "pdf": "https://arxiv.org/pdf/2505.24372", "abs": "https://arxiv.org/abs/2505.24372", "authors": ["Yichi Zhang", "Gongwei Chen", "Jun Zhu", "Jia Wan"], "title": "D2AF: A Dual-Driven Annotation and Filtering Framework for Visual Grounding", "categories": ["cs.CV"], "comment": "16pages, 8figures", "summary": "Visual Grounding is a task that aims to localize a target region in an image\nbased on a free-form natural language description. With the rise of Transformer\narchitectures, there is an increasing need for larger datasets to boost\nperformance. However, the high cost of manual annotation poses a challenge,\nhindering the scale of data and the ability of large models to enhance their\neffectiveness. Previous pseudo label generation methods heavily rely on\nhuman-labeled captions of the original dataset, limiting scalability and\ndiversity. To address this, we propose D2AF, a robust annotation framework for\nvisual grounding using only input images. This approach overcomes dataset size\nlimitations and enriches both the quantity and diversity of referring\nexpressions. Our approach leverages multimodal large models and object\ndetection models. By implementing dual-driven annotation strategies, we\neffectively generate detailed region-text pairs using both closed-set and\nopen-set approaches. We further conduct an in-depth analysis of data quantity\nand data distribution. Our findings demonstrate that increasing data volume\nenhances model performance. However, the degree of improvement depends on how\nwell the pseudo labels broaden the original data distribution. Based on these\ninsights, we propose a consistency and distribution aware filtering method to\nfurther improve data quality by effectively removing erroneous and redundant\ndata. This approach effectively eliminates noisy data, leading to improved\nperformance. Experiments on three visual grounding tasks demonstrate that our\nmethod significantly improves the performance of existing models and achieves\nstate-of-the-art results.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "consistency"], "score": 3}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24063", "pdf": "https://arxiv.org/pdf/2505.24063", "abs": "https://arxiv.org/abs/2505.24063", "authors": ["Jiacheng Xie", "Yang Yu", "Ziyang Zhang", "Shuai Zeng", "Jiaxuan He", "Ayush Vasireddy", "Xiaoting Tang", "Congyu Guo", "Lening Zhao", "Congcong Jing", "Guanghui An", "Dong Xu"], "title": "TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine", "categories": ["cs.CL", "cs.DB"], "comment": "22 pages, 4 figures", "summary": "Traditional Chinese Medicine (TCM), as an effective alternative medicine, has\nbeen receiving increasing attention. In recent years, the rapid development of\nlarge language models (LLMs) tailored for TCM has underscored the need for an\nobjective and comprehensive evaluation framework to assess their performance on\nreal-world tasks. However, existing evaluation datasets are limited in scope\nand primarily text-based, lacking a unified and standardized multimodal\nquestion-answering (QA) benchmark. To address this issue, we introduce\nTCM-Ladder, the first multimodal QA dataset specifically designed for\nevaluating large TCM language models. The dataset spans multiple core\ndisciplines of TCM, including fundamental theory, diagnostics, herbal formulas,\ninternal medicine, surgery, pharmacognosy, and pediatrics. In addition to\ntextual content, TCM-Ladder incorporates various modalities such as images and\nvideos. The datasets were constructed using a combination of automated and\nmanual filtering processes and comprise 52,000+ questions in total. These\nquestions include single-choice, multiple-choice, fill-in-the-blank, diagnostic\ndialogue, and visual comprehension tasks. We trained a reasoning model on\nTCM-Ladder and conducted comparative experiments against 9 state-of-the-art\ngeneral domain and 5 leading TCM-specific LLMs to evaluate their performance on\nthe datasets. Moreover, we propose Ladder-Score, an evaluation method\nspecifically designed for TCM question answering that effectively assesses\nanswer quality regarding terminology usage and semantic expression. To our\nknowledge, this is the first work to evaluate mainstream general domain and\nTCM-specific LLMs on a unified multimodal benchmark. The datasets and\nleaderboard are publicly available at https://tcmladder.com or\nhttps://54.211.107.106 and will be continuously updated.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "dialogue", "question answering"], "score": 5}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24417", "pdf": "https://arxiv.org/pdf/2505.24417", "abs": "https://arxiv.org/abs/2505.24417", "authors": ["Runnan Lu", "Yuxuan Zhang", "Jiaming Liu", "Haofan Wang", "Yiren Song"], "title": "EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Generating accurate multilingual text with diffusion models has long been\ndesired but remains challenging. Recent methods have made progress in rendering\ntext in a single language, but rendering arbitrary languages is still an\nunexplored area. This paper introduces EasyText, a text rendering framework\nbased on DiT (Diffusion Transformer), which connects denoising latents with\nmultilingual character tokens encoded as character tokens. We propose character\npositioning encoding and position encoding interpolation techniques to achieve\ncontrollable and precise text rendering. Additionally, we construct a\nlarge-scale synthetic text image dataset with 1 million multilingual image-text\nannotations as well as a high-quality dataset of 20K annotated images, which\nare used for pretraining and fine-tuning respectively. Extensive experiments\nand evaluations demonstrate the effectiveness and advancement of our approach\nin multilingual text rendering, visual quality, and layout-aware text\nintegration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24466", "pdf": "https://arxiv.org/pdf/2505.24466", "abs": "https://arxiv.org/abs/2505.24466", "authors": ["Yingjia Xu", "Jinlin Wu", "Zhen Chen", "Daming Gao", "Yang Yang", "Zhen Lei", "Min Cao"], "title": "SA-Person: Text-Based Person Retrieval with Scene-aware Re-ranking", "categories": ["cs.CV"], "comment": "22 pages, 7 figures. Under review", "summary": "Text-based person retrieval aims to identify a target individual from a\ngallery of images based on a natural language description. It presents a\nsignificant challenge due to the complexity of real-world scenes and the\nambiguity of appearance-related descriptions. Existing methods primarily\nemphasize appearance-based cross-modal retrieval, often neglecting the\ncontextual information embedded within the scene, which can offer valuable\ncomplementary insights for retrieval. To address this, we introduce\nSCENEPERSON-13W, a large-scale dataset featuring over 100,000 scenes with rich\nannotations covering both pedestrian appearance and environmental cues. Based\non this, we propose SA-Person, a two-stage retrieval framework. In the first\nstage, it performs discriminative appearance grounding by aligning textual cues\nwith pedestrian-specific regions. In the second stage, it introduces\nSceneRanker, a training-free, scene-aware re-ranking method leveraging\nmultimodal large language models to jointly reason over pedestrian appearance\nand the global scene context. Experiments on SCENEPERSON-13W validate the\neffectiveness of our framework in challenging scene-level retrieval scenarios.\nThe code and dataset will be made publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24481", "pdf": "https://arxiv.org/pdf/2505.24481", "abs": "https://arxiv.org/abs/2505.24481", "authors": ["Jing Huang", "Yongkang Zhao", "Yuhan Li", "Zhitao Dai", "Cheng Chen", "Qiying Lai"], "title": "ACM-UNet: Adaptive Integration of CNNs and Mamba for Efficient Medical Image Segmentation", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, 5 tables", "summary": "The U-shaped encoder-decoder architecture with skip connections has become a\nprevailing paradigm in medical image segmentation due to its simplicity and\neffectiveness. While many recent works aim to improve this framework by\ndesigning more powerful encoders and decoders, employing advanced convolutional\nneural networks (CNNs) for local feature extraction, Transformers or state\nspace models (SSMs) such as Mamba for global context modeling, or hybrid\ncombinations of both, these methods often struggle to fully utilize pretrained\nvision backbones (e.g., ResNet, ViT, VMamba) due to structural mismatches. To\nbridge this gap, we introduce ACM-UNet, a general-purpose segmentation\nframework that retains a simple UNet-like design while effectively\nincorporating pretrained CNNs and Mamba models through a lightweight adapter\nmechanism. This adapter resolves architectural incompatibilities and enables\nthe model to harness the complementary strengths of CNNs and SSMs-namely,\nfine-grained local detail extraction and long-range dependency modeling.\nAdditionally, we propose a hierarchical multi-scale wavelet transform module in\nthe decoder to enhance feature fusion and reconstruction fidelity. Extensive\nexperiments on the Synapse and ACDC benchmarks demonstrate that ACM-UNet\nachieves state-of-the-art performance while remaining computationally\nefficient. Notably, it reaches 85.12% Dice Score and 13.89mm HD95 on the\nSynapse dataset with 17.93G FLOPs, showcasing its effectiveness and\nscalability. Code is available at: https://github.com/zyklcode/ACM-UNet.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24187", "pdf": "https://arxiv.org/pdf/2505.24187", "abs": "https://arxiv.org/abs/2505.24187", "authors": ["Mikhail L. Arbuzov", "Alexey A. Shvets", "Sisong Beir"], "title": "Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The prevailing assumption of an exponential decay in large language model\n(LLM) reliability with sequence length, predicated on independent per-token\nerror probabilities, posits an inherent limitation for long autoregressive\noutputs. Our research fundamentally challenges this view by synthesizing\nemerging evidence that LLM errors are not uniformly distributed but are\nconcentrated at sparse \"key tokens\" ($5-10\\%$ of total tokens) representing\ncritical decision junctions. By distinguishing these high-impact tokens from\nthe increasingly predictable majority, we introduce a new reliability formula\nexplaining the sustained coherence of modern LLMs over thousands of tokens.\nConverging research streams reveal that long-context performance primarily\ndepends on accurately navigating a few crucial semantic decision points rather\nthan on uniform token-level accuracy, enabling targeted strategies that\nsignificantly outperform brute-force approaches. We thus propose a framework\nfor next-generation systems centered on selective preservation of semantically\nvital tokens, dynamic computational allocation at uncertain decision\nboundaries, multi-path exploration at ambiguities, and architectures aligned\nwith natural semantic domains. This marks a fundamental shift from raw scaling\nto strategic reasoning, promising breakthrough performance without\nproportionate computational scaling and offering a more nuanced understanding\nthat supersedes the exponential decay hypothesis, thereby opening pathways\ntoward substantially more powerful and efficient language systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24199", "pdf": "https://arxiv.org/pdf/2505.24199", "abs": "https://arxiv.org/abs/2505.24199", "authors": ["Yimin Du"], "title": "Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling", "categories": ["cs.CL"], "comment": "7 pages", "summary": "The quality of human preference data is crucial for training and evaluating\nlarge language models (LLMs), particularly in reinforcement learning from human\nfeedback (RLHF) and direct preference optimization (DPO) scenarios. Traditional\nside-by-side (SBS) annotation approaches often struggle with inherent\nuncertainty, annotator disagreement, and the complexity of preference\njudgments. This paper introduces a novel framework based on intuitionistic\nfuzzy sets (IFS) for modeling and aggregating human preferences in LLM data\nannotation tasks. Our approach captures not only the degree of preference but\nalso the uncertainty and hesitation inherent in human judgment through\nmembership, non-membership, and hesitation degrees. We propose an IFS-based\nannotation protocol that enables more nuanced preference modeling, develops\naggregation methods for handling annotator disagreement, and introduces quality\nmetrics for preference data assessment. Experimental validation on multiple\ndatasets demonstrates that our IFS-based approach significantly improves\nannotation consistency, reduces annotator fatigue, and produces higher-quality\npreference data compared to traditional binary and Likert-scale methods. The\nresulting preference datasets lead to improved model performance in downstream\ntasks, with 12.3\\% improvement in win-rate against baseline models and 15.7\\%\nreduction in annotation time. Our framework provides a principled approach to\nhandling uncertainty in human preference annotation and offers practical\nbenefits for large-scale LLM training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning", "preference", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "annotation", "consistency"], "score": 3}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24527", "pdf": "https://arxiv.org/pdf/2505.24527", "abs": "https://arxiv.org/abs/2505.24527", "authors": ["Simone Cammarasana", "Giuseppe PatanÃ¨"], "title": "Optimal Density Functions for Weighted Convolution in Learning Models", "categories": ["cs.CV", "cs.LG", "42A85"], "comment": "5 figures, 5 tables, 21 pages", "summary": "The paper introduces the weighted convolution, a novel approach to the\nconvolution for signals defined on regular grids (e.g., 2D images) through the\napplication of an optimal density function to scale the contribution of\nneighbouring pixels based on their distance from the central pixel. This choice\ndiffers from the traditional uniform convolution, which treats all neighbouring\npixels equally. Our weighted convolution can be applied to convolutional neural\nnetwork problems to improve the approximation accuracy. Given a convolutional\nnetwork, we define a framework to compute the optimal density function through\na minimisation model. The framework separates the optimisation of the\nconvolutional kernel weights (using stochastic gradient descent) from the\noptimisation of the density function (using DIRECT-L). Experimental results on\na learning model for an image-to-image task (e.g., image denoising) show that\nthe weighted convolution significantly reduces the loss (up to 53% improvement)\nand increases the test accuracy compared to standard convolution. While this\nmethod increases execution time by 11%, it is robust across several\nhyperparameters of the learning model. Future work will apply the weighted\nconvolution to real-case 2D and 3D image convolutional learning problems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24528", "pdf": "https://arxiv.org/pdf/2505.24528", "abs": "https://arxiv.org/abs/2505.24528", "authors": ["Pedram Ghamisi", "Weikang Yu", "Xiaokang Zhang", "Aldino Rizaldy", "Jian Wang", "Chufeng Zhou", "Richard Gloaguen", "Gustau Camps-Valls"], "title": "Geospatial Foundation Models to Enable Progress on Sustainable Development Goals", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Foundation Models (FMs) are large-scale, pre-trained AI systems that have\nrevolutionized natural language processing and computer vision, and are now\nadvancing geospatial analysis and Earth Observation (EO). They promise improved\ngeneralization across tasks, scalability, and efficient adaptation with minimal\nlabeled data. However, despite the rapid proliferation of geospatial FMs, their\nreal-world utility and alignment with global sustainability goals remain\nunderexplored. We introduce SustainFM, a comprehensive benchmarking framework\ngrounded in the 17 Sustainable Development Goals with extremely diverse tasks\nranging from asset wealth prediction to environmental hazard detection. This\nstudy provides a rigorous, interdisciplinary assessment of geospatial FMs and\noffers critical insights into their role in attaining sustainability goals. Our\nfindings show: (1) While not universally superior, FMs often outperform\ntraditional approaches across diverse tasks and datasets. (2) Evaluating FMs\nshould go beyond accuracy to include transferability, generalization, and\nenergy efficiency as key criteria for their responsible use. (3) FMs enable\nscalable, SDG-grounded solutions, offering broad utility for tackling complex\nsustainability challenges. Critically, we advocate for a paradigm shift from\nmodel-centric development to impact-driven deployment, and emphasize metrics\nsuch as energy efficiency, robustness to domain shifts, and ethical\nconsiderations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "criteria"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24241", "pdf": "https://arxiv.org/pdf/2505.24241", "abs": "https://arxiv.org/abs/2505.24241", "authors": ["Naibin Gu", "Yilong Chen", "Zhenyu Zhang", "Peng Fu", "Zheng Lin", "Shuohuan Wang", "Yu Sun", "Hua Wu", "Weiping Wang", "Haifeng Wang"], "title": "Advantageous Parameter Expansion Training Makes Better Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Although scaling up the number of trainable parameters in both pre-training\nand fine-tuning can effectively improve the performance of large language\nmodels, it also leads to increased computational overhead. When delving into\nthe parameter difference, we find that a subset of parameters, termed\nadvantageous parameters, plays a crucial role in determining model performance.\nFurther analysis reveals that stronger models tend to possess more such\nparameters. In this paper, we propose Advantageous Parameter EXpansion Training\n(APEX), a method that progressively expands advantageous parameters into the\nspace of disadvantageous ones, thereby increasing their proportion and\nenhancing training effectiveness. Further theoretical analysis from the\nperspective of matrix effective rank explains the performance gains of APEX.\nExtensive experiments on both instruction tuning and continued pre-training\ndemonstrate that, in instruction tuning, APEX outperforms full-parameter tuning\nwhile using only 52% of the trainable parameters. In continued pre-training,\nAPEX achieves the same perplexity level as conventional training with just 33%\nof the training data, and yields significant improvements on downstream tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24251", "pdf": "https://arxiv.org/pdf/2505.24251", "abs": "https://arxiv.org/abs/2505.24251", "authors": ["Xiaoyu Li", "Xiao Li", "Li Gao", "Yiding Liu", "Xiaoyang Wang", "Shuaiqiang Wang", "Junfeng Wang", "Dawei Yin"], "title": "Proactive Guidance of Multi-Turn Conversation in Industrial Search", "categories": ["cs.CL", "cs.IR"], "comment": "ACL'25 (Industry)", "summary": "The evolution of Large Language Models (LLMs) has significantly advanced\nmulti-turn conversation systems, emphasizing the need for proactive guidance to\nenhance users' interactions. However, these systems face challenges in\ndynamically adapting to shifts in users' goals and maintaining low latency for\nreal-time interactions. In the Baidu Search AI assistant, an industrial-scale\nmulti-turn search system, we propose a novel two-phase framework to provide\nproactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning\n(G-SFT), employs a goal adaptation agent that dynamically adapts to user goal\nshifts and provides goal-relevant contextual information. G-SFT also\nincorporates scalable knowledge transfer to distill insights from LLMs into a\nlightweight model for real-time interaction. The second phase, Click-oriented\nReinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically\nconstructs preference pairs from user click signals, and proactively improves\nclick-through rates through more engaging guidance. This dual-phase\narchitecture achieves complementary objectives: G-SFT ensures accurate goal\ntracking, while C-RL optimizes interaction quality through click signal-driven\nreinforcement learning. Extensive experiments demonstrate that our framework\nachieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and\n25.28% CTR in online deployment (149.06% relative improvement), while reducing\ninference latency by 69.55% through scalable knowledge distillation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24600", "pdf": "https://arxiv.org/pdf/2505.24600", "abs": "https://arxiv.org/abs/2505.24600", "authors": ["Omer Nacar", "Yasser Al-Habashi", "Serry Sibaee", "Adel Ammar", "Wadii Boulila"], "title": "SARD: A Large-Scale Synthetic Arabic OCR Dataset for Book-Style Text Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Arabic Optical Character Recognition (OCR) is essential for converting vast\namounts of Arabic print media into digital formats. However, training modern\nOCR models, especially powerful vision-language models, is hampered by the lack\nof large, diverse, and well-structured datasets that mimic real-world book\nlayouts. Existing Arabic OCR datasets often focus on isolated words or lines or\nare limited in scale, typographic variety, or structural complexity found in\nbooks. To address this significant gap, we introduce SARD (Large-Scale\nSynthetic Arabic OCR Dataset). SARD is a massive, synthetically generated\ndataset specifically designed to simulate book-style documents. It comprises\n843,622 document images containing 690 million words, rendered across ten\ndistinct Arabic fonts to ensure broad typographic coverage. Unlike datasets\nderived from scanned documents, SARD is free from real-world noise and\ndistortions, offering a clean and controlled environment for model training.\nIts synthetic nature provides unparalleled scalability and allows for precise\ncontrol over layout and content variation. We detail the dataset's composition\nand generation process and provide benchmark results for several OCR models,\nincluding traditional and deep learning approaches, highlighting the challenges\nand opportunities presented by this dataset. SARD serves as a valuable resource\nfor developing and evaluating robust OCR and vision-language models capable of\nprocessing diverse Arabic book-style texts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24264", "pdf": "https://arxiv.org/pdf/2505.24264", "abs": "https://arxiv.org/abs/2505.24264", "authors": ["Xin Quan", "Marco Valentino", "Louise A. Dennis", "AndrÃ© Freitas"], "title": "Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations", "categories": ["cs.CL", "cs.AI"], "comment": "Camera-ready for ACL 2025", "summary": "Natural language explanations play a fundamental role in Natural Language\nInference (NLI) by revealing how premises logically entail hypotheses. Recent\nwork has shown that the interaction of large language models (LLMs) with\ntheorem provers (TPs) can help verify and improve the validity of NLI\nexplanations. However, TPs require translating natural language into\nmachine-verifiable formal representations, a process that introduces the risk\nof semantic information loss and unfaithful interpretation, an issue compounded\nby LLMs' challenges in capturing critical logical structures with sufficient\nprecision. Moreover, LLMs are still limited in their capacity for rigorous and\nrobust proof construction within formal verification frameworks. To mitigate\nissues related to faithfulness and robustness, this paper investigates\nstrategies to (1) alleviate semantic loss during autoformalisation, (2)\nefficiently identify and correct syntactic errors in logical representations,\n(3) explicitly use logical expressions to guide LLMs in generating structured\nproof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback\nfor iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree\nusing different LLMs demonstrate that the proposed strategies yield significant\nimprovements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation\nrefinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover,\nwe show that specific interventions on the hybrid LLM-TP architecture can\nsubstantially improve efficiency, drastically reducing the number of iterations\nrequired for successful verification.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24634", "pdf": "https://arxiv.org/pdf/2505.24634", "abs": "https://arxiv.org/abs/2505.24634", "authors": ["Xuzhi Wang", "Wei Feng", "Lingdong Kong", "Liang Wan"], "title": "NUC-Net: Non-uniform Cylindrical Partition Network for Efficient LiDAR Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR semantic segmentation plays a vital role in autonomous driving.\nExisting voxel-based methods for LiDAR semantic segmentation apply uniform\npartition to the 3D LiDAR point cloud to form a structured representation based\non cartesian/cylindrical coordinates. Although these methods show impressive\nperformance, the drawback of existing voxel-based methods remains in two\naspects: (1) it requires a large enough input voxel resolution, which brings a\nlarge amount of computation cost and memory consumption. (2) it does not well\nhandle the unbalanced point distribution of LiDAR point cloud. In this paper,\nwe propose a non-uniform cylindrical partition network named NUC-Net to tackle\nthe above challenges. Specifically, we propose the Arithmetic Progression of\nInterval (API) method to non-uniformly partition the radial axis and generate\nthe voxel representation which is representative and efficient. Moreover, we\npropose a non-uniform multi-scale aggregation method to improve contextual\ninformation. Our method achieves state-of-the-art performance on SemanticKITTI\nand nuScenes datasets with much faster speed and much less training time. And\nour method can be a general component for LiDAR semantic segmentation, which\nsignificantly improves both the accuracy and efficiency of the uniform\ncounterpart by $4 \\times$ training faster and $2 \\times$ GPU memory reduction\nand $3 \\times$ inference speedup. We further provide theoretical analysis\ntowards understanding why NUC is effective and how point distribution affects\nperformance. Code is available at\n\\href{https://github.com/alanWXZ/NUC-Net}{https://github.com/alanWXZ/NUC-Net}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24302", "pdf": "https://arxiv.org/pdf/2505.24302", "abs": "https://arxiv.org/abs/2505.24302", "authors": ["Yike Wang", "Shangbin Feng", "Yulia Tsvetkov", "Hannaneh Hajishirzi"], "title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to support scientific\nresearch, but their knowledge of scientific advancements can quickly become\noutdated. We introduce ScienceMeter, a new framework for evaluating scientific\nknowledge update methods over scientific knowledge spanning the past, present,\nand future. ScienceMeter defines three metrics: knowledge preservation, the\nextent to which models' understanding of previously learned papers are\npreserved; knowledge acquisition, how well scientific claims from newly\nintroduced papers are acquired; and knowledge projection, the ability of the\nupdated model to anticipate or generalize to related scientific claims that may\nemerge in the future. Using ScienceMeter, we examine the scientific knowledge\nof LLMs on claim judgment and generation tasks across a curated dataset of\n15,444 scientific papers and 30,888 scientific claims from ten domains\nincluding medicine, biology, materials science, and computer science. We\nevaluate five representative knowledge update approaches including training-\nand inference-time methods. With extensive experiments, we find that the\nbest-performing knowledge update methods can preserve only 85.9% of existing\nknowledge, acquire 71.7% of new knowledge, and project 37.7% of future\nknowledge. Inference-based methods work for larger models, whereas smaller\nmodels require training to achieve comparable performance. Cross-domain\nanalysis reveals that performance on these objectives is correlated. Even when\napplying on specialized scientific LLMs, existing knowledge update methods fail\nto achieve these objectives collectively, underscoring that developing robust\nscientific knowledge update mechanisms is both crucial and challenging.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24332", "pdf": "https://arxiv.org/pdf/2505.24332", "abs": "https://arxiv.org/abs/2505.24332", "authors": ["Wenxuan Shi", "Haochen Tan", "Chuqiao Kuang", "Xiaoguang Li", "Xiaozhe Ren", "Chen Zhang", "Hanting Chen", "Yasheng Wang", "Lifeng Shang", "Fisher Yu", "Yunhe Wang"], "title": "Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Information seeking demands iterative evidence gathering and reflective\nreasoning, yet large language models (LLMs) still struggle with it in open-web\nquestion answering. Existing methods rely on static prompting rules or training\nwith Wikipedia-based corpora and retrieval environments, limiting adaptability\nto the real-world web environment where ambiguity, conflicting evidence, and\nnoise are prevalent. These constrained training settings hinder LLMs from\nlearning to dynamically decide when and where to search, and how to adjust\nsearch depth and frequency based on informational demands. We define this\nmissing capacity as Search Intensity Scaling (SIS)--the emergent skill to\nintensify search efforts under ambiguous or conflicting conditions, rather than\nsettling on overconfident, under-verification answers.\n  To study SIS, we introduce WebPuzzle, the first dataset designed to foster\ninformation-seeking behavior in open-world internet environments. WebPuzzle\nconsists of 24K training instances and 275 test questions spanning both\nwiki-based and open-web queries. Building on this dataset, we propose\nDeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by\nencouraging adaptive search policies through exploration under a real-world\nopen-web environment. Experimental results show that Pangu-7B-Reasoner\nempowered by DeepDiver achieve performance on real-web tasks comparable to the\n671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from\ncold-start supervised fine-tuning to a carefully designed RL phase, and present\nthat its capability of SIS generalizes from closed-form QA to open-ended tasks\nsuch as long-form writing. Our contributions advance adaptive information\nseeking in LLMs and provide a valuable benchmark and dataset for future\nresearch.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24649", "pdf": "https://arxiv.org/pdf/2505.24649", "abs": "https://arxiv.org/abs/2505.24649", "authors": ["Huu-Thien Tran", "Thanh-Dat Truong", "Khoa Luu"], "title": "BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "CVPRW 2025, 8 pages, 4 figures", "summary": "Large vision-language models have become widely adopted to advance in various\ndomains. However, developing a trustworthy system with minimal interpretable\ncharacteristics of large-scale models presents a significant challenge. One of\nthe most prevalent terms associated with the fallacy functions caused by these\nsystems is hallucination, where the language model generates a response that\ndoes not correspond to the visual content. To mitigate this problem, several\napproaches have been developed, and one prominent direction is to ameliorate\nthe decoding process. In this paper, we propose a new Bijective Maximum\nLikelihood Learning (BIMA) approach to hallucination mitigation using\nnormalizing flow theories. The proposed BIMA method can efficiently mitigate\nthe hallucination problem in prevailing vision-language models, resulting in\nsignificant improvements. Notably, BIMA achieves the average F1 score of 85.06%\non POPE benchmark and remarkably reduce CHAIRS and CHAIRI by 7.6% and 2.6%,\nrespectively. To the best of our knowledge, this is one of the first studies\nthat contemplates the bijection means to reduce hallucination induced by large\nvision-language models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24693", "pdf": "https://arxiv.org/pdf/2505.24693", "abs": "https://arxiv.org/abs/2505.24693", "authors": ["Julio Silva-RodrÃ­guez", "Ismail Ben Ayed", "Jose Dolz"], "title": "Conformal Prediction for Zero-Shot Models", "categories": ["cs.CV"], "comment": "CVPR 2025. Code: https://github.com/jusiro/CLIP-Conformal", "summary": "Vision-language models pre-trained at large scale have shown unprecedented\nadaptability and generalization to downstream tasks. Although its\ndiscriminative potential has been widely explored, its reliability and\nuncertainty are still overlooked. In this work, we investigate the capabilities\nof CLIP models under the split conformal prediction paradigm, which provides\ntheoretical guarantees to black-box models based on a small, labeled\ncalibration set. In contrast to the main body of literature on conformal\npredictors in vision classifiers, foundation models exhibit a particular\ncharacteristic: they are pre-trained on a one-time basis on an inaccessible\nsource domain, different from the transferred task. This domain drift\nnegatively affects the efficiency of the conformal sets and poses additional\nchallenges. To alleviate this issue, we propose Conf-OT, a transfer learning\nsetting that operates transductive over the combined calibration and query\nsets. Solving an optimal transport problem, the proposed method bridges the\ndomain gap between pre-training and adaptation without requiring additional\ndata splits but still maintaining coverage guarantees. We comprehensively\nexplore this conformal prediction strategy on a broad span of 15 datasets and\nthree non-conformity scores. Conf-OT provides consistent relative improvements\nof up to 20% on set efficiency while being 15 times faster than popular\ntransductive approaches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24733", "pdf": "https://arxiv.org/pdf/2505.24733", "abs": "https://arxiv.org/abs/2505.24733", "authors": ["Jiaxu Zhang", "Xianfang Zeng", "Xin Chen", "Wei Zuo", "Gang Yu", "Guosheng Lin", "Zhigang Tu"], "title": "DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents DreamDance, a novel character art animation framework\ncapable of producing stable, consistent character and scene motion conditioned\non precise camera trajectories. To achieve this, we re-formulate the animation\ntask as two inpainting-based steps: Camera-aware Scene Inpainting and\nPose-aware Video Inpainting. The first step leverages a pre-trained image\ninpainting model to generate multi-view scene images from the reference art and\noptimizes a stable large-scale Gaussian field, which enables coarse background\nvideo rendering with camera trajectories. However, the rendered video is rough\nand only conveys scene motion. To resolve this, the second step trains a\npose-aware video inpainting model that injects the dynamic character into the\nscene video while enhancing background quality. Specifically, this model is a\nDiT-based video generation model with a gating strategy that adaptively\nintegrates the character's appearance and pose information into the base\nbackground video. Through extensive experiments, we demonstrate the\neffectiveness and generalizability of DreamDance, producing high-quality and\nconsistent character animations with remarkable camera dynamics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24824", "pdf": "https://arxiv.org/pdf/2505.24824", "abs": "https://arxiv.org/abs/2505.24824", "authors": ["Marta LÃ³pez-Rauhut", "Hongyu Zhou", "Mathieu Aubry", "Loic Landrieu"], "title": "Segmenting France Across Four Centuries", "categories": ["cs.CV"], "comment": "20 pages, 8 figures, 3 tables", "summary": "Historical maps offer an invaluable perspective into territory evolution\nacross past centuries--long before satellite or remote sensing technologies\nexisted. Deep learning methods have shown promising results in segmenting\nhistorical maps, but publicly available datasets typically focus on a single\nmap type or period, require extensive and costly annotations, and are not\nsuited for nationwide, long-term analyses. In this paper, we introduce a new\ndataset of historical maps tailored for analyzing large-scale, long-term land\nuse and land cover evolution with limited annotations. Spanning metropolitan\nFrance (548,305 km^2), our dataset contains three map collections from the\n18th, 19th, and 20th centuries. We provide both comprehensive modern labels and\n22,878 km^2 of manually annotated historical labels for the 18th and 19th\ncentury maps. Our dataset illustrates the complexity of the segmentation task,\nfeaturing stylistic inconsistencies, interpretive ambiguities, and significant\nlandscape changes (e.g., marshlands disappearing in favor of forests). We\nassess the difficulty of these challenges by benchmarking three approaches: a\nfully-supervised model trained with historical labels, and two\nweakly-supervised models that rely only on modern annotations. The latter\neither use the modern labels directly or first perform image-to-image\ntranslation to address the stylistic gap between historical and contemporary\nmaps. Finally, we discuss how these methods can support long-term environment\nmonitoring, offering insights into centuries of landscape transformation. Our\nofficial project repository is publicly available at\nhttps://github.com/Archiel19/FRAx4.git.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24838", "pdf": "https://arxiv.org/pdf/2505.24838", "abs": "https://arxiv.org/abs/2505.24838", "authors": ["Brandon Man", "Ghadi Nehme", "Md Ferdous Alam", "Faez Ahmed"], "title": "VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Computer-Aided Design (CAD) is a time-consuming and complex process,\nrequiring precise, long-horizon user interactions with intricate 3D interfaces.\nWhile recent advances in AI-driven user interface (UI) agents show promise,\nmost existing datasets and methods focus on short, low-complexity tasks in\nmobile or web applications, failing to capture the demands of professional\nengineering tools. In this work, we introduce VideoCAD, the first attempt at\nengineering UI interaction learning for precision tasks. Specifically, VideoCAD\nis a large-scale synthetic dataset consisting of over 41K annotated video\nrecordings of CAD operations, generated using an automated framework for\ncollecting high-fidelity UI action data from human-made CAD designs. Compared\nto existing datasets, VideoCAD offers an order of magnitude higher complexity\nin UI interaction learning for real-world engineering tasks, having up to a 20x\nlonger time horizon than other datasets. We show two important downstream\napplications of VideoCAD: learning UI interactions from professional precision\n3D CAD tools and a visual question-answering (VQA) benchmark designed to\nevaluate multimodal large language models' (LLM) spatial reasoning and video\nunderstanding abilities. To learn the UI interactions, we propose\nVideoCADFormer - a state-of-the-art model in learning CAD interactions directly\nfrom video, which outperforms multiple behavior cloning baselines. Both\nVideoCADFormer and the VQA benchmark derived from VideoCAD reveal key\nchallenges in the current state of video-based UI understanding, including the\nneed for precise action grounding, multi-modal and spatial reasoning, and\nlong-horizon dependencies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24848", "pdf": "https://arxiv.org/pdf/2505.24848", "abs": "https://arxiv.org/abs/2505.24848", "authors": ["Charig Yang", "Samiul Alam", "Shakhrul Iman Siam", "Michael J. Proulx", "Lambert Mathias", "Kiran Somasundaram", "Luis Pesqueira", "James Fort", "Sheroze Sheriffdeen", "Omkar Parkhi", "Carl Ren", "Mi Zhang", "Yuning Chai", "Richard Newcombe", "Hyo Jin Kim"], "title": "Reading Recognition in the Wild", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "To enable egocentric contextual AI in always-on smart glasses, it is crucial\nto be able to keep a record of the user's interactions with the world,\nincluding during reading. In this paper, we introduce a new task of reading\nrecognition to determine when the user is reading. We first introduce the\nfirst-of-its-kind large-scale multimodal Reading in the Wild dataset,\ncontaining 100 hours of reading and non-reading videos in diverse and realistic\nscenarios. We then identify three modalities (egocentric RGB, eye gaze, head\npose) that can be used to solve the task, and present a flexible transformer\nmodel that performs the task using these modalities, either individually or\ncombined. We show that these modalities are relevant and complementary to the\ntask, and investigate how to efficiently and effectively encode each modality.\nAdditionally, we show the usefulness of this dataset towards classifying types\nof reading, extending current reading understanding studies conducted in\nconstrained settings to larger scale, diversity and realism. Code, model, and\ndata will be public.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24500", "pdf": "https://arxiv.org/pdf/2505.24500", "abs": "https://arxiv.org/abs/2505.24500", "authors": ["Guiyang Hou", "Xing Gao", "Yuchuan Wu", "Xiang Huang", "Wenqi Zhang", "Zhe Zheng", "Yongliang Shen", "Jialu Du", "Fei Huang", "Yongbin Li", "Weiming Lu"], "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence", "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 12 figures", "summary": "Recently, Large Language Models (LLMs) have made significant progress in\nIQ-related domains that require careful thinking, such as mathematics and\ncoding. However, enhancing LLMs' cognitive development in social domains,\nparticularly from a post-training perspective, remains underexplored.\nRecognizing that the social world follows a distinct timeline and requires a\nricher blend of cognitive modes (from intuitive reactions (System 1) and\nsurface-level thinking to deliberate thinking (System 2)) than mathematics,\nwhich primarily relies on System 2 cognition (careful, step-by-step reasoning),\nwe introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning\n(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we\nsystematically explore improving LLMs' social intelligence and validate the\neffectiveness of the TimeHC-RL method, through five other post-training\nparadigms and two test-time intervention paradigms on eight datasets with\ndiverse data patterns. Experimental results reveal the superiority of our\nproposed TimeHC-RL method compared to the widely adopted System 2 RL method. It\ngives the 7B backbone model wings, enabling it to rival the performance of\nadvanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic\nexploration from post-training and test-time interventions perspectives to\nimprove LLMs' social intelligence has uncovered several valuable insights.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24869", "pdf": "https://arxiv.org/pdf/2505.24869", "abs": "https://arxiv.org/abs/2505.24869", "authors": ["Ce Zhang", "Yan-Bo Lin", "Ziyang Wang", "Mohit Bansal", "Gedas Bertasius"], "title": "SiLVR: A Simple Language-based Video Reasoning Framework", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in test-time optimization have led to remarkable reasoning\ncapabilities in Large Language Models (LLMs), enabling them to solve highly\ncomplex problems in math and coding. However, the reasoning capabilities of\nmultimodal LLMs (MLLMs) still significantly lag, especially for complex\nvideo-language tasks. To address this issue, we present SiLVR, a Simple\nLanguage-based Video Reasoning framework that decomposes complex video\nunderstanding into two stages. In the first stage, SiLVR transforms raw video\ninto language-based representations using multisensory inputs, such as short\nclip captions and audio/speech subtitles. In the second stage, language\ndescriptions are fed into a powerful reasoning LLM to solve complex\nvideo-language understanding tasks. To handle long-context multisensory inputs,\nwe use an adaptive token reduction scheme, which dynamically determines the\ntemporal granularity with which to sample the tokens. Our simple, modular, and\ntraining-free video reasoning framework achieves the best-reported results on\nVideo-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.\nFurthermore, our empirical study focused on video reasoning capabilities shows\nthat, despite not being explicitly trained on video, strong reasoning LLMs can\neffectively aggregate multisensory input information from video, speech, and\naudio for complex temporal, causal, long-context, and knowledge acquisition\nreasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24538", "pdf": "https://arxiv.org/pdf/2505.24538", "abs": "https://arxiv.org/abs/2505.24538", "authors": ["Orfeas Menis Mastromichalakis", "Jason Liartis", "Kristina Rose", "Antoine Isaac", "Giorgos Stamou"], "title": "Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections", "categories": ["cs.CL"], "comment": null, "summary": "Cultural Heritage (CH) data hold invaluable knowledge, reflecting the\nhistory, traditions, and identities of societies, and shaping our understanding\nof the past and present. However, many CH collections contain outdated or\noffensive descriptions that reflect historical biases. CH Institutions (CHIs)\nface significant challenges in curating these data due to the vast scale and\ncomplexity of the task. To address this, we develop an AI-powered tool that\ndetects offensive terms in CH metadata and provides contextual insights into\ntheir historical background and contemporary perception. We leverage a\nmultilingual vocabulary co-created with marginalized communities, researchers,\nand CH professionals, along with traditional NLP techniques and Large Language\nModels (LLMs). Available as a standalone web app and integrated with major CH\nplatforms, the tool has processed over 7.9 million records, contextualizing the\ncontentious terms detected in their metadata. Rather than erasing these terms,\nour approach seeks to inform, making biases visible and providing actionable\ninsights for creating more inclusive and accessible CH collections.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24550", "pdf": "https://arxiv.org/pdf/2505.24550", "abs": "https://arxiv.org/abs/2505.24550", "authors": ["Xiaoang Xu", "Shuo Wang", "Xu Han", "Zhenghao Liu", "Huijia Wu", "Peipei Li", "Zhiyuan Liu", "Maosong Sun", "Zhaofeng He"], "title": "A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) achieve superior performance by extending the\nthought length. However, a lengthy thinking trajectory leads to reduced\nefficiency. Most of the existing methods are stuck in the assumption of\noverthinking and attempt to reason efficiently by compressing the\nChain-of-Thought, but this often leads to performance degradation. To address\nthis problem, we introduce A*-Thought, an efficient tree search-based unified\nframework designed to identify and isolate the most essential thoughts from the\nextensive reasoning chains produced by these models. It formulates the\nreasoning process of LRMs as a search tree, where each node represents a\nreasoning span in the giant reasoning space. By combining the A* search\nalgorithm with a cost function specific to the reasoning path, it can\nefficiently compress the chain of thought and determine a reasoning path with\nhigh information density and low cost. In addition, we also propose a\nbidirectional importance estimation mechanism, which further refines this\nsearch process and enhances its efficiency beyond uniform sampling. Extensive\nexperiments on several advanced math tasks show that A*-Thought effectively\nbalances performance and efficiency over a huge search space. Specifically,\nA*-Thought can improve the performance of QwQ-32B by 2.39$\\times$ with\nlow-budget and reduce the length of the output token by nearly 50% with\nhigh-budget. The proposed method is also compatible with several other LRMs,\ndemonstrating its generalization capability. The code can be accessed at:\nhttps://github.com/AI9Stars/AStar-Thought.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24876", "pdf": "https://arxiv.org/pdf/2505.24876", "abs": "https://arxiv.org/abs/2505.24876", "authors": ["Tajamul Ashraf", "Amal Saqib", "Hanan Ghani", "Muhra AlMahri", "Yuhao Li", "Noor Ahsan", "Umair Nawaz", "Jean Lahoud", "Hisham Cholakkal", "Mubarak Shah", "Philip Torr", "Fahad Shahbaz Khan", "Rao Muhammad Anwer", "Salman Khan"], "title": "Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Deep reasoning is fundamental for solving complex tasks, especially in\nvision-centric scenarios that demand sequential, multimodal understanding.\nHowever, existing benchmarks typically evaluate agents with fully synthetic,\nsingle-turn queries, limited visual modalities, and lack a framework to assess\nreasoning quality over multiple steps as required in real-world settings. To\naddress this, we introduce Agent-X, a large-scale benchmark for evaluating\nvision-centric agents multi-step and deep reasoning capabilities in real-world,\nmultimodal settings. Agent- X features 828 agentic tasks with authentic visual\ncontexts, including images, multi-image comparisons, videos, and instructional\ntext. These tasks span six major agentic environments: general visual\nreasoning, web browsing, security and surveillance, autonomous driving, sports,\nand math reasoning. Our benchmark requires agents to integrate tool use with\nexplicit, stepwise decision-making in these diverse settings. In addition, we\npropose a fine-grained, step-level evaluation framework that assesses the\ncorrectness and logical coherence of each reasoning step and the effectiveness\nof tool usage throughout the task. Our results reveal that even the\nbest-performing models, including GPT, Gemini, and Qwen families, struggle to\nsolve multi-step vision tasks, achieving less than 50% full-chain success.\nThese findings highlight key bottlenecks in current LMM reasoning and tool-use\ncapabilities and identify future research directions in vision-centric agentic\nreasoning models. Our data and code are publicly available at\nhttps://github.com/mbzuai-oryx/Agent-X", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24640", "pdf": "https://arxiv.org/pdf/2505.24640", "abs": "https://arxiv.org/abs/2505.24640", "authors": ["Jens-Joris Decorte", "Jeroen Van Hautte", "Chris Develder", "Thomas Demeester"], "title": "Efficient Text Encoders for Labor Market Analysis", "categories": ["cs.CL", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Labor market analysis relies on extracting insights from job advertisements,\nwhich provide valuable yet unstructured information on job titles and\ncorresponding skill requirements. While state-of-the-art methods for skill\nextraction achieve strong performance, they depend on large language models\n(LLMs), which are computationally expensive and slow. In this paper, we propose\n\\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level\nattention that is well-suited for the extreme multi-label classification task\nof skill classification. \\textbf{ConTeXT-match} significantly improves skill\nextraction efficiency and performance, achieving state-of-the-art results with\na lightweight bi-encoder model. To support robust evaluation, we introduce\n\\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill\nannotations that explicitly address the redundancy in the large label space.\nFinally, we present \\textbf{JobBERT V2}, an improved job title normalization\nmodel that leverages extracted skills to produce high-quality job title\nrepresentations. Experiments demonstrate that our models are efficient,\naccurate, and scalable, making them ideal for large-scale, real-time labor\nmarket analysis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24160", "pdf": "https://arxiv.org/pdf/2505.24160", "abs": "https://arxiv.org/abs/2505.24160", "authors": ["Junyu Chen", "Shuwen Wei", "Joel Honkamaa", "Pekka Marttinen", "Hang Zhang", "Min Liu", "Yichao Zhou", "Zuopeng Tan", "Zhuoyuan Wang", "Yi Wang", "Hongchao Zhou", "Shunbo Hu", "Yi Zhang", "Qian Tao", "Lukas FÃ¶rner", "Thomas Wendler", "Bailiang Jian", "Benedikt Wiestler", "Tim Hable", "Jin Kim", "Dan Ruan", "Frederic Madesta", "Thilo Sentker", "Wiebke Heyer", "Lianrui Zuo", "Yuwei Dai", "Jing Wu", "Jerry L. Prince", "Harrison Bai", "Yong Du", "Yihao Liu", "Alessa Hering", "Reuben Dorent", "Lasse Hansen", "Mattias P. Heinrich", "Aaron Carass"], "title": "Beyond the LUMIR challenge: The pathway to foundational registration models", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image challenges have played a transformative role in advancing the\nfield, catalyzing algorithmic innovation and establishing new performance\nstandards across diverse clinical applications. Image registration, a\nfoundational task in neuroimaging pipelines, has similarly benefited from the\nLearn2Reg initiative. Building on this foundation, we introduce the Large-scale\nUnsupervised Brain MRI Image Registration (LUMIR) challenge, a next-generation\nbenchmark designed to assess and advance unsupervised brain MRI registration.\nDistinct from prior challenges that leveraged anatomical label maps for\nsupervision, LUMIR removes this dependency by providing over 4,000 preprocessed\nT1-weighted brain MRIs for training without any label maps, encouraging\nbiologically plausible deformation modeling through self-supervision. In\naddition to evaluating performance on 590 held-out test subjects, LUMIR\nintroduces a rigorous suite of zero-shot generalization tasks, spanning\nout-of-domain imaging modalities (e.g., FLAIR, T2-weighted, T2*-weighted),\ndisease populations (e.g., Alzheimer's disease), acquisition protocols (e.g.,\n9.4T MRI), and species (e.g., macaque brains). A total of 1,158 subjects and\nover 4,000 image pairs were included for evaluation. Performance was assessed\nusing both segmentation-based metrics (Dice coefficient, 95th percentile\nHausdorff distance) and landmark-based registration accuracy (target\nregistration error). Across both in-domain and zero-shot tasks, deep\nlearning-based methods consistently achieved state-of-the-art accuracy while\nproducing anatomically plausible deformation fields. The top-performing deep\nlearning-based models demonstrated diffeomorphic properties and inverse\nconsistency, outperforming several leading optimization-based methods, and\nshowing strong robustness to most domain shifts, the exception being a drop in\nperformance on out-of-domain contrasts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24646", "pdf": "https://arxiv.org/pdf/2505.24646", "abs": "https://arxiv.org/abs/2505.24646", "authors": ["Yiqun Sun", "Qiang Huang", "Anthony K. H. Tung", "Jun Yu"], "title": "PRISM: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Semantic Text Embedding is a fundamental NLP task that encodes textual\ncontent into vector representations, where proximity in the embedding space\nreflects semantic similarity. While existing embedding models excel at\ncapturing general meaning, they often overlook ideological nuances, limiting\ntheir effectiveness in tasks that require an understanding of political bias.\nTo address this gap, we introduce PRISM, the first framework designed to\nProduce inteRpretable polItical biaS eMbeddings. PRISM operates in two key\nstages: (1) Controversial Topic Bias Indicator Mining, which systematically\nextracts fine-grained political topics and their corresponding bias indicators\nfrom weakly labeled news data, and (2) Cross-Encoder Political Bias Embedding,\nwhich assigns structured bias scores to news articles based on their alignment\nwith these indicators. This approach ensures that embeddings are explicitly\ntied to bias-revealing dimensions, enhancing both interpretability and\npredictive power. Through extensive experiments on two large-scale datasets, we\ndemonstrate that PRISM outperforms state-of-the-art text embedding models in\npolitical bias classification while offering highly interpretable\nrepresentations that facilitate diversified retrieval and ideological analysis.\nThe source code is available at https://github.com/dukesun99/ACL-PRISM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24305", "pdf": "https://arxiv.org/pdf/2505.24305", "abs": "https://arxiv.org/abs/2505.24305", "authors": ["Mingxu Zhang", "Xiaoqi Li", "Jiahui Xu", "Kaichen Zhou", "Hojin Bae", "Yan Shen", "Chuyan Xiong", "Jiaming Liu", "Hao Dong"], "title": "SR3D: Unleashing Single-view 3D Reconstruction for Transparent and Specular Object Grasping", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Recent advancements in 3D robotic manipulation have improved grasping of\neveryday objects, but transparent and specular materials remain challenging due\nto depth sensing limitations. While several 3D reconstruction and depth\ncompletion approaches address these challenges, they suffer from setup\ncomplexity or limited observation information utilization. To address this,\nleveraging the power of single view 3D object reconstruction approaches, we\npropose a training free framework SR3D that enables robotic grasping of\ntransparent and specular objects from a single view observation. Specifically,\ngiven single view RGB and depth images, SR3D first uses the external visual\nmodels to generate 3D reconstructed object mesh based on RGB image. Then, the\nkey idea is to determine the 3D object's pose and scale to accurately localize\nthe reconstructed object back into its original depth corrupted 3D scene.\nTherefore, we propose view matching and keypoint matching mechanisms,which\nleverage both the 2D and 3D's inherent semantic and geometric information in\nthe observation to determine the object's 3D state within the scene, thereby\nreconstructing an accurate 3D depth map for effective grasp detection.\nExperiments in both simulation and real world show the reconstruction\neffectiveness of SR3D.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24680", "pdf": "https://arxiv.org/pdf/2505.24680", "abs": "https://arxiv.org/abs/2505.24680", "authors": ["Xinrui Chen", "Haoli Bai", "Tao Yuan", "Ruikang Liu", "Kang Zhao", "Xianzhi Yu", "Lu Hou", "Tian Guan", "Yonghong He", "Chun Yuan"], "title": "A Simple Linear Patch Revives Layer-Pruned Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Layer pruning has become a popular technique for compressing large language\nmodels (LLMs) due to its simplicity. However, existing layer pruning methods\noften suffer from significant performance drops. We identify that this\ndegradation stems from the mismatch of activation magnitudes across layers and\ntokens at the pruning interface. To address this, we propose LinearPatch, a\nsimple plug-and-play technique to revive the layer-pruned LLMs. The proposed\nmethod adopts Hadamard transformation to suppress massive outliers in\nparticular tokens, and channel-wise scaling to align the activation magnitudes.\nThese operations can be fused into a single matrix, which functions as a patch\nto bridge the pruning interface with negligible inference overhead. LinearPatch\nretains up to 94.15% performance of the original model when pruning 5 layers of\nLLaMA-3-8B on the question answering benchmark, surpassing existing\nstate-of-the-art methods by 4%. In addition, the patch matrix can be further\noptimized with memory efficient offline knowledge distillation. With only 5K\nsamples, the retained performance of LinearPatch can be further boosted to\n95.16% within 30 minutes on a single computing card.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24605", "pdf": "https://arxiv.org/pdf/2505.24605", "abs": "https://arxiv.org/abs/2505.24605", "authors": ["Ivan Pereira-SÃ¡nchez", "Julia Navarro", "Ana BelÃ©n Petro", "Joan Duran"], "title": "Model-Guided Network with Cluster-Based Operators for Spatio-Spectral Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This paper addresses the problem of reconstructing a high-resolution\nhyperspectral image from a low-resolution multispectral observation. While\nspatial super-resolution and spectral super-resolution have been extensively\nstudied, joint spatio-spectral super-resolution remains relatively explored. We\npropose an end-to-end model-driven framework that explicitly decomposes the\njoint spatio-spectral super-resolution problem into spatial super-resolution,\nspectral super-resolution and fusion tasks. Each sub-task is addressed by\nunfolding a variational-based approach, where the operators involved in the\nproximal gradient iterative scheme are replaced with tailored learnable\nmodules. In particular, we design an upsampling operator for spatial\nsuper-resolution based on classical back-projection algorithms, adapted to\nhandle arbitrary scaling factors. Spectral reconstruction is performed using\nlearnable cluster-based upsampling and downsampling operators. For image\nfusion, we integrate low-frequency estimation and high-frequency injection\nmodules to combine the spatial and spectral information from spatial\nsuper-resolution and spectral super-resolution outputs. Additionally, we\nintroduce an efficient nonlocal post-processing step that leverages image\nself-similarity by combining a multi-head attention mechanism with residual\nconnections. Extensive evaluations on several datasets and sampling factors\ndemonstrate the effectiveness of our approach. The source code will be\navailable at https://github.com/TAMI-UIB/JSSUNet", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24713", "pdf": "https://arxiv.org/pdf/2505.24713", "abs": "https://arxiv.org/abs/2505.24713", "authors": ["Badr M. Abdullah", "Matthew Baas", "Bernd MÃ¶bius", "Dietrich Klakow"], "title": "Voice Conversion Improves Cross-Domain Robustness for Spoken Arabic Dialect Identification", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted in Interspeech 2025", "summary": "Arabic dialect identification (ADI) systems are essential for large-scale\ndata collection pipelines that enable the development of inclusive speech\ntechnologies for Arabic language varieties. However, the reliability of current\nADI systems is limited by poor generalization to out-of-domain speech. In this\npaper, we present an effective approach based on voice conversion for training\nADI models that achieves state-of-the-art performance and significantly\nimproves robustness in cross-domain scenarios. Evaluated on a newly collected\nreal-world test set spanning four different domains, our approach yields\nconsistent improvements of up to +34.1% in accuracy across domains.\nFurthermore, we present an analysis of our approach and demonstrate that voice\nconversion helps mitigate the speaker bias in the ADI dataset. We release our\nrobust ADI model and cross-domain evaluation dataset to support the development\nof inclusive speech technologies for Arabic.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24623", "pdf": "https://arxiv.org/pdf/2505.24623", "abs": "https://arxiv.org/abs/2505.24623", "authors": ["Wenyuan Li", "Guang Li", "Keisuke Maeda", "Takahiro Ogawa", "Miki Haseyama"], "title": "Hyperbolic Dataset Distillation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "To address the computational and storage challenges posed by large-scale\ndatasets in deep learning, dataset distillation has been proposed to synthesize\na compact dataset that replaces the original while maintaining comparable model\nperformance. Unlike optimization-based approaches that require costly bi-level\noptimization, distribution matching (DM) methods improve efficiency by aligning\nthe distributions of synthetic and original data, thereby eliminating nested\noptimization. DM achieves high computational efficiency and has emerged as a\npromising solution. However, existing DM methods, constrained to Euclidean\nspace, treat data as independent and identically distributed points,\noverlooking complex geometric and hierarchical relationships. To overcome this\nlimitation, we propose a novel hyperbolic dataset distillation method, termed\nHDD. Hyperbolic space, characterized by negative curvature and exponential\nvolume growth with distance, naturally models hierarchical and tree-like\nstructures. HDD embeds features extracted by a shallow network into the Lorentz\nhyperbolic space, where the discrepancy between synthetic and original data is\nmeasured by the hyperbolic (geodesic) distance between their centroids. By\noptimizing this distance, the hierarchical structure is explicitly integrated\ninto the distillation process, guiding synthetic samples to gravitate towards\nthe root-centric regions of the original data distribution while preserving\ntheir underlying geometric characteristics. Furthermore, we find that pruning\nin hyperbolic space requires only 20% of the distilled core set to retain model\nperformance, while significantly improving training stability. Notably, HDD is\nseamlessly compatible with most existing DM methods, and extensive experiments\non different datasets validate its effectiveness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24654", "pdf": "https://arxiv.org/pdf/2505.24654", "abs": "https://arxiv.org/abs/2505.24654", "authors": ["Maria Rafaela Gkeka", "Bowen Sun", "Evgenia Smirni", "Christos D. Antonopoulos", "Spyros Lalis", "Nikolaos Bellas"], "title": "Black-box Adversarial Attacks on CNN-based SLAM Algorithms", "categories": ["cs.RO", "cs.CV", "68T40, 68T45, 68M25,"], "comment": "9 pages, 8 figures", "summary": "Continuous advancements in deep learning have led to significant progress in\nfeature detection, resulting in enhanced accuracy in tasks like Simultaneous\nLocalization and Mapping (SLAM). Nevertheless, the vulnerability of deep neural\nnetworks to adversarial attacks remains a challenge for their reliable\ndeployment in applications, such as navigation of autonomous agents. Even\nthough CNN-based SLAM algorithms are a growing area of research there is a\nnotable absence of a comprehensive presentation and examination of adversarial\nattacks targeting CNN-based feature detectors, as part of a SLAM system. Our\nwork introduces black-box adversarial perturbations applied to the RGB images\nfed into the GCN-SLAM algorithm. Our findings on the TUM dataset [30] reveal\nthat even attacks of moderate scale can lead to tracking failure in as many as\n76% of the frames. Moreover, our experiments highlight the catastrophic impact\nof attacking depth instead of RGB input images on the SLAM system.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24754", "pdf": "https://arxiv.org/pdf/2505.24754", "abs": "https://arxiv.org/abs/2505.24754", "authors": ["Yingchaojie Feng", "Yiqun Sun", "Yandong Sun", "Minfeng Zhu", "Qiang Huang", "Anthony K. H. Tung", "Wei Chen"], "title": "Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to ACL 2025", "summary": "In this work, we investigate an important task named instruction-following\ntext embedding, which generates dynamic text embeddings that adapt to user\ninstructions, highlighting specific attributes of text. Despite recent\nadvancements, existing approaches suffer from significant computational\noverhead, as they require re-encoding the entire corpus for each new\ninstruction. To address this challenge, we propose GSTransform, a novel\ninstruction-following text embedding framework based on Guided Space\nTransformation. Our key observation is that instruction-relevant information is\ninherently encoded in generic embeddings but remains underutilized. Instead of\nrepeatedly encoding the corpus for each instruction, GSTransform is a\nlightweight transformation mechanism that adapts pre-computed embeddings in\nreal time to align with user instructions, guided by a small amount of text\ndata with instruction-focused label annotation. We conduct extensive\nexperiments on three instruction-awareness downstream tasks across nine\nreal-world datasets, demonstrating that GSTransform improves\ninstruction-following text embedding quality over state-of-the-art methods\nwhile achieving dramatic speedups of 6~300x in real-time processing on\nlarge-scale datasets. The source code is available at\nhttps://github.com/YingchaojieFeng/GSTransform.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24819", "pdf": "https://arxiv.org/pdf/2505.24819", "abs": "https://arxiv.org/abs/2505.24819", "authors": ["Haozhan Tang", "Tianyi Zhang", "Matthew Johnson-Roberson", "Weiming Zhi"], "title": "Bi-Manual Joint Camera Calibration and Scene Representation", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Robot manipulation, especially bimanual manipulation, often requires setting\nup multiple cameras on multiple robot manipulators. Before robot manipulators\ncan generate motion or even build representations of their environments, the\ncameras rigidly mounted to the robot need to be calibrated. Camera calibration\nis a cumbersome process involving collecting a set of images, with each\ncapturing a pre-determined marker. In this work, we introduce the Bi-Manual\nJoint Calibration and Representation Framework (Bi-JCR). Bi-JCR enables\nmultiple robot manipulators, each with cameras mounted, to circumvent taking\nimages of calibration markers. By leveraging 3D foundation models for dense,\nmarker-free multi-view correspondence, Bi-JCR jointly estimates: (i) the\nextrinsic transformation from each camera to its end-effector, (ii) the\ninter-arm relative poses between manipulators, and (iii) a unified,\nscale-consistent 3D representation of the shared workspace, all from the same\ncaptured RGB image sets. The representation, jointly constructed from images\ncaptured by cameras on both manipulators, lives in a common coordinate frame\nand supports collision checking and semantic segmentation to facilitate\ndownstream bimanual coordination tasks. We empirically evaluate the robustness\nof Bi-JCR on a variety of tabletop environments, and demonstrate its\napplicability on a variety of downstream tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24878", "pdf": "https://arxiv.org/pdf/2505.24878", "abs": "https://arxiv.org/abs/2505.24878", "authors": ["Yaxin Luo", "Zhaoyi Li", "Jiacheng Liu", "Jiacheng Cui", "Xiaohan Zhao", "Zhiqiang Shen"], "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Code at: https://github.com/MetaAgentX/OpenCaptchaWorld", "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24832", "pdf": "https://arxiv.org/pdf/2505.24832", "abs": "https://arxiv.org/abs/2505.24832", "authors": ["John X. Morris", "Chawin Sitawarin", "Chuan Guo", "Narine Kokhlikyan", "G. Edward Suh", "Alexander M. Rush", "Kamalika Chaudhuri", "Saeed Mahloujifar"], "title": "How much do language models memorize?", "categories": ["cs.CL"], "comment": null, "summary": "We propose a new method for estimating how much a model ``knows'' about a\ndatapoint and use it to measure the capacity of modern language models. Prior\nstudies of language model memorization have struggled to disentangle\nmemorization from generalization. We formally separate memorization into two\ncomponents: \\textit{unintended memorization}, the information a model contains\nabout a specific dataset, and \\textit{generalization}, the information a model\ncontains about the true data-generation process. When we completely eliminate\ngeneralization, we can compute the total memorization, which provides an\nestimate of model capacity: our measurements estimate that GPT-style models\nhave a capacity of approximately 3.6 bits per parameter. We train language\nmodels on datasets of increasing size and observe that models memorize until\ntheir capacity fills, at which point ``grokking'' begins, and unintended\nmemorization decreases as models begin to generalize. We train hundreds of\ntransformer language models ranging from $500K$ to $1.5B$ parameters and\nproduce a series of scaling laws relating model capacity and data size to\nmembership inference.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24834", "pdf": "https://arxiv.org/pdf/2505.24834", "abs": "https://arxiv.org/abs/2505.24834", "authors": ["Roksana Goworek", "Haim Dubossarsky"], "title": "Multilinguality Does not Make Sense: Investigating Factors Behind Zero-Shot Transfer in Sense-Aware Tasks", "categories": ["cs.CL"], "comment": "8 pages, 8 figures", "summary": "Cross-lingual transfer allows models to perform tasks in languages unseen\nduring training and is often assumed to benefit from increased multilinguality.\nIn this work, we challenge this assumption in the context of two underexplored,\nsense-aware tasks: polysemy disambiguation and lexical semantic change. Through\na large-scale analysis across 28 languages, we show that multilingual training\nis neither necessary nor inherently beneficial for effective transfer. Instead,\nwe find that confounding factors - such as fine-tuning data composition and\nevaluation artifacts - better account for the perceived advantages of\nmultilinguality. Our findings call for more rigorous evaluations in\nmultilingual NLP. We release fine-tuned models and benchmarks to support\nfurther research, with implications extending to low-resource and typologically\ndiverse languages.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24864", "pdf": "https://arxiv.org/pdf/2505.24864", "abs": "https://arxiv.org/abs/2505.24864", "authors": ["Mingjie Liu", "Shizhe Diao", "Ximing Lu", "Jian Hu", "Xin Dong", "Yejin Choi", "Jan Kautz", "Yi Dong"], "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "26 pages, 17 figures", "summary": "Recent advances in reasoning-centric language models have highlighted\nreinforcement learning (RL) as a promising method for aligning models with\nverifiable rewards. However, it remains contentious whether RL truly expands a\nmodel's reasoning capabilities or merely amplifies high-reward outputs already\nlatent in the base model's distribution, and whether continually scaling up RL\ncompute reliably leads to improved reasoning performance. In this work, we\nchallenge prevailing assumptions by demonstrating that prolonged RL (ProRL)\ntraining can uncover novel reasoning strategies that are inaccessible to base\nmodels, even under extensive sampling. We introduce ProRL, a novel training\nmethodology that incorporates KL divergence control, reference policy\nresetting, and a diverse suite of tasks. Our empirical analysis reveals that\nRL-trained models consistently outperform base models across a wide range of\npass@k evaluations, including scenarios where base models fail entirely\nregardless of the number of attempts. We further show that reasoning boundary\nimprovements correlates strongly with task competence of base model and\ntraining duration, suggesting that RL can explore and populate new regions of\nsolution space over time. These findings offer new insights into the conditions\nunder which RL meaningfully expands reasoning boundaries in language models and\nestablish a foundation for future work on long-horizon RL for reasoning. We\nrelease model weights to support further research:\nhttps://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23841", "pdf": "https://arxiv.org/pdf/2505.23841", "abs": "https://arxiv.org/abs/2505.23841", "authors": ["Hairu Wang", "Yuan Feng", "Yukun Cao", "Xike Xie", "S Kevin Zhou"], "title": "SkewRoute: Training-Free LLM Routing for Knowledge Graph Retrieval-Augmented Generation via Score Skewness of Retrieved Context", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large language models excel at many tasks but often incur high inference\ncosts during deployment. To mitigate hallucination, many systems use a\nknowledge graph to enhance retrieval-augmented generation (KG-RAG). However,\nthe large amount of retrieved knowledge contexts increase these inference costs\nfurther. A promising solution to balance performance and cost is LLM routing,\nwhich directs simple queries to smaller LLMs and complex ones to larger LLMs.\nHowever, no dedicated routing methods currently exist for RAG, and existing\ntraining-based routers face challenges scaling to this domain due to the need\nfor extensive training data. We observe that the score distributions produced\nby the retrieval scorer strongly correlate with query difficulty. Based on\nthis, we propose a novel, training-free routing framework, the first tailored\nto KG-RAG that effectively balances performance and cost in a plug-and-play\nmanner. Experiments show our method reduces calls to larger LLMs by up to 50%\nwithout sacrificing response quality, demonstrating its potential for efficient\nand scalable LLM deployment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23922", "pdf": "https://arxiv.org/pdf/2505.23922", "abs": "https://arxiv.org/abs/2505.23922", "authors": ["David Ma", "Huaqing Yuan", "Xingjian Wang", "Qianbo Zang", "Tianci Liu", "Xinyang He", "Yanbin Wei", "Jiawei Guo", "Ni Jiahui", "Zhenzhu Yang", "Meng Cao", "Shanghaoran Quan", "Yizhi Li", "Wangchunshu Zhou", "Jiaheng Liu", "Wenhao Huang", "Ge Zhang", "Shiwen Ni", "Xiaojie Jin"], "title": "ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Although long-video understanding demands that models capture hierarchical\ntemporal information -- from clip (seconds) and shot (tens of seconds) to event\n(minutes) and story (hours) -- existing benchmarks either neglect this\nmulti-scale design or scatter scale-specific questions across different videos,\npreventing direct comparison of model performance across timescales on the same\ncontent. To address this, we introduce ScaleLong, the first benchmark to\ndisentangle these factors by embedding questions targeting four hierarchical\ntimescales -- clip (seconds), shot (tens of seconds), event (minutes), and\nstory (hours) -- all within the same video content. This within-content\nmulti-timescale questioning design enables direct comparison of model\nperformance across timescales on identical videos. ScaleLong features 269 long\nvideos (avg.\\ 86\\,min) from 5 main categories and 36 sub-categories, with 4--8\ncarefully designed questions, including at least one question for each\ntimescale. Evaluating 23 MLLMs reveals a U-shaped performance curve, with\nhigher accuracy at the shortest and longest timescales and a dip at\nintermediate levels. Furthermore, ablation studies show that increased visual\ntoken capacity consistently enhances reasoning across all timescales. ScaleLong\noffers a fine-grained, multi-timescale benchmark for advancing MLLM\ncapabilities in long-video understanding. The code and dataset are available\nhttps://github.com/multimodal-art-projection/ScaleLong.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.23960", "pdf": "https://arxiv.org/pdf/2505.23960", "abs": "https://arxiv.org/abs/2505.23960", "authors": ["Henry Conklin"], "title": "Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "PhD Thesis, 204 pages; entropy estimation discussed from p.94", "summary": "Despite the remarkable success of large large-scale neural networks, we still\nlack unified notation for thinking about and describing their representational\nspaces. We lack methods to reliably describe how their representations are\nstructured, how that structure emerges over training, and what kinds of\nstructures are desirable. This thesis introduces quantitative methods for\nidentifying systematic structure in a mapping between spaces, and leverages\nthem to understand how deep-learning models learn to represent information,\nwhat representational structures drive generalisation, and how design decisions\ncondition the structures that emerge. To do this I identify structural\nprimitives present in a mapping, along with information theoretic\nquantifications of each. These allow us to analyse learning, structure, and\ngeneralisation across multi-agent reinforcement learning models,\nsequence-to-sequence models trained on a single task, and Large Language\nModels. I also introduce a novel, performant, approach to estimating the\nentropy of vector space, that allows this analysis to be applied to models\nranging in size from 1 million to 12 billion parameters.\n  The experiments here work to shed light on how large-scale distributed models\nof cognition learn, while allowing us to draw parallels between those systems\nand their human analogs. They show how the structures of language and the\nconstraints that give rise to them in many ways parallel the kinds of\nstructures that drive performance of contemporary neural networks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24292", "pdf": "https://arxiv.org/pdf/2505.24292", "abs": "https://arxiv.org/abs/2505.24292", "authors": ["Yueqi Zhang", "Peiwen Yuan", "Shaoxiong Feng", "Yiwei Li", "Xinglin Wang", "Jiayi Shi", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "title": "Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Human-AI conversation frequently relies on quoting earlier text-\"check it\nwith the formula I just highlighted\"-yet today's large language models (LLMs)\nlack an explicit mechanism for locating and exploiting such spans. We formalise\nthe challenge as span-conditioned generation, decomposing each turn into the\ndialogue history, a set of token-offset quotation spans, and an intent\nutterance. Building on this abstraction, we introduce a quotation-centric data\npipeline that automatically synthesises task-specific dialogues, verifies\nanswer correctness through multi-stage consistency checks, and yields both a\nheterogeneous training corpus and the first benchmark covering five\nrepresentative scenarios. To meet the benchmark's zero-overhead and\nparameter-efficiency requirements, we propose QuAda, a lightweight\ntraining-based method that attaches two bottleneck projections to every\nattention head, dynamically amplifying or suppressing attention to quoted spans\nat inference time while leaving the prompt unchanged and updating < 2.8% of\nbackbone weights. Experiments across models show that QuAda is suitable for all\nscenarios and generalises to unseen topics, offering an effective,\nplug-and-play solution for quotation-aware dialogue.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "dialogue"], "score": 3}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24324", "pdf": "https://arxiv.org/pdf/2505.24324", "abs": "https://arxiv.org/abs/2505.24324", "authors": ["Ivan Petrukha", "Yana Kurliak", "Nataliia Stulova"], "title": "SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation", "categories": ["cs.LG", "cs.CL", "cs.PL", "cs.SE"], "comment": "Accepted to FORGE'25 Benchmarking on 15.01.2025, to be published by\n  IEEE under the CC BY-NC-ND 4.0 license. This is the accepted version of the\n  article (5 pages, 2 figures, 1 table). DOI will be added upon publication", "summary": "In recent years, large language models (LLMs) have showcased significant\nadvancements in code generation. However, most evaluation benchmarks are\nprimarily oriented towards Python, making it difficult to evaluate other\nprogramming languages, such as Swift, with high quality. By examining widely\nestablished multilingual benchmarks like HumanEval-XL and MultiPL-E, we\nidentified critical issues specific to their Swift components, making them\ninsufficient or even irrelevant for assessing LLM coding capabilities on Swift.\nUnlike these existing approaches, which prioritize rapid scaling and\ngeneralization by automatically translating Python-centric benchmarks with\nLLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the\nfirst Swift-oriented benchmark consisting of 28 carefully hand-crafted\nproblems, and evaluate 44 popular Code LLMs on it. Our results show significant\nLLM scores drop for problems requiring language-specific features, most\nnoticeable in the models of smaller sizes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "code generation"], "score": 3}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24535", "pdf": "https://arxiv.org/pdf/2505.24535", "abs": "https://arxiv.org/abs/2505.24535", "authors": ["Narmeen Oozeer", "Luke Marks", "Fazl Barez", "Amirali Abdullah"], "title": "Beyond Linear Steering: Unified Multi-Attribute Control for Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Controlling multiple behavioral attributes in large language models (LLMs) at\ninference time is a challenging problem due to interference between attributes\nand the limitations of linear steering methods, which assume additive behavior\nin activation space and require per-attribute tuning. We introduce K-Steering,\na unified and flexible approach that trains a single non-linear multi-label\nclassifier on hidden activations and computes intervention directions via\ngradients at inference time. This avoids linearity assumptions, removes the\nneed for storing and tuning separate attribute vectors, and allows dynamic\ncomposition of behaviors without retraining. To evaluate our method, we propose\ntwo new benchmarks, ToneBank and DebateMix, targeting compositional behavioral\ncontrol. Empirical results across 3 model families, validated by both\nactivation-based classifiers and LLM-based judges, demonstrate that K-Steering\noutperforms strong baselines in accurately steering multiple behaviors.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24846", "pdf": "https://arxiv.org/pdf/2505.24846", "abs": "https://arxiv.org/abs/2505.24846", "authors": ["Jingyan Shen", "Jiarui Yao", "Rui Yang", "Yifan Sun", "Feng Luo", "Rui Pan", "Tong Zhang", "Han Zhao"], "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reward function", "RLHF", "reinforcement learning from human feedback", "human feedback", "preference learning", "reinforcement learning", "preference", "alignment"], "score": 9}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24850", "pdf": "https://arxiv.org/pdf/2505.24850", "abs": "https://arxiv.org/abs/2505.24850", "authors": ["Shuyao Xu", "Cheng Peng", "Jiangxuan Long", "Weidi Xu", "Wei Chu", "Yuan Qi"], "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": "27 pages, 10 figures. Code available at\n  https://github.com/Tim-Siu/reinforcement-distillation", "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["DPO"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "mathematical reasoning"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24859", "pdf": "https://arxiv.org/pdf/2505.24859", "abs": "https://arxiv.org/abs/2505.24859", "authors": ["Joschka Braun", "Carsten Eickhoff", "Seyed Ali Bahrainian"], "title": "Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization", "categories": ["cs.LG", "cs.CL"], "comment": "29 pages, 21 figures, preprint", "summary": "Steering vectors are a lightweight method for controlling text properties by\nadding a learned bias to language model activations at inference time. So far,\nsteering vectors have predominantly been evaluated in multiple-choice settings,\nwhile their effectiveness in free-form generation tasks remains understudied.\nMoving \"Beyond Multiple Choice,\" we thoroughly evaluate the effectiveness of\nsteering vectors in adaptively controlling topical focus, sentiment, toxicity,\nand readability in abstractive summaries of the NEWTS dataset. We find that\nsteering effectively controls the targeted summary properties, but high\nsteering strengths consistently degrade both intrinsic and extrinsic text\nquality. Compared to steering, prompting offers weaker control, while\npreserving text quality. Combining steering and prompting yields the strongest\ncontrol over text properties and offers the most favorable efficacy-quality\ntrade-off at moderate steering strengths. Our results underscore the practical\ntrade-off between control strength and text quality preservation when applying\nsteering vectors to free-form generation tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "summarization"], "score": 2}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24876", "pdf": "https://arxiv.org/pdf/2505.24876", "abs": "https://arxiv.org/abs/2505.24876", "authors": ["Tajamul Ashraf", "Amal Saqib", "Hanan Ghani", "Muhra AlMahri", "Yuhao Li", "Noor Ahsan", "Umair Nawaz", "Jean Lahoud", "Hisham Cholakkal", "Mubarak Shah", "Philip Torr", "Fahad Shahbaz Khan", "Rao Muhammad Anwer", "Salman Khan"], "title": "Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Deep reasoning is fundamental for solving complex tasks, especially in\nvision-centric scenarios that demand sequential, multimodal understanding.\nHowever, existing benchmarks typically evaluate agents with fully synthetic,\nsingle-turn queries, limited visual modalities, and lack a framework to assess\nreasoning quality over multiple steps as required in real-world settings. To\naddress this, we introduce Agent-X, a large-scale benchmark for evaluating\nvision-centric agents multi-step and deep reasoning capabilities in real-world,\nmultimodal settings. Agent- X features 828 agentic tasks with authentic visual\ncontexts, including images, multi-image comparisons, videos, and instructional\ntext. These tasks span six major agentic environments: general visual\nreasoning, web browsing, security and surveillance, autonomous driving, sports,\nand math reasoning. Our benchmark requires agents to integrate tool use with\nexplicit, stepwise decision-making in these diverse settings. In addition, we\npropose a fine-grained, step-level evaluation framework that assesses the\ncorrectness and logical coherence of each reasoning step and the effectiveness\nof tool usage throughout the task. Our results reveal that even the\nbest-performing models, including GPT, Gemini, and Qwen families, struggle to\nsolve multi-step vision tasks, achieving less than 50% full-chain success.\nThese findings highlight key bottlenecks in current LMM reasoning and tool-use\ncapabilities and identify future research directions in vision-centric agentic\nreasoning models. Our data and code are publicly available at\nhttps://github.com/mbzuai-oryx/Agent-X", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-06-02.jsonl"}
{"id": "2505.24878", "pdf": "https://arxiv.org/pdf/2505.24878", "abs": "https://arxiv.org/abs/2505.24878", "authors": ["Yaxin Luo", "Zhaoyi Li", "Jiacheng Liu", "Jiacheng Cui", "Xiaohan Zhao", "Zhiqiang Shen"], "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Code at: https://github.com/MetaAgentX/OpenCaptchaWorld", "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-02.jsonl"}
