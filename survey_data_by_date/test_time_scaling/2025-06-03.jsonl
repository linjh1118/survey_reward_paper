{"id": "2506.00027", "pdf": "https://arxiv.org/pdf/2506.00027", "abs": "https://arxiv.org/abs/2506.00027", "authors": ["Zhengyu Chen", "Yudong Wang", "Teng Xiao", "Ruochen Zhou", "Xuesheng Yang", "Wei Wang", "Zhifang Sui", "Jingang Wang"], "title": "From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in improving the reasoning capabilities of Large Language\nModels have underscored the efficacy of Process Reward Models (PRMs) in\naddressing intermediate errors through structured feedback mechanisms. This\nstudy analyzes PRMs from multiple perspectives, including training\nmethodologies, scalability, and generalization capabilities. We investigate the\ninterplay between pre-training and reward model training FLOPs to assess their\ninfluence on PRM efficiency and accuracy in complex reasoning tasks. Our\nanalysis reveals a pattern of diminishing returns in performance with\nincreasing PRM scale, highlighting the importance of balancing model size and\ncomputational cost. Furthermore, the diversity of training datasets\nsignificantly impacts PRM performance, emphasizing the importance of diverse\ndata to enhance both accuracy and efficiency. We further examine test-time\nscaling strategies, identifying Monte Carlo Tree Search as the most effective\nmethod when computational resources are abundant, while Best-of-N Sampling\nserves as a practical alternative under resource-limited conditions. Notably,\nour findings indicate that PRMs trained on mathematical datasets exhibit\nperformance comparable to those tailored for code generation, suggesting robust\ncross-domain generalization. Employing a gradient-based metric, we observe that\nPRMs exhibit a preference for selecting responses with similar underlying\npatterns, further informing their optimization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale", "monte carlo tree search"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "code generation", "mathematical reasoning"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01413", "pdf": "https://arxiv.org/pdf/2506.01413", "abs": "https://arxiv.org/abs/2506.01413", "authors": ["Yulei Qin", "Gang Li", "Zongyi Li", "Zihan Xu", "Yuchen Shi", "Zhekai Lin", "Xiao Cui", "Ke Li", "Xing Sun"], "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages of main body, 3 tables, 5 figures, 40 pages of appendix", "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nare available at https://github.com/yuleiqin/RAIF.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "compute scaling", "test-time compute"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01381", "pdf": "https://arxiv.org/pdf/2506.01381", "abs": "https://arxiv.org/abs/2506.01381", "authors": ["Yilong Lai", "Jialong Wu", "Zhenglin Wang", "Deyu Zhou"], "title": "AdaRewriter: Unleashing the Power of Prompting-based Conversational Query Reformulation via Test-Time Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "Prompting-based conversational query reformulation has emerged as a powerful\napproach for conversational search, refining ambiguous user queries into\nstandalone search queries. Best-of-N reformulation over the generated\ncandidates via prompting shows impressive potential scaling capability.\nHowever, both the previous tuning methods (training time) and adaptation\napproaches (test time) can not fully unleash their benefits. In this paper, we\npropose AdaRewriter, a novel framework for query reformulation using an\noutcome-supervised reward model via test-time adaptation. By training a\nlightweight reward model with contrastive ranking loss, AdaRewriter selects the\nmost promising reformulation during inference. Notably, it can operate\neffectively in black-box systems, including commercial LLM APIs. Experiments on\nfive conversational search datasets show that AdaRewriter significantly\noutperforms the existing methods across most settings, demonstrating the\npotential of test-time adaptation for conversational query reformulation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time", "scaling", "test-time adaptation"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "ranking"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00189", "pdf": "https://arxiv.org/pdf/2506.00189", "abs": "https://arxiv.org/abs/2506.00189", "authors": ["Di Zhang", "Weida Wang", "Junxian Li", "Xunzhi Wang", "Jiatong Li", "Jianbo Wu", "Jingdi Lei", "Haonan He", "Peng Ye", "Shufei Zhang", "Wanli Ouyang", "Yuqiang Li", "Dongzhan Zhou"], "title": "Control-R: Towards controllable test-time scaling", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "This paper target in addressing the challenges of underthinking and\noverthinking in long chain-of-thought (CoT) reasoning for Large Reasoning\nModels (LRMs) by introducing Reasoning Control Fields (RCF)--a novel test-time\napproach that injects structured control signals to guide reasoning from a tree\nsearch perspective. RCF enables models to adjust reasoning effort according to\ngiven control conditions when solving complex tasks. Additionally, we present\nthe Control-R-4K dataset, which consists of challenging problems annotated with\ndetailed reasoning processes and corresponding control fields. To further\nenhance reasoning control, we propose a Conditional Distillation Finetuning\n(CDF) method, which trains model--particularly Control-R-32B--to effectively\nadjust reasoning effort during test time. Experimental results on benchmarks\nsuch as AIME2024 and MATH500 demonstrate that our approach achieves\nstate-of-the-art performance at the 32B scale while enabling a controllable\nLong CoT reasoning process (L-CoT). Overall, this work introduces an effective\nparadigm for controllable test-time scaling reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time", "scaling", "scale"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01413", "pdf": "https://arxiv.org/pdf/2506.01413", "abs": "https://arxiv.org/abs/2506.01413", "authors": ["Yulei Qin", "Gang Li", "Zongyi Li", "Zihan Xu", "Yuchen Shi", "Zhekai Lin", "Xiao Cui", "Ke Li", "Xing Sun"], "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages of main body, 3 tables, 5 figures, 40 pages of appendix", "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nare available at https://github.com/yuleiqin/RAIF.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "compute scaling", "test-time compute"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00344", "pdf": "https://arxiv.org/pdf/2506.00344", "abs": "https://arxiv.org/abs/2506.00344", "authors": ["Sungjae Lee", "Hoyoung Kim", "Jeongyeon Hwang", "Eunhyeok Park", "Jungseul Ok"], "title": "Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling test-time computation--generating and analyzing multiple or\nsequential outputs for a single input--has become a promising strategy for\nimproving the reliability and quality of large language models (LLMs), as\nevidenced by advances in uncertainty quantification and multi-step reasoning. A\nkey shared component is semantic clustering, which groups outputs that differ\nin form but convey the same meaning. Semantic clustering enables estimation of\nthe distribution over the semantics of outputs and helps avoid redundant\nexploration of reasoning paths. However, existing approaches typically rely on\nexternal models, which introduce substantial computational overhead and often\nfail to capture context-aware semantics. We propose Latent Semantic Clustering\n(LSC), a lightweight and context-sensitive method that leverages the generator\nLLM's internal hidden states for clustering, eliminating the need for external\nmodels. Our extensive experiment across various LLMs and datasets shows that\nLSC significantly improves the computational efficiency of test-time scaling\nwhile maintaining or exceeding the performance of existing methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "multi-step reasoning"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01863", "pdf": "https://arxiv.org/pdf/2506.01863", "abs": "https://arxiv.org/abs/2506.01863", "authors": ["Andrei Panferov", "Alexandra Volkova", "Ionut-Vlad Modoranu", "Vage Egiazarian", "Mher Safaryan", "Dan Alistarh"], "title": "Unified Scaling Laws for Compressed Representations", "categories": ["cs.LG", "cs.CL"], "comment": "Preprint", "summary": "Scaling laws have shaped recent advances in machine learning by enabling\npredictable scaling of model performance based on model size, computation, and\ndata volume. Concurrently, the rise in computational cost for AI has motivated\nmodel compression techniques, notably quantization and sparsification, which\nhave emerged to mitigate the steep computational demands associated with\nlarge-scale training and inference. This paper investigates the interplay\nbetween scaling laws and compression formats, exploring whether a unified\nscaling framework can accurately predict model performance when training occurs\nover various compressed representations, such as sparse, scalar-quantized,\nsparse-quantized or even vector-quantized formats. Our key contributions\ninclude validating a general scaling law formulation and showing that it is\napplicable both individually but also composably across compression types.\nBased on this, our main finding is demonstrating both theoretically and\nempirically that there exists a simple \"capacity\" metric -- based on the\nrepresentation's ability to fit random Gaussian data -- which can robustly\npredict parameter efficiency across multiple compressed representations. On the\npractical side, we extend our formulation to directly compare the accuracy\npotential of different compressed formats, and to derive better algorithms for\ntraining over sparse-quantized formats.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale", "scaling law"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00333", "pdf": "https://arxiv.org/pdf/2506.00333", "abs": "https://arxiv.org/abs/2506.00333", "authors": ["Mingxuan Liu", "Tyler L. Hayes", "Massimiliano Mancini", "Elisa Ricci", "Riccardo Volpi", "Gabriela Csurka"], "title": "Test-time Vocabulary Adaptation for Language-driven Object Detection", "categories": ["cs.CV"], "comment": "Accepted as a conference paper at ICIP 2025", "summary": "Open-vocabulary object detection models allow users to freely specify a class\nvocabulary in natural language at test time, guiding the detection of desired\nobjects. However, vocabularies can be overly broad or even mis-specified,\nhampering the overall performance of the detector. In this work, we propose a\nplug-and-play Vocabulary Adapter (VocAda) to refine the user-defined\nvocabulary, automatically tailoring it to categories that are relevant for a\ngiven image. VocAda does not require any training, it operates at inference\ntime in three steps: i) it uses an image captionner to describe visible\nobjects, ii) it parses nouns from those captions, and iii) it selects relevant\nclasses from the user-defined vocabulary, discarding irrelevant ones.\nExperiments on COCO and Objects365 with three state-of-the-art detectors show\nthat VocAda consistently improves performance, proving its versatility. The\ncode is open source.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00475", "pdf": "https://arxiv.org/pdf/2506.00475", "abs": "https://arxiv.org/abs/2506.00475", "authors": ["Wei Tao", "Xiaoyang Qu", "Kai Lu", "Jiguang Wan", "Shenglin He", "Jianzong Wang"], "title": "BAGNet: A Boundary-Aware Graph Attention Network for 3D Point Cloud Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted by the 2025 International Joint Conference on Neural\n  Networks (IJCNN 2025)", "summary": "Since the point cloud data is inherently irregular and unstructured, point\ncloud semantic segmentation has always been a challenging task. The graph-based\nmethod attempts to model the irregular point cloud by representing it as a\ngraph; however, this approach incurs substantial computational cost due to the\nnecessity of constructing a graph for every point within a large-scale point\ncloud. In this paper, we observe that boundary points possess more intricate\nspatial structural information and develop a novel graph attention network\nknown as the Boundary-Aware Graph attention Network (BAGNet). On one hand,\nBAGNet contains a boundary-aware graph attention layer (BAGLayer), which\nemploys edge vertex fusion and attention coefficients to capture features of\nboundary points, reducing the computation time. On the other hand, BAGNet\nemploys a lightweight attention pooling layer to extract the global feature of\nthe point cloud to maintain model accuracy. Extensive experiments on standard\ndatasets demonstrate that BAGNet outperforms state-of-the-art methods in point\ncloud semantic segmentation with higher accuracy and less inference time.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00523", "pdf": "https://arxiv.org/pdf/2506.00523", "abs": "https://arxiv.org/abs/2506.00523", "authors": ["Xingtong Ge", "Xin Zhang", "Tongda Xu", "Yi Zhang", "Xinjie Zhang", "Yan Wang", "Jun Zhang"], "title": "SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation", "categories": ["cs.CV"], "comment": "under review", "summary": "The Distribution Matching Distillation (DMD) has been successfully applied to\ntext-to-image diffusion models such as Stable Diffusion (SD) 1.5. However,\nvanilla DMD suffers from convergence difficulties on large-scale flow-based\ntext-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze\nthe issues when applying vanilla DMD on large-scale models. Then, to overcome\nthe scalability challenge, we propose implicit distribution alignment (IDA) to\nregularize the distance between the generator and fake distribution.\nFurthermore, we propose intra-segment guidance (ISG) to relocate the timestep\nimportance distribution from the teacher model. With IDA alone, DMD converges\nfor SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1\ndev. Along with other improvements such as scaled up discriminator models, our\nfinal model, dubbed \\textbf{SenseFlow}, achieves superior performance in\ndistillation for both diffusion based text-to-image models such as SDXL, and\nflow-matching models such as SD 3.5 Large and FLUX. The source code will be\navaliable at https://github.com/XingtongGe/SenseFlow.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00290", "pdf": "https://arxiv.org/pdf/2506.00290", "abs": "https://arxiv.org/abs/2506.00290", "authors": ["Tianqi Chen", "Shujian Zhang", "Mingyuan Zhou"], "title": "DLM-One: Diffusion Language Models for One-Step Sequence Generation", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "This paper introduces DLM-One, a score-distillation-based framework for\none-step sequence generation with continuous diffusion language models (DLMs).\nDLM-One eliminates the need for iterative refinement by aligning the scores of\na student model's outputs in the continuous token embedding space with the\nscore function of a pretrained teacher DLM. We investigate whether DLM-One can\nachieve substantial gains in sampling efficiency for language modeling. Through\ncomprehensive experiments on DiffuSeq -- a representative continuous DLM -- we\nshow that DLM-One achieves up to ~500x speedup in inference time while\nmaintaining competitive performance on benchmark text generation tasks used to\nevaluate the teacher models. We further analyze the method's empirical behavior\nacross multiple datasets, providing initial insights into its generality and\npractical applicability. Our findings position one-step diffusion as a\npromising direction for efficient, high-quality language generation and broader\nadoption of continuous diffusion models operating in embedding space for\nnatural language processing.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "iterative refinement"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00338", "pdf": "https://arxiv.org/pdf/2506.00338", "abs": "https://arxiv.org/abs/2506.00338", "authors": ["Yifan Peng", "Shakeel Muhammad", "Yui Sudo", "William Chen", "Jinchuan Tian", "Chyi-Jiunn Lin", "Shinji Watanabe"], "title": "OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "The Open Whisper-style Speech Models (OWSM) project has developed a series of\nfully open speech foundation models using academic-scale resources, but their\ntraining data remains insufficient. This work enhances OWSM by integrating\nYODAS, a large-scale web-crawled dataset with a Creative Commons license.\nHowever, incorporating YODAS is nontrivial due to its wild nature, which\nintroduces challenges such as incorrect language labels and audio-text\nmisalignments. To address this, we develop a scalable data-cleaning pipeline\nusing public toolkits, yielding a dataset with 166,000 hours of speech across\n75 languages. Our new series of OWSM v4 models, trained on this curated dataset\nalongside existing OWSM data, significantly outperform previous versions on\nmultilingual benchmarks. Our models even match or surpass frontier industrial\nmodels like Whisper and MMS in multiple scenarios. We will publicly release the\ncleaned YODAS data, pre-trained models, and all associated scripts via the\nESPnet toolkit.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01109", "pdf": "https://arxiv.org/pdf/2506.01109", "abs": "https://arxiv.org/abs/2506.01109", "authors": ["Fengze Li", "Yangle Liu", "Jieming Ma", "Hai-Ning Liang", "Yaochun Shen", "Huangxiang Li", "Zhijing Wu"], "title": "CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Accurate fruit counting in real-world agricultural environments is a\nlongstanding challenge due to visual occlusions, semantic ambiguity, and the\nhigh computational demands of 3D reconstruction. Existing methods based on\nneural radiance fields suffer from low inference speed, limited generalization,\nand lack support for open-set semantic control. This paper presents\nFruitLangGS, a real-time 3D fruit counting framework that addresses these\nlimitations through spatial reconstruction, semantic embedding, and\nlanguage-guided instance estimation. FruitLangGS first reconstructs\norchard-scale scenes using an adaptive Gaussian splatting pipeline with\nradius-aware pruning and tile-based rasterization for efficient rendering. To\nenable semantic control, each Gaussian encodes a compressed CLIP-aligned\nlanguage embedding, forming a compact and queryable 3D representation. At\ninference time, prompt-based semantic filtering is applied directly in 3D\nspace, without relying on image-space segmentation or view-level fusion. The\nselected Gaussians are then converted into dense point clouds via\ndistribution-aware sampling and clustered to estimate fruit counts.\nExperimental results on real orchard data demonstrate that FruitLangGS achieves\nhigher rendering speed, semantic flexibility, and counting accuracy compared to\nprior approaches, offering a new perspective for language-driven, real-time\nneural rendering across open-world scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01247", "pdf": "https://arxiv.org/pdf/2506.01247", "abs": "https://arxiv.org/abs/2506.01247", "authors": ["Gerasimos Chatzoudis", "Zhuowei Li", "Gemma E. Moran", "Hao Wang", "Dimitris N. Metaxas"], "title": "Visual Sparse Steering: Improving Zero-shot Image Classification with Sparsity Guided Steering Vectors", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Steering vision foundation models at inference time without retraining or\naccess to large labeled datasets is a desirable yet challenging objective,\nparticularly in dynamic or resource-constrained settings. In this paper, we\nintroduce Visual Sparse Steering (VS2), a lightweight, test-time method that\nguides vision models using steering vectors derived from sparse features\nlearned by top-$k$ Sparse Autoencoders without requiring contrastive data.\nSpecifically, VS2 surpasses zero-shot CLIP by 4.12% on CIFAR-100, 1.08% on\nCUB-200, and 1.84% on Tiny-ImageNet. We further propose VS2++, a\nretrieval-augmented variant that selectively amplifies relevant sparse features\nusing pseudo-labeled neighbors at inference time. With oracle positive/negative\nsets, VS2++ achieves absolute top-1 gains over CLIP zero-shot of up to 21.44%\non CIFAR-100, 7.08% on CUB-200, and 20.47% on Tiny-ImageNet. Interestingly, VS2\nand VS2++ raise per-class accuracy by up to 25% and 38%, respectively, showing\nthat sparse steering benefits specific classes by disambiguating visually or\ntaxonomically proximate categories rather than providing a uniform boost.\nFinally, to better align the sparse features learned through the SAE\nreconstruction task with those relevant for downstream performance, we propose\nPrototype-Aligned Sparse Steering (PASS). By incorporating a\nprototype-alignment loss during SAE training, using labels only during training\nwhile remaining fully test-time unsupervised, PASS consistently, though\nmodestly, outperforms VS2, achieving a 6.12% gain over VS2 only on CIFAR-100\nwith ViT-B/32.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference time"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00842", "pdf": "https://arxiv.org/pdf/2506.00842", "abs": "https://arxiv.org/abs/2506.00842", "authors": ["Jiawei Gu", "Ziting Xian", "Yuanzhen Xie", "Ye Liu", "Enjie Liu", "Ruichao Zhong", "Mochi Gao", "Yunzhi Tan", "Bo Hu", "Zang Li"], "title": "Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) achieve strong performance on plain text tasks\nbut underperform on structured data like tables and databases. Potential\nchallenges arise from their underexposure during pre-training and rigid\ntext-to-structure transfer mechanisms. Unlike humans who seamlessly apply\nlearned patterns across data modalities, LLMs struggle to infer implicit\nrelationships embedded in tabular formats, especially in the absence of\nexplicit structural guidance. To bridge this cognitive gap, we introduce\nContrastive Retrieval-Augmented Generation on Experience (CoRE), a framework\nthat builds experience memory representations and enhances generalization\nthrough contrastive In-Context Learning (ICL) to simulate human-like knowledge\ntransfer. Experiments on Text-to-SQL and TableQA show CoRE significantly\nimproves performance, achieving average gains of 3.44% and 4.24%, with up to\n17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated\nExperience Memory expands training data 8-9x, enhancing diversity and domain\ncoverage. This training-free and continual method propels LLMs toward\nstructured knowledge expertise.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search", "MCTS"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01062", "pdf": "https://arxiv.org/pdf/2506.01062", "abs": "https://arxiv.org/abs/2506.01062", "authors": ["Thinh Pham", "Nguyen Nguyen", "Pratibha Zunjare", "Weiyuan Chen", "Yu-Min Tseng", "Tu Vu"], "title": "SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint. 22 pages, 7 figures, 11 tables", "summary": "We introduce SealQA, a new challenge benchmark for evaluating\nSEarch-Augmented Language models on fact-seeking questions where web search\nyields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:\n(1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and\nreasoning capabilities, with Seal-0 focusing on the most challenging questions\nwhere chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3)\nLongSeal, which extends SealQA to test long-context, multi-document reasoning\nin \"needle-in-a-haystack\" settings. Our evaluation reveals critical limitations\nin current models: Even frontier LLMs perform poorly across all SealQA flavors.\nOn Seal-0, frontier agentic models equipped with tools like o3 and o4-mini\nachieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning\nefforts. We find that advanced reasoning models such as DeepSeek-R1-671B and\no3-mini are highly vulnerable to noisy search results. Notably, increasing\ntest-time compute does not yield reliable gains across o3-mini, o4-mini, and\no3, with performance often plateauing or even declining early. Additionally,\nwhile recent models are less affected by the \"lost-in-the-middle\" issue, they\nstill fail to reliably identify relevant documents in LongSeal when faced with\nnumerous distractors. To facilitate future work, we release SealQA at\nhuggingface.co/datasets/vtllms/sealqa.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time compute"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01084", "pdf": "https://arxiv.org/pdf/2506.01084", "abs": "https://arxiv.org/abs/2506.01084", "authors": ["Saibo Geng", "Nathan Ranchin", "Yunzhen yao", "Maxime Peyrard", "Chris Wendler", "Michael Gastpar", "Robert West"], "title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression", "categories": ["cs.CL", "cs.LG"], "comment": "Code will be released at https://github.com/epfl-dlab/zip2zip", "summary": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized for general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a\nframework that enables LLMs to dynamically adjust token vocabulary at inference\ntime, allowing for fewer generated tokens and thus faster inference. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\n(LZW) compression that incrementally compresses tokens into reusable\n\"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for\nnewly formed hypertokens at runtime; and (3) a causal language modeling variant\nthat trains the model to operate on hypertokenized, compressed sequences. We\nshow that an existing LLM can be zip2zip-fied in 10 GPU-hours via\nparameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to\nuse hypertokens at inference time, reducing input and output sequence length by\n20-60\\%, with significant improvements in inference latency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "inference time"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01172", "pdf": "https://arxiv.org/pdf/2506.01172", "abs": "https://arxiv.org/abs/2506.01172", "authors": ["Byung-Doh Oh", "Hongao Zhu", "William Schuler"], "title": "The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is Not Due to Data Leakage", "categories": ["cs.CL"], "comment": "ACL Findings 2025; results with Natural Stories alignment issue\n  corrected (commit 4700daa)", "summary": "In psycholinguistic modeling, surprisal from larger pre-trained language\nmodels has been shown to be a poorer predictor of naturalistic human reading\ntimes. However, it has been speculated that this may be due to data leakage\nthat caused language models to see the text stimuli during training. This paper\npresents two studies to address this concern at scale. The first study reveals\nrelatively little leakage of five naturalistic reading time corpora in two\npre-training datasets in terms of length and frequency of token $n$-gram\noverlap. The second study replicates the negative relationship between language\nmodel size and the fit of surprisal to reading times using models trained on\n'leakage-free' data that overlaps only minimally with the reading time corpora.\nTaken together, this suggests that previous results using language models\ntrained on these corpora are not driven by the effects of data leakage.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01312", "pdf": "https://arxiv.org/pdf/2506.01312", "abs": "https://arxiv.org/abs/2506.01312", "authors": ["Chunhui Zhang", "Sirui", "Wang", "Zhongyu Ouyang", "Xiangchi Yuan", "Soroush Vosoughi"], "title": "Growing Through Experience: Scaling Episodic Grounding in Language Models", "categories": ["cs.CL"], "comment": "Accepted at The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Language models (LMs) require robust episodic grounding-the capacity to learn\nfrom and apply past experiences-to excel at physical planning tasks. Current\nepisodic grounding approaches struggle with scalability and integration,\nlimiting their effectiveness, especially for medium-sized LMs (7B parameters).\nWhile larger LMs (70-405B parameters) possess superior hierarchical\nrepresentations and extensive pre-trained knowledge, they encounter a\nfundamental scale paradox: despite their advanced abstraction capabilities,\nthey lack efficient mechanisms to leverage experience streams. We propose a\nscalable weak-to-strong episodic learning framework that effectively transfers\nepisodic behaviors from smaller to larger LMs. This framework integrates Monte\nCarlo tree search for structured experience collection with a novel\ndistillation method, preserving the inherent LM capabilities while embedding\nepisodic memory. Experiments demonstrate our method surpasses state-of-the-art\nproprietary LMs by 3.45% across diverse planning and question-answering tasks.\nLayer-wise probing further indicates significant improvements in task\nalignment, especially within deeper LM layers, highlighting stable\ngeneralization even for previously unseen scenarios with increased planning\ncomplexity-conditions where baseline methods degrade markedly.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00925", "pdf": "https://arxiv.org/pdf/2506.00925", "abs": "https://arxiv.org/abs/2506.00925", "authors": ["Mengdi Liu", "Xiaoxue Cheng", "Zhangyang Gao", "Hong Chang", "Cheng Tan", "Shiguang Shan", "Xilin Chen"], "title": "ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree Search", "categories": ["q-bio.BM", "cs.CV", "cs.LG"], "comment": null, "summary": "Designing protein sequences that fold into a target 3D structure, known as\nprotein inverse folding, is a fundamental challenge in protein engineering.\nWhile recent deep learning methods have achieved impressive performance by\nrecovering native sequences, they often overlook the one-to-many nature of the\nproblem: multiple diverse sequences can fold into the same structure. This\nmotivates the need for a generative model capable of designing diverse\nsequences while preserving structural consistency. To address this trade-off,\nwe introduce ProtInvTree, the first reward-guided tree-search framework for\nprotein inverse folding. ProtInvTree reformulates sequence generation as a\ndeliberate, step-wise decision-making process, enabling the exploration of\nmultiple design paths and exploitation of promising candidates through\nself-evaluation, lookahead, and backtracking. We propose a two-stage\nfocus-and-grounding action mechanism that decouples position selection and\nresidue generation. To efficiently evaluate intermediate states, we introduce a\njumpy denoising strategy that avoids full rollouts. Built upon pretrained\nprotein language models, ProtInvTree supports flexible test-time scaling by\nexpanding the search depth and breadth without retraining. Empirically,\nProtInvTree outperforms state-of-the-art baselines across multiple benchmarks,\ngenerating structurally consistent yet diverse sequences, including those far\nfrom the native ground truth.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01937", "pdf": "https://arxiv.org/pdf/2506.01937", "abs": "https://arxiv.org/abs/2506.01937", "authors": ["Saumya Malik", "Valentina Pyatkin", "Sander Land", "Jacob Morrison", "Noah A. Smith", "Hannaneh Hajishirzi", "Nathan Lambert"], "title": "RewardBench 2: Advancing Reward Model Evaluation", "categories": ["cs.CL"], "comment": "Data, models, and leaderboard available at\n  https://huggingface.co/collections/allenai/reward-bench-2-683d2612a4b3e38a3e53bb51", "summary": "Reward models are used throughout the post-training of language models to\ncapture nuanced signals from preference data and provide a training target for\noptimization across instruction following, reasoning, safety, and more domains.\nThe community has begun establishing best practices for evaluating reward\nmodels, from the development of benchmarks that test capabilities in specific\nskill areas to others that test agreement with human preferences. At the same\ntime, progress in evaluation has not been mirrored by the effectiveness of\nreward models in downstream tasks -- simpler direct alignment algorithms are\nreported to work better in many cases. This paper introduces RewardBench 2, a\nnew multi-skill reward modeling benchmark designed to bring new, challenging\ndata for accuracy-based reward model evaluation -- models score about 20 points\non average lower on RewardBench 2 compared to the first RewardBench -- while\nbeing highly correlated with downstream performance. Compared to most other\nbenchmarks, RewardBench 2 sources new human prompts instead of existing prompts\nfrom downstream evaluations, facilitating more rigorous evaluation practices.\nIn this paper, we describe our benchmark construction process and report how\nexisting models perform on it, while quantifying how performance on the\nbenchmark correlates with downstream use of the models in both inference-time\nscaling algorithms, like best-of-N sampling, and RLHF training algorithms like\nproximal policy optimization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward modeling", "RLHF", "proximal policy optimization", "policy optimization", "preference", "alignment"], "score": 7}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "reward model evaluation", "safety", "agreement", "accuracy"], "score": 6}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00222", "pdf": "https://arxiv.org/pdf/2506.00222", "abs": "https://arxiv.org/abs/2506.00222", "authors": ["Jiabao Brad Wang", "Amir Vaxman"], "title": "Power-Linear Polar Directional Fields", "categories": ["cs.GR"], "comment": "Accepted to SIGGRAPH 2025 Conference Track", "summary": "We introduce a novel method for directional-field design on meshes, enabling\nusers to specify singularities at any location on a mesh. Our method uses a\npiecewise power-linear representation for phase and scale, offering precise\ncontrol over field topology. The resulting fields are smooth and accommodate\nany singularity index and field symmetry. With this representation, we mitigate\nthe artifacts caused by coarse or uneven meshes. We showcase our approach on\nmeshes with diverse topologies and triangle qualities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00022", "pdf": "https://arxiv.org/pdf/2506.00022", "abs": "https://arxiv.org/abs/2506.00022", "authors": ["Shenghe Zheng", "Qianjia Cheng", "Junchi Yao", "Mengsong Wu", "haonan he", "Ning Ding", "Yu Cheng", "Shuyue Hu", "Lei Bai", "Dongzhan Zhou", "Ganqu Cui", "Peng Ye"], "title": "Scaling Physical Reasoning with the PHYSICS Dataset", "categories": ["cs.CL", "cs.LG", "physics.ed-ph"], "comment": "Work on physical datasets", "summary": "Large Language Models (LLMs) have achieved remarkable progress on advanced\nreasoning tasks such as mathematics and coding competitions. Meanwhile,\nphysics, despite being both reasoning-intensive and essential to real-world\nunderstanding, received limited academic and industrial attention. This paper\nintroduces PHYSICS, a dataset containing 16,568 high-quality physics problems\nspanning subjects and difficulty levels, to facilitate this issue.\nSpecifically, PHYSICS is curated with exercises from over 100 textbooks through\na carefully designed pipeline for quality control. It covers five major physics\ndomains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern\nPhysics. It also spans a wide range of difficulty levels, from high school to\ngraduate-level physics courses. To utilize the data for improving and\nevaluating the model's physical reasoning capabilities, we split the dataset\ninto training and test sets, and provide reasoning paths generated by powerful\nreasoning models for the training data to facilitate model training. In\naddition, for the evaluation part, we find that existing evaluation frameworks\nexhibit biases in aspects such as units, simplification, and precision in\nphysics domain. To balance efficiency and accuracy, we introduce a Rule+Model\nevaluation framework tailored to physics problems. Our evaluations on current\nstate-of-the-art open-source and proprietary models highlight the limitations\nof current models in handling physics-related tasks. We hope that our dataset\nand evaluation methodology will jointly advance the development of LLMs in the\nfield of physics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01077", "pdf": "https://arxiv.org/pdf/2506.01077", "abs": "https://arxiv.org/abs/2506.01077", "authors": ["Yueqian Guo", "Tianzhao Li", "Xin Lyu", "Jiehaolin Chen", "Zhaohan Wang", "Sirui Xiao", "Yurun Chen", "Yezi He", "Helin Li", "Fan Zhang"], "title": "TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans", "categories": ["cs.GR", "cs.HC", "68U05(Primary), 62M45(Secondary)"], "comment": "24 pages,12 figures", "summary": "Large Language Model (LLM)-driven digital humans have sparked a series of\nrecent studies on co-speech gesture generation systems. However, existing\napproaches struggle with real-time synthesis and long-text comprehension. This\npaper introduces Transformer-Based Rich Motion Matching (TRiMM), a novel\nmulti-modal framework for real-time 3D gesture generation. Our method\nincorporates three modules: 1) a cross-modal attention mechanism to achieve\nprecise temporal alignment between speech and gestures; 2) a long-context\nautoregressive model with a sliding window mechanism for effective sequence\nmodeling; 3) a large-scale gesture matching system that constructs an atomic\naction library and enables real-time retrieval. Additionally, we develop a\nlightweight pipeline implemented in the Unreal Engine for experimentation. Our\napproach achieves real-time inference at 120 fps and maintains a per-sentence\nlatency of 0.15 seconds on consumer-grade GPUs (Geforce RTX3060). Extensive\nsubjective and objective evaluations on the ZEGGS, and BEAT datasets\ndemonstrate that our model outperforms current state-of-the-art methods. TRiMM\nenhances the speed of co-speech gesture generation while ensuring gesture\nquality, enabling LLM-driven digital humans to respond to speech in real time\nand synthesize corresponding gestures. Our code is available at\nhttps://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00227", "pdf": "https://arxiv.org/pdf/2506.00227", "abs": "https://arxiv.org/abs/2506.00227", "authors": ["Anthony Gosselin", "Ge Ya Luo", "Luis Lara", "Florian Golemo", "Derek Nowrouzezahrai", "Liam Paull", "Alexia Jolicoeur-Martineau", "Christopher Pal"], "title": "Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Under review", "summary": "Video diffusion techniques have advanced significantly in recent years;\nhowever, they struggle to generate realistic imagery of car crashes due to the\nscarcity of accident events in most driving datasets. Improving traffic safety\nrequires realistic and controllable accident simulations. To tackle the\nproblem, we propose Ctrl-Crash, a controllable car crash video generation model\nthat conditions on signals such as bounding boxes, crash types, and an initial\nimage frame. Our approach enables counterfactual scenario generation where\nminor variations in input can lead to dramatically different crash outcomes. To\nsupport fine-grained control at inference time, we leverage classifier-free\nguidance with independently tunable scales for each conditioning signal.\nCtrl-Crash achieves state-of-the-art performance across quantitative video\nquality metrics (e.g., FVD and JEDi) and qualitative measurements based on a\nhuman-evaluation of physical realism and video quality compared to prior\ndiffusion-based methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "fine-grained"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00238", "pdf": "https://arxiv.org/pdf/2506.00238", "abs": "https://arxiv.org/abs/2506.00238", "authors": ["Ehsan Karimi", "Maryam Rahnemoonfar"], "title": "ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG", "I.2.7; I.2.10; I.5.1"], "comment": "Accepted by the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)", "summary": "Natural disasters usually affect vast areas and devastate infrastructures.\nPerforming a timely and efficient response is crucial to minimize the impact on\naffected communities, and data-driven approaches are the best choice. Visual\nquestion answering (VQA) models help management teams to achieve in-depth\nunderstanding of damages. However, recently published models do not possess the\nability to answer open-ended questions and only select the best answer among a\npredefined list of answers. If we want to ask questions with new additional\npossible answers that do not exist in the predefined list, the model needs to\nbe fin-tuned/retrained on a new collected and annotated dataset, which is a\ntime-consuming procedure. In recent years, large-scale Vision-Language Models\n(VLMs) have earned significant attention. These models are trained on extensive\ndatasets and demonstrate strong performance on both unimodal and multimodal\nvision/language downstream tasks, often without the need for fine-tuning. In\nthis paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and\ninvestigate the performance of on post-disaster FloodNet dataset. Since the\nproposed method takes advantage of zero-shot learning, it can be applied on new\ndatasets without fine-tuning. In addition, ZeShot-VQA is able to process and\ngenerate answers that has been not seen during the training procedure, which\ndemonstrates its flexibility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00077", "pdf": "https://arxiv.org/pdf/2506.00077", "abs": "https://arxiv.org/abs/2506.00077", "authors": ["Edward Wang", "Tianyu Wang", "Avanti Athreya", "Vince Lyzinski", "Carey E. Priebe"], "title": "Gaussian mixture models as a proxy for interacting language models", "categories": ["cs.CL", "cs.LG", "stat.ML", "62R07"], "comment": null, "summary": "Large language models (LLMs) are a powerful tool with the ability to match\nhuman capabilities and behavior in many settings. Retrieval-augmented\ngeneration (RAG) further allows LLMs to generate diverse output depending on\nthe contents of their RAG database. This motivates their use in the social\nsciences to study human behavior between individuals when large-scale\nexperiments are infeasible. However, LLMs depend on complex, computationally\nexpensive algorithms. In this paper, we introduce interacting Gaussian mixture\nmodels (GMMs) as an alternative to similar frameworks using LLMs. We compare a\nsimplified model of GMMs to select experimental simulations of LLMs whose\nupdating and response depend on feedback from other LLMs. We find that\ninteracting GMMs capture important features of the dynamics in interacting\nLLMs, and we investigate key similarities and differences between interacting\nLLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture\nmodels, potential modifications, and future research directions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00325", "pdf": "https://arxiv.org/pdf/2506.00325", "abs": "https://arxiv.org/abs/2506.00325", "authors": ["Long Xu", "Peng Gao", "Wen-Jia Tang", "Fei Wang", "Ru-Yue Yuan"], "title": "Towards Effective and Efficient Adversarial Defense with Diffusion Models for Robust Visual Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Although deep learning-based visual tracking methods have made significant\nprogress, they exhibit vulnerabilities when facing carefully designed\nadversarial attacks, which can lead to a sharp decline in tracking performance.\nTo address this issue, this paper proposes for the first time a novel\nadversarial defense method based on denoise diffusion probabilistic models,\ntermed DiffDf, aimed at effectively improving the robustness of existing visual\ntracking methods against adversarial attacks. DiffDf establishes a multi-scale\ndefense mechanism by combining pixel-level reconstruction loss, semantic\nconsistency loss, and structural similarity loss, effectively suppressing\nadversarial perturbations through a gradual denoising process. Extensive\nexperimental results on several mainstream datasets show that the DiffDf method\ndemonstrates excellent generalization performance for trackers with different\narchitectures, significantly improving various evaluation metrics while\nachieving real-time inference speeds of over 30 FPS, showcasing outstanding\ndefense performance and efficiency. Codes are available at\nhttps://github.com/pgao-lab/DiffDf.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00327", "pdf": "https://arxiv.org/pdf/2506.00327", "abs": "https://arxiv.org/abs/2506.00327", "authors": ["Shreshth Saini", "Ru-Ling Liao", "Yan Ye", "Alan C. Bovik"], "title": "Latent Guidance in Diffusion Models for Perceptual Evaluations", "categories": ["cs.CV", "cs.AI"], "comment": "24 Pages, 7 figures, 10 Tables", "summary": "Despite recent advancements in latent diffusion models that generate\nhigh-dimensional image data and perform various downstream tasks, there has\nbeen little exploration into perceptual consistency within these models on the\ntask of No-Reference Image Quality Assessment (NR-IQA). In this paper, we\nhypothesize that latent diffusion models implicitly exhibit perceptually\nconsistent local regions within the data manifold. We leverage this insight to\nguide on-manifold sampling using perceptual features and input measurements.\nSpecifically, we propose Perceptual Manifold Guidance (PMG), an algorithm that\nutilizes pretrained latent diffusion models and perceptual quality features to\nobtain perceptually consistent multi-scale and multi-timestep feature maps from\nthe denoising U-Net. We empirically demonstrate that these hyperfeatures\nexhibit high correlation with human perception in IQA tasks. Our method can be\napplied to any existing pretrained latent diffusion model and is\nstraightforward to integrate. To the best of our knowledge, this paper is the\nfirst work on guiding diffusion model with perceptual features for NR-IQA.\nExtensive experiments on IQA datasets show that our method, LGDM, achieves\nstate-of-the-art performance, underscoring the superior generalization\ncapabilities of diffusion models for NR-IQA tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "consistency"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00087", "pdf": "https://arxiv.org/pdf/2506.00087", "abs": "https://arxiv.org/abs/2506.00087", "authors": ["Peng Xie", "Xingyuan Liu", "Tsz Wai Chan", "Yequan Bie", "Yangqiu Song", "Yang Wang", "Hao Chen", "Kani Chen"], "title": "SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic Code-Switching Dataset", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Code-switching (CS) is the alternating use of two or more languages within a\nconversation or utterance, often influenced by social context and speaker\nidentity. This linguistic phenomenon poses challenges for Automatic Speech\nRecognition (ASR) systems, which are typically designed for a single language\nand struggle to handle multilingual inputs. The growing global demand for\nmultilingual applications, including Code-Switching ASR (CSASR), Text-to-Speech\n(CSTTS), and Cross-Lingual Information Retrieval (CLIR), highlights the\ninadequacy of existing monolingual datasets.\n  Although some code-switching datasets exist, most are limited to bilingual\nmixing within homogeneous ethnic groups, leaving a critical need for a\nlarge-scale, diverse benchmark akin to ImageNet in computer vision.\n  To bridge this gap, we introduce \\textbf{LinguaMaster}, a multi-agent\ncollaboration framework specifically designed for efficient and scalable\nmultilingual data synthesis. Leveraging this framework, we curate\n\\textbf{SwitchLingua}, the first large-scale multilingual and multi-ethnic\ncode-switching dataset, including: (1) 420K CS textual samples across 12\nlanguages, and (2) over 80 hours of audio recordings from 174 speakers\nrepresenting 18 countries/regions and 63 racial/ethnic backgrounds, based on\nthe textual data. This dataset captures rich linguistic and cultural diversity,\noffering a foundational resource for advancing multilingual and multicultural\nresearch. Furthermore, to address the issue that existing ASR evaluation\nmetrics lack sensitivity to code-switching scenarios, we propose the\n\\textbf{Semantic-Aware Error Rate (SAER)}, a novel evaluation metric that\nincorporates semantic information, providing a more accurate and context-aware\nassessment of system performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00365", "pdf": "https://arxiv.org/pdf/2506.00365", "abs": "https://arxiv.org/abs/2506.00365", "authors": ["Ngoc Tuyen Do", "Tri Nhu Do"], "title": "Feature Fusion and Knowledge-Distilled Multi-Modal Multi-Target Detection", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "In the surveillance and defense domain, multi-target detection and\nclassification (MTD) is considered essential yet challenging due to\nheterogeneous inputs from diverse data sources and the computational complexity\nof algorithms designed for resource-constrained embedded devices, particularly\nfor Al-based solutions. To address these challenges, we propose a feature\nfusion and knowledge-distilled framework for multi-modal MTD that leverages\ndata fusion to enhance accuracy and employs knowledge distillation for improved\ndomain adaptation. Specifically, our approach utilizes both RGB and thermal\nimage inputs within a novel fusion-based multi-modal model, coupled with a\ndistillation training pipeline. We formulate the problem as a posterior\nprobability optimization task, which is solved through a multi-stage training\npipeline supported by a composite loss function. This loss function effectively\ntransfers knowledge from a teacher model to a student model. Experimental\nresults demonstrate that our student model achieves approximately 95% of the\nteacher model's mean Average Precision while reducing inference time by\napproximately 50%, underscoring its suitability for practical MTD deployment\nscenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00394", "pdf": "https://arxiv.org/pdf/2506.00394", "abs": "https://arxiv.org/abs/2506.00394", "authors": ["Ziwei Zhao", "Xizi Wang", "Yuchen Wang", "Feng Cheng", "David Crandall"], "title": "Sequence-Based Identification of First-Person Camera Wearers in Third-Person Views", "categories": ["cs.CV"], "comment": null, "summary": "The increasing popularity of egocentric cameras has generated growing\ninterest in studying multi-camera interactions in shared environments. Although\nlarge-scale datasets such as Ego4D and Ego-Exo4D have propelled egocentric\nvision research, interactions between multiple camera wearers remain\nunderexplored-a key gap for applications like immersive learning and\ncollaborative robotics. To bridge this, we present TF2025, an expanded dataset\nwith synchronized first- and third-person views. In addition, we introduce a\nsequence-based method to identify first-person wearers in third-person footage,\ncombining motion cues and person re-identification.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00406", "pdf": "https://arxiv.org/pdf/2506.00406", "abs": "https://arxiv.org/abs/2506.00406", "authors": ["Huahui Yi", "Wei Xu", "Ziyuan Qin", "Xi Chen", "Xiaohu Wu", "Kang Li", "Qicheng Lao"], "title": "iDPA: Instance Decoupled Prompt Attention for Incremental Medical Object Detection", "categories": ["cs.CV"], "comment": "accepted to ICML 2025", "summary": "Existing prompt-based approaches have demonstrated impressive performance in\ncontinual learning, leveraging pre-trained large-scale models for\nclassification tasks; however, the tight coupling between foreground-background\ninformation and the coupled attention between prompts and image-text tokens\npresent significant challenges in incremental medical object detection tasks,\ndue to the conceptual gap between medical and natural domains. To overcome\nthese challenges, we introduce the \\method~framework, which comprises two main\ncomponents: 1) Instance-level Prompt Generation (\\ipg), which decouples\nfine-grained instance-level knowledge from images and generates prompts that\nfocus on dense predictions, and 2) Decoupled Prompt Attention (\\dpa), which\ndecouples the original prompt attention, enabling a more direct and efficient\ntransfer of prompt information while reducing memory usage and mitigating\ncatastrophic forgetting. We collect 13 clinical, cross-modal, multi-organ, and\nmulti-category datasets, referred to as \\dataset, and experiments demonstrate\nthat \\method~outperforms existing SOTA methods, with FAP improvements of\n5.44\\%, 4.83\\%, 12.88\\%, and 4.59\\% in full data, 1-shot, 10-shot, and 50-shot\nsettings, respectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00433", "pdf": "https://arxiv.org/pdf/2506.00433", "abs": "https://arxiv.org/abs/2506.00433", "authors": ["Luigi Sigillo", "Shengfeng He", "Danilo Comminiello"], "title": "Latent Wavelet Diffusion: Enabling 4K Image Synthesis for Free", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "High-resolution image synthesis remains a core challenge in generative\nmodeling, particularly in balancing computational efficiency with the\npreservation of fine-grained visual detail. We present Latent Wavelet Diffusion\n(LWD), a lightweight framework that enables any latent diffusion model to scale\nto ultra-high-resolution image generation (2K to 4K) for free. LWD introduces\nthree key components: (1) a scale-consistent variational autoencoder objective\nthat enhances the spectral fidelity of latent representations; (2) wavelet\nenergy maps that identify and localize detail-rich spatial regions within the\nlatent space; and (3) a time-dependent masking strategy that focuses denoising\nsupervision on high-frequency components during training. LWD requires no\narchitectural modifications and incurs no additional computational overhead.\nDespite its simplicity, it consistently improves perceptual quality and reduces\nFID in ultra-high-resolution image synthesis, outperforming strong baseline\nmodels. These results highlight the effectiveness of frequency-aware,\nsignal-driven supervision as a principled and efficient approach for\nhigh-resolution generative modeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00200", "pdf": "https://arxiv.org/pdf/2506.00200", "abs": "https://arxiv.org/abs/2506.00200", "authors": ["Johannes Moll", "Louisa Fay", "Asfandyar Azhar", "Sophie Ostmeier", "Tim Lueth", "Sergios Gatidis", "Curtis Langlotz", "Jean-Benoit Delbrouck"], "title": "Structuring Radiology Reports: Challenging LLMs with Lightweight Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Radiology reports are critical for clinical decision-making but often lack a\nstandardized format, limiting both human interpretability and machine learning\n(ML) applications. While large language models (LLMs) have shown strong\ncapabilities in reformatting clinical text, their high computational\nrequirements, lack of transparency, and data privacy concerns hinder practical\ndeployment. To address these challenges, we explore lightweight encoder-decoder\nmodels (<300M parameters)-specifically T5 and BERT2BERT-for structuring\nradiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark\nthese models against eight open-source LLMs (1B-70B), adapted using prefix\nprompting, in-context learning (ICL), and low-rank adaptation (LoRA)\nfinetuning. Our best-performing lightweight model outperforms all LLMs adapted\nusing prompt-based techniques on a human-annotated test set. While some\nLoRA-finetuned LLMs achieve modest gains over the lightweight model on the\nFindings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,\nGREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of\nsubstantially greater computational resources. For example, LLaMA-3-70B\nincurred more than 400 times the inference time, cost, and carbon emissions\ncompared to the lightweight model. These results underscore the potential of\nlightweight, task-specific models as sustainable and privacy-preserving\nsolutions for structuring clinical text in resource-constrained healthcare\nsettings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00204", "pdf": "https://arxiv.org/pdf/2506.00204", "abs": "https://arxiv.org/abs/2506.00204", "authors": ["Linyuan Gong", "Alvin Cheung", "Mostafa Elhoushi", "Sida Wang"], "title": "Structure-Aware Fill-in-the-Middle Pretraining for Code", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "14 pages", "summary": "Fill-in-the-Middle (FIM) is a common pretraining method for code LLMs, where\nmodels complete code segments given surrounding context. However, existing LLMs\ntreat code as plain text and mask random character spans. We propose and\nevaluate AST-FIM, a pretraining strategy that leverages Abstract Syntax Trees\n(ASTs) to mask complete syntactic structures at scale, ensuring coherent\ntraining examples better aligned with universal code structures and common code\nediting patterns such as blocks, expressions, or functions. To evaluate\nreal-world fill-in-the-middle (FIM) programming tasks, we introduce\nReal-FIM-Eval, a benchmark derived from 30,000+ GitHub commits across 12\nlanguages. On infilling tasks, experiments on 1B and 8B parameter models show\nthat AST-FIM is particularly beneficial for real-world code editing as it\noutperforms standard random-character FIM by up to 5 pts on standard FIM\nbenchmarks. Our code is publicly available at\nhttps://github.com/gonglinyuan/ast_fim.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00513", "pdf": "https://arxiv.org/pdf/2506.00513", "abs": "https://arxiv.org/abs/2506.00513", "authors": ["Yaxiong Wang", "Zhenqiang Zhang", "Lechao Cheng", "Zhun Zhong", "Dan Guo", "Meng Wang"], "title": "SSAM: Self-Supervised Association Modeling for Test-Time Adaption", "categories": ["cs.CV"], "comment": "10 papges", "summary": "Test-time adaption (TTA) has witnessed important progress in recent years,\nthe prevailing methods typically first encode the image and the text and design\nstrategies to model the association between them. Meanwhile, the image encoder\nis usually frozen due to the absence of explicit supervision in TTA scenarios.\nWe identify a critical limitation in this paradigm: While test-time images\noften exhibit distribution shifts from training data, existing methods\npersistently freeze the image encoder due to the absence of explicit\nsupervision during adaptation. This practice overlooks the image encoder's\ncrucial role in bridging distribution shift between training and test. To\naddress this challenge, we propose SSAM (Self-Supervised Association Modeling),\na new TTA framework that enables dynamic encoder refinement through dual-phase\nassociation learning. Our method operates via two synergistic components: 1)\nSoft Prototype Estimation (SPE), which estimates probabilistic category\nassociations to guide feature space reorganization, and 2) Prototype-anchored\nImage Reconstruction (PIR), enforcing encoder stability through\ncluster-conditional image feature reconstruction. Comprehensive experiments\nacross diverse baseline methods and benchmarks demonstrate that SSAM can\nsurpass state-of-the-art TTA baselines by a clear margin while maintaining\ncomputational efficiency. The framework's architecture-agnostic design and\nminimal hyperparameter dependence further enhance its practical applicability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00210", "pdf": "https://arxiv.org/pdf/2506.00210", "abs": "https://arxiv.org/abs/2506.00210", "authors": ["Ziji Zhang", "Michael Yang", "Zhiyu Chen", "Yingying Zhuang", "Shu-Ting Pi", "Qun Liu", "Rajashekar Maragoud", "Vy Nguyen", "Anurag Beniwal"], "title": "REIC: RAG-Enhanced Intent Classification at Scale", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Accurate intent classification is critical for efficient routing in customer\nservice, ensuring customers are connected with the most suitable agents while\nreducing handling times and operational costs. However, as companies expand\ntheir product lines, intent classification faces scalability challenges due to\nthe increasing number of intents and variations in taxonomy across different\nverticals. In this paper, we introduce REIC, a Retrieval-augmented generation\nEnhanced Intent Classification approach, which addresses these challenges\neffectively. REIC leverages retrieval-augmented generation (RAG) to dynamically\nincorporate relevant knowledge, enabling precise classification without the\nneed for frequent retraining. Through extensive experiments on real-world\ndatasets, we demonstrate that REIC outperforms traditional fine-tuning,\nzero-shot, and few-shot methods in large-scale customer service settings. Our\nresults highlight its effectiveness in both in-domain and out-of-domain\nscenarios, demonstrating its potential for real-world deployment in adaptive\nand large-scale intent classification systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00232", "pdf": "https://arxiv.org/pdf/2506.00232", "abs": "https://arxiv.org/abs/2506.00232", "authors": ["Ruofan Wu", "Youngwon Lee", "Fan Shu", "Danmei Xu", "Seung-won Hwang", "Zhewei Yao", "Yuxiong He", "Feng Yan"], "title": "ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems are increasingly diverse, yet\nmany suffer from monolithic designs that tightly couple core functions like\nquery reformulation, retrieval, reasoning, and verification. This limits their\ninterpretability, systematic evaluation, and targeted improvement, especially\nfor complex multi-hop question answering. We introduce ComposeRAG, a novel\nmodular abstraction that decomposes RAG pipelines into atomic, composable\nmodules. Each module, such as Question Decomposition, Query Rewriting,\nRetrieval Decision, and Answer Verification, acts as a parameterized\ntransformation on structured inputs/outputs, allowing independent\nimplementation, upgrade, and analysis. To enhance robustness against errors in\nmulti-step reasoning, ComposeRAG incorporates a self-reflection mechanism that\niteratively revisits and refines earlier steps upon verification failure.\nEvaluated on four challenging multi-hop QA benchmarks, ComposeRAG consistently\noutperforms strong baselines in both accuracy and grounding fidelity.\nSpecifically, it achieves up to a 15% accuracy improvement over\nfine-tuning-based methods and up to a 5% gain over reasoning-specialized\npipelines under identical retrieval conditions. Crucially, ComposeRAG\nsignificantly enhances grounding: its verification-first design reduces\nungrounded answers by over 10% in low-quality retrieval settings, and by\napproximately 3% even with strong corpora. Comprehensive ablation studies\nvalidate the modular architecture, demonstrating distinct and additive\ncontributions from each component. These findings underscore ComposeRAG's\ncapacity to deliver flexible, transparent, scalable, and high-performing\nmulti-hop reasoning with improved grounding and interpretability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00250", "pdf": "https://arxiv.org/pdf/2506.00250", "abs": "https://arxiv.org/abs/2506.00250", "authors": ["Mohammad Javad Ranjbar Kalahroodi", "Amirhossein Sheikholselami", "Sepehr Karimi", "Sepideh Ranjbar Kalahroodi", "Heshaam Faili", "Azadeh Shakery"], "title": "PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain", "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance on a wide\nrange of NLP benchmarks, often surpassing human-level accuracy. However, their\nreliability in high-stakes domains such as medicine, particularly in\nlow-resource languages, remains underexplored. In this work, we introduce\nPersianMedQA, a large-scale, expert-validated dataset of multiple-choice\nPersian medical questions, designed to evaluate LLMs across both Persian and\nEnglish. We benchmark over 40 state-of-the-art models, including\ngeneral-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and\nchain-of-thought (CoT) settings. Our results show that closed-source general\nmodels (e.g., GPT-4.1) consistently outperform all other categories, achieving\n83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models\nsuch as Dorna underperform significantly (e.g., 35.9% in Persian), often\nstruggling with both instruction-following and domain reasoning. We also\nanalyze the impact of translation, showing that while English performance is\ngenerally higher, Persian responses are sometimes more accurate due to cultural\nand clinical contextual cues. Finally, we demonstrate that model size alone is\ninsufficient for robust performance without strong domain or language\nadaptation. PersianMedQA provides a foundation for evaluating multilingual and\nculturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be\naccessed at:\nhttps://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA](https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00562", "pdf": "https://arxiv.org/pdf/2506.00562", "abs": "https://arxiv.org/abs/2506.00562", "authors": ["Yule Zhu", "Ping Liu", "Zhedong Zheng", "Wei Liu"], "title": "SEED: A Benchmark Dataset for Sequential Facial Attribute Editing with Diffusion Models", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Diffusion models have recently enabled precise and photorealistic facial\nediting across a wide range of semantic attributes. Beyond single-step\nmodifications, a growing class of applications now demands the ability to\nanalyze and track sequences of progressive edits, such as stepwise changes to\nhair, makeup, or accessories. However, sequential editing introduces\nsignificant challenges in edit attribution and detection robustness, further\ncomplicated by the lack of large-scale, finely annotated benchmarks tailored\nexplicitly for this task. We introduce SEED, a large-scale Sequentially Edited\nfacE Dataset constructed via state-of-the-art diffusion models. SEED contains\nover 90,000 facial images with one to four sequential attribute modifications,\ngenerated using diverse diffusion-based editing pipelines (LEdits, SDXL, SD3).\nEach image is annotated with detailed edit sequences, attribute masks, and\nprompts, facilitating research on sequential edit tracking, visual provenance\nanalysis, and manipulation robustness assessment. To benchmark this task, we\npropose FAITH, a frequency-aware transformer-based model that incorporates\nhigh-frequency cues to enhance sensitivity to subtle sequential changes.\nComprehensive experiments, including systematic comparisons of multiple\nfrequency-domain methods, demonstrate the effectiveness of FAITH and the unique\nchallenges posed by SEED. SEED offers a challenging and flexible resource for\nstudying progressive diffusion-based edits at scale. Dataset and code will be\npublicly released at: https://github.com/Zeus1037/SEED.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00568", "pdf": "https://arxiv.org/pdf/2506.00568", "abs": "https://arxiv.org/abs/2506.00568", "authors": ["Ke Niu", "Zhuofan Chen", "Haiyang Yu", "Yuwen Chen", "Teng Fu", "Mengyang Zhao", "Bin Li", "Xiangyang Xue"], "title": "CReFT-CAD: Boosting Orthographic Projection Reasoning for CAD via Reinforcement Fine-Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing.\nOrthographic projection reasoning underpins the entire CAD workflow,\nencompassing design, manufacturing, and simulation. However, prevailing\ndeep-learning approaches employ standard 3D reconstruction pipelines as an\nalternative, which often introduce imprecise dimensions and limit the\nparametric editability required for CAD workflows. Recently, some researchers\nadopt vision-language models (VLMs), particularly supervised fine-tuning (SFT),\nto tackle CAD-related challenges. SFT shows promise but often devolves into\npattern memorization, yielding poor out-of-distribution performance on complex\nreasoning tasks. To address these gaps, we introduce CReFT-CAD, a two-stage\nfine-tuning paradigm that first employs a curriculum-driven reinforcement\nlearning stage with difficulty-aware rewards to build reasoning ability\nsteadily, and then applies supervised post-tuning to hone instruction following\nand semantic extraction. Complementing this, we release TriView2CAD, the first\nlarge-scale, open-source benchmark for orthographic projection reasoning,\ncomprising 200,000 synthetic and 3,000 real-world orthographic projections with\nprecise dimension annotations and six interoperable data modalities. We\nbenchmark leading VLMs on orthographic projection reasoning and demonstrate\nthat CReFT-CAD substantially improves reasoning accuracy and\nout-of-distribution generalizability in real-world scenarios, offering valuable\ninsights for advancing CAD reasoning research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "dimension"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00267", "pdf": "https://arxiv.org/pdf/2506.00267", "abs": "https://arxiv.org/abs/2506.00267", "authors": ["Cihan Xiao", "Ruixing Liang", "Xiangyu Zhang", "Mehmet Emre Tiryaki", "Veronica Bae", "Lavanya Shankar", "Rong Yang", "Ethan Poon", "Emmanuel Dupoux", "Sanjeev Khudanpur", "Leibny Paola Garcia Perera"], "title": "CASPER: A Large Scale Spontaneous Speech Dataset", "categories": ["cs.CL"], "comment": null, "summary": "The success of large language models has driven interest in developing\nsimilar speech processing capabilities. However, a key challenge is the\nscarcity of high-quality spontaneous speech data, as most existing datasets\ncontain scripted dialogues. To address this, we present a novel pipeline for\neliciting and recording natural dialogues and release our Stage 1 dataset with\n200+ hours of spontaneous speech. Our approach fosters fluid, natural\nconversations while encouraging a diverse range of topics and interactive\nexchanges. Unlike traditional methods, it facilitates genuine interactions,\nproviding a reproducible framework for future data collection. This paper\nintroduces our dataset and methodology, laying the groundwork for addressing\nthe shortage of spontaneous speech data. We plan to expand this dataset in\nfuture stages, offering a growing resource for the research community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00596", "pdf": "https://arxiv.org/pdf/2506.00596", "abs": "https://arxiv.org/abs/2506.00596", "authors": ["Danfeng li", "Hui Zhang", "Sheng Wang", "Jiacheng Li", "Zuxuan Wu"], "title": "Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise Shape and Semantic Control", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent advances in diffusion models, top-tier text-to-image (T2I)\nmodels still struggle to achieve precise spatial layout control, i.e.\naccurately generating entities with specified attributes and locations.\nSegmentation-mask-to-image (S2I) generation has emerged as a promising solution\nby incorporating pixel-level spatial guidance and regional text prompts.\nHowever, existing S2I methods fail to simultaneously ensure semantic\nconsistency and shape consistency. To address these challenges, we propose\nSeg2Any, a novel S2I framework built upon advanced multimodal diffusion\ntransformers (e.g. FLUX). First, to achieve both semantic and shape\nconsistency, we decouple segmentation mask conditions into regional semantic\nand high-frequency shape components. The regional semantic condition is\nintroduced by a Semantic Alignment Attention Mask, ensuring that generated\nentities adhere to their assigned text prompts. The high-frequency shape\ncondition, representing entity boundaries, is encoded as an Entity Contour Map\nand then introduced as an additional modality via multi-modal attention to\nguide image spatial structure. Second, to prevent attribute leakage across\nentities in multi-entity scenarios, we introduce an Attribute Isolation\nAttention Mask mechanism, which constrains each entity's image tokens to attend\nexclusively to themselves during image self-attention. To support open-set S2I\ngeneration, we construct SACap-1M, a large-scale dataset containing 1 million\nimages with 5.9 million segmented entities and detailed regional captions,\nalong with a SACap-Eval benchmark for comprehensive S2I evaluation. Extensive\nexperiments demonstrate that Seg2Any achieves state-of-the-art performance on\nboth open-set and closed-set S2I benchmarks, particularly in fine-grained\nspatial and attribute control of entities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "consistency", "fine-grained"], "score": 5}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00277", "pdf": "https://arxiv.org/pdf/2506.00277", "abs": "https://arxiv.org/abs/2506.00277", "authors": ["Hans W. A. Hanley", "Zakir Durumeric"], "title": "Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "Accepted to The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Contextual large language model embeddings are increasingly utilized for\ntopic modeling and clustering. However, current methods often scale poorly,\nrely on opaque similarity metrics, and struggle in multilingual settings. In\nthis work, we present a novel, scalable, interpretable, hierarchical, and\nmultilingual approach to clustering news articles and social media data. To do\nthis, we first train multilingual Matryoshka embeddings that can determine\nstory similarity at varying levels of granularity based on which subset of the\ndimensions of the embeddings is examined. This embedding model achieves\nstate-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson\n$\\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering\nalgorithm that leverages the hierarchical nature of Matryoshka embeddings to\nidentify unique news stories, narratives, and themes. We conclude by\nillustrating how our approach can identify and cluster stories, narratives, and\noverarching themes within real-world news datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00599", "pdf": "https://arxiv.org/pdf/2506.00599", "abs": "https://arxiv.org/abs/2506.00599", "authors": ["Junwen Huang", "Jizhong Liang", "Jiaqi Hu", "Martin Sundermeyer", "Peter KT Yu", "Nassir Navab", "Benjamin Busam"], "title": "XYZ-IBD: High-precision Bin-picking Dataset for Object 6D Pose Estimation Capturing Real-world Industrial Complexity", "categories": ["cs.CV"], "comment": null, "summary": "We introduce XYZ-IBD, a bin-picking dataset for 6D pose estimation that\ncaptures real-world industrial complexity, including challenging object\ngeometries, reflective materials, severe occlusions, and dense clutter. The\ndataset reflects authentic robotic manipulation scenarios with\nmillimeter-accurate annotations. Unlike existing datasets that primarily focus\non household objects, which approach saturation,XYZ-IBD represents the unsolved\nrealistic industrial conditions. The dataset features 15 texture-less,\nmetallic, and mostly symmetrical objects of varying shapes and sizes. These\nobjects are heavily occluded and randomly arranged in bins with high density,\nreplicating the challenges of real-world bin-picking. XYZ-IBD was collected\nusing two high-precision industrial cameras and one commercially available\ncamera, providing RGB, grayscale, and depth images. It contains 75 multi-view\nreal-world scenes, along with a large-scale synthetic dataset rendered under\nsimulated bin-picking conditions. We employ a meticulous annotation pipeline\nthat includes anti-reflection spray, multi-view depth fusion, and\nsemi-automatic annotation, achieving millimeter-level pose labeling accuracy\nrequired for industrial manipulation. Quantification in simulated environments\nconfirms the reliability of the ground-truth annotations. We benchmark\nstate-of-the-art methods on 2D detection, 6D pose estimation, and depth\nestimation tasks on our dataset, revealing significant performance degradation\nin our setups compared to current academic household benchmarks. By capturing\nthe complexity of real-world bin-picking scenarios, XYZ-IBD introduces more\nrealistic and challenging problems for future research. The dataset and\nbenchmark are publicly available at https://xyz-ibd.github.io/XYZ-IBD/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00600", "pdf": "https://arxiv.org/pdf/2506.00600", "abs": "https://arxiv.org/abs/2506.00600", "authors": ["Xianghui Ze", "Beiyi Zhu", "Zhenbo Song", "Jianfeng Lu", "Yujiao Shi"], "title": "SatDreamer360: Geometry Consistent Street-View Video Generation from Satellite Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Generating continuous ground-level video from satellite imagery is a\nchallenging task with significant potential for applications in simulation,\nautonomous navigation, and digital twin cities. Existing approaches primarily\nfocus on synthesizing individual ground-view images, often relying on auxiliary\ninputs like height maps or handcrafted projections, and fall short in producing\ntemporally consistent sequences. In this paper, we propose {SatDreamer360}, a\nnovel framework that generates geometrically and temporally consistent\nground-view video from a single satellite image and a predefined trajectory. To\nbridge the large viewpoint gap, we introduce a compact tri-plane representation\nthat encodes scene geometry directly from the satellite image. A ray-based\npixel attention mechanism retrieves view-dependent features from the tri-plane,\nenabling accurate cross-view correspondence without requiring additional\ngeometric priors. To ensure multi-frame consistency, we propose an\nepipolar-constrained temporal attention module that aligns features across\nframes using the known relative poses along the trajectory. To support\nevaluation, we introduce {VIGOR++}, a large-scale dataset for cross-view video\ngeneration, with dense trajectory annotations and high-quality ground-view\nsequences. Extensive experiments demonstrate that SatDreamer360 achieves\nsuperior performance in fidelity, coherence, and geometric alignment across\ndiverse urban scenes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00667", "pdf": "https://arxiv.org/pdf/2506.00667", "abs": "https://arxiv.org/abs/2506.00667", "authors": ["Vasilii Korolkov"], "title": "Scene Detection Policies and Keyframe Extraction Strategies for Large-Scale Video Analysis", "categories": ["cs.CV", "cs.MM", "68T07", "I.2.10; I.4.8; I.5.1"], "comment": "24 pages, 8 figures, submitted as a preprint. ArXiv preprint only,\n  not submitted to a journal yet", "summary": "Robust scene segmentation and keyframe extraction are essential preprocessing\nsteps in video understanding pipelines, supporting tasks such as indexing,\nsummarization, and semantic retrieval. However, existing methods often lack\ngeneralizability across diverse video types and durations. We present a\nunified, adaptive framework for automatic scene detection and keyframe\nselection that handles formats ranging from short-form media to long-form\nfilms, archival content, and surveillance footage. Our system dynamically\nselects segmentation policies based on video length: adaptive thresholding for\nshort videos, hybrid strategies for mid-length ones, and interval-based\nsplitting for extended recordings. This ensures consistent granularity and\nefficient processing across domains. For keyframe selection, we employ a\nlightweight module that scores sampled frames using a composite metric of\nsharpness, luminance, and temporal spread, avoiding complex saliency models\nwhile ensuring visual relevance. Designed for high-throughput workflows, the\nsystem is deployed in a commercial video analysis platform and has processed\ncontent from media, education, research, and security domains. It offers a\nscalable and interpretable solution suitable for downstream applications such\nas UI previews, embedding pipelines, and content filtering. We discuss\npractical implementation details and outline future enhancements, including\naudio-aware segmentation and reinforcement-learned frame scoring.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00391", "pdf": "https://arxiv.org/pdf/2506.00391", "abs": "https://arxiv.org/abs/2506.00391", "authors": ["Ge Qu", "Jinyang Li", "Bowen Qin", "Xiaolong Li", "Nan Huo", "Chenhao Ma", "Reynold Cheng"], "title": "SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main", "summary": "Current self-correction approaches in text-to-SQL face two critical\nlimitations: 1) Conventional self-correction methods rely on recursive\nself-calls of LLMs, resulting in multiplicative computational overhead, and 2)\nLLMs struggle to implement effective error detection and correction for\ndeclarative SQL queries, as they fail to demonstrate the underlying reasoning\npath. In this work, we propose SHARE, an SLM-based Hierarchical Action\ncorREction assistant that enables LLMs to perform more precise error\nlocalization and efficient correction. SHARE orchestrates three specialized\nSmall Language Models (SLMs) in a sequential pipeline, where it first\ntransforms declarative SQL queries into stepwise action trajectories that\nreveal underlying reasoning, followed by a two-phase granular refinement. We\nfurther propose a novel hierarchical self-evolution strategy for data-efficient\ntraining. Experimental results demonstrate that SHARE effectively enhances\nself-correction capabilities while proving robust across various LLMs.\nFurthermore, our comprehensive analysis shows that SHARE maintains strong\nperformance even in low-resource training settings, which is particularly\nvaluable for text-to-SQL applications with data privacy constraints.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00742", "pdf": "https://arxiv.org/pdf/2506.00742", "abs": "https://arxiv.org/abs/2506.00742", "authors": ["Zeqi Gu", "Yin Cui", "Zhaoshuo Li", "Fangyin Wei", "Yunhao Ge", "Jinwei Gu", "Ming-Yu Liu", "Abe Davis", "Yifan Ding"], "title": "ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image Intermediary", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR", "summary": "Designing 3D scenes is traditionally a challenging task that demands both\nartistic expertise and proficiency with complex software. Recent advances in\ntext-to-3D generation have greatly simplified this process by letting users\ncreate scenes based on simple text descriptions. However, as these methods\ngenerally require extra training or in-context learning, their performance is\noften hindered by the limited availability of high-quality 3D data. In\ncontrast, modern text-to-image models learned from web-scale images can\ngenerate scenes with diverse, reliable spatial layouts and consistent, visually\nappealing styles. Our key insight is that instead of learning directly from 3D\nscenes, we can leverage generated 2D images as an intermediary to guide 3D\nsynthesis. In light of this, we introduce ArtiScene, a training-free automated\npipeline for scene design that integrates the flexibility of free-form\ntext-to-image generation with the diversity and reliability of 2D intermediary\nlayouts.\n  First, we generate 2D images from a scene description, then extract the shape\nand appearance of objects to create 3D models. These models are assembled into\nthe final scene using geometry, position, and pose information derived from the\nsame intermediary image. Being generalizable to a wide range of scenes and\nstyles, ArtiScene outperforms state-of-the-art benchmarks by a large margin in\nlayout and aesthetic quality by quantitative metrics. It also averages a 74.89%\nwinning rate in extensive user studies and 95.07% in GPT-4o evaluation. Project\npage: https://artiscene-cvpr.github.io/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00400", "pdf": "https://arxiv.org/pdf/2506.00400", "abs": "https://arxiv.org/abs/2506.00400", "authors": ["Zixin Ding", "Junyuan Hong", "Jiachen T. Wang", "Zinan Lin", "Zhangyang Wang", "Yuxin Chen"], "title": "Scaling Textual Gradients via Sampling-Based Momentum", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As prompts play an increasingly critical role in large language models\n(LLMs), optimizing textual prompts has become a crucial challenge. The Textual\nGradient Descent (TGD) framework has emerged as a promising data-driven\napproach that iteratively refines textual prompts using LLM - suggested updates\n(or textual gradients) over minibatches of training samples. In this paper, we\nempirically demonstrate that scaling the number of training examples initially\nimproves but later degrades TGD's performance across multiple downstream NLP\ntasks. However, while data scaling improves results for most tasks, it also\nsignificantly increases the computational cost when leveraging LLMs. To address\nthis, we draw inspiration from numerical gradient descent and propose Textual\nStochastic Gradient Descent with Momentum (TSGD-M) - a method that facilitates\nscalable in-context learning by reweighting prompt sampling based on past batch\ndistributions. Across nine NLP tasks spanning three domains - including\nBIG-Bench Hard (BBH), natural language understanding tasks, and reasoning tasks\n- TSGD-M significantly outperforms TGD baselines that do not incorporate\nreweighted sampling, while also reducing variance in most tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00820", "pdf": "https://arxiv.org/pdf/2506.00820", "abs": "https://arxiv.org/abs/2506.00820", "authors": ["Jiatong Li", "Libo Zhu", "Haotong Qin", "Jingkai Wang", "Linghe Kong", "Guihai Chen", "Yulun Zhang", "Xiaokang Yang"], "title": "QuantFace: Low-Bit Post-Training Quantization for One-Step Diffusion Face Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have been achieving remarkable performance in face\nrestoration. However, the heavy computations of diffusion models make it\ndifficult to deploy them on devices like smartphones. In this work, we propose\nQuantFace, a novel low-bit quantization for one-step diffusion face restoration\nmodels, where the full-precision (\\ie, 32-bit) weights and activations are\nquantized to 4$\\sim$6-bit. We first analyze the data distribution within\nactivations and find that they are highly variant. To preserve the original\ndata information, we employ rotation-scaling channel balancing. Furthermore, we\npropose Quantization-Distillation Low-Rank Adaptation (QD-LoRA) that jointly\noptimizes for quantization and distillation performance. Finally, we propose an\nadaptive bit-width allocation strategy. We formulate such a strategy as an\ninteger programming problem, which combines quantization error and perceptual\nmetrics to find a satisfactory resource allocation. Extensive experiments on\nthe synthetic and real-world datasets demonstrate the effectiveness of\nQuantFace under 6-bit and 4-bit. QuantFace achieves significant advantages over\nrecent leading low-bit quantization methods for face restoration. The code is\navailable at https://github.com/jiatongli2024/QuantFace.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00836", "pdf": "https://arxiv.org/pdf/2506.00836", "abs": "https://arxiv.org/abs/2506.00836", "authors": ["Baolu Li", "Hongkai Yu", "Huiming Sun", "Jin Ma", "Yuewei Lin", "Lu Ma", "Yonghua Du"], "title": "Advancing from Automated to Autonomous Beamline by Leveraging Computer Vision", "categories": ["cs.CV"], "comment": null, "summary": "The synchrotron light source, a cutting-edge large-scale user facility,\nrequires autonomous synchrotron beamline operations, a crucial technique that\nshould enable experiments to be conducted automatically, reliably, and safely\nwith minimum human intervention. However, current state-of-the-art synchrotron\nbeamlines still heavily rely on human safety oversight. To bridge the gap\nbetween automated and autonomous operation, a computer vision-based system is\nproposed, integrating deep learning and multiview cameras for real-time\ncollision detection. The system utilizes equipment segmentation, tracking, and\ngeometric analysis to assess potential collisions with transfer learning that\nenhances robustness. In addition, an interactive annotation module has been\ndeveloped to improve the adaptability to new object classes. Experiments on a\nreal beamline dataset demonstrate high accuracy, real-time performance, and\nstrong potential for autonomous synchrotron beamline operations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "safety", "accuracy"], "score": 4}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00871", "pdf": "https://arxiv.org/pdf/2506.00871", "abs": "https://arxiv.org/abs/2506.00871", "authors": ["Ryo Fujii", "Hideo Saito", "Ryo Hachiuma"], "title": "Towards Predicting Any Human Trajectory In Context", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Predicting accurate future trajectories of pedestrians is essential for\nautonomous systems but remains a challenging task due to the need for\nadaptability in different environments and domains. A common approach involves\ncollecting scenario-specific data and performing fine-tuning via\nbackpropagation. However, this process is often impractical on edge devices due\nto constrained computational resources. To address this challenge, we introduce\nTrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory\nprediction that enables rapid adaptation without fine-tuning on the\nscenario-specific data. We propose a spatio-temporal similarity-based example\nselection (STES) method that selects relevant examples from previously observed\ntrajectories within the same scene by identifying similar motion patterns at\ncorresponding locations. To further refine this selection, we introduce\nprediction-guided example selection (PG-ES), which selects examples based on\nboth the past trajectory and the predicted future trajectory, rather than\nrelying solely on the past trajectory. This approach allows the model to\naccount for long-term dynamics when selecting examples. Finally, instead of\nrelying on small real-world datasets with limited scenario diversity, we train\nour model on a large-scale synthetic dataset to enhance its prediction ability\nby leveraging in-context examples. Extensive experiments demonstrate that\nTrajICL achieves remarkable adaptation across both in-domain and cross-domain\nscenarios, outperforming even fine-tuned approaches across multiple public\nbenchmarks. The code will be released at\nhttps://fujiry0.github.io/TrajICL-project-page.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00507", "pdf": "https://arxiv.org/pdf/2506.00507", "abs": "https://arxiv.org/abs/2506.00507", "authors": ["Dohyun Lee", "Seungil Chad Lee", "Chanwoo Yang", "Yujin Baek", "Jaegul Choo"], "title": "Exploring In-context Example Generation for Machine Translation", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large language models (LLMs) have demonstrated strong performance across\nvarious tasks, leveraging their exceptional in-context learning ability with\nonly a few examples. Accordingly, the selection of optimal in-context examples\nhas been actively studied in the field of machine translation. However, these\nstudies presuppose the presence of a demonstration pool with human-annotated\npairs, making them less applicable to low-resource languages where such an\nassumption is challenging to meet. To overcome this limitation, this paper\nexplores the research direction of in-context example generation for machine\ntranslation. Specifically, we propose Demonstration Augmentation for\nTranslation (DAT), a simple yet effective approach that generates example pairs\nwithout relying on any external resources. This method builds upon two prior\ncriteria, relevance and diversity, which have been highlighted in previous work\nas key factors for in-context example selection. Through experiments and\nanalysis on low-resource languages where human-annotated pairs are scarce, we\nshow that DAT achieves superior translation quality compared to the baselines.\nFurthermore, we investigate the potential of progressively accumulating\ngenerated pairs during test time to build and reuse a demonstration pool. Our\nimplementation is publicly available at https://github.com/aiclaudev/DAT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00908", "pdf": "https://arxiv.org/pdf/2506.00908", "abs": "https://arxiv.org/abs/2506.00908", "authors": ["Xianbing Sun", "Yan Hong", "Jiahui Zhan", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Liqing Zhang", "Jianfu Zhang"], "title": "DS-VTON: High-Quality Virtual Try-on via Disentangled Dual-Scale Generation", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent progress, most existing virtual try-on methods still struggle\nto simultaneously address two core challenges: accurately aligning the garment\nimage with the target human body, and preserving fine-grained garment textures\nand patterns. In this paper, we propose DS-VTON, a dual-scale virtual try-on\nframework that explicitly disentangles these objectives for more effective\nmodeling. DS-VTON consists of two stages: the first stage generates a\nlow-resolution try-on result to capture the semantic correspondence between\ngarment and body, where reduced detail facilitates robust structural alignment.\nThe second stage introduces a residual-guided diffusion process that\nreconstructs high-resolution outputs by refining the residual between the two\nscales, focusing on texture fidelity. In addition, our method adopts a fully\nmask-free generation paradigm, eliminating reliance on human parsing maps or\nsegmentation masks. By leveraging the semantic priors embedded in pretrained\ndiffusion models, this design more effectively preserves the person's\nappearance and geometric consistency. Extensive experiments demonstrate that\nDS-VTON achieves state-of-the-art performance in both structural alignment and\ntexture preservation across multiple standard virtual try-on benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00956", "pdf": "https://arxiv.org/pdf/2506.00956", "abs": "https://arxiv.org/abs/2506.00956", "authors": ["Geonu Lee", "Yujeong Oh", "Geonhui Jang", "Soyoung Lee", "Jeonghyo Song", "Sungmin Cha", "YoungJoon Yoo"], "title": "Continual-MEGA: A Large-scale Benchmark for Generalizable Continual Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we introduce a new benchmark for continual learning in anomaly\ndetection, aimed at better reflecting real-world deployment scenarios. Our\nbenchmark, Continual-MEGA, includes a large and diverse dataset that\nsignificantly expands existing evaluation settings by combining carefully\ncurated existing datasets with our newly proposed dataset, ContinualAD. In\naddition to standard continual learning with expanded quantity, we propose a\nnovel scenario that measures zero-shot generalization to unseen classes, those\nnot observed during continual adaptation. This setting poses a new problem\nsetting that continual adaptation also enhances zero-shot performance. We also\npresent a unified baseline algorithm that improves robustness in few-shot\ndetection and maintains strong generalization. Through extensive evaluations,\nwe report three key findings: (1) existing methods show substantial room for\nimprovement, particularly in pixel-level defect localization; (2) our proposed\nmethod consistently outperforms prior approaches; and (3) the newly introduced\nContinualAD dataset enhances the performance of strong anomaly detection\nmodels. We release the benchmark and code in\nhttps://github.com/Continual-Mega/Continual-Mega.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00979", "pdf": "https://arxiv.org/pdf/2506.00979", "abs": "https://arxiv.org/abs/2506.00979", "authors": ["Wayne Zhang", "Changjiang Jiang", "Zhonghao Zhang", "Chenyang Si", "Fengchang Yu", "Wei Peng"], "title": "IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection", "categories": ["cs.CV", "cs.AI"], "comment": "20pages,13figures,7 tables", "summary": "The rapid advancement of Artificial Intelligence Generated Content (AIGC) in\nvisual domains has resulted in highly realistic synthetic images and videos,\ndriven by sophisticated generative frameworks such as diffusion-based\narchitectures. While these breakthroughs open substantial opportunities, they\nsimultaneously raise critical concerns about content authenticity and\nintegrity. Many current AIGC detection methods operate as black-box binary\nclassifiers, which offer limited interpretability, and no approach supports\ndetecting both images and videos in a unified framework. This dual limitation\ncompromises model transparency, reduces trustworthiness, and hinders practical\ndeployment. To address these challenges, we introduce IVY-FAKE , a novel,\nunified, and large-scale dataset specifically designed for explainable\nmultimodal AIGC detection. Unlike prior benchmarks, which suffer from\nfragmented modality coverage and sparse annotations, IVY-FAKE contains over\n150,000 richly annotated training samples (images and videos) and 18,700\nevaluation examples, each accompanied by detailed natural-language reasoning\nbeyond simple binary labels. Building on this, we propose Ivy Explainable\nDetector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture\nthat jointly performs explainable detection for both image and video content.\nOur unified vision-language model achieves state-of-the-art performance across\nmultiple image and video detection benchmarks, highlighting the significant\nadvancements enabled by our dataset and modeling framework. Our data is\npublicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00996", "pdf": "https://arxiv.org/pdf/2506.00996", "abs": "https://arxiv.org/abs/2506.00996", "authors": ["Kinam Kim", "Junha Hyung", "Jaegul Choo"], "title": "Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models", "categories": ["cs.CV"], "comment": "project page: https://kinam0252.github.io/TIC-FT/", "summary": "Recent advances in text-to-video diffusion models have enabled high-quality\nvideo synthesis, but controllable generation remains challenging, particularly\nunder limited data and compute. Existing fine-tuning methods for conditional\ngeneration often rely on external encoders or architectural modifications,\nwhich demand large datasets and are typically restricted to spatially aligned\nconditioning, limiting flexibility and scalability. In this work, we introduce\nTemporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach\nfor adapting pretrained video diffusion models to diverse conditional\ngeneration tasks. Our key idea is to concatenate condition and target frames\nalong the temporal axis and insert intermediate buffer frames with\nprogressively increasing noise levels. These buffer frames enable smooth\ntransitions, aligning the fine-tuning process with the pretrained model's\ntemporal dynamics. TIC-FT requires no architectural changes and achieves strong\nperformance with as few as 10-30 training samples. We validate our method\nacross a range of tasks, including image-to-video and video-to-video\ngeneration, using large-scale base models such as CogVideoX-5B and Wan-14B.\nExtensive experiments show that TIC-FT outperforms existing baselines in both\ncondition fidelity and visual quality, while remaining highly efficient in both\ntraining and inference. For additional results, visit\nhttps://kinam0252.github.io/TIC-FT/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01040", "pdf": "https://arxiv.org/pdf/2506.01040", "abs": "https://arxiv.org/abs/2506.01040", "authors": ["Zuzheng Kuang", "Haixia Bi", "Chen Xu", "Jian Sun"], "title": "ECP-Mamba: An Efficient Multi-scale Self-supervised Contrastive Learning Method with State Space Model for PolSAR Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Recently, polarimetric synthetic aperture radar (PolSAR) image classification\nhas been greatly promoted by deep neural networks. However,current deep\nlearning-based PolSAR classification methods encounter difficulties due to its\ndependence on extensive labeled data and the computational inefficiency of\narchitectures like Transformers. This paper presents ECP-Mamba, an efficient\nframework integrating multi-scale self-supervised contrastive learning with a\nstate space model (SSM) backbone. Specifically, ECP-Mamba addresses annotation\nscarcity through a multi-scale predictive pretext task based on local-to-global\nfeature correspondences, which uses a simplified self-distillation paradigm\nwithout negative sample pairs. To enhance computational efficiency,the Mamba\narchitecture (a selective SSM) is first tailored for pixel-wise PolSAR\nclassification task by designing a spiral scan strategy. This strategy\nprioritizes causally relevant features near the central pixel, leveraging the\nlocalized nature of pixel-wise classification tasks. Additionally, the\nlightweight Cross Mamba module is proposed to facilitates complementary\nmulti-scale feature interaction with minimal overhead. Extensive experiments\nacross four benchmark datasets demonstrate ECP-Mamba's effectiveness in\nbalancing high accuracy with resource efficiency. On the Flevoland 1989\ndataset, ECP-Mamba achieves state-of-the-art performance with an overall\naccuracy of 99.70%, average accuracy of 99.64% and Kappa coefficient of\n99.62e-2. Our code will be available at\nhttps://github.com/HaixiaBi1982/ECP_Mamba.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "kappa", "accuracy"], "score": 5}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01064", "pdf": "https://arxiv.org/pdf/2506.01064", "abs": "https://arxiv.org/abs/2506.01064", "authors": ["Yudong Zhang", "Ruobing Xie", "Yiqing Huang", "Jiansheng Chen", "Xingwu Sun", "Zhanhui Kang", "Di Wang", "Yu Wang"], "title": "Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in large vision-language models (LVLMs) have showcased their\nremarkable capabilities across a wide range of multimodal vision-language\ntasks. However, these models remain vulnerable to visual adversarial attacks,\nwhich can substantially compromise their performance. Despite their potential\nimpact, the development of effective methods for purifying such adversarial\nexamples has received relatively limited attention. In this paper, we introduce\nF3, a novel adversarial purification framework that employs a counterintuitive\n\"fighting fire with fire\" strategy: intentionally introducing simple\nperturbations to adversarial examples to mitigate their harmful effects.\nSpecifically, F3 leverages cross-modal attentions derived from randomly\nperturbed adversary examples as reference targets. By injecting noise into\nthese adversarial examples, F3 effectively refines their attention, resulting\nin cleaner and more reliable model outputs. Remarkably, this seemingly\nparadoxical approach of employing noise to counteract adversarial attacks\nyields impressive purification results. Furthermore, F3 offers several distinct\nadvantages: it is training-free and straightforward to implement, and exhibits\nsignificant computational efficiency improvements compared to existing\npurification methods. These attributes render F3 particularly suitable for\nlarge-scale industrial applications where both robust performance and\noperational efficiency are critical priorities. The code will be made publicly\navailable.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01073", "pdf": "https://arxiv.org/pdf/2506.01073", "abs": "https://arxiv.org/abs/2506.01073", "authors": ["Mingzhe Hu", "Yuan Gao", "Yuheng Li", "Ricahrd LJ Qiu", "Chih-Wei Chang", "Keyur D. Shah", "Priyanka Kapoor", "Beth Bradshaw", "Yuan Shao", "Justin Roper", "Jill Remick", "Zhen Tian", "Xiaofeng Yang"], "title": "A Large Convolutional Neural Network for Clinical Target and Multi-organ Segmentation in Gynecologic Brachytherapy with Multi-stage Learning", "categories": ["cs.CV"], "comment": null, "summary": "Purpose: Accurate segmentation of clinical target volumes (CTV) and\norgans-at-risk is crucial for optimizing gynecologic brachytherapy (GYN-BT)\ntreatment planning. However, anatomical variability, low soft-tissue contrast\nin CT imaging, and limited annotated datasets pose significant challenges. This\nstudy presents GynBTNet, a novel multi-stage learning framework designed to\nenhance segmentation performance through self-supervised pretraining and\nhierarchical fine-tuning strategies. Methods: GynBTNet employs a three-stage\ntraining strategy: (1) self-supervised pretraining on large-scale CT datasets\nusing sparse submanifold convolution to capture robust anatomical\nrepresentations, (2) supervised fine-tuning on a comprehensive multi-organ\nsegmentation dataset to refine feature extraction, and (3) task-specific\nfine-tuning on a dedicated GYN-BT dataset to optimize segmentation performance\nfor clinical applications. The model was evaluated against state-of-the-art\nmethods using the Dice Similarity Coefficient (DSC), 95th percentile Hausdorff\nDistance (HD95), and Average Surface Distance (ASD). Results: Our GynBTNet\nachieved superior segmentation performance, significantly outperforming nnU-Net\nand Swin-UNETR. Notably, it yielded a DSC of 0.837 +/- 0.068 for CTV, 0.940 +/-\n0.052 for the bladder, 0.842 +/- 0.070 for the rectum, and 0.871 +/- 0.047 for\nthe uterus, with reduced HD95 and ASD compared to baseline models.\nSelf-supervised pretraining led to consistent performance improvements,\nparticularly for structures with complex boundaries. However, segmentation of\nthe sigmoid colon remained challenging, likely due to anatomical ambiguities\nand inter-patient variability. Statistical significance analysis confirmed that\nGynBTNet's improvements were significant compared to baseline models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00722", "pdf": "https://arxiv.org/pdf/2506.00722", "abs": "https://arxiv.org/abs/2506.00722", "authors": ["Siddhant Arora", "Jinchuan Tian", "Hayato Futami", "Jee-weon Jung", "Jiatong Shi", "Yosuke Kashiwagi", "Emiru Tsunoo", "Shinji Watanabe"], "title": "Chain-of-Thought Training for Open E2E Spoken Dialogue Systems", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Unlike traditional cascaded pipelines, end-to-end (E2E) spoken dialogue\nsystems preserve full differentiability and capture non-phonemic information,\nmaking them well-suited for modeling spoken interactions. However, existing E2E\napproaches often require large-scale training data and generates responses\nlacking semantic coherence. We propose a simple yet effective strategy\nleveraging a chain-of-thought (CoT) formulation, ensuring that training on\nconversational data remains closely aligned with the multimodal language model\n(LM)'s pre-training on speech recognition~(ASR), text-to-speech synthesis\n(TTS), and text LM tasks. Our method achieves over 1.5 ROUGE-1 improvement over\nthe baseline, successfully training spoken dialogue systems on publicly\navailable human-human conversation datasets, while being compute-efficient\nenough to train on just 300 hours of public human-human conversation data, such\nas the Switchboard. We will publicly release our models and training code.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01085", "pdf": "https://arxiv.org/pdf/2506.01085", "abs": "https://arxiv.org/abs/2506.01085", "authors": ["Shivam Chandhok", "Qian Yang", "Oscar Manas", "Kanishk Jain", "Leonid Sigal", "Aishwarya Agrawal"], "title": "Learning What Matters: Prioritized Concept Learning via Relative Error-driven Sample Selection", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint", "summary": "Instruction tuning has been central to the success of recent vision-language\nmodels (VLMs), but it remains expensive-requiring large-scale datasets,\nhigh-quality annotations, and large compute budgets. We propose PRioritized\ncOncept learninG via Relative Error-driven Sample Selection (PROGRESS), a data-\nand compute-efficient framework that enables VLMs to dynamically select what to\nlearn next based on their evolving needs during training. At each stage, the\nmodel tracks its learning progress across skills and selects the most\ninformative samples-those it has not already mastered and that are not too\ndifficult to learn at the current stage of training. This strategy effectively\ncontrols skill acquisition and the order in which skills are learned.\nSpecifically, we sample from skills showing the highest learning progress,\nprioritizing those with the most rapid improvement. Unlike prior methods,\nPROGRESS requires no upfront answer annotations, queries answers only on a need\nbasis, avoids reliance on additional supervision from auxiliary VLMs, and does\nnot require compute-heavy gradient computations for data selection. Experiments\nacross multiple instruction-tuning datasets of varying scales demonstrate that\nPROGRESS consistently outperforms state-of-the-art baselines with much less\ndata and supervision. Additionally, we show strong cross-architecture\ngeneralization and transferability to larger models, validating PROGRESS as a\nscalable solution for efficient learning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00740", "pdf": "https://arxiv.org/pdf/2506.00740", "abs": "https://arxiv.org/abs/2506.00740", "authors": ["Harveen Singh Chadha", "Aswin Shanmugam Subramanian", "Vikas Joshi", "Shubham Bansal", "Jian Xue", "Rupeshkumar Mehta", "Jinyu Li"], "title": "Length Aware Speech Translation for Video Dubbing", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "This paper was accepted to Interspeech 2025", "summary": "In video dubbing, aligning translated audio with the source audio is a\nsignificant challenge. Our focus is on achieving this efficiently, tailored for\nreal-time, on-device video dubbing scenarios. We developed a phoneme-based\nend-to-end length-sensitive speech translation (LSST) model, which generates\ntranslations of varying lengths short, normal, and long using predefined tags.\nAdditionally, we introduced length-aware beam search (LABS), an efficient\napproach to generate translations of different lengths in a single decoding\npass. This approach maintained comparable BLEU scores compared to a baseline\nwithout length awareness while significantly enhancing synchronization quality\nbetween source and target audio, achieving a mean opinion score (MOS) gain of\n0.34 for Spanish and 0.65 for Korean, respectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["beam search"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01118", "pdf": "https://arxiv.org/pdf/2506.01118", "abs": "https://arxiv.org/abs/2506.01118", "authors": ["Pimchanok Sukjai", "Apiradee Boonmee"], "title": "Revolutionizing Radiology Workflow with Factual and Efficient CXR Report Generation", "categories": ["cs.CV"], "comment": null, "summary": "The escalating demand for medical image interpretation underscores the\ncritical need for advanced artificial intelligence solutions to enhance the\nefficiency and accuracy of radiological diagnoses. This paper introduces\nCXR-PathFinder, a novel Large Language Model (LLM)-centric foundation model\nspecifically engineered for automated chest X-ray (CXR) report generation. We\npropose a unique training paradigm, Clinician-Guided Adversarial Fine-Tuning\n(CGAFT), which meticulously integrates expert clinical feedback into an\nadversarial learning framework to mitigate factual inconsistencies and improve\ndiagnostic precision. Complementing this, our Knowledge Graph Augmentation\nModule (KGAM) acts as an inference-time safeguard, dynamically verifying\ngenerated medical statements against authoritative knowledge bases to minimize\nhallucinations and ensure standardized terminology. Leveraging a comprehensive\ndataset of millions of paired CXR images and expert reports, our experiments\ndemonstrate that CXR-PathFinder significantly outperforms existing\nstate-of-the-art medical vision-language models across various quantitative\nmetrics, including clinical accuracy (Macro F1 (14): 46.5, Micro F1 (14):\n59.5). Furthermore, blinded human evaluation by board-certified radiologists\nconfirms CXR-PathFinder's superior clinical utility, completeness, and\naccuracy, establishing its potential as a reliable and efficient aid for\nradiological practice. The developed method effectively balances high\ndiagnostic fidelity with computational efficiency, providing a robust solution\nfor automated medical report generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01130", "pdf": "https://arxiv.org/pdf/2506.01130", "abs": "https://arxiv.org/abs/2506.01130", "authors": ["Yiliang Chen", "Zhixi Li", "Cheng Xu", "Alex Qinyang Liu", "Xuemiao Xu", "Jeremy Yuen-Chun Teoh", "Shengfeng He", "Jing Qin"], "title": "ProstaTD: A Large-scale Multi-source Dataset for Structured Surgical Triplet Detection", "categories": ["cs.CV"], "comment": null, "summary": "Surgical triplet detection has emerged as a pivotal task in surgical video\nanalysis, with significant implications for performance assessment and the\ntraining of novice surgeons. However, existing datasets such as CholecT50\nexhibit critical limitations: they lack precise spatial bounding box\nannotations, provide inconsistent and clinically ungrounded temporal labels,\nand rely on a single data source, which limits model generalizability.To\naddress these shortcomings, we introduce ProstaTD, a large-scale,\nmulti-institutional dataset for surgical triplet detection, developed from the\ntechnically demanding domain of robot-assisted prostatectomy. ProstaTD offers\nclinically defined temporal boundaries and high-precision bounding box\nannotations for each structured triplet action. The dataset comprises 60,529\nvideo frames and 165,567 annotated triplet instances, collected from 21\nsurgeries performed across multiple institutions, reflecting a broad range of\nsurgical practices and intraoperative conditions. The annotation process was\nconducted under rigorous medical supervision and involved more than 50\ncontributors, including practicing surgeons and medically trained annotators,\nthrough multiple iterative phases of labeling and verification. ProstaTD is the\nlargest and most diverse surgical triplet dataset to date, providing a robust\nfoundation for fair benchmarking, the development of reliable surgical AI\nsystems, and scalable tools for procedural training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00784", "pdf": "https://arxiv.org/pdf/2506.00784", "abs": "https://arxiv.org/abs/2506.00784", "authors": ["Shaily Bhatt", "Tal August", "Maria Antoniak"], "title": "Research Borderlands: Analysing Writing Across Research Cultures", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Improving cultural competence of language technologies is important. However\nmost recent works rarely engage with the communities they study, and instead\nrely on synthetic setups and imperfect proxies of culture. In this work, we\ntake a human-centered approach to discover and measure language-based cultural\nnorms, and cultural competence of LLMs. We focus on a single kind of culture,\nresearch cultures, and a single task, adapting writing across research\ncultures. Through a set of interviews with interdisciplinary researchers, who\nare experts at moving between cultures, we create a framework of structural,\nstylistic, rhetorical, and citational norms that vary across research cultures.\nWe operationalise these features with a suite of computational metrics and use\nthem for (a) surfacing latent cultural norms in human-written research papers\nat scale; and (b) highlighting the lack of cultural competence of LLMs, and\ntheir tendency to homogenise writing. Overall, our work illustrates the\nefficacy of a human-centered approach to measuring cultural norms in\nhuman-written and LLM-generated texts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00789", "pdf": "https://arxiv.org/pdf/2506.00789", "abs": "https://arxiv.org/abs/2506.00789", "authors": ["Yixiao Zeng", "Tianyu Cao", "Danqing Wang", "Xinran Zhao", "Zimeng Qiu", "Morteza Ziyadi", "Tongshuang Wu", "Lei Li"], "title": "RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances recency and factuality in\nanswers. However, existing evaluations rarely test how well these systems cope\nwith real-world noise, conflicting between internal and external retrieved\ncontexts, or fast-changing facts. We introduce Retrieval-Aware Robustness\nEvaluation (RARE), a unified framework and large-scale benchmark that jointly\nstress-tests query and document perturbations over dynamic, time-sensitive\ncorpora. One of the central features of RARE is a knowledge-graph-driven\nsynthesis pipeline (RARE-Get) that automatically extracts single and multi-hop\nrelations from the customized corpus and generates multi-level question sets\nwithout manual intervention. Leveraging this pipeline, we construct a dataset\n(RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and\npolicy documents and 48,322 questions whose distribution evolves as the\nunderlying sources change. To quantify resilience, we formalize\nretrieval-conditioned robustness metrics (RARE-Met) that capture a model's\nability to remain correct or recover when queries, documents, or real-world\nretrieval results are systematically altered. Our results show that RAG systems\nexhibit surprising vulnerability to perturbations, with document robustness\nconsistently being the weakest point regardless of generator size or\narchitecture. RAG systems consistently show lower robustness on multi-hop\nqueries than single-hop queries across all domains.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "factuality"], "score": 4}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01214", "pdf": "https://arxiv.org/pdf/2506.01214", "abs": "https://arxiv.org/abs/2506.01214", "authors": ["Ali Zia", "Renuka Sharma", "Abdelwahed Khamis", "Xuesong Li", "Muhammad Husnain", "Numan Shafi", "Saeed Anwar", "Sabine Schmoelzl", "Eric Stone", "Lars Petersson", "Vivien Rolland"], "title": "A Review on Coarse to Fine-Grained Animal Action Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This review provides an in-depth exploration of the field of animal action\nrecognition, focusing on coarse-grained (CG) and fine-grained (FG) techniques.\nThe primary aim is to examine the current state of research in animal behaviour\nrecognition and to elucidate the unique challenges associated with recognising\nsubtle animal actions in outdoor environments. These challenges differ\nsignificantly from those encountered in human action recognition due to factors\nsuch as non-rigid body structures, frequent occlusions, and the lack of\nlarge-scale, annotated datasets. The review begins by discussing the evolution\nof human action recognition, a more established field, highlighting how it\nprogressed from broad, coarse actions in controlled settings to the demand for\nfine-grained recognition in dynamic environments. This shift is particularly\nrelevant for animal action recognition, where behavioural variability and\nenvironmental complexity present unique challenges that human-centric models\ncannot fully address. The review then underscores the critical differences\nbetween human and animal action recognition, with an emphasis on high\nintra-species variability, unstructured datasets, and the natural complexity of\nanimal habitats. Techniques like spatio-temporal deep learning frameworks\n(e.g., SlowFast) are evaluated for their effectiveness in animal behaviour\nanalysis, along with the limitations of existing datasets. By assessing the\nstrengths and weaknesses of current methodologies and introducing a\nrecently-published dataset, the review outlines future directions for advancing\nfine-grained action recognition, aiming to improve accuracy and\ngeneralisability in behaviour analysis across species.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01331", "pdf": "https://arxiv.org/pdf/2506.01331", "abs": "https://arxiv.org/abs/2506.01331", "authors": ["Jinjin Zhang", "Qiuyu Huang", "Junjie Liu", "Xiefan Guo", "Di Huang"], "title": "Ultra-High-Resolution Image Synthesis: Data, Method and Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Ultra-high-resolution image synthesis holds significant potential, yet\nremains an underexplored challenge due to the absence of standardized\nbenchmarks and computational constraints. In this paper, we establish\nAesthetic-4K, a meticulously curated dataset containing dedicated training and\nevaluation subsets specifically designed for comprehensive research on\nultra-high-resolution image synthesis. This dataset consists of high-quality 4K\nimages accompanied by descriptive captions generated by GPT-4o. Furthermore, we\npropose Diffusion-4K, an innovative framework for the direct generation of\nultra-high-resolution images. Our approach incorporates the Scale Consistent\nVariational Auto-Encoder (SC-VAE) and Wavelet-based Latent Fine-tuning (WLF),\nwhich are designed for efficient visual token compression and the capture of\nintricate details in ultra-high-resolution images, thereby facilitating direct\ntraining with photorealistic 4K data. This method is applicable to various\nlatent diffusion models and demonstrates its efficacy in synthesizing highly\ndetailed 4K images. Additionally, we propose novel metrics, namely the GLCM\nScore and Compression Ratio, to assess the texture richness and fine details in\nlocal patches, in conjunction with holistic measures such as FID, Aesthetics,\nand CLIPScore, enabling a thorough and multifaceted evaluation of\nultra-high-resolution image synthesis. Consequently, Diffusion-4K achieves\nimpressive performance in ultra-high-resolution image synthesis, particularly\nwhen powered by state-of-the-art large-scale diffusion models (eg, Flux-12B).\nThe source code is publicly available at\nhttps://github.com/zhang0jhon/diffusion-4k.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01349", "pdf": "https://arxiv.org/pdf/2506.01349", "abs": "https://arxiv.org/abs/2506.01349", "authors": ["Yuho Shoji", "Takahiro Toizumi", "Atsushi Ito"], "title": "Target Driven Adaptive Loss For Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "We propose a target driven adaptive (TDA) loss to enhance the performance of\ninfrared small target detection (IRSTD). Prior works have used loss functions,\nsuch as binary cross-entropy loss and IoU loss, to train segmentation models\nfor IRSTD. Minimizing these loss functions guides models to extract pixel-level\nfeatures or global image context. However, they have two issues: improving\ndetection performance for local regions around the targets and enhancing\nrobustness to small scale and low local contrast. To address these issues, the\nproposed TDA loss introduces a patch-based mechanism, and an adaptive\nadjustment strategy to scale and local contrast. The proposed TDA loss leads\nthe model to focus on local regions around the targets and pay particular\nattention to targets with smaller scales and lower local contrast. We evaluate\nthe proposed method on three datasets for IRSTD. The results demonstrate that\nthe proposed TDA loss achieves better detection performance than existing\nlosses on these datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01366", "pdf": "https://arxiv.org/pdf/2506.01366", "abs": "https://arxiv.org/abs/2506.01366", "authors": ["Cong Guan", "Osamu Yoshie"], "title": "CLIP-driven rain perception: Adaptive deraining with pattern-aware network routing and mask-guided cross-attention", "categories": ["cs.CV"], "comment": null, "summary": "Existing deraining models process all rainy images within a single network.\nHowever, different rain patterns have significant variations, which makes it\nchallenging for a single network to handle diverse types of raindrops and\nstreaks. To address this limitation, we propose a novel CLIP-driven rain\nperception network (CLIP-RPN) that leverages CLIP to automatically perceive\nrain patterns by computing visual-language matching scores and adaptively\nrouting to sub-networks to handle different rain patterns, such as varying\nraindrop densities, streak orientations, and rainfall intensity. CLIP-RPN\nestablishes semantic-aware rain pattern recognition through CLIP's cross-modal\nvisual-language alignment capabilities, enabling automatic identification of\nprecipitation characteristics across different rain scenarios. This rain\npattern awareness drives an adaptive subnetwork routing mechanism where\nspecialized processing branches are dynamically activated based on the detected\nrain type, significantly enhancing the model's capacity to handle diverse\nrainfall conditions. Furthermore, within sub-networks of CLIP-RPN, we introduce\na mask-guided cross-attention mechanism (MGCA) that predicts precise rain masks\nat multi-scale to facilitate contextual interactions between rainy regions and\nclean background areas by cross-attention. We also introduces a dynamic loss\nscheduling mechanism (DLS) to adaptively adjust the gradients for the\noptimization process of CLIP-RPN. Compared with the commonly used $l_1$ or\n$l_2$ loss, DLS is more compatible with the inherent dynamics of the network\ntraining process, thus achieving enhanced outcomes. Our method achieves\nstate-of-the-art performance across multiple datasets, particularly excelling\nin complex mixed datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00883", "pdf": "https://arxiv.org/pdf/2506.00883", "abs": "https://arxiv.org/abs/2506.00883", "authors": ["Farong Wen", "Yijin Guo", "Junying Wang", "Jiaohao Xiao", "Yingjie Zhou", "Chunyi Li", "Zicheng Zhang", "Guangtao Zhai"], "title": "Improve MLLM Benchmark Efficiency through Interview", "categories": ["cs.CL"], "comment": null, "summary": "The rapid development of Multimodal Large Language Models (MLLM) has led to a\nwide range of MLLM applications, and a number of benchmark datasets have sprung\nup in order to assess MLLM abilities. However, full-coverage Q&A testing on\nlarge-scale data is resource-intensive and time-consuming. To address this\nissue, we propose the MLLM Interview (MITV) strategy, which aims to quickly\nobtain MLLM performance metrics by quizzing fewer question. First, First, we\nconstructed the interview dataset, which was built on an existing MLLM\nassessment dataset, by adding difficulty labels based on the performance of\nsome typical MLLMs in this dataset. Second, we propose an MLLM Interview\nstrategy, which obtains an initial performance situation of the large model by\nquizzing a small number of topics and then continuously tries to test the\nmodel's limits. Through extensive experiments, the result shows that the MITV\nstrategy proposed in this paper performs well on MLLM benchmark datasets, and\nit is able to obtain the model evaluation capability faster through a small\nnumber of questions and answers.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01380", "pdf": "https://arxiv.org/pdf/2506.01380", "abs": "https://arxiv.org/abs/2506.01380", "authors": ["Xinle Cheng", "Tianyu He", "Jiayi Xu", "Junliang Guo", "Di He", "Jiang Bian"], "title": "Playing with Transformer at 30+ FPS via Next-Frame Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autoregressive video models offer distinct advantages over bidirectional\ndiffusion models in creating interactive video content and supporting streaming\napplications with arbitrary duration. In this work, we present Next-Frame\nDiffusion (NFD), an autoregressive diffusion transformer that incorporates\nblock-wise causal attention, enabling iterative sampling and efficient\ninference via parallel token generation within each frame. Nonetheless,\nachieving real-time video generation remains a significant challenge for such\nmodels, primarily due to the high computational cost associated with diffusion\nsampling and the hardware inefficiencies inherent to autoregressive generation.\nTo address this, we introduce two innovations: (1) We extend consistency\ndistillation to the video domain and adapt it specifically for video models,\nenabling efficient inference with few sampling steps; (2) To fully leverage\nparallel computation, motivated by the observation that adjacent frames often\nshare the identical action input, we propose speculative sampling. In this\napproach, the model generates next few frames using current action input, and\ndiscard speculatively generated frames if the input action differs. Experiments\non a large-scale action-conditioned video generation benchmark demonstrate that\nNFD beats autoregressive baselines in terms of both visual quality and sampling\nefficiency. We, for the first time, achieves autoregressive video generation at\nover 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00955", "pdf": "https://arxiv.org/pdf/2506.00955", "abs": "https://arxiv.org/abs/2506.00955", "authors": ["Zhu Li", "Yuqing Zhang", "Xiyuan Gao", "Shekhar Nayak", "Matt Coler"], "title": "Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Sarcasm fundamentally alters meaning through tone and context, yet detecting\nit in speech remains a challenge due to data scarcity. In addition, existing\ndetection systems often rely on multimodal data, limiting their applicability\nin contexts where only speech is available. To address this, we propose an\nannotation pipeline that leverages large language models (LLMs) to generate a\nsarcasm dataset. Using a publicly available sarcasm-focused podcast, we employ\nGPT-4o and LLaMA 3 for initial sarcasm annotations, followed by human\nverification to resolve disagreements. We validate this approach by comparing\nannotation quality and detection performance on a publicly available sarcasm\ndataset using a collaborative gating architecture. Finally, we introduce\nPodSarc, a large-scale sarcastic speech dataset created through this pipeline.\nThe detection model achieves a 73.63% F1 score, demonstrating the dataset's\npotential as a benchmark for sarcasm detection research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01430", "pdf": "https://arxiv.org/pdf/2506.01430", "abs": "https://arxiv.org/abs/2506.01430", "authors": ["Chenxi Xie", "Minghan Li", "Shuai Li", "Yuhui Wu", "Qiaosi Yi", "Lei Zhang"], "title": "DNAEdit: Direct Noise Alignment for Text-Guided Rectified Flow Editing", "categories": ["cs.CV"], "comment": "Project URL: https://xiechenxi99.github.io/DNAEdit", "summary": "Leveraging the powerful generation capability of large-scale pretrained\ntext-to-image models, training-free methods have demonstrated impressive image\nediting results. Conventional diffusion-based methods, as well as recent\nrectified flow (RF)-based methods, typically reverse synthesis trajectories by\ngradually adding noise to clean images, during which the noisy latent at the\ncurrent timestep is used to approximate that at the next timesteps, introducing\naccumulated drift and degrading reconstruction accuracy. Considering the fact\nthat in RF the noisy latent is estimated through direct interpolation between\nGaussian noises and clean images at each timestep, we propose Direct Noise\nAlignment (DNA), which directly refines the desired Gaussian noise in the noise\ndomain, significantly reducing the error accumulation in previous methods.\nSpecifically, DNA estimates the velocity field of the interpolated noised\nlatent at each timestep and adjusts the Gaussian noise by computing the\ndifference between the predicted and expected velocity field. We validate the\neffectiveness of DNA and reveal its relationship with existing RF-based\ninversion methods. Additionally, we introduce a Mobile Velocity Guidance (MVG)\nto control the target prompt-guided generation process, balancing image\nbackground preservation and target object editability. DNA and MVG collectively\nconstitute our proposed method, namely DNAEdit. Finally, we introduce\nDNA-Bench, a long-prompt benchmark, to evaluate the performance of advanced\nimage editing models. Experimental results demonstrate that our DNAEdit\nachieves superior performance to state-of-the-art text-guided editing methods.\nCodes and benchmark will be available at \\href{\nhttps://xiechenxi99.github.io/DNAEdit/}{https://xiechenxi99.github.io/DNAEdit/}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00980", "pdf": "https://arxiv.org/pdf/2506.00980", "abs": "https://arxiv.org/abs/2506.00980", "authors": ["Sina J. Semnani", "Pingyue Zhang", "Wanyue Zhai", "Haozhuo Li", "Ryan Beauchamp", "Trey Billing", "Katayoun Kishi", "Manling Li", "Monica S. Lam"], "title": "LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World", "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "This paper presents LEMONADE, a large-scale conflict event dataset comprising\n39,786 events across 20 languages and 171 countries, with extensive coverage of\nregion-specific entities. LEMONADE is based on a partially reannotated subset\nof the Armed Conflict Location & Event Data (ACLED), which has documented\nglobal conflict events for over a decade.\n  To address the challenge of aggregating multilingual sources for global event\nanalysis, we introduce abstractive event extraction (AEE) and its subtask,\nabstractive entity linking (AEL). Unlike conventional span-based event\nextraction, our approach detects event arguments and entities through holistic\ndocument understanding and normalizes them across the multilingual dataset. We\nevaluate various large language models (LLMs) on these tasks, adapt existing\nzero-shot event extraction systems, and benchmark supervised models.\nAdditionally, we introduce ZEST, a novel zero-shot retrieval-based system for\nAEL.\n  Our best zero-shot system achieves an end-to-end F1 score of 58.3%, with LLMs\noutperforming specialized event extraction models such as GoLLIE. For entity\nlinking, ZEST achieves an F1 score of 45.7%, significantly surpassing OneNet, a\nstate-of-the-art zero-shot baseline that achieves only 23.7%. However, these\nzero-shot results lag behind the best supervised systems by 20.1% and 37.0% in\nthe end-to-end and AEL tasks, respectively, highlighting the need for further\nresearch.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01443", "pdf": "https://arxiv.org/pdf/2506.01443", "abs": "https://arxiv.org/abs/2506.01443", "authors": ["Jakob Schmid", "Azin Jahedi", "Noah Berenguel Senn", "Andrs Bruhn"], "title": "MS-RAFT-3D: A Multi-Scale Architecture for Recurrent Image-Based Scene Flow", "categories": ["cs.CV"], "comment": "ICIP 2025", "summary": "Although multi-scale concepts have recently proven useful for recurrent\nnetwork architectures in the field of optical flow and stereo, they have not\nbeen considered for image-based scene flow so far. Hence, based on a\nsingle-scale recurrent scene flow backbone, we develop a multi-scale approach\nthat generalizes successful hierarchical ideas from optical flow to image-based\nscene flow. By considering suitable concepts for the feature and the context\nencoder, the overall coarse-to-fine framework and the training loss, we succeed\nto design a scene flow approach that outperforms the current state of the art\non KITTI and Spring by 8.7%(3.89 vs. 4.26) and 65.8% (9.13 vs. 26.71),\nrespectively. Our code is available at\nhttps://github.com/cv-stuttgart/MS-RAFT-3D.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00985", "pdf": "https://arxiv.org/pdf/2506.00985", "abs": "https://arxiv.org/abs/2506.00985", "authors": ["Valeriya Goloviznina", "Alexander Sergeev", "Mikhail Melnichenko", "Evgeny Kotelnikov"], "title": "Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction and Clustering", "categories": ["cs.CL"], "comment": "Accepted for CompLing-2025 conference", "summary": "Diary analysis presents challenges, particularly in extracting meaningful\ninformation from large corpora, where traditional methods often fail to deliver\nsatisfactory results. This study introduces a novel method based on Large\nLanguage Models (LLMs) to identify and cluster the various purposes of diary\nwriting. By \"purposes,\" we refer to the intentions behind diary writing, such\nas documenting life events, self-reflection, or practicing language skills. Our\napproach is applied to Soviet-era diaries (1922-1929) from the Prozhito digital\narchive, a rich collection of personal narratives. We evaluate different\nproprietary and open-source LLMs, finding that GPT-4o and o1-mini achieve the\nbest performance, while a template-based baseline is significantly less\neffective. Additionally, we analyze the retrieved purposes based on gender, age\nof the authors, and the year of writing. Furthermore, we examine the types of\nerrors made by the models, providing a deeper understanding of their\nlimitations and potential areas for improvement in future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01466", "pdf": "https://arxiv.org/pdf/2506.01466", "abs": "https://arxiv.org/abs/2506.01466", "authors": ["Shuyu Yang", "Yilun Wang", "Yaxiong Wang", "Li Zhu", "Zhedong Zheng"], "title": "Towards Scalable Video Anomaly Retrieval: A Synthetic Video-Text Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "Video anomaly retrieval aims to localize anomalous events in videos using\nnatural language queries to facilitate public safety. However, existing\ndatasets suffer from severe limitations: (1) data scarcity due to the long-tail\nnature of real-world anomalies, and (2) privacy constraints that impede\nlarge-scale collection. To address the aforementioned issues in one go, we\nintroduce SVTA (Synthetic Video-Text Anomaly benchmark), the first large-scale\ndataset for cross-modal anomaly retrieval, leveraging generative models to\novercome data availability challenges. Specifically, we collect and generate\nvideo descriptions via the off-the-shelf LLM (Large Language Model) covering 68\nanomaly categories, e.g., throwing, stealing, and shooting. These descriptions\nencompass common long-tail events. We adopt these texts to guide the video\ngenerative model to produce diverse and high-quality videos. Finally, our SVTA\ninvolves 41,315 videos (1.36M frames) with paired captions, covering 30 normal\nactivities, e.g., standing, walking, and sports, and 68 anomalous events, e.g.,\nfalling, fighting, theft, explosions, and natural disasters. We adopt three\nwidely-used video-text retrieval baselines to comprehensively test our SVTA,\nrevealing SVTA's challenging nature and its effectiveness in evaluating a\nrobust cross-modal retrieval method. SVTA eliminates privacy risks associated\nwith real-world anomaly collection while maintaining realistic scenarios. The\ndataset demo is available at: [https://svta-mm.github.io/SVTA.github.io/].", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01468", "pdf": "https://arxiv.org/pdf/2506.01468", "abs": "https://arxiv.org/abs/2506.01468", "authors": ["Alam Noor", "Luis Almeida", "Mohamed Daoudi", "Kai Li", "Eduardo Tovar"], "title": "Sheep Facial Pain Assessment Under Weighted Graph Neural Networks", "categories": ["cs.CV"], "comment": "2025 19th International Conference on Automatic Face and Gesture\n  Recognition (FG)", "summary": "Accurately recognizing and assessing pain in sheep is key to discern animal\nhealth and mitigating harmful situations. However, such accuracy is limited by\nthe ability to manage automatic monitoring of pain in those animals. Facial\nexpression scoring is a widely used and useful method to evaluate pain in both\nhumans and other living beings. Researchers also analyzed the facial\nexpressions of sheep to assess their health state and concluded that facial\nlandmark detection and pain level prediction are essential. For this purpose,\nwe propose a novel weighted graph neural network (WGNN) model to link sheep's\ndetected facial landmarks and define pain levels. Furthermore, we propose a new\nsheep facial landmarks dataset that adheres to the parameters of the Sheep\nFacial Expression Scale (SPFES). Currently, there is no comprehensive\nperformance benchmark that specifically evaluates the use of graph neural\nnetworks (GNNs) on sheep facial landmark data to detect and measure pain\nlevels. The YOLOv8n detector architecture achieves a mean average precision\n(mAP) of 59.30% with the sheep facial landmarks dataset, among seven other\ndetection models. The WGNN framework has an accuracy of 92.71% for tracking\nmultiple facial parts expressions with the YOLOv8n lightweight on-board device\ndeployment-capable model.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01471", "pdf": "https://arxiv.org/pdf/2506.01471", "abs": "https://arxiv.org/abs/2506.01471", "authors": ["Yiping Li", "Ronald de Jong", "Sahar Nasirihaghighi", "Tim Jaspers", "Romy van Jaarsveld", "Gino Kuiper", "Richard van Hillegersberg", "Fons van der Sommen", "Jelle Ruurda", "Marcel Breeuwer", "Yasmina Al Khalil"], "title": "SemiVT-Surge: Semi-Supervised Video Transformer for Surgical Phase Recognition", "categories": ["cs.CV"], "comment": "Accepted for MICCAI 2025", "summary": "Accurate surgical phase recognition is crucial for computer-assisted\ninterventions and surgical video analysis. Annotating long surgical videos is\nlabor-intensive, driving research toward leveraging unlabeled data for strong\nperformance with minimal annotations. Although self-supervised learning has\ngained popularity by enabling large-scale pretraining followed by fine-tuning\non small labeled subsets, semi-supervised approaches remain largely\nunderexplored in the surgical domain. In this work, we propose a video\ntransformer-based model with a robust pseudo-labeling framework. Our method\nincorporates temporal consistency regularization for unlabeled data and\ncontrastive learning with class prototypes, which leverages both labeled data\nand pseudo-labels to refine the feature space. Through extensive experiments on\nthe private RAMIE (Robot-Assisted Minimally Invasive Esophagectomy) dataset and\nthe public Cholec80 dataset, we demonstrate the effectiveness of our approach.\nBy incorporating unlabeled data, we achieve state-of-the-art performance on\nRAMIE with a 4.9% accuracy increase and obtain comparable results to full\nsupervision while using only 1/4 of the labeled data on Cholec80. Our findings\nestablish a strong benchmark for semi-supervised surgical phase recognition,\npaving the way for future research in this domain.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01047", "pdf": "https://arxiv.org/pdf/2506.01047", "abs": "https://arxiv.org/abs/2506.01047", "authors": ["Phan Anh Duong", "Cat Luong", "Divyesh Bommana", "Tianyu Jiang"], "title": "CHEER-Ekman: Fine-grained Embodied Emotion Classification", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Emotions manifest through physical experiences and bodily reactions, yet\nidentifying such embodied emotions in text remains understudied. We present an\nembodied emotion classification dataset, CHEER-Ekman, extending the existing\nbinary embodied emotion dataset with Ekman's six basic emotion categories.\nUsing automatic best-worst scaling with large language models, we achieve\nperformance superior to supervised approaches on our new dataset. Our\ninvestigation reveals that simplified prompting instructions and\nchain-of-thought reasoning significantly improve emotion recognition accuracy,\nenabling smaller models to achieve competitive performance with larger ones.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01493", "pdf": "https://arxiv.org/pdf/2506.01493", "abs": "https://arxiv.org/abs/2506.01493", "authors": ["Yuya Kobayashi", "Yuhta Takida", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "Efficiency without Compromise: CLIP-aided Text-to-Image GANs with Increased Diversity", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at IJCNN 2025", "summary": "Recently, Generative Adversarial Networks (GANs) have been successfully\nscaled to billion-scale large text-to-image datasets. However, training such\nmodels entails a high training cost, limiting some applications and research\nusage. To reduce the cost, one promising direction is the incorporation of\npre-trained models. The existing method of utilizing pre-trained models for a\ngenerator significantly reduced the training cost compared with the other\nlarge-scale GANs, but we found the model loses the diversity of generation for\na given prompt by a large margin. To build an efficient and high-fidelity\ntext-to-image GAN without compromise, we propose to use two specialized\ndiscriminators with Slicing Adversarial Networks (SANs) adapted for\ntext-to-image tasks. Our proposed model, called SCAD, shows a notable\nenhancement in diversity for a given prompt with better sample fidelity. We\nalso propose to use a metric called Per-Prompt Diversity (PPD) to evaluate the\ndiversity of text-to-image models quantitatively. SCAD achieved a zero-shot FID\ncompetitive with the latest large-scale GANs at two orders of magnitude less\ntraining cost.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01539", "pdf": "https://arxiv.org/pdf/2506.01539", "abs": "https://arxiv.org/abs/2506.01539", "authors": ["Tianjiao Zhang", "Fei Zhang", "Jiangchao Yao", "Ya Zhang", "Yanfeng Wang"], "title": "G4Seg: Generation for Inexact Segmentation Refinement with Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 12 figures, IEEE International Conference on Multimedia &\n  Expo 2025", "summary": "This paper considers the problem of utilizing a large-scale text-to-image\ndiffusion model to tackle the challenging Inexact Segmentation (IS) task.\nUnlike traditional approaches that rely heavily on discriminative-model-based\nparadigms or dense visual representations derived from internal attention\nmechanisms, our method focuses on the intrinsic generative priors in Stable\nDiffusion~(SD). Specifically, we exploit the pattern discrepancies between\noriginal images and mask-conditional generated images to facilitate a\ncoarse-to-fine segmentation refinement by establishing a semantic\ncorrespondence alignment and updating the foreground probability. Comprehensive\nquantitative and qualitative experiments validate the effectiveness and\nsuperiority of our plug-and-play design, underscoring the potential of\nleveraging generation discrepancies to model dense representations and\nencouraging further exploration of generative approaches for solving\ndiscriminative tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01546", "pdf": "https://arxiv.org/pdf/2506.01546", "abs": "https://arxiv.org/abs/2506.01546", "authors": ["Xiaodong Wang", "Zhirong Wu", "Peixi Peng"], "title": "LongDWM: Cross-Granularity Distillation for Building a Long-Term Driving World Model", "categories": ["cs.CV"], "comment": "project homepage: https://wang-xiaodong1899.github.io/longdwm/", "summary": "Driving world models are used to simulate futures by video generation based\non the condition of the current state and actions. However, current models\noften suffer serious error accumulations when predicting the long-term future,\nwhich limits the practical application. Recent studies utilize the Diffusion\nTransformer (DiT) as the backbone of driving world models to improve learning\nflexibility. However, these models are always trained on short video clips\n(high fps and short duration), and multiple roll-out generations struggle to\nproduce consistent and reasonable long videos due to the training-inference\ngap. To this end, we propose several solutions to build a simple yet effective\nlong-term driving world model. First, we hierarchically decouple world model\nlearning into large motion learning and bidirectional continuous motion\nlearning. Then, considering the continuity of driving scenes, we propose a\nsimple distillation method where fine-grained video flows are self-supervised\nsignals for coarse-grained flows. The distillation is designed to improve the\ncoherence of infinite video generation. The coarse-grained and fine-grained\nmodules are coordinated to generate long-term and temporally coherent videos.\nIn the public benchmark NuScenes, compared with the state-of-the-art front-view\nmodel, our model improves FVD by $27\\%$ and reduces inference time by $85\\%$\nfor the video task of generating 110+ frames. More videos (including 90s\nduration) are available at https://Wang-Xiaodong1899.github.io/longdwm/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01156", "pdf": "https://arxiv.org/pdf/2506.01156", "abs": "https://arxiv.org/abs/2506.01156", "authors": ["Nhan Phan", "Mikko Kuronen", "Maria Kautonen", "Riikka Ullakonoja", "Anna von Zansen", "Yaroslav Getman", "Ekaterina Voskoboinik", "Tams Grsz", "Mikko Kurimo"], "title": "Mispronunciation Detection Without L2 Pronunciation Dataset in Low-Resource Setting: A Case Study in Finland Swedish", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025 conference", "summary": "Mispronunciation detection (MD) models are the cornerstones of many language\nlearning applications. Unfortunately, most systems are built for English and\nother major languages, while low-resourced language varieties, such as Finland\nSwedish (FS), lack such tools. In this paper, we introduce our MD model for FS,\ntrained on 89 hours of first language (L1) speakers' spontaneous speech and\ntested on 33 minutes of L2 transcribed read-aloud speech.\n  We trained a multilingual wav2vec 2.0 model with entropy regularization,\nfollowed by temperature scaling and top-k normalization after the inference to\nbetter adapt it for MD. The main novelty of our method lies in its simplicity,\nrequiring minimal L2 data. The process is also language-independent, making it\nsuitable for other low-resource languages. Our proposed algorithm allows us to\nbalance Recall (43.2%) and Precision (29.8%), compared with the baseline\nmodel's Recall (77.5%) and Precision (17.6%).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01586", "pdf": "https://arxiv.org/pdf/2506.01586", "abs": "https://arxiv.org/abs/2506.01586", "authors": ["Zhuohang Dang", "Minnan Luo", "Chengyou Jia", "Hangwei Qian", "Xiaojun Chang", "Ivor W. Tsang"], "title": "Multi-Modal Dataset Distillation in the Wild", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent multi-modal models have shown remarkable versatility in real-world\napplications. However, their rapid development encounters two critical data\nchallenges. First, the training process requires large-scale datasets, leading\nto substantial storage and computational costs. Second, these data are\ntypically web-crawled with inevitable noise, i.e., partially mismatched pairs,\nseverely degrading model performance. To these ends, we propose Multi-modal\ndataset Distillation in the Wild, i.e., MDW, the first framework to distill\nnoisy multi-modal datasets into compact clean ones for effective and efficient\nmodel training. Specifically, MDW introduces learnable fine-grained\ncorrespondences during distillation and adaptively optimizes distilled data to\nemphasize correspondence-discriminative regions, thereby enhancing distilled\ndata's information density and efficacy. Moreover, to capture robust\ncross-modal correspondence prior knowledge from real data, MDW proposes\ndual-track collaborative learning to avoid the risky data noise, alleviating\ninformation loss with certifiable noise tolerance. Extensive experiments\nvalidate MDW's theoretical and empirical efficacy with remarkable scalability,\nsurpassing prior methods by over 15% across various compression ratios,\nhighlighting its appealing practicality for applications with diverse efficacy\nand resource needs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01206", "pdf": "https://arxiv.org/pdf/2506.01206", "abs": "https://arxiv.org/abs/2506.01206", "authors": ["Daewon Choi", "Seunghyuk Oh", "Saket Dingliwal", "Jihoon Tack", "Kyuyoung Kim", "Woomin Song", "Seojin Kim", "Insu Han", "Jinwoo Shin", "Aram Galstyan", "Shubham Katiyar", "Sravan Babu Bodapati"], "title": "Mamba Drafters for Speculative Decoding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding has emerged as a promising approach to accelerating\nlarge language model (LLM) generation using a fast drafter while maintaining\nalignment with the target model's distribution. However, existing approaches\nface a trade-off: external drafters offer flexibility but can suffer from\nslower drafting, while self-speculation methods use drafters tailored to the\ntarget model but require re-training. In this paper, we introduce novel\ndrafters based on Mamba, a state-of-the-art state space model (SSM), as a\nsolution that combines the best aspects of both approaches. By leveraging the\nlinear structure of SSMs, our approach avoids the quadratic complexity inherent\nin traditional Transformer-based methods, enabling faster drafting and lower\nmemory usage while maintaining the flexibility to work across different target\nmodels. We further enhance efficiency with a novel test-time tree search\nalgorithm for generating high-quality draft candidates. Our empirical\nevaluation demonstrates that Mamba-based drafters not only outperform existing\nexternal drafting methods but are also comparable to state-of-the-art\nself-speculation approaches while using less memory and maintaining their\ncross-model adaptability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01667", "pdf": "https://arxiv.org/pdf/2506.01667", "abs": "https://arxiv.org/abs/2506.01667", "authors": ["Yan Shu", "Bin Ren", "Zhitong Xiong", "Danda Pani Paudel", "Luc Van Gool", "Begum Demir", "Nicu Sebe", "Paolo Rota"], "title": "EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation with Large Multimodal Models", "categories": ["cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) have demonstrated strong performance in\nvarious vision-language tasks. However, they often struggle to comprehensively\nunderstand Earth Observation (EO) data, which is critical for monitoring the\nenvironment and the effects of human activity on it. In this work, we present\nEarthMind, a novel vision-language framework for multi-granular and\nmulti-sensor EO data understanding. EarthMind features two core components: (1)\nSpatial Attention Prompting (SAP), which reallocates attention within the LLM\nto enhance pixel-level understanding; and (2) Cross-modal Fusion, which aligns\nheterogeneous modalities into a shared space and adaptively reweighs tokens\nbased on their information density for effective fusion. To facilitate\nmulti-sensor fusion evaluation, we propose EarthMind-Bench, a comprehensive\nbenchmark with over 2,000 human-annotated multi-sensor image-question pairs,\ncovering a wide range of perception and reasoning tasks. Extensive experiments\ndemonstrate the effectiveness of EarthMind. It achieves state-of-the-art\nperformance on EarthMind-Bench, surpassing GPT-4o despite being only 4B in\nscale. Moreover, EarthMind outperforms existing methods on multiple public EO\nbenchmarks, showcasing its potential to handle both multi-granular and\nmulti-sensor challenges in a unified framework.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01215", "pdf": "https://arxiv.org/pdf/2506.01215", "abs": "https://arxiv.org/abs/2506.01215", "authors": ["Woomin Song", "Sai Muralidhar Jayanthi", "Srikanth Ronanki", "Kanthashree Mysore Sathyendra", "Jinwoo Shin", "Aram Galstyan", "Shubham Katiyar", "Sravan Babu Bodapati"], "title": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models increasingly gain popularity in real-world\napplications, processing extremely long contexts, often exceeding the model's\npre-trained context limits, has emerged as a critical challenge. While existing\napproaches to efficient long-context processing show promise, recurrent\ncompression-based methods struggle with information preservation, whereas\nrandom access approaches require substantial memory resources. We introduce\nREFORM, a novel inference framework that efficiently handles long contexts\nthrough a two-phase approach. First, it incrementally processes input chunks\nwhile maintaining a compressed KV cache, constructs cross-layer context\nembeddings, and utilizes early exit strategy for improved efficiency. Second,\nit identifies and gathers essential tokens via similarity matching and\nselectively recomputes the KV cache. Compared to baselines, REFORM achieves\nover 50% and 27% performance gains on RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on Infinite-Bench and MM-NIAH,\ndemonstrating flexibility across diverse tasks and domains. Additionally,\nREFORM reduces inference time by 30% and peak memory usage by 5%, achieving\nboth efficiency and superior performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01674", "pdf": "https://arxiv.org/pdf/2506.01674", "abs": "https://arxiv.org/abs/2506.01674", "authors": ["Yipeng Du", "Tiehan Fan", "Kepan Nan", "Rui Xie", "Penghao Zhou", "Xiang Li", "Jian Yang", "Zhenheng Yang", "Ying Tai"], "title": "MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Despite advancements in Multimodal Large Language Models (MLLMs), their\nproficiency in fine-grained video motion understanding remains critically\nlimited. They often lack inter-frame differencing and tend to average or ignore\nsubtle visual cues. Furthermore, while visual prompting has shown potential in\nstatic images, its application to video's temporal complexities, particularly\nfor fine-grained motion understanding, remains largely unexplored. We\ninvestigate whether inherent capability can be unlocked and boost MLLMs' motion\nperception and enable distinct visual signatures tailored to decouple object\nand camera motion cues. In this study, we introduce MotionSight, a novel\nzero-shot method pioneering object-centric visual spotlight and motion blur as\nvisual prompts to effectively improve fine-grained motion understanding without\ntraining. To convert this into valuable data assets, we curated MotionVid-QA,\nthe first large-scale dataset for fine-grained video motion understanding, with\nhierarchical annotations including SFT and preference data, {\\Theta}(40K) video\nclips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves\nstate-of-the-art open-source performance and competitiveness with commercial\nmodels. In particular, for fine-grained motion understanding we present a novel\nzero-shot technique and a large-scale, high-quality dataset. All the code and\nannotations will be publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01701", "pdf": "https://arxiv.org/pdf/2506.01701", "abs": "https://arxiv.org/abs/2506.01701", "authors": ["Haoru Tan", "Sitong Wu", "Wei Huang", "Shizhen Zhao", "Xiaojuan Qi"], "title": "Data Pruning by Information Maximization", "categories": ["cs.CV", "cs.AI"], "comment": "ICLR 2025", "summary": "In this paper, we present InfoMax, a novel data pruning method, also known as\ncoreset selection, designed to maximize the information content of selected\nsamples while minimizing redundancy. By doing so, InfoMax enhances the overall\ninformativeness of the coreset. The information of individual samples is\nmeasured by importance scores, which capture their influence or difficulty in\nmodel learning. To quantify redundancy, we use pairwise sample similarities,\nbased on the premise that similar samples contribute similarly to the learning\nprocess. We formalize the coreset selection problem as a discrete quadratic\nprogramming (DQP) task, with the objective of maximizing the total information\ncontent, represented as the sum of individual sample contributions minus the\nredundancies introduced by similar samples within the coreset. To ensure\npractical scalability, we introduce an efficient gradient-based solver,\ncomplemented by sparsification techniques applied to the similarity matrix and\ndataset partitioning strategies. This enables InfoMax to seamlessly scale to\ndatasets with millions of samples. Extensive experiments demonstrate the\nsuperior performance of InfoMax in various data pruning tasks, including image\nclassification, vision-language pre-training, and instruction tuning for large\nlanguage models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01724", "pdf": "https://arxiv.org/pdf/2506.01724", "abs": "https://arxiv.org/abs/2506.01724", "authors": ["Tong Wang", "Jiaqi Wang", "Shu Kong"], "title": "Active Learning via Vision-Language Model Adaptation with Open Data", "categories": ["cs.CV"], "comment": "Here is the project webpage: https://leowangtong.github.io/ALOR/", "summary": "Pretrained on web-scale open data, VLMs offer powerful capabilities for\nsolving downstream tasks after being adapted to task-specific labeled data.\nYet, data labeling can be expensive and may demand domain expertise. Active\nLearning (AL) aims to reduce this expense by strategically selecting the most\ninformative data for labeling and model training. Recent AL methods have\nexplored VLMs but have not leveraged publicly available open data, such as\nVLM's pretraining data. In this work, we leverage such data by retrieving\ntask-relevant examples to augment the task-specific examples. As expected,\nincorporating them significantly improves AL. Given that our method exploits\nopen-source VLM and open data, we refer to it as Active Learning with Open\nResources (ALOR). Additionally, most VLM-based AL methods use prompt tuning\n(PT) for model adaptation, likely due to its ability to directly utilize\npretrained parameters and the assumption that doing so reduces the risk of\noverfitting to limited labeled data. We rigorously compare popular adaptation\napproaches, including linear probing (LP), finetuning (FT), and contrastive\ntuning (CT). We reveal two key findings: (1) All adaptation approaches benefit\nfrom incorporating retrieved data, and (2) CT resoundingly outperforms other\napproaches across AL methods. Further analysis of retrieved data reveals a\nnaturally imbalanced distribution of task-relevant classes, exposing inherent\nbiases within the VLM. This motivates our novel Tail First Sampling (TFS)\nstrategy for AL, an embarrassingly simple yet effective method that prioritizes\nsampling data from underrepresented classes to label. Extensive experiments\ndemonstrate that our final method, contrastively finetuning VLM on both\nretrieved and TFS-selected labeled data, significantly outperforms existing\nmethods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01254", "pdf": "https://arxiv.org/pdf/2506.01254", "abs": "https://arxiv.org/abs/2506.01254", "authors": ["Yimin Du"], "title": "Memory-Efficient FastText: A Comprehensive Approach Using Double-Array Trie Structures and Mark-Compact Memory Management", "categories": ["cs.CL"], "comment": "10 pages", "summary": "FastText has established itself as a fundamental algorithm for learning word\nrepresentations, demonstrating exceptional capability in handling\nout-of-vocabulary words through character-level n-gram embeddings. However, its\nhash-based bucketing mechanism introduces critical limitations for large-scale\nindustrial deployment: hash collisions cause semantic drift, and memory\nrequirements become prohibitively expensive when dealing with real-world\nvocabularies containing millions of terms. This paper presents a comprehensive\nmemory optimization framework that fundamentally reimagines FastText's memory\nmanagement through the integration of double-array trie (DA-trie) structures\nand mark-compact garbage collection principles. Our approach leverages the\nlinguistic insight that n-grams sharing common prefixes or suffixes exhibit\nhighly correlated embeddings due to co-occurrence patterns in natural language.\nBy systematically identifying and merging semantically similar embeddings based\non structural relationships, we achieve compression ratios of 4:1 to 10:1 while\nmaintaining near-perfect embedding quality. The algorithm consists of four\nsophisticated phases: prefix trie construction with embedding mapping,\nprefix-based similarity compression, suffix-based similarity compression, and\nmark-compact memory reorganization. Comprehensive experiments on a 30-million\nChinese vocabulary dataset demonstrate memory reduction from over 100GB to\napproximately 30GB with negligible performance degradation. Our industrial\ndeployment results show significant cost reduction, faster loading times, and\nimproved model reliability through the elimination of hash collision artifacts.\nCode and experimental implementations are available at:\nhttps://github.com/initial-d/me_fasttext", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01257", "pdf": "https://arxiv.org/pdf/2506.01257", "abs": "https://arxiv.org/abs/2506.01257", "authors": ["Jiancheng Ye", "Sophie Bronstein", "Jiarui Hai", "Malak Abu Hashish"], "title": "DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "DeepSeek-R1 is a cutting-edge open-source large language model (LLM)\ndeveloped by DeepSeek, showcasing advanced reasoning capabilities through a\nhybrid architecture that integrates mixture of experts (MoE), chain of thought\n(CoT) reasoning, and reinforcement learning. Released under the permissive MIT\nlicense, DeepSeek-R1 offers a transparent and cost-effective alternative to\nproprietary models like GPT-4o and Claude-3 Opus; it excels in structured\nproblem-solving domains such as mathematics, healthcare diagnostics, code\ngeneration, and pharmaceutical research. The model demonstrates competitive\nperformance on benchmarks like the United States Medical Licensing Examination\n(USMLE) and American Invitational Mathematics Examination (AIME), with strong\nresults in pediatric and ophthalmologic clinical decision support tasks. Its\narchitecture enables efficient inference while preserving reasoning depth,\nmaking it suitable for deployment in resource-constrained settings. However,\nDeepSeek-R1 also exhibits increased vulnerability to bias, misinformation,\nadversarial manipulation, and safety failures - especially in multilingual and\nethically sensitive contexts. This survey highlights the model's strengths,\nincluding interpretability, scalability, and adaptability, alongside its\nlimitations in general language fluency and safety alignment. Future research\npriorities include improving bias mitigation, natural language comprehension,\ndomain-specific validation, and regulatory compliance. Overall, DeepSeek-R1\nrepresents a major advance in open, scalable AI, underscoring the need for\ncollaborative governance to ensure responsible and equitable deployment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01263", "pdf": "https://arxiv.org/pdf/2506.01263", "abs": "https://arxiv.org/abs/2506.01263", "authors": ["Yu Nakagome", "Michael Hentschel"], "title": "WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard CTC-based Keyword Spotting and Inter-layer Biasing", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Despite recent advances in end-to-end speech recognition methods, the output\ntends to be biased to the training data's vocabulary, resulting in inaccurate\nrecognition of proper nouns and other unknown terms. To address this issue, we\npropose a method to improve recognition accuracy of such rare words in\nCTC-based models without additional training or text-to-speech systems.\nSpecifically, keyword spotting is performed using acoustic features of\nintermediate layers during inference, and a bias is applied to the subsequent\nlayers of the acoustic model for detected keywords. For keyword detection, we\nadopt a wildcard CTC that is both fast and tolerant of ambiguous matches,\nallowing flexible handling of words that are difficult to match strictly. Since\nthis method does not require retraining of existing models, it can be easily\napplied to even large-scale models. In experiments on Japanese speech\nrecognition, the proposed method achieved a 29% improvement in the F1 score for\nunknown words.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01783", "pdf": "https://arxiv.org/pdf/2506.01783", "abs": "https://arxiv.org/abs/2506.01783", "authors": ["Honglu Zhang", "Zhiqin Fang", "Ningning Zhao", "Saihui Hou", "Long Ma", "Renwang Pei", "Zhaofeng He"], "title": "FaceCoT: A Benchmark Dataset for Face Anti-Spoofing with Chain-of-Thought Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Face Anti-Spoofing (FAS) typically depends on a single visual modality when\ndefending against presentation attacks such as print attacks, screen replays,\nand 3D masks, resulting in limited generalization across devices, environments,\nand attack types. Meanwhile, Multimodal Large Language Models (MLLMs) have\nrecently achieved breakthroughs in image-text understanding and semantic\nreasoning, suggesting that integrating visual and linguistic co-inference into\nFAS can substantially improve both robustness and interpretability. However,\nthe lack of a high-quality vision-language multimodal dataset has been a\ncritical bottleneck. To address this, we introduce FaceCoT (Face\nChain-of-Thought), the first large-scale Visual Question Answering (VQA)\ndataset tailored for FAS. FaceCoT covers 14 spoofing attack types and enriches\nmodel learning with high-quality CoT VQA annotations. Meanwhile, we develop a\ncaption model refined via reinforcement learning to expand the dataset and\nenhance annotation quality. Furthermore, we introduce a CoT-Enhanced\nProgressive Learning (CEPL) strategy to better leverage the CoT data and boost\nmodel performance on FAS tasks. Extensive experiments demonstrate that models\ntrained with FaceCoT and CEPL outperform state-of-the-art methods on multiple\nbenchmark datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "question answering"], "score": 4}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01266", "pdf": "https://arxiv.org/pdf/2506.01266", "abs": "https://arxiv.org/abs/2506.01266", "authors": ["Yuanhe Tian", "Mingjie Deng", "Guoqing Jin", "Yan Song"], "title": "Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 1 figure", "summary": "Existing approaches for Large language model (LLM) detoxification generally\nrely on training on large-scale non-toxic or human-annotated preference data,\ndesigning prompts to instruct the LLM to generate safe content, or modifying\nthe model parameters to remove toxic information, which are computationally\nexpensive, lack robustness, and often compromise LLMs' fluency and contextual\nunderstanding. In this paper, we propose a simple yet effective approach for\nLLM detoxification, which leverages a compact, pre-trained calibration model\nthat guides the detoxification process of a target LLM via a lightweight\nintervention in its generation pipeline. By learning a detoxified embedding\nspace from non-toxic data, the calibration model effectively steers the LLM\naway from generating harmful content. This approach only requires a one-time\ntraining of the calibration model that is able to be seamlessly applied to\nmultiple LLMs without compromising fluency or contextual understanding.\nExperiment results on the benchmark dataset demonstrate that our approach\nreduces toxicity while maintaining reasonable content expression.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01853", "pdf": "https://arxiv.org/pdf/2506.01853", "abs": "https://arxiv.org/abs/2506.01853", "authors": ["Junliang Ye", "Zhengyi Wang", "Ruowen Zhao", "Shenghao Xie", "Jun Zhu"], "title": "ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding", "categories": ["cs.CV"], "comment": "Project page: https://github.com/JAMESYJL/ShapeLLM-Omni", "summary": "Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to\ngrowing appreciation for native multimodal large language models. However, its\nmultimodal capabilities remain confined to images and text. Yet beyond images,\nthe ability to understand and generate 3D content is equally crucial. To\naddress this gap, we propose ShapeLLM-Omni-a native 3D large language model\ncapable of understanding and generating 3D assets and text in any sequence.\nFirst, we train a 3D vector-quantized variational autoencoder (VQVAE), which\nmaps 3D objects into a discrete latent space to achieve efficient and accurate\nshape representation and reconstruction. Building upon the 3D-aware discrete\ntokens, we innovatively construct a large-scale continuous training dataset\nnamed 3D-Alpaca, encompassing generation, comprehension, and editing, thus\nproviding rich resources for future research and training. Finally, by\nperforming instruction-based training of the Qwen-2.5-vl-7B-Instruct model on\nthe 3D-Alpaca dataset. Our work provides an effective attempt at extending\nmultimodal models with basic 3D capabilities, which contributes to future\nresearch in 3D-native AI. Project page:\nhttps://github.com/JAMESYJL/ShapeLLM-Omni", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "Alpaca"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01902", "pdf": "https://arxiv.org/pdf/2506.01902", "abs": "https://arxiv.org/abs/2506.01902", "authors": ["Xinliu Zhong", "Kayhan Batmanghelich", "Li Sun"], "title": "Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination", "categories": ["cs.CV", "cs.CL"], "comment": "6 pages, 1 figure, accepted by 2024 IEEE Conference on Artificial\n  Intelligence (CAI)", "summary": "Vision-language models pre-trained on large scale of unlabeled biomedical\nimages and associated reports learn generalizable semantic representations.\nThese multi-modal representations can benefit various downstream tasks in the\nbiomedical domain. Contrastive learning is widely used to pre-train\nvision-language models for general natural images and associated captions.\nDespite its popularity, we found biomedical texts have complex and\ndomain-specific semantics that are often neglected by common contrastive\nmethods. To address this issue, we propose a novel method, perturbed report\ndiscrimination, for pre-train biomedical vision-language models. First, we\ncurate a set of text perturbation methods that keep the same words, but disrupt\nthe semantic structure of the sentence. Next, we apply different types of\nperturbation to reports, and use the model to distinguish the original report\nfrom the perturbed ones given the associated image. Parallel to this, we\nenhance the sensitivity of our method to higher level of granularity for both\nmodalities by contrasting attention-weighted image sub-regions and sub-words in\nthe image-text pairs. We conduct extensive experiments on multiple downstream\ntasks, and our method outperforms strong baseline methods. The results\ndemonstrate that our approach learns more semantic meaningful and robust\nmulti-modal representations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01341", "pdf": "https://arxiv.org/pdf/2506.01341", "abs": "https://arxiv.org/abs/2506.01341", "authors": ["Yiran Zhang", "Mo Wang", "Xiaoyang Li", "Kaixuan Ren", "Chencheng Zhu", "Usman Naseem"], "title": "TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Despite impressive advances in large language models (LLMs), existing\nbenchmarks often focus on single-turn or single-step tasks, failing to capture\nthe kind of iterative reasoning required in real-world settings. To address\nthis limitation, we introduce TurnBench, a novel benchmark that evaluates\nmulti-turn, multi-step reasoning through an interactive code-breaking task\ninspired by a \"Turing Machine Board Game.\" In each episode, a model must\nuncover hidden logical or arithmetic rules by making sequential guesses,\nreceiving structured feedback, and integrating clues across multiple rounds.\nThis dynamic setup requires models to reason over time, adapt based on past\ninformation, and maintain consistency across steps-capabilities underexplored\nin current benchmarks. TurnBench includes two modes: Classic, which tests\nstandard reasoning, and Nightmare, which introduces increased complexity and\nrequires robust inferential chains. To support fine-grained analysis, we\nprovide ground-truth annotations for intermediate reasoning steps. Our\nevaluation of state-of-the-art LLMs reveals significant gaps: the best model\nachieves 81.5% accuracy in Classic mode, but performance drops to 17.8% in\nNightmare mode. In contrast, human participants achieve 100% in both,\nunderscoring the challenge TurnBench poses to current models. By incorporating\nfeedback loops and hiding task rules, TurnBench reduces contamination risks and\nprovides a rigorous testbed for diagnosing and advancing multi-step, multi-turn\nreasoning in LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "testbed", "consistency", "accuracy", "fine-grained"], "score": 6}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01347", "pdf": "https://arxiv.org/pdf/2506.01347", "abs": "https://arxiv.org/abs/2506.01347", "authors": ["Xinyu Zhu", "Mengzhou Xia", "Zhepei Wei", "Wei-Lin Chen", "Danqi Chen", "Yu Meng"], "title": "The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor training language models (LMs) on reasoning tasks that elicit emergent long\nchains of thought (CoTs). Unlike supervised learning, it updates the model\nusing both correct and incorrect samples via policy gradients. To better\nunderstand its mechanism, we decompose the learning signal into reinforcing\ncorrect responses and penalizing incorrect ones, referred to as Positive and\nNegative Sample Reinforcement (PSR and NSR), respectively. We train\nQwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset and uncover a\nsurprising result: training with only negative samples -- without reinforcing\ncorrect responses -- can be highly effective: it consistently improves\nperformance over the base model across the entire Pass@$k$ spectrum ($k$ up to\n$256$), often matching or surpassing PPO and GRPO. In contrast, reinforcing\nonly correct responses improves Pass@$1$ but degrades performance at higher\n$k$, due to reduced diversity. These inference-scaling trends highlight that\nsolely penalizing incorrect responses may contribute more to performance than\npreviously recognized. Through gradient analysis, we show that NSR works by\nsuppressing incorrect generations and redistributing probability mass toward\nother plausible candidates, guided by the model's prior beliefs. It refines the\nmodel's existing knowledge rather than introducing entirely new behaviors.\nBuilding on this insight, we propose a simple variant of the RL objective that\nupweights NSR, and show that it consistently improves overall Pass@$k$\nperformance on MATH, AIME 2025, and AMC23. Our code is available at\nhttps://github.com/TianHongZXY/RLVR-Decomposed.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "mathematical reasoning"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01933", "pdf": "https://arxiv.org/pdf/2506.01933", "abs": "https://arxiv.org/abs/2506.01933", "authors": ["Wenyan Cong", "Yiqing Liang", "Yancheng Zhang", "Ziyi Yang", "Yan Wang", "Boris Ivanovic", "Marco Pavone", "Chen Chen", "Zhangyang Wang", "Zhiwen Fan"], "title": "E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models", "categories": ["cs.CV"], "comment": "Project Page: https://e3dbench.github.io/", "summary": "Spatial intelligence, encompassing 3D reconstruction, perception, and\nreasoning, is fundamental to applications such as robotics, aerial imaging, and\nextended reality. A key enabler is the real-time, accurate estimation of core\n3D attributes (camera parameters, point clouds, depth maps, and 3D point\ntracks) from unstructured or streaming imagery. Inspired by the success of\nlarge foundation models in language and 2D vision, a new class of end-to-end 3D\ngeometric foundation models (GFMs) has emerged, directly predicting dense 3D\nrepresentations in a single feed-forward pass, eliminating the need for slow or\nunavailable precomputed camera parameters. Since late 2023, the field has\nexploded with diverse variants, but systematic evaluation is lacking. In this\nwork, we present the first comprehensive benchmark for 3D GFMs, covering five\ncore tasks: sparse-view depth estimation, video depth estimation, 3D\nreconstruction, multi-view pose estimation, novel view synthesis, and spanning\nboth standard and challenging out-of-distribution datasets. Our standardized\ntoolkit automates dataset handling, evaluation protocols, and metric\ncomputation to ensure fair, reproducible comparisons. We evaluate 16\nstate-of-the-art GFMs, revealing their strengths and limitations across tasks\nand domains, and derive key insights to guide future model scaling and\noptimization. All code, evaluation scripts, and processed data will be publicly\nreleased to accelerate research in 3D spatial intelligence.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01935", "pdf": "https://arxiv.org/pdf/2506.01935", "abs": "https://arxiv.org/abs/2506.01935", "authors": ["Sai Tanmay Reddy Chakkera", "Aggelina Chatziagapi", "Md Moniruzzaman", "Chen-Ping Yu", "Yi-Hsuan Tsai", "Dimitris Samaras"], "title": "Low-Rank Head Avatar Personalization with Registers", "categories": ["cs.CV"], "comment": "23 pages, 16 figures. Project page:\n  https://starc52.github.io/publications/2025-05-28-LoRAvatar/", "summary": "We introduce a novel method for low-rank personalization of a generic model\nfor head avatar generation. Prior work proposes generic models that achieve\nhigh-quality face animation by leveraging large-scale datasets of multiple\nidentities. However, such generic models usually fail to synthesize unique\nidentity-specific details, since they learn a general domain prior. To adapt to\nspecific subjects, we find that it is still challenging to capture\nhigh-frequency facial details via popular solutions like low-rank adaptation\n(LoRA). This motivates us to propose a specific architecture, a Register\nModule, that enhances the performance of LoRA, while requiring only a small\nnumber of parameters to adapt to an unseen identity. Our module is applied to\nintermediate features of a pre-trained model, storing and re-purposing\ninformation in a learnable 3D feature space. To demonstrate the efficacy of our\npersonalization method, we collect a dataset of talking videos of individuals\nwith distinctive facial details, such as wrinkles and tattoos. Our approach\nfaithfully captures unseen faces, outperforming existing methods quantitatively\nand qualitatively. We will release the code, models, and dataset to the public.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01940", "pdf": "https://arxiv.org/pdf/2506.01940", "abs": "https://arxiv.org/abs/2506.01940", "authors": ["Yaroslava Lochman", "Carl Olsson", "Christopher Zach"], "title": "Fast and Robust Rotation Averaging with Anisotropic Coordinate Descent", "categories": ["cs.CV"], "comment": null, "summary": "Anisotropic rotation averaging has recently been explored as a natural\nextension of respective isotropic methods. In the anisotropic formulation,\nuncertainties of the estimated relative rotations -- obtained via standard\ntwo-view optimization -- are propagated to the optimization of absolute\nrotations. The resulting semidefinite relaxations are able to recover global\nminima but scale poorly with the problem size. Local methods are fast and also\nadmit robust estimation but are sensitive to initialization. They usually\nemploy minimum spanning trees and therefore suffer from drift accumulation and\ncan get trapped in poor local minima. In this paper, we attempt to bridge the\ngap between optimality, robustness and efficiency of anisotropic rotation\naveraging. We analyze a family of block coordinate descent methods initially\nproposed to optimize the standard chordal distances, and derive a much simpler\nformulation and an anisotropic extension obtaining a fast general solver. We\nintegrate this solver into the extended anisotropic large-scale robust rotation\naveraging pipeline. The resulting algorithm achieves state-of-the-art\nperformance on public structure-from-motion datasets. Project page:\nhttps://ylochman.github.io/acd", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01942", "pdf": "https://arxiv.org/pdf/2506.01942", "abs": "https://arxiv.org/abs/2506.01942", "authors": ["Salwa K. Al Khatib", "Ahmed ElHagry", "Shitong Shao", "Zhiqiang Shen"], "title": "OD3: Optimization-free Dataset Distillation for Object Detection", "categories": ["cs.CV"], "comment": "Equal Contribution of the first three authors", "summary": "Training large neural networks on large-scale datasets requires substantial\ncomputational resources, particularly for dense prediction tasks such as object\ndetection. Although dataset distillation (DD) has been proposed to alleviate\nthese demands by synthesizing compact datasets from larger ones, most existing\nwork focuses solely on image classification, leaving the more complex detection\nsetting largely unexplored. In this paper, we introduce OD3, a novel\noptimization-free data distillation framework specifically designed for object\ndetection. Our approach involves two stages: first, a candidate selection\nprocess in which object instances are iteratively placed in synthesized images\nbased on their suitable locations, and second, a candidate screening process\nusing a pre-trained observer model to remove low-confidence objects. We perform\nour data synthesis framework on MS COCO and PASCAL VOC, two popular detection\ndatasets, with compression ratios ranging from 0.25% to 5%. Compared to the\nprior solely existing dataset distillation method on detection and conventional\ncore set selection methods, OD3 delivers superior accuracy, establishes new\nstate-of-the-art results, surpassing prior best method by more than 14% on COCO\nmAP50 at a compression ratio of 1.0%. Code and condensed datasets are available\nat: https://github.com/VILA-Lab/OD3.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01419", "pdf": "https://arxiv.org/pdf/2506.01419", "abs": "https://arxiv.org/abs/2506.01419", "authors": ["Joseph Marvin Imperial", "Abdullah Barayan", "Regina Stodden", "Rodrigo Wilkens", "Ricardo Munoz Sanchez", "Lingyun Gao", "Melissa Torgbi", "Dawn Knight", "Gail Forey", "Reka R. Jablonkai", "Ekaterina Kochmar", "Robert Reynolds", "Eugenio Ribeiro", "Horacio Saggion", "Elena Volodina", "Sowmya Vajjala", "Thomas Francois", "Fernando Alva-Manchego", "Harish Tayyar Madabushi"], "title": "UniversalCEFR: Enabling Open Multilingual Research on Language Proficiency Assessment", "categories": ["cs.CL"], "comment": null, "summary": "We introduce UniversalCEFR, a large-scale multilingual multidimensional\ndataset of texts annotated according to the CEFR (Common European Framework of\nReference) scale in 13 languages. To enable open research in both automated\nreadability and language proficiency assessment, UniversalCEFR comprises\n505,807 CEFR-labeled texts curated from educational and learner-oriented\nresources, standardized into a unified data format to support consistent\nprocessing, analysis, and modeling across tasks and languages. To demonstrate\nits utility, we conduct benchmark experiments using three modelling paradigms:\na) linguistic feature-based classification, b) fine-tuning pre-trained LLMs,\nand c) descriptor-based prompting of instruction-tuned LLMs. Our results\nfurther support using linguistic features and fine-tuning pretrained models in\nmultilingual CEFR level assessment. Overall, UniversalCEFR aims to establish\nbest practices in data distribution in language proficiency research by\nstandardising dataset formats and promoting their accessibility to the global\nresearch community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01420", "pdf": "https://arxiv.org/pdf/2506.01420", "abs": "https://arxiv.org/abs/2506.01420", "authors": ["Kyuyoung Kim", "Hyunjun Jeon", "Jinwoo Shin"], "title": "Self-Refining Language Model Anonymizers via Adversarial Distillation", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly used in sensitive domains,\nwhere their ability to infer personal data from seemingly benign text poses\nemerging privacy risks. While recent LLM-based anonymization methods help\nmitigate such risks, they often rely on proprietary models (e.g., GPT-4),\nraising concerns about cost and the potential exposure of sensitive data to\nuntrusted external systems. To address this, we introduce SElf-refining\nAnonymization with Language model (SEAL), a novel distillation framework for\ntraining small language models (SLMs) to perform effective anonymization\nwithout relying on external costly models at inference time. We leverage\nadversarial interactions between an LLM anonymizer and an inference model to\ncollect trajectories of anonymized texts and inferred attributes, which are\nused to distill anonymization, adversarial inference, and utility evaluation\ncapabilities into SLMs via supervised fine-tuning and preference learning. The\nresulting models learn to both anonymize text and critique their outputs,\nenabling iterative improvement of anonymization quality via self-refinement.\nExperiments on SynthPAI, a dataset of synthetic personal profiles and text\ncomments, demonstrate that SLMs trained with SEAL achieve substantial\nimprovements in anonymization capabilities. Notably, 8B models attain a\nprivacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with\nself-refinement, even surpass it in terms of privacy. These results show the\neffectiveness of our adversarial distillation framework in training SLMs as\nefficient anonymizers. To facilitate further research, we release the full\ndataset used in our experiments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01439", "pdf": "https://arxiv.org/pdf/2506.01439", "abs": "https://arxiv.org/abs/2506.01439", "authors": ["Yosuke Kashiwagi", "Hayato Futami", "Emiru Tsunoo", "Satoshi Asakawa"], "title": "Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer with large speech data", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "This paper reports on the development of a large-scale speech recognition\nmodel, Whale. Similar to models such as Whisper and OWSM, Whale leverages both\na large model size and a diverse, extensive dataset. Whale's architecture\nintegrates w2v-BERT self-supervised model, an encoder-decoder backbone built on\nE-Branchformer, and a joint CTC-attention decoding strategy. The training\ncorpus comprises varied speech data, of not only public corpora but also\nin-house data, thereby enhancing the model's robustness to different speaking\nstyles and acoustic conditions. Through evaluations on multiple benchmarks,\nWhale achieved comparable performance to existing models. In particular, it\nachieves a word error rate of 2.4% on the Librispeech test-clean set and a\ncharacter error rate of 3.4% on the CSJ eval3 set, outperforming Whisper\nlarge-v3 and OWSM v3.1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01484", "pdf": "https://arxiv.org/pdf/2506.01484", "abs": "https://arxiv.org/abs/2506.01484", "authors": ["Shuzhou Yuan", "Ercong Nie", "Lukas Kouba", "Ashish Yashwanth Kangen", "Helmut Schmid", "Hinrich Schutze", "Michael Farber"], "title": "LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech Detoxification", "categories": ["cs.CL"], "comment": null, "summary": "Detoxification, the task of rewriting harmful language into non-toxic text,\nhas become increasingly important amid the growing prevalence of toxic content\nonline. However, high-quality parallel datasets for detoxification, especially\nfor hate speech, remain scarce due to the cost and sensitivity of human\nannotation. In this paper, we propose a novel LLM-in-the-loop pipeline\nleveraging GPT-4o-mini for automated detoxification. We first replicate the\nParaDetox pipeline by replacing human annotators with an LLM and show that the\nLLM performs comparably to human annotation. Building on this, we construct\nPARADEHATE, a large-scale parallel dataset specifically for hatespeech\ndetoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate\ntext pairs and evaluate a wide range of baseline methods. Experimental results\nshow that models such as BART, fine-tuned on PARADEHATE, achieve better\nperformance in style accuracy, content preservation, and fluency, demonstrating\nthe effectiveness of LLM-generated detoxification text as a scalable\nalternative to human annotation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "accuracy"], "score": 4}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01495", "pdf": "https://arxiv.org/pdf/2506.01495", "abs": "https://arxiv.org/abs/2506.01495", "authors": ["Ping Wu", "Guobin Shen", "Dongcheng Zhao", "Yuwei Wang", "Yiting Dong", "Yu Shi", "Enmeng Lu", "Feifei Zhao", "Yi Zeng"], "title": "CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Ensuring that Large Language Models (LLMs) align with mainstream human values\nand ethical norms is crucial for the safe and sustainable development of AI.\nCurrent value evaluation and alignment are constrained by Western cultural bias\nand incomplete domestic frameworks reliant on non-native rules; furthermore,\nthe lack of scalable, rule-driven scenario generation methods makes evaluations\ncostly and inadequate across diverse cultural contexts. To address these\nchallenges, we propose a hierarchical value framework grounded in core Chinese\nvalues, encompassing three main dimensions, 12 core values, and 50 derived\nvalues. Based on this framework, we construct a large-scale Chinese Values\nCorpus (CVC) containing over 250,000 value rules enhanced and expanded through\nhuman annotation. Experimental results show that CVC-guided scenarios\noutperform direct generation ones in value boundaries and content diversity. In\nthe evaluation across six sensitive themes (e.g., surrogacy, suicide), seven\nmainstream LLMs preferred CVC-generated options in over 70.5% of cases, while\nfive Chinese human annotators showed an 87.5% alignment with CVC, confirming\nits universality, cultural relevance, and strong alignment with Chinese values.\nAdditionally, we construct 400,000 rule-based moral dilemma scenarios that\nobjectively capture nuanced distinctions in conflicting value prioritization\nacross 17 LLMs. Our work establishes a culturally-adaptive benchmarking\nframework for comprehensive value evaluation and alignment, representing\nChinese characteristics. All data are available at\nhttps://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at\nhttps://github.com/Beijing-AISI/CVC.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00294", "pdf": "https://arxiv.org/pdf/2506.00294", "abs": "https://arxiv.org/abs/2506.00294", "authors": ["Luis Felipe Strano Moraes", "Ignacio Becker", "Pavlos Protopapas", "Guillermo Cabrera-Vives"], "title": "Applying Vision Transformers on Spectral Analysis of Astronomical Objects", "categories": ["astro-ph.IM", "cs.CV"], "comment": "9 pages, 9 figures", "summary": "We apply pre-trained Vision Transformers (ViTs), originally developed for\nimage recognition, to the analysis of astronomical spectral data. By converting\ntraditional one-dimensional spectra into two-dimensional image representations,\nwe enable ViTs to capture both local and global spectral features through\nspatial self-attention. We fine-tune a ViT pretrained on ImageNet using\nmillions of spectra from the SDSS and LAMOST surveys, represented as spectral\nplots. Our model is evaluated on key tasks including stellar object\nclassification and redshift ($z$) estimation, where it demonstrates strong\nperformance and scalability. We achieve classification accuracy higher than\nSupport Vector Machines and Random Forests, and attain $R^2$ values comparable\nto AstroCLIP's spectrum encoder, even when generalizing across diverse object\ntypes. These results demonstrate the effectiveness of using pretrained vision\nmodels for spectroscopic data analysis. To our knowledge, this is the first\napplication of ViTs to large-scale, which also leverages real spectroscopic\ndata and does not rely on synthetic inputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01531", "pdf": "https://arxiv.org/pdf/2506.01531", "abs": "https://arxiv.org/abs/2506.01531", "authors": ["Wenhao Liu", "Zhenyi Lu", "Xinyu Hu", "Jierui Zhang", "Dailin Li", "Jiacheng Cen", "Huilin Cao", "Haiteng Wang", "Yuhan Li", "Kun Xie", "Dandan Li", "Pei Zhang", "Chengbo Zhang", "Yuxiang Ren", "Xiaohong Huang", "Yan Ma"], "title": "STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework", "categories": ["cs.CL"], "comment": "accepted by ACL2025", "summary": "High-quality math datasets are crucial for advancing the reasoning abilities\nof large language models (LLMs). However, existing datasets often suffer from\nthree key issues: outdated and insufficient challenging content, neglecting\nhuman-like reasoning, and limited reliability due to single-LLM generation. To\naddress these, we introduce $\\textbf{STORM-BORN}$, an ultra-challenging dataset\nof mathematical derivations sourced from cutting-edge academic papers, which\nincludes dense human-like approximations and heuristic cues. To ensure the\nreliability and quality, we propose a novel human-in-the-loop, multi-agent data\ngeneration framework, integrating reasoning-dense filters, multi-agent\ncollaboration, and human mathematicians' evaluations. We curated a set of 2,000\nsynthetic samples and deliberately selected the 100 most difficult problems.\nEven most advanced models like GPT-o1 solved fewer than $5\\%$ of them.\nFine-tuning on STORM-BORN boosts accuracy by $7.84\\%$ (LLaMA3-8B) and $9.12\\%$\n(Qwen2.5-7B). As AI approaches mathematician-level reasoning, STORM-BORN\nprovides both a high-difficulty benchmark and a human-like reasoning training\nresource. Our code and dataset are publicly available at\nhttps://github.com/lwhere/STORM-BORN.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01578", "pdf": "https://arxiv.org/pdf/2506.01578", "abs": "https://arxiv.org/abs/2506.01578", "authors": ["Philipp Schoenegger", "Cameron R. Jones", "Philip E. Tetlock", "Barbara Mellers"], "title": "Prompt Engineering Large Language Models' Forecasting Capabilities", "categories": ["cs.CL"], "comment": null, "summary": "Large language model performance can be improved in a large number of ways.\nMany such techniques, like fine-tuning or advanced tool usage, are\ntime-intensive and expensive. Although prompt engineering is significantly\ncheaper and often works for simpler tasks, it remains unclear whether prompt\nengineering suffices for more complex domains like forecasting. Here we show\nthat small prompt modifications rarely boost forecasting accuracy beyond a\nminimal baseline. In our first study, we tested 38 prompts across Claude 3.5\nSonnet, Claude 3.5 Haiku, GPT-4o, and Llama 3.1 405B. In our second, we\nintroduced compound prompts and prompts from external sources, also including\nthe reasoning models o1 and o1-mini. Our results show that most prompts lead to\nnegligible gains, although references to base rates yield slight benefits.\nSurprisingly, some strategies showed strong negative effects on accuracy:\nespecially encouraging the model to engage in Bayesian reasoning. These results\nsuggest that, in the context of complex tasks like forecasting, basic prompt\nrefinements alone offer limited gains, implying that more robust or specialized\ntechniques may be required for substantial performance improvements in AI\nforecasting.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01615", "pdf": "https://arxiv.org/pdf/2506.01615", "abs": "https://arxiv.org/abs/2506.01615", "authors": ["Pasunuti Prasanjith", "Prathmesh B More", "Anoop Kunchukuttan", "Raj Dabre"], "title": "IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG Systems", "categories": ["cs.CL"], "comment": "WIP", "summary": "Retrieval-Augmented Generation (RAG) systems enable language models to access\nrelevant information and generate accurate, well-grounded, and contextually\ninformed responses. However, for Indian languages, the development of\nhigh-quality RAG systems is hindered by the lack of two critical resources: (1)\nevaluation benchmarks for retrieval and generation tasks, and (2) large-scale\ntraining datasets for multilingual retrieval. Most existing benchmarks and\ndatasets are centered around English or high-resource languages, making it\ndifficult to extend RAG capabilities to the diverse linguistic landscape of\nIndia. To address the lack of evaluation benchmarks, we create IndicMSMarco, a\nmultilingual benchmark for evaluating retrieval quality and response generation\nin 13 Indian languages, created via manual translation of 1000 diverse queries\nfrom MS MARCO-dev set. To address the need for training data, we build a\nlarge-scale dataset of (question, answer, relevant passage) tuples derived from\nthe Wikipedias of 19 Indian languages using state-of-the-art LLMs.\nAdditionally, we include translated versions of the original MS MARCO dataset\nto further enrich the training data and ensure alignment with real-world\ninformation-seeking tasks. Resources are available here:\nhttps://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00785", "pdf": "https://arxiv.org/pdf/2506.00785", "abs": "https://arxiv.org/abs/2506.00785", "authors": ["Sahiti Yerramilli", "Nilay Pande", "Rynaa Grover", "Jayant Sravan Tamarapalli"], "title": "GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper introduces GeoChain, a large-scale benchmark for evaluating\nstep-by-step geographic reasoning in multimodal large language models (MLLMs).\nLeveraging 1.46 million Mapillary street-level images, GeoChain pairs each\nimage with a 21-step chain-of-thought (CoT) question sequence (over 30 million\nQ&A pairs). These sequences guide models from coarse attributes to fine-grained\nlocalization across four reasoning categories - visual, spatial, cultural, and\nprecise geolocation - annotated by difficulty. Images are also enriched with\nsemantic segmentation (150 classes) and a visual locatability score. Our\nbenchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5\nvariants) on a diverse 2,088-image subset reveals consistent challenges: models\nfrequently exhibit weaknesses in visual grounding, display erratic reasoning,\nand struggle to achieve accurate localization, especially as the reasoning\ncomplexity escalates. GeoChain offers a robust diagnostic methodology, critical\nfor fostering significant advancements in complex geographic reasoning within\nMLLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00868", "pdf": "https://arxiv.org/pdf/2506.00868", "abs": "https://arxiv.org/abs/2506.00868", "authors": ["Parul Gupta", "Shreya Ghosh", "Tom Gedeon", "Thanh-Toan Do", "Abhinav Dhall"], "title": "Multiverse Through Deepfakes: The MultiFakeVerse Dataset of Person-Centric Visual and Conceptual Manipulations", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "The rapid advancement of GenAI technology over the past few years has\nsignificantly contributed towards highly realistic deepfake content generation.\nDespite ongoing efforts, the research community still lacks a large-scale and\nreasoning capability driven deepfake benchmark dataset specifically tailored\nfor person-centric object, context and scene manipulations. In this paper, we\naddress this gap by introducing MultiFakeVerse, a large scale person-centric\ndeepfake dataset, comprising 845,286 images generated through manipulation\nsuggestions and image manipulations both derived from vision-language models\n(VLM). The VLM instructions were specifically targeted towards modifications to\nindividuals or contextual elements of a scene that influence human perception\nof importance, intent, or narrative. This VLM-driven approach enables semantic,\ncontext-aware alterations such as modifying actions, scenes, and human-object\ninteractions rather than synthetic or low-level identity swaps and\nregion-specific edits that are common in existing datasets. Our experiments\nreveal that current state-of-the-art deepfake detection models and human\nobservers struggle to detect these subtle yet meaningful manipulations. The\ncode and dataset are available on\n\\href{https://github.com/Parul-Gupta/MultiFakeVerse}{GitHub}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01687", "pdf": "https://arxiv.org/pdf/2506.01687", "abs": "https://arxiv.org/abs/2506.01687", "authors": ["Anya Sims", "Thom Foster", "Klara Kaleb", "Tuan-Duy H. Nguyen", "Joseph Lee", "Jakob N. Foerster", "Yee Whye Teh", "Cong Lu"], "title": "StochasTok: Improving Fine-Grained Subword Understanding in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Subword-level understanding is integral to numerous tasks, including\nunderstanding multi-digit numbers, spelling mistakes, abbreviations, rhyming,\nand wordplay. Despite this, current large language models (LLMs) still often\nstruggle with seemingly simple subword-level tasks like How many 'r's in\n'strawberry'?. A key factor behind these failures is tokenization which\nobscures the fine-grained structure of words. Current alternatives, such as\ncharacter-level and dropout tokenization methods, significantly increase\ncomputational costs and provide inconsistent improvements. In this paper we\nrevisit tokenization and introduce StochasTok, a simple, efficient stochastic\ntokenization scheme that randomly splits tokens during training, allowing LLMs\nto 'see' their internal structure. Our experiments show that pretraining with\nStochasTok substantially improves LLMs' downstream performance across multiple\nsubword-level language games, including character counting, substring\nidentification, and math tasks. Furthermore, StochasTok's simplicity allows\nseamless integration at any stage of the training pipeline; and we demonstrate\nthat post-training with StochasTok can instill improved subword understanding\ninto existing pretrained models, thus avoiding costly pretraining from scratch.\nThese dramatic improvements achieved with a minimal change suggest StochasTok\nholds exciting potential when applied to larger, more capable models. Code\nopen-sourced at: https://github.com/anyasims/stochastok.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["strawberry"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00958", "pdf": "https://arxiv.org/pdf/2506.00958", "abs": "https://arxiv.org/abs/2506.00958", "authors": ["Youngmin Kim", "Jiwan Chung", "Jisoo Kim", "Sunghyun Lee", "Sangkyu Lee", "Junhyeok Kim", "Cheoljong Yang", "Youngjae Yu"], "title": "Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Accepted to ACL 2025 (Main), Our code and dataset:\n  https://github.com/winston1214/nonverbal-conversation", "summary": "Nonverbal communication is integral to human interaction, with gestures,\nfacial expressions, and body language conveying critical aspects of intent and\nemotion. However, existing large language models (LLMs) fail to effectively\nincorporate these nonverbal elements, limiting their capacity to create fully\nimmersive conversational experiences. We introduce MARS, a multimodal language\nmodel designed to understand and generate nonverbal cues alongside text,\nbridging this gap in conversational AI. Our key innovation is VENUS, a\nlarge-scale dataset comprising annotated videos with time-aligned text, facial\nexpressions, and body language. Leveraging VENUS, we train MARS with a\nnext-token prediction objective, combining text with vector-quantized nonverbal\nrepresentations to achieve multimodal understanding and generation within a\nunified framework. Based on various analyses of the VENUS datasets, we validate\nits substantial scale and high effectiveness. Our quantitative and qualitative\nresults demonstrate that MARS successfully generates text and nonverbal\nlanguages, corresponding to conversational input.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01713", "pdf": "https://arxiv.org/pdf/2506.01713", "abs": "https://arxiv.org/abs/2506.01713", "authors": ["Zhongwei Wan", "Zhihao Dou", "Che Liu", "Yu Zhang", "Dongfei Cui", "Qinjian Zhao", "Hui Shen", "Jing Xiong", "Yi Xin", "Yifan Jiang", "Yangfan He", "Mi Zhang", "Shen Yan"], "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning", "categories": ["cs.CL"], "comment": "Under review", "summary": "Multimodal large language models (MLLMs) have shown promising capabilities in\nreasoning tasks, yet still struggle with complex problems requiring explicit\nself-reflection and self-correction, especially compared to their unimodal\ntext-based counterparts. Existing reflection methods are simplistic and\nstruggle to generate meaningful and instructive feedback, as the reasoning\nability and knowledge limits of pre-trained models are largely fixed during\ninitial training. To overcome these challenges, we propose Multimodal\nSelf-Reflection enhanced reasoning with Group Relative Policy Optimization\n(SRPO), a two-stage reflection-aware reinforcement learning (RL) framework\nexplicitly designed to enhance multimodal LLM reasoning. In the first stage, we\nconstruct a high-quality, reflection-focused dataset under the guidance of an\nadvanced MLLM, which generates reflections based on initial responses to help\nthe policy model learn both reasoning and self-reflection. In the second stage,\nwe introduce a novel reward mechanism within the GRPO framework that encourages\nconcise and cognitively meaningful reflection while avoiding redundancy.\nExtensive experiments across multiple multimodal reasoning benchmarks,\nincluding MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B\nand Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms\nstate-of-the-art models, achieving notable improvements in both reasoning\naccuracy and reflection quality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01320", "pdf": "https://arxiv.org/pdf/2506.01320", "abs": "https://arxiv.org/abs/2506.01320", "authors": ["Taehoon Yoon", "Yunhong Min", "Kyeongmin Yeo", "Minhyuk Sung"], "title": "$$-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce $\\Psi$-Sampler, an SMC-based framework incorporating pCNL-based\ninitial particle sampling for effective inference-time reward alignment with a\nscore-based generative model. Inference-time reward alignment with score-based\ngenerative models has recently gained significant traction, following a broader\nparadigm shift from pre-training to post-training optimization. At the core of\nthis trend is the application of Sequential Monte Carlo (SMC) to the denoising\nprocess. However, existing methods typically initialize particles from the\nGaussian prior, which inadequately captures reward-relevant regions and results\nin reduced sampling efficiency. We demonstrate that initializing from the\nreward-aware posterior significantly improves alignment performance. To enable\nposterior sampling in high-dimensional latent spaces, we introduce the\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines\ndimension-robust proposals with gradient-informed dynamics. This approach\nenables efficient and scalable posterior sampling and consistently improves\nperformance across various reward alignment tasks, including layout-to-image\ngeneration, quantity-aware generation, and aesthetic-preference generation, as\ndemonstrated in our experiments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01353", "pdf": "https://arxiv.org/pdf/2506.01353", "abs": "https://arxiv.org/abs/2506.01353", "authors": ["Nie Lin", "Yansen Wang", "Dongqi Han", "Weibang Jiang", "Jingyuan Li", "Ryosuke Furuta", "Yoichi Sato", "Dongsheng Li"], "title": "EgoBrain: Synergizing Minds and Eyes For Human Action Understanding", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "21 pages, 12 figures", "summary": "The integration of brain-computer interfaces (BCIs), in particular\nelectroencephalography (EEG), with artificial intelligence (AI) has shown\ntremendous promise in decoding human cognition and behavior from neural\nsignals. In particular, the rise of multimodal AI models have brought new\npossibilities that have never been imagined before. Here, we present EgoBrain\n--the world's first large-scale, temporally aligned multimodal dataset that\nsynchronizes egocentric vision and EEG of human brain over extended periods of\ntime, establishing a new paradigm for human-centered behavior analysis. This\ndataset comprises 61 hours of synchronized 32-channel EEG recordings and\nfirst-person video from 40 participants engaged in 29 categories of daily\nactivities. We then developed a muiltimodal learning framework to fuse EEG and\nvision for action understanding, validated across both cross-subject and\ncross-environment challenges, achieving an action recognition accuracy of\n66.70%. EgoBrain paves the way for a unified framework for brain-computer\ninterface with multiple modalities. All data, tools, and acquisition protocols\nare openly shared to foster open science in cognitive computing.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01600", "pdf": "https://arxiv.org/pdf/2506.01600", "abs": "https://arxiv.org/abs/2506.01600", "authors": ["Tenny Yin", "Zhiting Mei", "Tao Sun", "Lihan Zha", "Emily Zhou", "Jeremy Bao", "Miyu Yamane", "Ola Shorinwa", "Anirudha Majumdar"], "title": "WoMAP: World Models For Embodied Open-Vocabulary Object Localization", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Language-instructed active object localization is a critical challenge for\nrobots, requiring efficient exploration of partially observable environments.\nHowever, state-of-the-art approaches either struggle to generalize beyond\ndemonstration datasets (e.g., imitation learning methods) or fail to generate\nphysically grounded actions (e.g., VLMs). To address these limitations, we\nintroduce WoMAP (World Models for Active Perception): a recipe for training\nopen-vocabulary object localization policies that: (i) uses a Gaussian\nSplatting-based real-to-sim-to-real pipeline for scalable data generation\nwithout the need for expert demonstrations, (ii) distills dense rewards signals\nfrom open-vocabulary object detectors, and (iii) leverages a latent world model\nfor dynamics and rewards prediction to ground high-level action proposals at\ninference time. Rigorous simulation and hardware experiments demonstrate\nWoMAP's superior performance in a broad range of zero-shot object localization\ntasks, with more than 9x and 2x higher success rates compared to VLM and\ndiffusion policy baselines, respectively. Further, we show that WoMAP achieves\nstrong generalization and sim-to-real transfer on a TidyBot.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01840", "pdf": "https://arxiv.org/pdf/2506.01840", "abs": "https://arxiv.org/abs/2506.01840", "authors": ["Igor Sterner", "Simone Teufel"], "title": "Minimal Pair-Based Evaluation of Code-Switching", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "There is a lack of an evaluation methodology that estimates the extent to\nwhich large language models (LLMs) use code-switching (CS) in the same way as\nbilinguals. Existing methods do not have wide language coverage, fail to\naccount for the diverse range of CS phenomena, or do not scale. We propose an\nintervention based on minimal pairs of CS. Each minimal pair contains one\nnaturally occurring CS sentence and one minimally manipulated variant. We\ncollect up to 1,000 such pairs each for 11 language pairs. Our human\nexperiments show that, for every language pair, bilinguals consistently prefer\nthe naturally occurring CS sentence. Meanwhile our experiments with current\nLLMs show that the larger the model, the more consistently it assigns higher\nprobability to the naturally occurring CS sentence than to the variant. In\naccordance with theoretical claims, the largest probability differences arise\nin those pairs where the manipulated material consisted of closed-class words.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01846", "pdf": "https://arxiv.org/pdf/2506.01846", "abs": "https://arxiv.org/abs/2506.01846", "authors": ["Igor Sterner", "Simone Teufel"], "title": "Code-Switching and Syntax: A Large-Scale Experiment", "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "The theoretical code-switching (CS) literature provides numerous pointwise\ninvestigations that aim to explain patterns in CS, i.e. why bilinguals switch\nlanguage in certain positions in a sentence more often than in others. A\nresulting consensus is that CS can be explained by the syntax of the\ncontributing languages. There is however no large-scale, multi-language,\ncross-phenomena experiment that tests this claim. When designing such an\nexperiment, we need to make sure that the system that is predicting where\nbilinguals tend to switch has access only to syntactic information. We provide\nsuch an experiment here. Results show that syntax alone is sufficient for an\nautomatic system to distinguish between sentences in minimal pairs of CS, to\nthe same degree as bilingual humans. Furthermore, the learnt syntactic patterns\ngeneralise well to unseen language pairs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01928", "pdf": "https://arxiv.org/pdf/2506.01928", "abs": "https://arxiv.org/abs/2506.01928", "authors": ["Subham Sekhar Sahoo", "Zhihan Yang", "Yash Akhauri", "Johnna Liu", "Deepansha Singh", "Zhoujun Cheng", "Zhengzhong Liu", "Eric Xing", "John Thickstun", "Arash Vahdat"], "title": "Esoteric Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01939", "pdf": "https://arxiv.org/pdf/2506.01939", "abs": "https://arxiv.org/abs/2506.01939", "authors": ["Shenzhi Wang", "Le Yu", "Chang Gao", "Chujie Zheng", "Shixuan Liu", "Rui Lu", "Kai Dang", "Xionghui Chen", "Jianxin Yang", "Zhenru Zhang", "Yuqiong Liu", "An Yang", "Andrew Zhao", "Yang Yue", "Shiji Song", "Bowen Yu", "Gao Huang", "Junyang Lin"], "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 17 figures, 2 tables", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful approach to enhancing the reasoning capabilities of Large Language\nModels (LLMs), while its mechanisms are not yet well understood. In this work,\nwe undertake a pioneering exploration of RLVR through the novel perspective of\ntoken entropy patterns, comprehensively analyzing how different tokens\ninfluence reasoning performance. By examining token entropy patterns in\nChain-of-Thought (CoT) reasoning, we observe that only a small fraction of\ntokens exhibit high entropy, and these tokens act as critical forks that steer\nthe model toward diverse reasoning pathways. Furthermore, studying how entropy\npatterns evolve during RLVR training reveals that RLVR largely adheres to the\nbase model's entropy patterns, primarily adjusting the entropy of high-entropy\ntokens. These findings highlight the significance of high-entropy tokens (i.e.,\nforking tokens) to RLVR. We ultimately improve RLVR by restricting policy\ngradient updates to forking tokens and uncover a finding even beyond the 80/20\nrule: utilizing only 20% of the tokens while maintaining performance comparable\nto full-gradient updates on the Qwen3-8B base model and significantly\nsurpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71\non AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,\nhighlighting a strong scaling trend. In contrast, training exclusively on the\n80% lowest-entropy tokens leads to a marked decline in performance. These\nfindings indicate that the efficacy of RLVR primarily arises from optimizing\nthe high-entropy tokens that decide reasoning directions. Collectively, our\nresults highlight the potential to understand RLVR through a token-entropy\nperspective and optimize RLVR by leveraging high-entropy minority tokens to\nfurther improve LLM reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01954", "pdf": "https://arxiv.org/pdf/2506.01954", "abs": "https://arxiv.org/abs/2506.01954", "authors": ["Jennifer Chen", "Aidar Myrzakhan", "Yaxin Luo", "Hassaan Muhammad Khan", "Sondos Mahmoud Bsharat", "Zhiqiang Shen"], "title": "DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Main. Code is available at https://github.com/VILA-Lab/DRAG", "summary": "Retrieval-Augmented Generation (RAG) methods have proven highly effective for\ntasks requiring factual consistency and robust knowledge retrieval. However,\nlarge-scale RAG systems consume significant computational resources and are\nprone to generating hallucinated content from Humans. In this work, we\nintroduce $\\texttt{DRAG}$, a novel framework for distilling RAG knowledge from\nlarge-scale Language Models (LLMs) into small LMs (SLMs). Our approach\nleverages evidence- and knowledge graph-based distillation, ensuring that the\ndistilled model retains critical factual knowledge while significantly reducing\nmodel size and computational cost. By aligning the smaller model's predictions\nwith a structured knowledge graph and ranked evidence, $\\texttt{DRAG}$\neffectively mitigates hallucinations and improves factual accuracy. We further\npresent a case demonstrating how our framework mitigates user privacy risks and\nintroduce a corresponding benchmark. Experimental evaluations on multiple\nbenchmarks demonstrate that our method outperforms the prior competitive RAG\nmethods like MiniRAG for SLMs by up to 27.7% using the same models, preserving\nhigh-level efficiency and reliability. With $\\texttt{DRAG}$, we provide a\npractical and resource-efficient roadmap to deploying enhanced retrieval and\ngeneration capabilities in small-sized LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00054", "pdf": "https://arxiv.org/pdf/2506.00054", "abs": "https://arxiv.org/abs/2506.00054", "authors": ["Chaitanya Sharma"], "title": "Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to\nenhance large language models (LLMs) by conditioning generation on external\nevidence retrieved at inference time. While RAG addresses critical limitations\nof parametric knowledge storage-such as factual inconsistency and domain\ninflexibility-it introduces new challenges in retrieval quality, grounding\nfidelity, pipeline efficiency, and robustness against noisy or adversarial\ninputs. This survey provides a comprehensive synthesis of recent advances in\nRAG systems, offering a taxonomy that categorizes architectures into\nretriever-centric, generator-centric, hybrid, and robustness-oriented designs.\nWe systematically analyze enhancements across retrieval optimization, context\nfiltering, decoding control, and efficiency improvements, supported by\ncomparative performance analyses on short-form and multi-hop question answering\ntasks. Furthermore, we review state-of-the-art evaluation frameworks and\nbenchmarks, highlighting trends in retrieval-aware evaluation, robustness\ntesting, and federated retrieval settings. Our analysis reveals recurring\ntrade-offs between retrieval precision and generation flexibility, efficiency\nand faithfulness, and modularity and coordination. We conclude by identifying\nopen challenges and future research directions, including adaptive retrieval\narchitectures, real-time retrieval integration, structured reasoning over\nmulti-hop evidence, and privacy-preserving retrieval mechanisms. This survey\naims to consolidate current knowledge in RAG research and serve as a foundation\nfor the next generation of retrieval-augmented language modeling systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "question answering"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00166", "pdf": "https://arxiv.org/pdf/2506.00166", "abs": "https://arxiv.org/abs/2506.00166", "authors": ["Kundan Krishna", "Joseph Y Cheng", "Charles Maalouf", "Leon A Gatys"], "title": "Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "16 pages, 2 figures, including references and appendix", "summary": "Existing paradigms for ensuring AI safety, such as guardrail models and\nalignment training, often compromise either inference efficiency or development\nflexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework\naddressing these challenges by decoupling safety-specific computations from a\ntask-optimized base model. DSA utilizes lightweight adapters that leverage the\nbase model's internal representations, enabling diverse and flexible safety\nfunctionalities with minimal impact on inference cost. Empirically, DSA-based\nsafety guardrails substantially outperform comparably sized standalone models,\nnotably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and\nalso excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe\nmodel inputs and responses (0.93 vs. 0.90 on AEGIS2.0 & BeaverTails).\nFurthermore, DSA-based safety alignment allows dynamic, inference-time\nadjustment of alignment strength and a fine-grained trade-off between\ninstruction following performance and model safety. Importantly, combining the\nDSA safety guardrail with DSA safety alignment facilitates context-dependent\nalignment strength, boosting safety on StrongReject by 93% while maintaining\n98% performance on MTBench -- a total reduction in alignment tax of 8\npercentage points compared to standard safety alignment fine-tuning. Overall,\nDSA presents a promising path towards more modular, efficient, and adaptable AI\nsafety and alignment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "fine-grained"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00185", "pdf": "https://arxiv.org/pdf/2506.00185", "abs": "https://arxiv.org/abs/2506.00185", "authors": ["Lilit Grigoryan", "Vladimir Bataev", "Andrei Andrusenko", "Hainan Xu", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "Pushing the Limits of Beam Search Decoding for Transducer-based ASR models", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Transducer models have emerged as a promising choice for end-to-end ASR\nsystems, offering a balanced trade-off between recognition accuracy, streaming\ncapabilities, and inference speed in greedy decoding. However, beam search\nsignificantly slows down Transducers due to repeated evaluations of key network\ncomponents, limiting practical applications. This paper introduces a universal\nmethod to accelerate beam search for Transducers, enabling the implementation\nof two optimized algorithms: ALSD++ and AES++. The proposed method utilizes\nbatch operations, a tree-based hypothesis structure, novel blank scoring for\nenhanced shallow fusion, and CUDA graph execution for efficient GPU inference.\nThis narrows the speed gap between beam and greedy modes to only 10-20% for the\nwhole system, achieves 14-30% relative improvement in WER compared to greedy\ndecoding, and improves shallow fusion for low-resource up to 11% compared to\nexisting implementations. All the algorithms are open sourced.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["beam search"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00238", "pdf": "https://arxiv.org/pdf/2506.00238", "abs": "https://arxiv.org/abs/2506.00238", "authors": ["Ehsan Karimi", "Maryam Rahnemoonfar"], "title": "ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG", "I.2.7; I.2.10; I.5.1"], "comment": "Accepted by the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)", "summary": "Natural disasters usually affect vast areas and devastate infrastructures.\nPerforming a timely and efficient response is crucial to minimize the impact on\naffected communities, and data-driven approaches are the best choice. Visual\nquestion answering (VQA) models help management teams to achieve in-depth\nunderstanding of damages. However, recently published models do not possess the\nability to answer open-ended questions and only select the best answer among a\npredefined list of answers. If we want to ask questions with new additional\npossible answers that do not exist in the predefined list, the model needs to\nbe fin-tuned/retrained on a new collected and annotated dataset, which is a\ntime-consuming procedure. In recent years, large-scale Vision-Language Models\n(VLMs) have earned significant attention. These models are trained on extensive\ndatasets and demonstrate strong performance on both unimodal and multimodal\nvision/language downstream tasks, often without the need for fine-tuning. In\nthis paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and\ninvestigate the performance of on post-disaster FloodNet dataset. Since the\nproposed method takes advantage of zero-shot learning, it can be applied on new\ndatasets without fine-tuning. In addition, ZeShot-VQA is able to process and\ngenerate answers that has been not seen during the training procedure, which\ndemonstrates its flexibility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00308", "pdf": "https://arxiv.org/pdf/2506.00308", "abs": "https://arxiv.org/abs/2506.00308", "authors": ["Hayoung Jung", "Shravika Mittal", "Ananya Aatreya", "Navreet Kaur", "Munmun De Choudhury", "Tanushree Mitra"], "title": "MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "34 pages, 14 figures, 21 tables. In submission", "summary": "Understanding the prevalence of misinformation in health topics online can\ninform public health policies and interventions. However, measuring such\nmisinformation at scale remains a challenge, particularly for high-stakes but\nunderstudied topics like opioid-use disorder (OUD)--a leading cause of death in\nthe U.S. We present the first large-scale study of OUD-related myths on\nYouTube, a widely-used platform for health information. With clinical experts,\nwe validate 8 pervasive myths and release an expert-labeled video dataset. To\nscale labeling, we introduce MythTriage, an efficient triage pipeline that uses\na lightweight model for routine cases and defers harder ones to a\nhigh-performing, but costlier, large language model (LLM). MythTriage achieves\nup to 0.86 macro F1-score while estimated to reduce annotation time and\nfinancial cost by over 76% compared to experts and full LLM labeling. We\nanalyze 2.9K search results and 343K recommendations, uncovering how myths\npersist on YouTube and offering actionable insights for public health and\nplatform moderation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00462", "pdf": "https://arxiv.org/pdf/2506.00462", "abs": "https://arxiv.org/abs/2506.00462", "authors": ["Ioan-Paul Ciobanu", "Andrei-Iulian Hiji", "Nicolae-Catalin Ristea", "Paul Irofti", "Cristian Rusu", "Radu Tudor Ionescu"], "title": "XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "comment": null, "summary": "Recent advances in audio generation led to an increasing number of deepfakes,\nmaking the general public more vulnerable to financial scams, identity theft,\nand misinformation. Audio deepfake detectors promise to alleviate this issue,\nwith many recent studies reporting accuracy rates close to 99%. However, these\nmethods are typically tested in an in-domain setup, where the deepfake samples\nfrom the training and test sets are produced by the same generative models. To\nthis end, we introduce XMAD-Bench, a large-scale cross-domain multilingual\naudio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In\nour novel dataset, the speakers, the generative methods, and the real audio\nsources are distinct across training and test splits. This leads to a\nchallenging cross-domain evaluation setup, where audio deepfake detectors can\nbe tested ``in the wild''. Our in-domain and cross-domain experiments indicate\na clear disparity between the in-domain performance of deepfake detectors,\nwhich is usually as high as 100%, and the cross-domain performance of the same\nmodels, which is sometimes similar to random chance. Our benchmark highlights\nthe need for the development of robust audio deepfake detectors, which maintain\ntheir generalization capacity across different languages, speakers, generative\nmethods, and data sources. Our benchmark is publicly released at\nhttps://github.com/ristea/xmad-bench/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00653", "pdf": "https://arxiv.org/pdf/2506.00653", "abs": "https://arxiv.org/abs/2506.00653", "authors": ["Femi Bello", "Anubrata Das", "Fanzhi Zeng", "Fangcong Yin", "Leqi Liu"], "title": "Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "It has been hypothesized that neural networks with similar architectures\ntrained on similar data learn shared representations relevant to the learning\ntask. We build on this idea by extending the conceptual framework where\nrepresentations learned across models trained on the same data can be expressed\nas linear combinations of a \\emph{universal} set of basis features. These basis\nfeatures underlie the learning task itself and remain consistent across models,\nregardless of scale. From this framework, we propose the \\textbf{Linear\nRepresentation Transferability (LRT)} Hypothesis -- that there exists an affine\ntransformation between the representation spaces of different models. To test\nthis hypothesis, we learn affine mappings between the hidden states of models\nof different sizes and evaluate whether steering vectors -- directions in\nhidden state space associated with specific model behaviors -- retain their\nsemantic effect when transferred from small to large language models using the\nlearned mappings. We find strong empirical evidence that such affine mappings\ncan preserve steering behaviors. These findings suggest that representations\nlearned by small models can be used to guide the behavior of large models, and\nthat the LRT hypothesis may be a promising direction on understanding\nrepresentation alignment across model scales.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00871", "pdf": "https://arxiv.org/pdf/2506.00871", "abs": "https://arxiv.org/abs/2506.00871", "authors": ["Ryo Fujii", "Hideo Saito", "Ryo Hachiuma"], "title": "Towards Predicting Any Human Trajectory In Context", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Predicting accurate future trajectories of pedestrians is essential for\nautonomous systems but remains a challenging task due to the need for\nadaptability in different environments and domains. A common approach involves\ncollecting scenario-specific data and performing fine-tuning via\nbackpropagation. However, this process is often impractical on edge devices due\nto constrained computational resources. To address this challenge, we introduce\nTrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory\nprediction that enables rapid adaptation without fine-tuning on the\nscenario-specific data. We propose a spatio-temporal similarity-based example\nselection (STES) method that selects relevant examples from previously observed\ntrajectories within the same scene by identifying similar motion patterns at\ncorresponding locations. To further refine this selection, we introduce\nprediction-guided example selection (PG-ES), which selects examples based on\nboth the past trajectory and the predicted future trajectory, rather than\nrelying solely on the past trajectory. This approach allows the model to\naccount for long-term dynamics when selecting examples. Finally, instead of\nrelying on small real-world datasets with limited scenario diversity, we train\nour model on a large-scale synthetic dataset to enhance its prediction ability\nby leveraging in-context examples. Extensive experiments demonstrate that\nTrajICL achieves remarkable adaptation across both in-domain and cross-domain\nscenarios, outperforming even fine-tuned approaches across multiple public\nbenchmarks. The code will be released at\nhttps://fujiry0.github.io/TrajICL-project-page.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.00958", "pdf": "https://arxiv.org/pdf/2506.00958", "abs": "https://arxiv.org/abs/2506.00958", "authors": ["Youngmin Kim", "Jiwan Chung", "Jisoo Kim", "Sunghyun Lee", "Sangkyu Lee", "Junhyeok Kim", "Cheoljong Yang", "Youngjae Yu"], "title": "Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Accepted to ACL 2025 (Main), Our code and dataset:\n  https://github.com/winston1214/nonverbal-conversation", "summary": "Nonverbal communication is integral to human interaction, with gestures,\nfacial expressions, and body language conveying critical aspects of intent and\nemotion. However, existing large language models (LLMs) fail to effectively\nincorporate these nonverbal elements, limiting their capacity to create fully\nimmersive conversational experiences. We introduce MARS, a multimodal language\nmodel designed to understand and generate nonverbal cues alongside text,\nbridging this gap in conversational AI. Our key innovation is VENUS, a\nlarge-scale dataset comprising annotated videos with time-aligned text, facial\nexpressions, and body language. Leveraging VENUS, we train MARS with a\nnext-token prediction objective, combining text with vector-quantized nonverbal\nrepresentations to achieve multimodal understanding and generation within a\nunified framework. Based on various analyses of the VENUS datasets, we validate\nits substantial scale and high effectiveness. Our quantitative and qualitative\nresults demonstrate that MARS successfully generates text and nonverbal\nlanguages, corresponding to conversational input.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01478", "pdf": "https://arxiv.org/pdf/2506.01478", "abs": "https://arxiv.org/abs/2506.01478", "authors": ["Tung-Lam Ngo", "Ba-Hoang Tran", "Duy-Cat Can", "Trung-Hieu Do", "Oliver Y. Chn", "Hoang-Quynh Le"], "title": "MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic Drug-Drug Interactions", "categories": ["cs.LG", "cs.CL", "cs.MM", "q-bio.QM"], "comment": null, "summary": "Understanding the interaction between different drugs (drug-drug interaction\nor DDI) is critical for ensuring patient safety and optimizing therapeutic\noutcomes. Existing DDI datasets primarily focus on textual information,\noverlooking multimodal data that reflect complex drug mechanisms. In this\npaper, we (1) introduce MUDI, a large-scale Multimodal biomedical dataset for\nUnderstanding pharmacodynamic Drug-drug Interactions, and (2) benchmark\nlearning methods to study it. In brief, MUDI provides a comprehensive\nmultimodal representation of drugs by combining pharmacological text, chemical\nformulas, molecular structure graphs, and images across 310,532 annotated drug\npairs labeled as Synergism, Antagonism, or New Effect. Crucially, to\neffectively evaluate machine-learning based generalization, MUDI consists of\nunseen drug pairs in the test set. We evaluate benchmark models using both late\nfusion voting and intermediate fusion strategies. All data, annotations,\nevaluation scripts, and baselines are released under an open research license.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety"], "score": 4}}, "source_file": "2025-06-03.jsonl"}
{"id": "2506.01902", "pdf": "https://arxiv.org/pdf/2506.01902", "abs": "https://arxiv.org/abs/2506.01902", "authors": ["Xinliu Zhong", "Kayhan Batmanghelich", "Li Sun"], "title": "Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination", "categories": ["cs.CV", "cs.CL"], "comment": "6 pages, 1 figure, accepted by 2024 IEEE Conference on Artificial\n  Intelligence (CAI)", "summary": "Vision-language models pre-trained on large scale of unlabeled biomedical\nimages and associated reports learn generalizable semantic representations.\nThese multi-modal representations can benefit various downstream tasks in the\nbiomedical domain. Contrastive learning is widely used to pre-train\nvision-language models for general natural images and associated captions.\nDespite its popularity, we found biomedical texts have complex and\ndomain-specific semantics that are often neglected by common contrastive\nmethods. To address this issue, we propose a novel method, perturbed report\ndiscrimination, for pre-train biomedical vision-language models. First, we\ncurate a set of text perturbation methods that keep the same words, but disrupt\nthe semantic structure of the sentence. Next, we apply different types of\nperturbation to reports, and use the model to distinguish the original report\nfrom the perturbed ones given the associated image. Parallel to this, we\nenhance the sensitivity of our method to higher level of granularity for both\nmodalities by contrasting attention-weighted image sub-regions and sub-words in\nthe image-text pairs. We conduct extensive experiments on multiple downstream\ntasks, and our method outperforms strong baseline methods. The results\ndemonstrate that our approach learns more semantic meaningful and robust\nmulti-modal representations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-03.jsonl"}
