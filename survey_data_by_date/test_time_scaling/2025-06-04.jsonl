{"id": "2506.02338", "pdf": "https://arxiv.org/pdf/2506.02338", "abs": "https://arxiv.org/abs/2506.02338", "authors": ["Hyungjoo Chae", "Dongjin Kang", "Jihyuk Kim", "Beong-woo Kwak", "Sunghyun Park", "Haeju Park", "Jinyoung Yeo", "Moontae Lee", "Kyungjae Lee"], "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL", "categories": ["cs.CL"], "comment": "ACL 2025 Industry", "summary": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "o1", "reasoning model"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03140", "pdf": "https://arxiv.org/pdf/2506.03140", "abs": "https://arxiv.org/abs/2506.03140", "authors": ["Yawen Luo", "Jianhong Bai", "Xiaoyu Shi", "Menghan Xia", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Tianfan Xue"], "title": "CamCloneMaster: Enabling Reference-based Camera Control for Video Generation", "categories": ["cs.CV"], "comment": "Project Page: https://camclonemaster.github.io/", "summary": "Camera control is crucial for generating expressive and cinematic videos.\nExisting methods rely on explicit sequences of camera parameters as control\nconditions, which can be cumbersome for users to construct, particularly for\nintricate camera movements. To provide a more intuitive camera control method,\nwe propose CamCloneMaster, a framework that enables users to replicate camera\nmovements from reference videos without requiring camera parameters or\ntest-time fine-tuning. CamCloneMaster seamlessly supports reference-based\ncamera control for both Image-to-Video and Video-to-Video tasks within a\nunified framework. Furthermore, we present the Camera Clone Dataset, a\nlarge-scale synthetic dataset designed for camera clone learning, encompassing\ndiverse scenes, subjects, and camera movements. Extensive experiments and user\nstudies demonstrate that CamCloneMaster outperforms existing methods in terms\nof both camera controllability and visual quality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scale", "test-time fine-tuning"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02453", "pdf": "https://arxiv.org/pdf/2506.02453", "abs": "https://arxiv.org/abs/2506.02453", "authors": ["Kunyu Wang", "Xueyang Fu", "Yunfei Bao", "Chengjie Ge", "Chengzhi Cao", "Wei Zhai", "Zheng-Jun Zha"], "title": "PAID: Pairwise Angular-Invariant Decomposition for Continual Test-Time Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Continual Test-Time Adaptation (CTTA) aims to online adapt a pre-trained\nmodel to changing environments during inference. Most existing methods focus on\nexploiting target data, while overlooking another crucial source of\ninformation, the pre-trained weights, which encode underutilized\ndomain-invariant priors. This paper takes the geometric attributes of\npre-trained weights as a starting point, systematically analyzing three key\ncomponents: magnitude, absolute angle, and pairwise angular structure. We find\nthat the pairwise angular structure remains stable across diverse corrupted\ndomains and encodes domain-invariant semantic information, suggesting it should\nbe preserved during adaptation. Based on this insight, we propose PAID\n(Pairwise Angular-Invariant Decomposition), a prior-driven CTTA method that\ndecomposes weight into magnitude and direction, and introduces a learnable\northogonal matrix via Householder reflections to globally rotate direction\nwhile preserving the pairwise angular structure. During adaptation, only the\nmagnitudes and the orthogonal matrices are updated. PAID achieves consistent\nimprovements over recent SOTA methods on four widely used CTTA benchmarks,\ndemonstrating that preserving pairwise angular structure offers a simple yet\neffective principle for CTTA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02536", "pdf": "https://arxiv.org/pdf/2506.02536", "abs": "https://arxiv.org/abs/2506.02536", "authors": ["Xin Liu", "Lu Wang"], "title": "Answer Convergence as a Signal for Early Stopping in Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) prompting enhances reasoning in large language models\n(LLMs) but often leads to verbose and redundant outputs, thus increasing\ninference cost. We hypothesize that many reasoning steps are unnecessary for\nproducing correct answers. To investigate this, we start with a systematic\nstudy to examine what is the minimum reasoning required for a model to reach a\nstable decision. We find that on math reasoning tasks like math, models\ntypically converge to their final answers after 60\\% of the reasoning steps,\nsuggesting substantial redundancy in the remaining content. Based on these\ninsights, we propose three inference-time strategies to improve efficiency: (1)\nearly stopping via answer consistency, (2) boosting the probability of\ngenerating end-of-reasoning signals, and (3) a supervised method that learns\nwhen to stop based on internal activations. Experiments across five benchmarks\nand five open-weights LLMs show that our methods significantly reduce token\nusage with little or no accuracy drop. In particular, on NaturalQuestions,\nAnswer Consistency reduces tokens by over 40\\% while further improving\naccuracy. Our work underscores the importance of cost-effective reasoning\nmethods that operate at inference time, offering practical benefits for\nreal-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "inference time"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02591", "pdf": "https://arxiv.org/pdf/2506.02591", "abs": "https://arxiv.org/abs/2506.02591", "authors": ["Minh Duc Bui", "Kyung Eun Park", "Goran GlavaÅ¡", "Fabian David Schmidt", "Katharina von der Wense"], "title": "On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main (Camera-Ready Version)", "summary": "Measurement systems (e.g., currencies) differ across cultures, but the\nconversions between them are well defined so that humans can state facts using\nany measurement system of their choice. Being available to users from diverse\ncultural backgrounds, large language models (LLMs) should also be able to\nprovide accurate information irrespective of the measurement system at hand.\nUsing newly compiled datasets we test if this is the case for seven open-source\nLLMs, addressing three key research questions: (RQ1) What is the default system\nused by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their\naccuracy vary across different measurement systems? (RQ3) Can LLMs mitigate\npotential challenges w.r.t. underrepresented systems via reasoning? Our\nfindings show that LLMs default to the measurement system predominantly used in\nthe data. Additionally, we observe considerable instability and variance in\nperformance across different measurement systems. While this instability can in\npart be mitigated by employing reasoning methods such as chain-of-thought\n(CoT), this implies longer responses and thereby significantly increases\ntest-time compute (and inference costs), marginalizing users from cultural\nbackgrounds that use underrepresented measurement systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time compute"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02671", "pdf": "https://arxiv.org/pdf/2506.02671", "abs": "https://arxiv.org/abs/2506.02671", "authors": ["Xiao Chen", "Jiazhen Huang", "Qinting Jiang", "Fanding Huang", "Xianghua Fu", "Jingyan Jiang", "Zhi Wang"], "title": "Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language Models with AdaptNet", "categories": ["cs.CV"], "comment": null, "summary": "Test-time adaptation (TTA) has emerged as a critical technique for enhancing\nthe generalization capability of vision-language models (VLMs) during\ninference. However, existing approaches often incur substantial computational\ncosts and exhibit poor scalability, primarily due to sample-wise adaptation\ngranularity and reliance on costly auxiliary designs such as data augmentation.\nTo address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel\nadapter-based TTA framework that leverages a lightweight, learnable AdaptNet to\nenable efficient and scalable model adaptation. As SAIL's core, a frozen\npre-trained VLM collaborates with AdaptNet through a confidence-based\ninterpolation weight, generating robust predictions during inference. These\npredictions serve as self-supervised targets to align AdaptNet's outputs\nthrough efficient batch-wise processing, dramatically reducing computational\ncosts without modifying the VLM or requiring memory caches. To mitigate\ncatastrophic forgetting during continual adaptation, we propose a\ngradient-aware reset strategy driven by a gradient drift indicator (GDI), which\ndynamically detects domain transitions and strategically resets AdaptNet for\nstable adaptation. Extensive experiments across diverse benchmarks on two\nscenarios demonstrate that SAIL achieves state-of-the-art performance while\nmaintaining low computational costs. These results highlight SAIL's\neffectiveness, efficiency and scalability for real-world deployment. The code\nwill be released upon acceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02803", "pdf": "https://arxiv.org/pdf/2506.02803", "abs": "https://arxiv.org/abs/2506.02803", "authors": ["Sifan Li", "Yujun Cai", "Yiwei Wang"], "title": "SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03136", "pdf": "https://arxiv.org/pdf/2506.03136", "abs": "https://arxiv.org/abs/2506.03136", "authors": ["Yinjie Wang", "Ling Yang", "Ye Tian", "Ke Shen", "Mengdi Wang"], "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning", "categories": ["cs.CL"], "comment": "Project: https://github.com/Gen-Verse/CURE", "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "code generation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02096", "pdf": "https://arxiv.org/pdf/2506.02096", "abs": "https://arxiv.org/abs/2506.02096", "authors": ["Zijian Wu", "Jinjie Ni", "Xiangyan Liu", "Zichen Liu", "Hang Yan", "Michael Qizhe Shieh"], "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose \\textbf{SynthRL}-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02890", "pdf": "https://arxiv.org/pdf/2506.02890", "abs": "https://arxiv.org/abs/2506.02890", "authors": ["Jakub Krajewski", "Marcin Chochowski", "Daniel Korzekwa"], "title": "Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Mixture of Experts (MoE) architectures have emerged as pivotal for scaling\nLarge Language Models (LLMs) efficiently. Fine-grained MoE approaches -\nutilizing more numerous, smaller experts - have demonstrated potential in\nimproving model convergence and quality. This work proposes a set of training\nrecipes and provides a comprehensive empirical evaluation of fine-grained MoE,\ndirectly comparing its scaling properties against standard MoE configurations\nfor models with up to 56B total (17B active) parameters. We investigate\nconvergence speed, model performance on downstream benchmarks, and practical\ntraining considerations across various setups. Overall, at the largest scale we\nshow that fine-grained MoE achieves better validation loss and higher accuracy\nacross a set of downstream benchmarks. This study offers empirical grounding\nand practical insights for leveraging fine-grained MoE in the development of\nfuture large-scale models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02096", "pdf": "https://arxiv.org/pdf/2506.02096", "abs": "https://arxiv.org/abs/2506.02096", "authors": ["Zijian Wu", "Jinjie Ni", "Xiangyan Liu", "Zichen Liu", "Hang Yan", "Michael Qizhe Shieh"], "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose \\textbf{SynthRL}-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02803", "pdf": "https://arxiv.org/pdf/2506.02803", "abs": "https://arxiv.org/abs/2506.02803", "authors": ["Sifan Li", "Yujun Cai", "Yiwei Wang"], "title": "SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02774", "pdf": "https://arxiv.org/pdf/2506.02774", "abs": "https://arxiv.org/abs/2506.02774", "authors": ["Zheng Liu", "He Zhu", "Xinyang Li", "Yirun Wang", "Yujiao Shi", "Wei Li", "Jingwen Leng", "Minyi Guo", "Yu Feng"], "title": "Voyager: Real-Time Splatting City-Scale 3D Gaussians on Your Phone", "categories": ["cs.GR"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is an emerging technique for photorealistic 3D\nscene rendering. However, rendering city-scale 3DGS scenes on mobile devices,\ne.g., your smartphones, remains a significant challenge due to the limited\nresources on mobile devices. A natural solution is to offload computation to\nthe cloud; however, naively streaming rendered frames from the cloud to the\nclient introduces high latency and requires bandwidth far beyond the capacity\nof current wireless networks.\n  In this paper, we propose an effective solution to enable city-scale 3DGS\nrendering on mobile devices. Our key insight is that, under normal user motion,\nthe number of newly visible Gaussians per second remains roughly constant.\nLeveraging this, we stream only the necessary Gaussians to the client.\nSpecifically, on the cloud side, we propose asynchronous level-of-detail search\nto identify the necessary Gaussians for the client. On the client side, we\naccelerate rendering via a lookup table-based rasterization. Combined with\nholistic runtime optimizations, our system can deliver low-latency, city-scale\n3DGS rendering on mobile devices. Compared to existing solutions, Voyager\nachieves over 100$\\times$ reduction on data transfer and up to 8.9$\\times$\nspeedup while retaining comparable rendering quality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.01961", "pdf": "https://arxiv.org/pdf/2506.01961", "abs": "https://arxiv.org/abs/2506.01961", "authors": ["Jinzhu Yang"], "title": "Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System", "categories": ["cs.CL"], "comment": null, "summary": "This study is dedicated to exploring the application of prompt learning\nmethods to advance Named Entity Recognition (NER) within the medical domain. In\nrecent years, the emergence of large-scale models has driven significant\nprogress in NER tasks, particularly with the introduction of the BioBERT\nlanguage model, which has greatly enhanced NER capabilities in medical texts.\nOur research introduces the Prompt-bioMRC model, which integrates both hard\ntemplate and soft prompt designs aimed at refining the precision and efficiency\nof medical entity recognition. Through extensive experimentation across diverse\nmedical datasets, our findings consistently demonstrate that our approach\nsurpasses traditional models. This enhancement not only validates the efficacy\nof our methodology but also highlights its potential to provide reliable\ntechnological support for applications like intelligent diagnosis systems. By\nleveraging advanced NER techniques, this study contributes to advancing\nautomated medical data processing, facilitating more accurate medical\ninformation extraction, and supporting efficient healthcare decision-making\nprocesses.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02011", "pdf": "https://arxiv.org/pdf/2506.02011", "abs": "https://arxiv.org/abs/2506.02011", "authors": ["Minjae Lee", "Minhyuk Seo", "Tingyu Qu", "Tinne Tuytelaars", "Jonghyun Choi"], "title": "OASIS: Online Sample Selection for Continual Visual Instruction Tuning", "categories": ["cs.CV"], "comment": null, "summary": "In continual visual instruction tuning (CVIT) scenarios, where multi-modal\ndata continuously arrive in an online streaming manner, training delays from\nlarge-scale data significantly hinder real-time adaptation. While existing data\nselection strategies reduce training overheads, they rely on pre-trained\nreference models, which are impractical in CVIT setups due to unknown future\ndata. Recent reference model-free online sample selection methods address this\nissue but typically select a fixed number of samples per batch (e.g., top-k),\ncausing them to suffer from distribution shifts where informativeness varies\nacross batches. To address these limitations, we propose OASIS, an adaptive\nonline sample selection approach for CVIT that: (1) dynamically adjusts\nselected samples per batch based on relative inter-batch informativeness, and\n(2) minimizes redundancy of selected samples through iterative selection score\nupdates. Empirical results across various MLLMs, such as LLaVA-1.5 and\nQwen-VL-2.5, show that OASIS achieves comparable performance to full-data\ntraining using only 25% of the data and outperforms the state-of-the-art.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02012", "pdf": "https://arxiv.org/pdf/2506.02012", "abs": "https://arxiv.org/abs/2506.02012", "authors": ["Zehua Liu", "Xiaolou Li", "Li Guo", "Lantian Li", "Dong Wang"], "title": "Leveraging Large Language Models in Visual Speech Recognition: Model Scaling, Context-Aware Decoding, and Iterative Polishing", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Visual Speech Recognition (VSR) transcribes speech by analyzing lip\nmovements. Recently, Large Language Models (LLMs) have been integrated into VSR\nsystems, leading to notable performance improvements. However, the potential of\nLLMs has not been extensively studied, and how to effectively utilize LLMs in\nVSR tasks remains unexplored. This paper systematically explores how to better\nleverage LLMs for VSR tasks and provides three key contributions: (1) Scaling\nTest: We study how the LLM size affects VSR performance, confirming a scaling\nlaw in the VSR task. (2) Context-Aware Decoding: We add contextual text to\nguide the LLM decoding, improving recognition accuracy. (3) Iterative\nPolishing: We propose iteratively refining LLM outputs, progressively reducing\nrecognition errors. Extensive experiments demonstrate that by these designs,\nthe great potential of LLMs can be largely harnessed, leading to significant\nVSR performance improvement.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02000", "pdf": "https://arxiv.org/pdf/2506.02000", "abs": "https://arxiv.org/abs/2506.02000", "authors": ["Abhay Gupta", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts", "categories": ["cs.CL"], "comment": null, "summary": "Current large language models (LLMs) struggle to answer questions that span\ntens of thousands of tokens, especially when multi-hop reasoning is involved.\nWhile prior benchmarks explore long-context comprehension or multi-hop\nreasoning in isolation, none jointly vary context length and reasoning depth in\nnatural narrative settings. We introduce NovelHopQA, the first benchmark to\nevaluate k1-4 hop QA over 64k-128k-token excerpts from 83 full-length\npublic-domain novels. A keyword-guided pipeline builds hop-separated chains\ngrounded in coherent storylines. We evaluate six state-of-the-art (SOTA) models\nand apply oracle-context filtering to ensure all questions are genuinely\nanswerable. Human annotators validate both alignment and hop depth. We noticed\nconsistent accuracy drops with increased hops and context length, even in\nfrontier models-revealing that sheer scale does not guarantee robust reasoning.\nOur failure mode analysis highlights common breakdowns, such as missed\nfinal-hop integration and long-range drift. NovelHopQA offers a controlled\ndiagnostic setting to stress-test multi-hop reasoning at scale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02380", "pdf": "https://arxiv.org/pdf/2506.02380", "abs": "https://arxiv.org/abs/2506.02380", "authors": ["Zihao Ding", "Cheng-Tse Lee", "Mufeng Zhu", "Tao Guan", "Yuan-Chun Sun", "Cheng-Hsin Hsu", "Yao Liu"], "title": "EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for Real-World 3DGS Scenes in VR", "categories": ["cs.MM", "cs.CV", "cs.GR", "cs.HC"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is an emerging media representation that\nreconstructs real-world 3D scenes in high fidelity, enabling\n6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,\ndeveloping and evaluating 3DGS-enabled applications and optimizing their\nrendering performance, require realistic user navigation data. Such data is\ncurrently unavailable for photorealistic 3DGS reconstructions of real-world\nscenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available\n6-DoF navigation dataset featuring traces from 46 participants exploring twelve\ndiverse, real-world 3DGS scenes. The dataset was collected at two sites, using\nthe Meta Quest Pro headsets, recording the head pose and eye gaze data for each\nrendered frame during free world standing 6-DoF navigation. For each of the\ntwelve scenes, we performed careful scene initialization to correct for scene\ntilt and scale, ensuring a perceptually-comfortable VR experience. We also\nrelease our open-source SIBR viewer software fork with record-and-replay\nfunctionalities and a suite of utility tools for data processing, conversion,\nand visualization. The EyeNavGS dataset and its accompanying software tools\nprovide valuable resources for advancing research in 6-DoF viewport prediction,\nadaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The\nEyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02021", "pdf": "https://arxiv.org/pdf/2506.02021", "abs": "https://arxiv.org/abs/2506.02021", "authors": ["Yinjie Zhao", "Heng Zhao", "Bihan Wen", "Yew-Soon Ong", "Joey Tianyi Zhou"], "title": "Dynamic-Aware Video Distillation: Optimizing Temporal Resolution Based on Video Semantics", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of vision tasks and the scaling on datasets and\nmodels, redundancy reduction in vision datasets has become a key area of\nresearch. To address this issue, dataset distillation (DD) has emerged as a\npromising approach to generating highly compact synthetic datasets with\nsignificantly less redundancy while preserving essential information. However,\nwhile DD has been extensively studied for image datasets, DD on video datasets\nremains underexplored. Video datasets present unique challenges due to the\npresence of temporal information and varying levels of redundancy across\ndifferent classes. Existing DD approaches assume a uniform level of temporal\nredundancy across all different video semantics, which limits their\neffectiveness on video datasets. In this work, we propose Dynamic-Aware Video\nDistillation (DAViD), a Reinforcement Learning (RL) approach to predict the\noptimal Temporal Resolution of the synthetic videos. A teacher-in-the-loop\nreward function is proposed to update the RL agent policy. To the best of our\nknowledge, this is the first study to introduce adaptive temporal resolution\nbased on video semantics in video dataset distillation. Our approach\nsignificantly outperforms existing DD methods, demonstrating substantial\nimprovements in performance. This work paves the way for future research on\nmore efficient and semantic-adaptive video dataset distillation research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02126", "pdf": "https://arxiv.org/pdf/2506.02126", "abs": "https://arxiv.org/abs/2506.02126", "authors": ["Juncheng Wu", "Sheng Liu", "Haoqin Tu", "Hang Yu", "Xiaoke Huang", "James Zou", "Cihang Xie", "Yuyin Zhou"], "title": "Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains", "categories": ["cs.CL"], "comment": "17 pages, preprint", "summary": "Recent advances in reasoning-enhanced Large Language Models such as\nOpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex\ntasks. However, the quality and transparency of their internal reasoning\nprocesses remain underexplored. This work moves beyond the final-answer\naccuracy and investigates step-by-step reasoning in the medical and\nmathematical domains by explicitly decomposing the thinking trajectories into\ntwo parts: knowledge and reasoning. Specifically, we introduce a fine-grained\nevaluation framework that judges: (1) the correctness of knowledge used\n(measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured\nby Information Gain (InfoGain)). Using this framework, we study R1-distilled\nand base Qwen models trained with supervised fine-tuning (SFT) and/or\nreinforcement learning (RL) in the medical and math domains. Three intriguing\nfindings emerge: (1) The general reasoning abilities in R1-distilled models do\nnot transfer effectively to the medical domain through either SFT or RL. (2)\nSFT raises final-answer accuracy in both domains, but often at the cost of\nreasoning quality: InfoGain drops by 38.9% on average compared with untrained\nmodels; In the medical domain, however, SFT remains crucial because domain\nknowledge is indispensable. (3) RL enhances medical reasoning by pruning\ninaccurate or irrelevant knowledge from reasoning paths, thereby improving both\nreasoning accuracy and knowledge correctness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02095", "pdf": "https://arxiv.org/pdf/2506.02095", "abs": "https://arxiv.org/abs/2506.02095", "authors": ["Hyojin Bahng", "Caroline Chan", "Fredo Durand", "Phillip Isola"], "title": "Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Learning alignment between language and vision is a fundamental challenge,\nespecially as multimodal data becomes increasingly detailed and complex.\nExisting methods often rely on collecting human or AI preferences, which can be\ncostly and time-intensive. We propose an alternative approach that leverages\ncycle consistency as a supervisory signal. Given an image and generated text,\nwe map the text back to image space using a text-to-image model and compute the\nsimilarity between the original image and its reconstruction. Analogously, for\ntext-to-image generation, we measure the textual similarity between an input\ncaption and its reconstruction through the cycle. We use the cycle consistency\nscore to rank candidates and construct a preference dataset of 866K comparison\npairs. The reward model trained on our dataset outperforms state-of-the-art\nalignment metrics on detailed captioning, with superior inference-time\nscalability when used as a verifier for Best-of-N sampling. Furthermore,\nperforming DPO and Diffusion DPO using our dataset enhances performance across\na wide range of vision-language tasks and text-to-image generation. Our\ndataset, model, and code are at https://cyclereward.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "comparison", "alignment", "DPO"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "consistency"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02150", "pdf": "https://arxiv.org/pdf/2506.02150", "abs": "https://arxiv.org/abs/2506.02150", "authors": ["Stefano Fogarollo", "Gregor Laimer", "Reto Bale", "Matthias Harders"], "title": "Implicit Deformable Medical Image Registration with Learnable Kernels", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025 Provisional Accept", "summary": "Deformable medical image registration is an essential task in\ncomputer-assisted interventions. This problem is particularly relevant to\noncological treatments, where precise image alignment is necessary for tracking\ntumor growth, assessing treatment response, and ensuring accurate delivery of\ntherapies. Recent AI methods can outperform traditional techniques in accuracy\nand speed, yet they often produce unreliable deformations that limit their\nclinical adoption. In this work, we address this challenge and introduce a\nnovel implicit registration framework that can predict accurate and reliable\ndeformations. Our insight is to reformulate image registration as a signal\nreconstruction problem: we learn a kernel function that can recover the dense\ndisplacement field from sparse keypoint correspondences. We integrate our\nmethod in a novel hierarchical architecture, and estimate the displacement\nfield in a coarse-to-fine manner. Our formulation also allows for efficient\nrefinement at test time, permitting clinicians to easily adjust registrations\nwhen needed. We validate our method on challenging intra-patient thoracic and\nabdominal zero-shot registration tasks, using public and internal datasets from\nthe local University Hospital. Our method not only shows competitive accuracy\nto state-of-the-art approaches, but also bridges the generalization gap between\nimplicit and explicit registration techniques. In particular, our method\ngenerates deformations that better preserve anatomical relationships and\nmatches the performance of specialized commercial systems, underscoring its\npotential for clinical adoption.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02212", "pdf": "https://arxiv.org/pdf/2506.02212", "abs": "https://arxiv.org/abs/2506.02212", "authors": ["Ella Rannon", "David Burstein"], "title": "Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics", "categories": ["cs.CL", "cs.AI", "q-bio.GN"], "comment": null, "summary": "Natural Language Processing (NLP) has transformed various fields beyond\nlinguistics by applying techniques originally developed for human language to\nthe analysis of biological sequences. This review explores the application of\nNLP methods to biological sequence data, focusing on genomics, transcriptomics,\nand proteomics. We examine how various NLP methods, from classic approaches\nlike word2vec to advanced models employing transformers and hyena operators,\nare being adapted to analyze DNA, RNA, protein sequences, and entire genomes.\nThe review also examines tokenization strategies and model architectures,\nevaluating their strengths, limitations, and suitability for different\nbiological tasks. We further cover recent advances in NLP applications for\nbiological data, such as structure prediction, gene expression, and\nevolutionary analysis, highlighting the potential of these methods for\nextracting meaningful insights from large-scale genomic data. As language\nmodels continue to advance, their integration into bioinformatics holds immense\npromise for advancing our understanding of biological processes in all domains\nof life.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02244", "pdf": "https://arxiv.org/pdf/2506.02244", "abs": "https://arxiv.org/abs/2506.02244", "authors": ["Bowen Xue", "Giuseppe Claudio Guarnera", "Shuang Zhao", "Zahra Montazeri"], "title": "Motion aware video generative model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in diffusion-based video generation have yielded\nunprecedented quality in visual content and semantic coherence. However,\ncurrent approaches predominantly rely on statistical learning from vast\ndatasets without explicitly modeling the underlying physics of motion,\nresulting in subtle yet perceptible non-physical artifacts that diminish the\nrealism of generated videos. This paper introduces a physics-informed frequency\ndomain approach to enhance the physical plausibility of generated videos. We\nfirst conduct a systematic analysis of the frequency-domain characteristics of\ndiverse physical motions (translation, rotation, scaling), revealing that each\nmotion type exhibits distinctive and identifiable spectral signatures. Building\non this theoretical foundation, we propose two complementary components: (1) a\nphysical motion loss function that quantifies and optimizes the conformity of\ngenerated videos to ideal frequency-domain motion patterns, and (2) a frequency\ndomain enhancement module that progressively learns to adjust video features to\nconform to physical motion constraints while preserving original network\nfunctionality through a zero-initialization strategy. Experiments across\nmultiple video diffusion architectures demonstrate that our approach\nsignificantly enhances motion quality and physical plausibility without\ncompromising visual quality or semantic alignment. Our frequency-domain\nphysical motion framework generalizes effectively across different video\ngeneration architectures, offering a principled approach to incorporating\nphysical constraints into deep learning-based video synthesis pipelines. This\nwork seeks to establish connections between data-driven models and\nphysics-based motion models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02265", "pdf": "https://arxiv.org/pdf/2506.02265", "abs": "https://arxiv.org/abs/2506.02265", "authors": ["Samuel Li", "Pujith Kachana", "Prajwal Chidananda", "Saurabh Nair", "Yasutaka Furukawa", "Matthew Brown"], "title": "Rig3R: Rig-Aware Conditioning for Learned 3D Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Estimating agent pose and 3D scene structure from multi-camera rigs is a\ncentral task in embodied AI applications such as autonomous driving. Recent\nlearned approaches such as DUSt3R have shown impressive performance in\nmultiview settings. However, these models treat images as unstructured\ncollections, limiting effectiveness in scenarios where frames are captured from\nsynchronized rigs with known or inferable structure.\n  To this end, we introduce Rig3R, a generalization of prior multiview\nreconstruction models that incorporates rig structure when available, and\nlearns to infer it when not. Rig3R conditions on optional rig metadata\nincluding camera ID, time, and rig poses to develop a rig-aware latent space\nthat remains robust to missing information. It jointly predicts pointmaps and\ntwo types of raymaps: a pose raymap relative to a global frame, and a rig\nraymap relative to a rig-centric frame consistent across time. Rig raymaps\nallow the model to infer rig structure directly from input images when metadata\nis missing.\n  Rig3R achieves state-of-the-art performance in 3D reconstruction, camera pose\nestimation, and rig discovery, outperforming both traditional and learned\nmethods by 17-45% mAA across diverse real-world rig datasets, all in a single\nforward pass without post-processing or iterative refinement.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02294", "pdf": "https://arxiv.org/pdf/2506.02294", "abs": "https://arxiv.org/abs/2506.02294", "authors": ["Niclas Popp", "Kevin Alexander Laube", "Matthias Hein", "Lukas Schott"], "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation", "categories": ["cs.CV"], "comment": null, "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02356", "pdf": "https://arxiv.org/pdf/2506.02356", "abs": "https://arxiv.org/abs/2506.02356", "authors": ["Woojeong Jin", "Seongchan Kim", "Seungryong Kim"], "title": "InterRVOS: Interaction-aware Referring Video Object Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Referring video object segmentation aims to segment the object in a video\ncorresponding to a given natural language expression. While prior works have\nexplored various referring scenarios, including motion-centric or\nmulti-instance expressions, most approaches still focus on localizing a single\ntarget object in isolation. However, in comprehensive video understanding, an\nobject's role is often defined by its interactions with other entities, which\nare largely overlooked in existing datasets and models. In this work, we\nintroduce Interaction-aware referring video object sgementation (InterRVOS), a\nnew task that requires segmenting both actor and target entities involved in an\ninteraction. Each interactoin is described through a pair of complementary\nexpressions from different semantic perspectives, enabling fine-grained\nmodeling of inter-object relationships. To tackle this task, we propose\nInterRVOS-8K, the large-scale and automatically constructed dataset containing\ndiverse interaction-aware expressions with corresponding masks, including\nchallenging cases such as motion-only multi-instance expressions. We also\npresent a baseline architecture, ReVIOSa, designed to handle actor-target\nsegmentation from a single expression, achieving strong performance in both\nstandard and interaction-focused settings. Furthermore, we introduce an\nactor-target-aware evalaution setting that enables a more targeted assessment\nof interaction understanding. Experimental results demonstrate that our\napproach outperforms prior methods in modeling complex object interactions for\nreferring video object segmentation task, establishing a strong foundation for\nfuture research in interaction-centric video understanding. Our project page is\navailable at\n\\href{https://cvlab-kaist.github.io/InterRVOS}{https://cvlab-kaist.github.io/InterRVOS}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02358", "pdf": "https://arxiv.org/pdf/2506.02358", "abs": "https://arxiv.org/abs/2506.02358", "authors": ["Tianze Wang", "Zhang Zhang", "Chao Sun"], "title": "RoadFormer : Local-Global Feature Fusion for Road Surface Classification in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "The classification of the type of road surface (RSC) aims to utilize pavement\nfeatures to identify the roughness, wet and dry conditions, and material\ninformation of the road surface. Due to its ability to effectively enhance road\nsafety and traffic management, it has received widespread attention in recent\nyears. In autonomous driving, accurate RSC allows vehicles to better understand\nthe road environment, adjust driving strategies, and ensure a safer and more\nefficient driving experience. For a long time, vision-based RSC has been\nfavored. However, existing visual classification methods have overlooked the\nexploration of fine-grained classification of pavement types (such as similar\npavement textures). In this work, we propose a pure vision-based fine-grained\nRSC method for autonomous driving scenarios, which fuses local and global\nfeature information through the stacking of convolutional and transformer\nmodules. We further explore the stacking strategies of local and global feature\nextraction modules to find the optimal feature extraction strategy. In\naddition, since fine-grained tasks also face the challenge of relatively large\nintra-class differences and relatively small inter-class differences, we\npropose a Foreground-Background Module (FBM) that effectively extracts\nfine-grained context features of the pavement, enhancing the classification\nability for complex pavements. Experiments conducted on a large-scale pavement\ndataset containing one million samples and a simplified dataset reorganized\nfrom this dataset achieved Top-1 classification accuracies of 92.52% and\n96.50%, respectively, improving by 5.69% to 12.84% compared to SOTA methods.\nThese results demonstrate that RoadFormer outperforms existing methods in RSC\ntasks, providing significant progress in improving the reliability of pavement\nperception in autonomous driving systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "reliability", "fine-grained"], "score": 4}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02359", "pdf": "https://arxiv.org/pdf/2506.02359", "abs": "https://arxiv.org/abs/2506.02359", "authors": ["Brent A. Griffin", "Manushree Gangwar", "Jacob Sela", "Jason J. Corso"], "title": "Auto-Labeling Data for Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Great labels make great models. However, traditional labeling approaches for\ntasks like object detection have substantial costs at scale. Furthermore,\nalternatives to fully-supervised object detection either lose functionality or\nrequire larger models with prohibitive computational costs for inference at\nscale. To that end, this paper addresses the problem of training standard\nobject detection models without any ground truth labels. Instead, we configure\npreviously-trained vision-language foundation models to generate\napplication-specific pseudo \"ground truth\" labels. These auto-generated labels\ndirectly integrate with existing model training frameworks, and we subsequently\ntrain lightweight detection models that are computationally efficient. In this\nway, we avoid the costs of traditional labeling, leverage the knowledge of\nvision-language models, and keep the efficiency of lightweight models for\npractical application. We perform exhaustive experiments across multiple\nlabeling configurations, downstream inference models, and datasets to establish\nbest practices and set an extensive auto-labeling benchmark. From our results,\nwe find that our approach is a viable alternative to standard labeling in that\nit maintains competitive performance on multiple datasets and substantially\nreduces labeling time and costs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02351", "pdf": "https://arxiv.org/pdf/2506.02351", "abs": "https://arxiv.org/abs/2506.02351", "authors": ["Jeonghun Kang", "Soonmok Kwon", "Joonseok Lee", "Byung-Hak Kim"], "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "To appear in the First REALM (Research on Agent Language Models)\n  workshop at ACL 2025", "summary": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking\nor computer vision-driven event detection -- can identify scoring plays but\noften miss strategic depth, momentum shifts, and storyline progression. Manual\ncuration remains the gold standard but is resource-intensive and not scalable.\nWe introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight\nsummarization that integrates structured sports analytics with natural language\nreasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and\nLeverage Index -- to quantify play importance, while an LLM module enhances\nselection based on contextual narrative value. This hybrid approach ensures\nboth quantitative rigor and qualitative richness, surpassing the limitations of\npurely statistical or vision-based systems. Evaluated on five diverse Korean\nBaseball Organization League games, DIAMOND improves F1-score from 42.9%\n(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.\nThough limited in scale, our results highlight the potential of modular,\ninterpretable agent-based frameworks for event-level summarization in sports\nand beyond.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02366", "pdf": "https://arxiv.org/pdf/2506.02366", "abs": "https://arxiv.org/abs/2506.02366", "authors": ["Qin Xie", "Qinghua Zhang", "Shuyin Xia"], "title": "Approximate Borderline Sampling using Granular-Ball for Classification Tasks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Data sampling enhances classifier efficiency and robustness through data\ncompression and quality improvement. Recently, the sampling method based on\ngranular-ball (GB) has shown promising performance in generality and noisy\nclassification tasks. However, some limitations remain, including the absence\nof borderline sampling strategies and issues with class boundary blurring or\nshrinking due to overlap between GBs. In this paper, an approximate borderline\nsampling method using GBs is proposed for classification tasks. First, a\nrestricted diffusion-based GB generation (RD-GBG) method is proposed, which\nprevents GB overlaps by constrained expansion, preserving precise geometric\nrepresentation of GBs via redefined ones. Second, based on the concept of\nheterogeneous nearest neighbor, a GB-based approximate borderline sampling\n(GBABS) method is proposed, which is the first general sampling method capable\nof both borderline sampling and improving the quality of class noise datasets.\nAdditionally, since RD-GBG incorporates noise detection and GBABS focuses on\nborderline samples, GBABS performs outstandingly on class noise datasets\nwithout the need for an optimal purity threshold. Experimental results\ndemonstrate that the proposed methods outperform the GB-based sampling method\nand several representative sampling methods. Our source code is publicly\navailable at https://github.com/CherylTse/GBABS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02367", "pdf": "https://arxiv.org/pdf/2506.02367", "abs": "https://arxiv.org/abs/2506.02367", "authors": ["Jiayi Su", "Dequan Jin"], "title": "ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery", "categories": ["cs.CV", "68T07", "I.5.1"], "comment": "22 pages, 3 figures", "summary": "Generalized category discovery (GCD) is a highly popular task in open-world\nrecognition, aiming to identify unknown class samples using known class data.\nBy leveraging pre-training, meta-training, and fine-tuning, ViT achieves\nexcellent few-shot learning capabilities. Its MLP head is a feedforward\nnetwork, trained synchronously with the entire network in the same process,\nincreasing the training cost and difficulty without fully leveraging the power\nof the feature extractor. This paper proposes a new architecture by replacing\nthe MLP head with a neural field-based one. We first present a new static\nneural field function to describe the activity distribution of the neural field\nand then use two static neural field functions to build an efficient few-shot\nclassifier. This neural field-based (NF) classifier consists of two coupled\nstatic neural fields. It stores the feature information of support samples by\nits elementary field, the known categories by its high-level field, and the\ncategory information of support samples by its cross-field connections. We\nreplace the MLP head with the proposed NF classifier, resulting in a novel\narchitecture ViTNF, and simplify the three-stage training mode by pre-training\nthe feature extractor on source tasks and training the NF classifier with\nsupport samples in meta-testing separately, significantly reducing ViT's demand\nfor training samples and the difficulty of model training. To enhance the\nmodel's capability in identifying new categories, we provide an effective\nalgorithm to determine the lateral interaction scale of the elementary field.\nExperimental results demonstrate that our model surpasses existing\nstate-of-the-art methods on CIFAR-100, ImageNet-100, CUB-200, and Standard\nCars, achieving dramatic accuracy improvements of 19\\% and 16\\% in new and all\nclasses, respectively, indicating a notable advantage in GCD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02404", "pdf": "https://arxiv.org/pdf/2506.02404", "abs": "https://arxiv.org/abs/2506.02404", "authors": ["Yilin Xiao", "Junnan Dong", "Chuang Zhou", "Su Dong", "Qianwen Zhang", "Di Yin", "Xing Sun", "Xiao Huang"], "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing\nrecognition for its potential to enhance large language models (LLMs) by\nstructurally organizing domain-specific corpora and facilitating complex\nreasoning. However, current evaluations of GraphRAG models predominantly rely\non traditional question-answering datasets. Their limited scope in questions\nand evaluation metrics fails to comprehensively assess the reasoning capacity\nimprovements enabled by GraphRAG models. To address this gap, we introduce\nGraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously\nevaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\)\nChallenging question design. Featuring college-level, domain-specific questions\nthat demand multi-hop reasoning, the benchmark ensures that simple content\nretrieval is insufficient for problem-solving. For example, some questions\nrequire mathematical reasoning or programming. \\((ii)\\) Diverse task coverage.\nThe dataset includes a broad spectrum of reasoning tasks, multiple-choice,\ntrue/false, multi-select, open-ended, and fill-in-the-blank. It spans 16\ndisciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework.\nGraphRAG-Bench provides comprehensive assessment across the entire GraphRAG\npipeline, including graph construction, knowledge retrieval, and answer\ngeneration. Beyond final-answer correctness, it evaluates the logical coherence\nof the reasoning process. By applying nine contemporary GraphRAG methods to\nGraphRAG-Bench, we demonstrate its utility in quantifying how graph-based\nstructuring improves model reasoning capabilities. Our analysis reveals\ncritical insights about graph architectures, retrieval efficacy, and reasoning\ncapabilities, offering actionable guidance for the research community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "mathematical reasoning"], "score": 4}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02408", "pdf": "https://arxiv.org/pdf/2506.02408", "abs": "https://arxiv.org/abs/2506.02408", "authors": ["Wenhao Tang", "Rong Qin", "Heng Fang", "Fengtao Zhou", "Hao Chen", "Xiang Li", "Ming-Ming Cheng"], "title": "Revisiting End-to-End Learning with Slide-level Supervision in Computational Pathology", "categories": ["cs.CV"], "comment": null, "summary": "Pre-trained encoders for offline feature extraction followed by multiple\ninstance learning (MIL) aggregators have become the dominant paradigm in\ncomputational pathology (CPath), benefiting cancer diagnosis and prognosis.\nHowever, performance limitations arise from the absence of encoder fine-tuning\nfor downstream tasks and disjoint optimization with MIL. While slide-level\nsupervised end-to-end (E2E) learning is an intuitive solution to this issue, it\nfaces challenges such as high computational demands and suboptimal results.\nThese limitations motivate us to revisit E2E learning. We argue that prior work\nneglects inherent E2E optimization challenges, leading to performance\ndisparities compared to traditional two-stage methods. In this paper, we\npioneer the elucidation of optimization challenge caused by sparse-attention\nMIL and propose a novel MIL called ABMILX. It mitigates this problem through\nglobal correlation-based attention refinement and multi-head mechanisms. With\nthe efficient multi-scale random patch sampling strategy, an E2E trained ResNet\nwith ABMILX surpasses SOTA foundation models under the two-stage paradigm\nacross multiple challenging benchmarks, while remaining computationally\nefficient (<10 RTX3090 hours). We show the potential of E2E learning in CPath\nand calls for greater research focus in this area. The code is\nhttps://github.com/DearCaat/E2E-WSI-ABMILX.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02448", "pdf": "https://arxiv.org/pdf/2506.02448", "abs": "https://arxiv.org/abs/2506.02448", "authors": ["Baoyu Liang", "Qile Su", "Shoutai Zhu", "Yuchen Liang", "Chao Tong"], "title": "VidEvent: A Large Dataset for Understanding Dynamic Evolution of Events in Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite the significant impact of visual events on human cognition,\nunderstanding events in videos remains a challenging task for AI due to their\ncomplex structures, semantic hierarchies, and dynamic evolution. To address\nthis, we propose the task of video event understanding that extracts event\nscripts and makes predictions with these scripts from videos. To support this\ntask, we introduce VidEvent, a large-scale dataset containing over 23,000\nwell-labeled events, featuring detailed event structures, broad hierarchies,\nand logical relations extracted from movie recap videos. The dataset was\ncreated through a meticulous annotation process, ensuring high-quality and\nreliable event data. We also provide comprehensive baseline models offering\ndetailed descriptions of their architecture and performance metrics. These\nmodels serve as benchmarks for future research, facilitating comparisons and\nimprovements. Our analysis of VidEvent and the baseline models highlights the\ndataset's potential to advance video event understanding and encourages the\nexploration of innovative algorithms and models. The dataset and related\nresources are publicly available at www.videvent.top.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02462", "pdf": "https://arxiv.org/pdf/2506.02462", "abs": "https://arxiv.org/abs/2506.02462", "authors": ["Kunyu Wang", "Xueyang Fu", "Xin Lu", "Chengjie Ge", "Chengzhi Cao", "Wei Zhai", "Zheng-Jun Zha"], "title": "Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning", "categories": ["cs.CV"], "comment": "Accepted as CVPR 2025 oral paper", "summary": "Continual test-time adaptive object detection (CTTA-OD) aims to online adapt\na source pre-trained detector to ever-changing environments during inference\nunder continuous domain shifts. Most existing CTTA-OD methods prioritize\neffectiveness while overlooking computational efficiency, which is crucial for\nresource-constrained scenarios. In this paper, we propose an efficient CTTA-OD\nmethod via pruning. Our motivation stems from the observation that not all\nlearned source features are beneficial; certain domain-sensitive feature\nchannels can adversely affect target domain performance. Inspired by this, we\nintroduce a sensitivity-guided channel pruning strategy that quantifies each\nchannel based on its sensitivity to domain discrepancies at both image and\ninstance levels. We apply weighted sparsity regularization to selectively\nsuppress and prune these sensitive channels, focusing adaptation efforts on\ninvariant ones. Additionally, we introduce a stochastic channel reactivation\nmechanism to restore pruned channels, enabling recovery of potentially useful\nfeatures and mitigating the risks of early pruning. Extensive experiments on\nthree benchmarks show that our method achieves superior adaptation performance\nwhile reducing computational overhead by 12% in FLOPs compared to the recent\nSOTA method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02494", "pdf": "https://arxiv.org/pdf/2506.02494", "abs": "https://arxiv.org/abs/2506.02494", "authors": ["Junzhe Zhang", "Huixuan Zhang", "Xinyu Hu", "Li Lin", "Mingqi Gao", "Shi Qiu", "Xiaojun Wan"], "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Evaluation is important for multimodal generation tasks. With the rapid\nprogress of MLLMs, there is growing interest in applying MLLMs to build general\nevaluation systems. However, existing work overlooks two aspects: (1) the\ndevelopment of evaluation capabilities for text-to-image (T2I) generation task,\nand (2) the incorporation of large-scale human evaluation data. In this paper,\nwe introduce Minos-Corpus, a large-scale multimodal evaluation dataset that\ncombines evaluation data from both human and GPT. The corpus contains\nevaluation data across both image-to-text(I2T) and T2I generation tasks. Based\non this corpus, we propose Data Selection and Balance, Mix-SFT training\nmethods, and apply DPO to develop Minos, a multimodal evaluation model built\nupon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among\nall open-source evaluation models of similar scale on the average of evaluation\nperformance on all tasks, and outperforms all open-source and closed-source\nmodels on evaluation of T2I generation task. Extensive experiments demonstrate\nthe importance of leveraging high-quality human evaluation data and jointly\ntraining on evaluation data from both I2T and T2I generation tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["DPO"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02519", "pdf": "https://arxiv.org/pdf/2506.02519", "abs": "https://arxiv.org/abs/2506.02519", "authors": ["Sohan Patnaik", "Milan Aggarwal", "Sumit Bhatia", "Balaji Krishnamurthy"], "title": "Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning", "categories": ["cs.CL"], "comment": "Accepted at ACL Main 2025", "summary": "LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions\nby generating step-by-step rationales. Prior works have utilized this\ncapability to improve smaller and cheaper LMs (say, with 7B parameters).\nHowever, various practical constraints, such as copyright and legal issues,\nowing to lack of transparency in the pre-training data of large (often closed)\nmodels, prevent their use in commercial settings. Little focus has been given\nto improving the innate reasoning ability of smaller models without distilling\ninformation from larger LLMs. To address this, we propose COLLATE, a trainable\nframework that tunes a (small) LLM to generate those outputs from a pool of\ndiverse rationales that selectively improves the downstream task. COLLATE\nenforces multiple instances of the same LLM to exhibit distinct behavior and\nemploys them to generate rationales to obtain diverse outputs. The LLM is then\ntuned via preference optimization to choose the candidate rationale which\nmaximizes the likelihood of ground-truth answer. COLLATE outperforms several\ntrainable and prompting baselines on 5 datasets across 3 domains: maths problem\nsolving, natural language inference, and commonsense reasoning. We show the eff\nicacy of COLLATE on LLMs from different model families across varying parameter\nscales (1B to 8B) and demonstrate the benefit of multiple rationale providers\nguided by the end task through ablations. Code is released here\n(https://github.com/Sohanpatnaik106/collate).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02493", "pdf": "https://arxiv.org/pdf/2506.02493", "abs": "https://arxiv.org/abs/2506.02493", "authors": ["Jiachen Liu", "Rui Yu", "Sili Chen", "Sharon X. Huang", "Hengkai Guo"], "title": "Towards In-the-wild 3D Plane Reconstruction from a Single Image", "categories": ["cs.CV"], "comment": "CVPR 2025 Highlighted Paper", "summary": "3D plane reconstruction from a single image is a crucial yet challenging\ntopic in 3D computer vision. Previous state-of-the-art (SOTA) methods have\nfocused on training their system on a single dataset from either indoor or\noutdoor domain, limiting their generalizability across diverse testing data. In\nthis work, we introduce a novel framework dubbed ZeroPlane, a Transformer-based\nmodel targeting zero-shot 3D plane detection and reconstruction from a single\nimage, over diverse domains and environments. To enable data-driven models\nacross multiple domains, we have curated a large-scale planar benchmark,\ncomprising over 14 datasets and 560,000 high-resolution, dense planar\nannotations for diverse indoor and outdoor scenes. To address the challenge of\nachieving desirable planar geometry on multi-dataset training, we propose to\ndisentangle the representation of plane normal and offset, and employ an\nexemplar-guided, classification-then-regression paradigm to learn plane and\noffset respectively. Additionally, we employ advanced backbones as image\nencoder, and present an effective pixel-geometry-enhanced plane embedding\nmodule to further facilitate planar reconstruction. Extensive experiments\nacross multiple zero-shot evaluation datasets have demonstrated that our\napproach significantly outperforms previous methods on both reconstruction\naccuracy and generalizability, especially over in-the-wild data. Our code and\ndata are available at: https://github.com/jcliu0428/ZeroPlane.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02534", "pdf": "https://arxiv.org/pdf/2506.02534", "abs": "https://arxiv.org/abs/2506.02534", "authors": ["Sining Chen", "Yilei Shi", "Xiao Xiang Zhu"], "title": "Enhancing Monocular Height Estimation via Weak Supervision from Imperfect Labels", "categories": ["cs.CV"], "comment": null, "summary": "Monocular height estimation is considered the most efficient and\ncost-effective means of 3D perception in remote sensing, and it has attracted\nmuch attention since the emergence of deep learning. While training neural\nnetworks requires a large amount of data, data with perfect labels are scarce\nand only available within developed regions. The trained models therefore lack\ngeneralizability, which limits the potential for large-scale application of\nexisting methods. We tackle this problem for the first time, by introducing\ndata with imperfect labels into training pixel-wise height estimation networks,\nincluding labels that are incomplete, inexact, and inaccurate compared to\nhigh-quality labels. We propose an ensemble-based pipeline compatible with any\nmonocular height estimation network. Taking the challenges of noisy labels,\ndomain shift, and long-tailed distribution of height values into consideration,\nwe carefully design the architecture and loss functions to leverage the\ninformation concealed in imperfect labels using weak supervision through\nbalanced soft losses and ordinal constraints. We conduct extensive experiments\non two datasets with different resolutions, DFC23 (0.5 to 1 m) and GBH (3 m).\nThe results indicate that the proposed pipeline outperforms baselines by\nachieving more balanced performance across various domains, leading to\nimprovements of average root mean square errors up to 22.94 %, and 18.62 % on\nDFC23 and GBH, respectively. The efficacy of each design component is validated\nthrough ablation studies. Code is available at\nhttps://github.com/zhu-xlab/weakim2h.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02555", "pdf": "https://arxiv.org/pdf/2506.02555", "abs": "https://arxiv.org/abs/2506.02555", "authors": ["Zhitao Zeng", "Zhu Zhuo", "Xiaojun Jia", "Erli Zhang", "Junde Wu", "Jiaan Zhang", "Yuxuan Wang", "Chang Han Low", "Jian Jiang", "Zilong Zheng", "Xiaochun Cao", "Yutong Ban", "Qi Dou", "Yang Liu", "Yueming Jin"], "title": "SurgVLM: A Large Vision-Language Model and Systematic Evaluation Benchmark for Surgical Intelligence", "categories": ["cs.CV", "68T45", "I.2.10"], "comment": "29 pages, 5 figures", "summary": "Foundation models have achieved transformative success across biomedical\ndomains by enabling holistic understanding of multimodal data. However, their\napplication in surgery remains underexplored. Surgical intelligence presents\nunique challenges - requiring surgical visual perception, temporal analysis,\nand reasoning. Existing general-purpose vision-language models fail to address\nthese needs due to insufficient domain-specific supervision and the lack of a\nlarge-scale high-quality surgical database. To bridge this gap, we propose\nSurgVLM, one of the first large vision-language foundation models for surgical\nintelligence, where this single universal model can tackle versatile surgical\ntasks. To enable this, we construct a large-scale multimodal surgical database,\nSurgVLM-DB, comprising over 1.81 million frames with 7.79 million\nconversations, spanning more than 16 surgical types and 18 anatomical\nstructures. We unify and reorganize 23 public datasets across 10 surgical\ntasks, followed by standardizing labels and doing hierarchical vision-language\nalignment to facilitate comprehensive coverage of gradually finer-grained\nsurgical tasks, from visual perception, temporal analysis, to high-level\nreasoning. Building upon this comprehensive dataset, we propose SurgVLM, which\nis built upon Qwen2.5-VL, and undergoes instruction tuning to 10+ surgical\ntasks. We further construct a surgical multimodal benchmark, SurgVLM-Bench, for\nmethod evaluation. SurgVLM-Bench consists of 6 popular and widely-used datasets\nin surgical domain, covering several crucial downstream tasks. Based on\nSurgVLM-Bench, we evaluate the performance of our SurgVLM (3 SurgVLM variants:\nSurgVLM-7B, SurgVLM-32B, and SurgVLM-72B), and conduct comprehensive\ncomparisons with 14 mainstream commercial VLMs (e.g., GPT-4o, Gemini 2.0 Flash,\nQwen2.5-Max).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02614", "pdf": "https://arxiv.org/pdf/2506.02614", "abs": "https://arxiv.org/abs/2506.02614", "authors": ["Guohang Zhuang", "Weixi Song", "Jinyang Huang", "Chenwei Yang", "Yan Lu"], "title": "High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of space exploration, space debris has attracted\nmore attention due to its potential extreme threat, leading to the need for\nreal-time and accurate debris tracking. However, existing methods are mainly\nbased on traditional signal processing, which cannot effectively process the\ncomplex background and dense space debris. In this paper, we propose a deep\nlearning-based Space Debris Tracking Network~(SDT-Net) to achieve highly\naccurate debris tracking. SDT-Net effectively represents the feature of debris,\nenhancing the efficiency and stability of end-to-end model learning. To train\nand evaluate this model effectively, we also produce a large-scale dataset\nSpace Debris Tracking Dataset (SDTD) by a novel observation-based data\nsimulation scheme. SDTD contains 18,040 video sequences with a total of 62,562\nframes and covers 250,000 synthetic space debris. Extensive experiments\nvalidate the effectiveness of our model and the challenging of our dataset.\nFurthermore, we test our model on real data from the Antarctic Station,\nachieving a MOTA score of 70.6%, which demonstrates its strong transferability\nto real-world scenarios. Our dataset and code will be released soon.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02615", "pdf": "https://arxiv.org/pdf/2506.02615", "abs": "https://arxiv.org/abs/2506.02615", "authors": ["Safaa Abdullahi Moallim Mohamud", "Minjin Baek", "Dong Seog Han"], "title": "Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this paper, we present a hierarchical question-answering (QA) approach for\nscene understanding in autonomous vehicles, balancing cost-efficiency with\ndetailed visual interpretation. The method fine-tunes a compact vision-language\nmodel (VLM) on a custom dataset specific to the geographical area in which the\nvehicle operates to capture key driving-related visual elements. At the\ninference stage, the hierarchical QA strategy decomposes the scene\nunderstanding task into high-level and detailed sub-questions. Instead of\ngenerating lengthy descriptions, the VLM navigates a structured question tree,\nwhere answering high-level questions (e.g., \"Is it possible for the ego vehicle\nto turn left at the intersection?\") triggers more detailed sub-questions (e.g.,\n\"Is there a vehicle approaching the intersection from the opposite\ndirection?\"). To optimize inference time, questions are dynamically skipped\nbased on previous answers, minimizing computational overhead. The extracted\nanswers are then synthesized using handcrafted templates to ensure coherent,\ncontextually accurate scene descriptions. We evaluate the proposed approach on\nthe custom dataset using GPT reference-free scoring, demonstrating its\ncompetitiveness with state-of-the-art methods like GPT-4o in capturing key\nscene details while achieving significantly lower inference time. Moreover,\nqualitative results from real-time deployment highlight the proposed approach's\ncapacity to capture key driving elements with minimal latency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02692", "pdf": "https://arxiv.org/pdf/2506.02692", "abs": "https://arxiv.org/abs/2506.02692", "authors": ["Shu Yang", "Fengtao Zhou", "Leon Mayer", "Fuxiang Huang", "Yiliang Chen", "Yihui Wang", "Sunan He", "Yuxiang Nie", "Xi Wang", "Ãmer SÃ¼mer", "Yueming Jin", "Huihui Sun", "Shuchang Xu", "Alex Qinyang Liu", "Zheng Li", "Jing Qin", "Jeremy YuenChun Teoh", "Lena Maier-Hein", "Hao Chen"], "title": "Large-scale Self-supervised Video Foundation Model for Intelligent Surgery", "categories": ["cs.CV"], "comment": null, "summary": "Computer-Assisted Intervention (CAI) has the potential to revolutionize\nmodern surgery, with surgical scene understanding serving as a critical\ncomponent in supporting decision-making, improving procedural efficacy, and\nensuring intraoperative safety. While existing AI-driven approaches alleviate\nannotation burdens via self-supervised spatial representation learning, their\nlack of explicit temporal modeling during pre-training fundamentally restricts\nthe capture of dynamic surgical contexts, resulting in incomplete\nspatiotemporal understanding. In this work, we introduce the first video-level\nsurgical pre-training framework that enables joint spatiotemporal\nrepresentation learning from large-scale surgical video data. To achieve this,\nwe constructed a large-scale surgical video dataset comprising 3,650 videos and\napproximately 3.55 million frames, spanning more than 20 surgical procedures\nand over 10 anatomical structures. Building upon this dataset, we propose\nSurgVISTA (Surgical Video-level Spatial-Temporal Architecture), a\nreconstruction-based pre-training method that captures intricate spatial\nstructures and temporal dynamics through joint spatiotemporal modeling.\nAdditionally, SurgVISTA incorporates image-level knowledge distillation guided\nby a surgery-specific expert to enhance the learning of fine-grained anatomical\nand semantic features. To validate its effectiveness, we established a\ncomprehensive benchmark comprising 13 video-level datasets spanning six\nsurgical procedures across four tasks. Extensive experiments demonstrate that\nSurgVISTA consistently outperforms both natural- and surgical-domain\npre-trained models, demonstrating strong potential to advance intelligent\nsurgical systems in clinically meaningful scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "safety", "fine-grained"], "score": 5}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02911", "pdf": "https://arxiv.org/pdf/2506.02911", "abs": "https://arxiv.org/abs/2506.02911", "authors": ["Yin Fang", "Qiao Jin", "Guangzhi Xiong", "Bowen Jin", "Xianrui Zhong", "Siru Ouyang", "Aidong Zhang", "Jiawei Han", "Zhiyong Lu"], "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "cs.LG"], "comment": "28 pages; 16 tables; 7 figures; Code:\n  https://github.com/ncbi-nlp/cell-o1", "summary": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02738", "pdf": "https://arxiv.org/pdf/2506.02738", "abs": "https://arxiv.org/abs/2506.02738", "authors": ["Negin Baghbanzadeh", "Sajad Ashkezari", "Elham Dolatabadi", "Arash Afkanpour"], "title": "Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Compound figures, which are multi-panel composites containing diverse\nsubfigures, are ubiquitous in biomedical literature, yet large-scale subfigure\nextraction remains largely unaddressed. Prior work on subfigure extraction has\nbeen limited in both dataset size and generalizability, leaving a critical open\nquestion: How does high-fidelity image-text alignment via large-scale subfigure\nextraction impact representation learning in vision-language models? We address\nthis gap by introducing a scalable subfigure extraction pipeline based on\ntransformer-based object detection, trained on a synthetic corpus of 500,000\ncompound figures, and achieving state-of-the-art performance on both ImageCLEF\n2016 and synthetic benchmarks. Using this pipeline, we release OPEN-PMC-18M, a\nlarge-scale high quality biomedical vision-language dataset comprising 18\nmillion clinically relevant subfigure-caption pairs spanning radiology,\nmicroscopy, and visible light photography. We train and evaluate\nvision-language models on our curated datasets and show improved performance\nacross retrieval, zero-shot classification, and robustness benchmarks,\noutperforming existing baselines. We release our dataset, models, and code to\nsupport reproducible benchmarks and further study into biomedical\nvision-language modeling and representation learning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02741", "pdf": "https://arxiv.org/pdf/2506.02741", "abs": "https://arxiv.org/abs/2506.02741", "authors": ["Pengchong Hu", "Zhizhong Han"], "title": "VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians", "categories": ["cs.CV"], "comment": "ICML 2025", "summary": "Jointly estimating camera poses and mapping scenes from RGBD images is a\nfundamental task in simultaneous localization and mapping (SLAM).\nState-of-the-art methods employ 3D Gaussians to represent a scene, and render\nthese Gaussians through splatting for higher efficiency and better rendering.\nHowever, these methods cannot scale up to extremely large scenes, due to the\ninefficient tracking and mapping strategies that need to optimize all 3D\nGaussians in the limited GPU memories throughout the training to maintain the\ngeometry and color consistency to previous RGBD observations. To resolve this\nissue, we propose novel tracking and mapping strategies to work with a novel 3D\nrepresentation, dubbed view-tied 3D Gaussians, for RGBD SLAM systems. View-tied\n3D Gaussians is a kind of simplified Gaussians, which is tied to depth pixels,\nwithout needing to learn locations, rotations, and multi-dimensional variances.\nTying Gaussians to views not only significantly saves storage but also allows\nus to employ many more Gaussians to represent local details in the limited GPU\nmemory. Moreover, our strategies remove the need of maintaining all Gaussians\nlearnable throughout the training, while improving rendering quality, and\ntracking accuracy. We justify the effectiveness of these designs, and report\nbetter performance over the latest methods on the widely used benchmarks in\nterms of rendering and tracking accuracy and scalability. Please see our\nproject page for code and videos at\nhttps://machineperceptionlab.github.io/VTGaussian-SLAM-Project .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy", "multi-dimensional"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02751", "pdf": "https://arxiv.org/pdf/2506.02751", "abs": "https://arxiv.org/abs/2506.02751", "authors": ["Chuanyu Fu", "Yuqi Zhang", "Kunbin Yao", "Guanying Chen", "Yuan Xiong", "Chuan Huang", "Shuguang Cui", "Xiaochun Cao"], "title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS", "categories": ["cs.CV"], "comment": "Project page: https://fcyycf.github.io/RobustSplat/", "summary": "3D Gaussian Splatting (3DGS) has gained significant attention for its\nreal-time, photo-realistic rendering in novel-view synthesis and 3D modeling.\nHowever, existing methods struggle with accurately modeling scenes affected by\ntransient objects, leading to artifacts in the rendered images. We identify\nthat the Gaussian densification process, while enhancing scene detail capture,\nunintentionally contributes to these artifacts by growing additional Gaussians\nthat model transient disturbances. To address this, we propose RobustSplat, a\nrobust solution based on two critical designs. First, we introduce a delayed\nGaussian growth strategy that prioritizes optimizing static scene structure\nbefore allowing Gaussian splitting/cloning, mitigating overfitting to transient\nobjects in early optimization. Second, we design a scale-cascaded mask\nbootstrapping approach that first leverages lower-resolution feature similarity\nsupervision for reliable initial transient mask estimation, taking advantage of\nits stronger semantic consistency and robustness to noise, and then progresses\nto high-resolution supervision to achieve more precise mask prediction.\nExtensive experiments on multiple challenging datasets show that our method\noutperforms existing methods, clearly demonstrating the robustness and\neffectiveness of our method. Our project page is\nhttps://fcyycf.github.io/RobustSplat/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02973", "pdf": "https://arxiv.org/pdf/2506.02973", "abs": "https://arxiv.org/abs/2506.02973", "authors": ["Dingwei Chen", "Ziqiang Liu", "Feiteng Fang", "Chak Tou Leong", "Shiwen Ni", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Min Yang", "Chengming Li"], "title": "Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\nunderstanding and generation. However, their tendency to produce factually\ninconsistent outputs, commonly referred to as ''hallucinations'', remains a\ncritical challenge. Existing approaches, such as retrieval-based and\ninference-time correction methods, primarily address this issue at the input or\noutput level, often overlooking the intrinsic information refinement process\nand the role of premature layers. Meanwhile, alignment- and fine-tuning-based\nmethods are resource-intensive. In this paper, we propose PLI (Premature Layers\nInterpolation), a novel, training-free, and plug-and-play intervention designed\nto enhance factuality. PLI mitigates hallucinations by inserting premature\nlayers formed through mathematical interpolation with adjacent layers. Inspired\nby stable diffusion and sampling steps, PLI extends the depth of information\nprocessing and transmission in LLMs, improving factual coherence. Experiments\non four publicly available datasets demonstrate that PLI effectively reduces\nhallucinations while outperforming existing baselines in most cases. Further\nanalysis suggests that the success of layer interpolation is closely linked to\nLLMs' internal mechanisms. To promote reproducibility, we will release our code\nand data upon acceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02979", "pdf": "https://arxiv.org/pdf/2506.02979", "abs": "https://arxiv.org/abs/2506.02979", "authors": ["Atsumoto Ohashi", "Shinya Iizuka", "Jingjing Jiang", "Ryuichiro Higashinaka"], "title": "Towards a Japanese Full-duplex Spoken Dialogue System", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Full-duplex spoken dialogue systems, which can model simultaneous\nbidirectional features of human conversations such as speech overlaps and\nbackchannels, have attracted significant attention recently. However, the study\nof full-duplex spoken dialogue systems for the Japanese language has been\nlimited, and the research on their development in Japanese remains scarce. In\nthis paper, we present the first publicly available full-duplex spoken dialogue\nmodel in Japanese, which is built upon Moshi, a full-duplex dialogue model in\nEnglish. Our model is trained through a two-stage process: pre-training on a\nlarge-scale spoken dialogue data in Japanese, followed by fine-tuning on\nhigh-quality stereo spoken dialogue data. We further enhance the model's\nperformance by incorporating synthetic dialogue data generated by a\nmulti-stream text-to-speech system. Evaluation experiments demonstrate that the\ntrained model outperforms Japanese baseline models in both naturalness and\nmeaningfulness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dialogue"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02987", "pdf": "https://arxiv.org/pdf/2506.02987", "abs": "https://arxiv.org/abs/2506.02987", "authors": ["Richard Armitage"], "title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "12 pages, 1 Table", "summary": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02853", "pdf": "https://arxiv.org/pdf/2506.02853", "abs": "https://arxiv.org/abs/2506.02853", "authors": ["Mingjie Wei", "Xuemei Xie", "Yutong Zhong", "Guangming Shi"], "title": "Learning Pyramid-structured Long-range Dependencies for 3D Human Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM)", "summary": "Action coordination in human structure is indispensable for the spatial\nconstraints of 2D joints to recover 3D pose. Usually, action coordination is\nrepresented as a long-range dependence among body parts. However, there are two\nmain challenges in modeling long-range dependencies. First, joints should not\nonly be constrained by other individual joints but also be modulated by the\nbody parts. Second, existing methods make networks deeper to learn dependencies\nbetween non-linked parts. They introduce uncorrelated noise and increase the\nmodel size. In this paper, we utilize a pyramid structure to better learn\npotential long-range dependencies. It can capture the correlation across joints\nand groups, which complements the context of the human sub-structure. In an\neffective cross-scale way, it captures the pyramid-structured long-range\ndependence. Specifically, we propose a novel Pyramid Graph Attention (PGA)\nmodule to capture long-range cross-scale dependencies. It concatenates\ninformation from various scales into a compact sequence, and then computes the\ncorrelation between scales in parallel. Combining PGA with graph convolution\nmodules, we develop a Pyramid Graph Transformer (PGFormer) for 3D human pose\nestimation, which is a lightweight multi-scale transformer architecture. It\nencapsulates human sub-structures into self-attention by pooling. Extensive\nexperiments show that our approach achieves lower error and smaller model size\nthan state-of-the-art methods on Human3.6M and MPI-INF-3DHP datasets. The code\nis available at https://github.com/MingjieWe/PGFormer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03101", "pdf": "https://arxiv.org/pdf/2506.03101", "abs": "https://arxiv.org/abs/2506.03101", "authors": ["Jonas F. Lotz", "AntÃ³nio V. Lopes", "Stephan Peitz", "Hendra Setiawan", "Leonardo Emili"], "title": "Beyond Text Compression: Evaluating Tokenizers Across Scales", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "The choice of tokenizer can profoundly impact language model performance, yet\naccessible and reliable evaluations of tokenizer quality remain an open\nchallenge. Inspired by scaling consistency, we show that smaller models can\naccurately predict significant differences in tokenizer impact on larger models\nat a fraction of the compute cost. By systematically evaluating both\nEnglish-centric and multilingual tokenizers, we find that tokenizer choice has\nnegligible effects on tasks in English but results in consistent performance\ndifferences in multilingual settings. We propose new intrinsic tokenizer\nmetrics inspired by Zipf's law that correlate more strongly with downstream\nperformance than text compression when modeling unseen languages. By combining\nseveral metrics to capture multiple aspects of tokenizer behavior, we develop a\nreliable framework for intrinsic tokenizer evaluations. Our work offers a more\nefficient path to informed tokenizer selection in future language model\ndevelopment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02868", "pdf": "https://arxiv.org/pdf/2506.02868", "abs": "https://arxiv.org/abs/2506.02868", "authors": ["Amal S. Perera", "David Fernandez", "Chandi Witharana", "Elias Manos", "Michael Pimenta", "Anna K. Liljedahl", "Ingmar Nitze", "Yili Yang", "Todd Nicholson", "Chia-Yu Hsu", "Wenwen Li", "Guido Grosse"], "title": "Pan-Arctic Permafrost Landform and Human-built Infrastructure Feature Detection with Vision Transformers and Location Embeddings", "categories": ["cs.CV", "I.4.6; I.5.4; I.5.2; I.2.10"], "comment": "20 pages, 2 column IEEE format, 13 Figures", "summary": "Accurate mapping of permafrost landforms, thaw disturbances, and human-built\ninfrastructure at pan-Arctic scale using sub-meter satellite imagery is\nincreasingly critical. Handling petabyte-scale image data requires\nhigh-performance computing and robust feature detection models. While\nconvolutional neural network (CNN)-based deep learning approaches are widely\nused for remote sensing (RS),similar to the success in transformer based large\nlanguage models, Vision Transformers (ViTs) offer advantages in capturing\nlong-range dependencies and global context via attention mechanisms. ViTs\nsupport pretraining via self-supervised learning-addressing the common\nlimitation of labeled data in Arctic feature detection and outperform CNNs on\nbenchmark datasets. Arctic also poses challenges for model generalization,\nespecially when features with the same semantic class exhibit diverse spectral\ncharacteristics. To address these issues for Arctic feature detection, we\nintegrate geospatial location embeddings into ViTs to improve adaptation across\nregions. This work investigates: (1) the suitability of pre-trained ViTs as\nfeature extractors for high-resolution Arctic remote sensing tasks, and (2) the\nbenefit of combining image and location embeddings. Using previously published\ndatasets for Arctic feature detection, we evaluate our models on three\ntasks-detecting ice-wedge polygons (IWP), retrogressive thaw slumps (RTS), and\nhuman-built infrastructure. We empirically explore multiple configurations to\nfuse image embeddings and location embeddings. Results show that ViTs with\nlocation embeddings outperform prior CNN-based models on two of the three tasks\nincluding F1 score increase from 0.84 to 0.92 for RTS detection, demonstrating\nthe potential of transformer-based models with spatial awareness for Arctic RS\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03122", "pdf": "https://arxiv.org/pdf/2506.03122", "abs": "https://arxiv.org/abs/2506.03122", "authors": ["Prashanth Vijayaraghavan", "Luyao Shi", "Ehsan Degan", "Vandana Mukherjee", "Xin Zhang"], "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation", "categories": ["cs.CL"], "comment": "9 Pages (Content), 4 Pages (Appendix), 7 figures, ICML'2025", "summary": "Analog circuit topology synthesis is integral to Electronic Design Automation\n(EDA), enabling the automated creation of circuit structures tailored to\nspecific design requirements. However, the vast design search space and strict\nconstraint adherence make efficient synthesis challenging. Leveraging the\nversatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel\nreinforcement learning (RL)-based framework for automated analog circuit\nsynthesis. The framework operates in two phases: instruction tuning, where an\nLLM learns to generate circuit topologies from structured prompts encoding\ndesign constraints, and RL refinement, which further improves the\ninstruction-tuned model using reward models that evaluate validity, efficiency,\nand output voltage. The refined model is then used directly to generate\ntopologies that satisfy the design constraints. Empirical results show that\nAUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by\n~14% compared to the best baselines, while reducing duplicate generation rates\nby ~38%. It achieves over 60% success in synthesizing valid circuits with\nlimited training data, demonstrating strong generalization. These findings\nhighlight the framework's effectiveness in scaling to complex circuits while\nmaintaining efficiency and constraint adherence, marking a significant\nadvancement in AI-driven circuit design.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03145", "pdf": "https://arxiv.org/pdf/2506.03145", "abs": "https://arxiv.org/abs/2506.03145", "authors": ["Pralaypati Ta", "Sriram Venkatesaperumal", "Keerthi Ram", "Mohanasankar Sivaprakasam"], "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.01963", "pdf": "https://arxiv.org/pdf/2506.01963", "abs": "https://arxiv.org/abs/2506.01963", "authors": ["Andrew Kiruluta", "Preethi Raju", "Priscilla Burity"], "title": "Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We present a novel non attention based architecture for large language models\n(LLMs) that efficiently handles very long context windows, on the order of\nhundreds of thousands to potentially millions of tokens. Unlike traditional\nTransformer designs, which suffer from quadratic memory and computation\noverload due to the nature of the self attention mechanism, our model avoids\ntoken to token attention entirely. Instead, it combines the following\ncomplementary components: State Space blocks (inspired by S4) that learn\ncontinuous time convolution kernels and scale near linearly with sequence\nlength, Multi Resolution Convolution layers that capture local context at\ndifferent dilation levels, a lightweight Recurrent Supervisor to maintain a\nglobal hidden state across sequential chunks, and Retrieval Augmented External\nMemory that stores and retrieves high-level chunk embeddings without\nreintroducing quadratic operations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.01967", "pdf": "https://arxiv.org/pdf/2506.01967", "abs": "https://arxiv.org/abs/2506.01967", "authors": ["Patrik CzakÃ³", "GÃ¡bor KertÃ©sz", "SÃ¡ndor SzÃ©nÃ¡si"], "title": "Turning LLM Activations Quantization-Friendly", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "6 pages, 5 figures. Accepted to SACI 2025 conference proceedings", "summary": "Quantization effectively reduces the serving costs of Large Language Models\n(LLMs) by speeding up data movement through compressed parameters and enabling\nfaster operations via integer arithmetic. However, activating integer\narithmetic requires quantizing both weights and activations, which poses\nchallenges due to the significant outliers in LLMs that increase quantization\nerror. In this work, we investigate these outliers with an emphasis on their\neffect on layer-wise quantization error, then examine how smoothing and\nrotation transform the observed values. Our primary contributions include\nintroducing a new metric to measure and visualize quantization difficulty based\non channel magnitudes, as well as proposing a hybrid approach that applies\nchannel-wise scaling before rotation, supported by a mathematical formulation\nof its benefits.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02975", "pdf": "https://arxiv.org/pdf/2506.02975", "abs": "https://arxiv.org/abs/2506.02975", "authors": ["Yicheng Xiao", "Lin Song", "Rui Yang", "Cheng Cheng", "Zunnan Xu", "Zhaoyang Zhang", "Yixiao Ge", "Xiu Li", "Ying Shan"], "title": "HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the advancement of language models, unified multimodal understanding and\ngeneration have made significant strides, with model architectures evolving\nfrom separated components to unified single-model frameworks. This paper\nexplores an efficient training paradigm to build a single transformer for\nunified multimodal understanding and generation. Specifically, we propose a\nmultimodal warmup strategy utilizing prior knowledge to extend capabilities. To\naddress cross-modal compatibility challenges, we introduce feature pre-scaling\nand multimodal AdaLN techniques. Integrating the proposed technologies, we\npresent the HaploOmni, a new single multimodal transformer. With limited\ntraining costs, HaploOmni achieves competitive performance across multiple\nimage and video understanding and generation benchmarks over advanced unified\nmodels. All codes will be made public at https://github.com/Tencent/HaploVLM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03007", "pdf": "https://arxiv.org/pdf/2506.03007", "abs": "https://arxiv.org/abs/2506.03007", "authors": ["Jiarui Wang", "Huiyu Duan", "Juntong Wang", "Ziheng Jia", "Woo Yi Yang", "Xiaorong Zhu", "Yu Zhao", "Jiaying Qian", "Yuke Xing", "Guangtao Zhai", "Xiongkuo Min"], "title": "DFBench: Benchmarking Deepfake Image Detection Capability of Large Multimodal Models", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of generative models, the realism of AI-generated\nimages has significantly improved, posing critical challenges for verifying\ndigital content authenticity. Current deepfake detection methods often depend\non datasets with limited generation models and content diversity that fail to\nkeep pace with the evolving complexity and increasing realism of the\nAI-generated content. Large multimodal models (LMMs), widely adopted in various\nvision tasks, have demonstrated strong zero-shot capabilities, yet their\npotential in deepfake detection remains largely unexplored. To bridge this gap,\nwe present \\textbf{DFBench}, a large-scale DeepFake Benchmark featuring (i)\nbroad diversity, including 540,000 images across real, AI-edited, and\nAI-generated content, (ii) latest model, the fake images are generated by 12\nstate-of-the-art generation models, and (iii) bidirectional benchmarking and\nevaluating for both the detection accuracy of deepfake detectors and the\nevasion capability of generative models. Based on DFBench, we propose\n\\textbf{MoA-DF}, Mixture of Agents for DeepFake detection, leveraging a\ncombined probability strategy from multiple LMMs. MoA-DF achieves\nstate-of-the-art performance, further proving the effectiveness of leveraging\nLMMs for deepfake detection. Database and codes are publicly available at\nhttps://github.com/IntMeGroup/DFBench.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02077", "pdf": "https://arxiv.org/pdf/2506.02077", "abs": "https://arxiv.org/abs/2506.02077", "authors": ["Yoonjun Cho", "Soeun Kim", "Dongjae Jeon", "Kyelim Lee", "Beomsoo Lee", "Albert No"], "title": "Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Decomposing weight matrices into quantization and low-rank components\n($\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$) is a widely used\ntechnique for compressing large language models (LLMs). Existing joint\noptimization methods iteratively alternate between quantization and low-rank\napproximation. However, these methods tend to prioritize one component at the\nexpense of the other, resulting in suboptimal decompositions that fail to\nleverage each component's unique strengths. In this work, we introduce\nOutlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank\ncomponents the specific role of capturing activation-sensitive weights. This\nstructured decomposition mitigates outliers' negative impact on quantization,\nenabling more effective balance between quantization and low-rank\napproximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B\ndemonstrate that incorporating ODLRI into the joint optimization framework\nconsistently reduces activation-aware error, minimizes quantization scale, and\nimproves perplexity and zero-shot accuracy in low-bit settings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02208", "pdf": "https://arxiv.org/pdf/2506.02208", "abs": "https://arxiv.org/abs/2506.02208", "authors": ["Hongling Xu", "Qi Zhu", "Heyuan Deng", "Jinpeng Li", "Lu Hou", "Yasheng Wang", "Lifeng Shang", "Ruifeng Xu", "Fei Mi"], "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in large language model (LLM) post-training have leveraged\ntwo distinct paradigms to enhance reasoning capabilities: reinforcement\nlearning (RL) and knowledge distillation (KD). While RL enables the emergence\nof complex reasoning behaviors, it often suffers from low sample efficiency\nwhen the initial policy struggles to explore high-reward trajectories.\nConversely, KD improves learning efficiency via mimicking the teacher model but\ntends to generalize poorly to out-of-domain scenarios. In this work, we present\n\\textbf{KDRL}, a \\textit{unified post-training framework} that jointly\noptimizes a reasoning model through teacher supervision (KD) and\nself-exploration (RL). Specifically, KDRL leverages policy gradient\noptimization to simultaneously minimize the reverse Kullback-Leibler divergence\n(RKL) between the student and teacher distributions while maximizing the\nexpected rule-based rewards. We first formulate a unified objective that\nintegrates GRPO and KD, and systematically explore how different KL\napproximations, KL coefficients, and reward-guided KD strategies affect the\noverall post-training dynamics and performance. Empirical results on multiple\nreasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD\nbaselines while achieving a favorable balance between performance and reasoning\ntoken efficiency. These findings indicate that integrating KD and RL serves as\nan effective and efficient strategy to train reasoning LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy gradient", "reinforcement learning"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02475", "pdf": "https://arxiv.org/pdf/2506.02475", "abs": "https://arxiv.org/abs/2506.02475", "authors": ["Jiaxi Hu", "Yongqi Pan", "Jusen Du", "Disen Lan", "Xiaqiang Tang", "Qingsong Wen", "Yuxuan Liang", "Weigao Sun"], "title": "Comba: Improving Nonlinear RNNs with Closed-loop Control", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and\nRWKV-7 have achieved performance improvements by supervising the recurrent\nmemory management through Delta learning rule. Unlike previous state-space\nmodels (e.g., Mamba) and gated linear attentions (e.g., GLA), these models\nintroduce interactions between the recurrent state and the key vector,\nresulting in a nonlinear recursive structure. In this paper, we first introduce\nthe concept of Nonlinear RNNs with a comprehensive analysis on the advantages\nand limitations of these models. Then, based on closed-loop control theory, we\npropose a novel Nonlinear RNN variant named Comba, which adopts a\nscalar-plus-low-rank state transition, with both state feedback and output\nfeedback corrections. We also implement a hardware-efficient chunk-wise\nparallel kernel in Triton and train models with 340M/1.3B parameters on\nlarge-scale corpus. Comba demonstrates its superior performance and computation\nefficiency in both language and vision modeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03107", "pdf": "https://arxiv.org/pdf/2506.03107", "abs": "https://arxiv.org/abs/2506.03107", "authors": ["Di Chang", "Mingdeng Cao", "Yichun Shi", "Bo Liu", "Shengqu Cai", "Shijie Zhou", "Weilin Huang", "Gordon Wetzstein", "Mohammad Soleymani", "Peng Wang"], "title": "ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions", "categories": ["cs.CV"], "comment": "Website: https://boese0601.github.io/bytemorph Dataset:\n  https://huggingface.co/datasets/ByteDance-Seed/BM-6M Benchmark:\n  https://huggingface.co/datasets/ByteDance-Seed/BM-Bench Code:\n  https://github.com/ByteDance-Seed/BM-code Demo:\n  https://huggingface.co/spaces/Boese0601/ByteMorph-Demo", "summary": "Editing images with instructions to reflect non-rigid motions, camera\nviewpoint shifts, object deformations, human articulations, and complex\ninteractions, poses a challenging yet underexplored problem in computer vision.\nExisting approaches and datasets predominantly focus on static scenes or rigid\ntransformations, limiting their capacity to handle expressive edits involving\ndynamic motion. To address this gap, we introduce ByteMorph, a comprehensive\nframework for instruction-based image editing with an emphasis on non-rigid\nmotions. ByteMorph comprises a large-scale dataset, ByteMorph-6M, and a strong\nbaseline model built upon the Diffusion Transformer (DiT), named ByteMorpher.\nByteMorph-6M includes over 6 million high-resolution image editing pairs for\ntraining, along with a carefully curated evaluation benchmark ByteMorph-Bench.\nBoth capture a wide variety of non-rigid motion types across diverse\nenvironments, human figures, and object categories. The dataset is constructed\nusing motion-guided data generation, layered compositing techniques, and\nautomated captioning to ensure diversity, realism, and semantic coherence. We\nfurther conduct a comprehensive evaluation of recent instruction-based image\nediting methods from both academic and commercial domains.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03110", "pdf": "https://arxiv.org/pdf/2506.03110", "abs": "https://arxiv.org/abs/2506.03110", "authors": ["Shuai Yi", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "title": "Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025(spotlight)", "summary": "Vision Transformer (ViT) has achieved remarkable success due to its\nlarge-scale pretraining on general domains, but it still faces challenges when\napplying it to downstream distant domains that have only scarce training data,\nwhich gives rise to the Cross-Domain Few-Shot Learning (CDFSL) task. Inspired\nby Self-Attention's insensitivity to token orders, we find an interesting\nphenomenon neglected in current works: disrupting the continuity of image\ntokens (i.e., making pixels not smoothly transited across patches) in ViT leads\nto a noticeable performance decline in the general (source) domain but only a\nmarginal decrease in downstream target domains. This questions the role of\nimage tokens' continuity in ViT's generalization under large domain gaps. In\nthis paper, we delve into this phenomenon for an interpretation. We find\ncontinuity aids ViT in learning larger spatial patterns, which are harder to\ntransfer than smaller ones, enlarging domain distances. Meanwhile, it implies\nthat only smaller patterns within each patch could be transferred under extreme\ndomain gaps. Based on this interpretation, we further propose a simple yet\neffective method for CDFSL that better disrupts the continuity of image tokens,\nencouraging the model to rely less on large patterns and more on smaller ones.\nExtensive experiments show the effectiveness of our method in reducing domain\ngaps and outperforming state-of-the-art works. Codes and models are available\nat https://github.com/shuaiyi308/ReCIT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03114", "pdf": "https://arxiv.org/pdf/2506.03114", "abs": "https://arxiv.org/abs/2506.03114", "authors": ["Michelle Chen", "David Russell", "Amritha Pallavoor", "Derek Young", "Jane Wu"], "title": "Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery", "categories": ["cs.CV"], "comment": "Code:\n  https://github.com/open-forest-observatory/tree-detection-framework", "summary": "Large-scale delineation of individual trees from remote sensing imagery is\ncrucial to the advancement of ecological research, particularly as climate\nchange and other environmental factors rapidly transform forest landscapes\nacross the world. Current RGB tree segmentation methods rely on training\nspecialized machine learning models with labeled tree datasets. While these\nlearning-based approaches can outperform manual data collection when accurate,\nthe existing models still depend on training data that's hard to scale. In this\npaper, we investigate the efficacy of using a state-of-the-art image\nsegmentation model, Segment Anything Model 2 (SAM2), in a zero-shot manner for\nindividual tree detection and segmentation. We evaluate a pretrained SAM2 model\non two tasks in this domain: (1) zero-shot segmentation and (2) zero-shot\ntransfer by using predictions from an existing tree detection model as prompts.\nOur results suggest that SAM2 not only has impressive generalization\ncapabilities, but also can form a natural synergy with specialized methods\ntrained on in-domain labeled data. We find that applying large pretrained\nmodels to problems in remote sensing is a promising avenue for future progress.\nWe make our code available at:\nhttps://github.com/open-forest-observatory/tree-detection-framework.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03117", "pdf": "https://arxiv.org/pdf/2506.03117", "abs": "https://arxiv.org/abs/2506.03117", "authors": ["Zeliang Zhang", "Gaowen Liu", "Charles Fleming", "Ramana Rao Kompella", "Chenliang Xu"], "title": "Targeted Forgetting of Image Subgroups in CLIP Models", "categories": ["cs.CV"], "comment": "12 Figures,5 Pages. The project page is\n  \\url{https://zhangaipi.github.io/forget_clip/}", "summary": "Foundation models (FMs) such as CLIP have demonstrated impressive zero-shot\nperformance across various tasks by leveraging large-scale, unsupervised\npre-training. However, they often inherit harmful or unwanted knowledge from\nnoisy internet-sourced datasets, compromising their reliability in real-world\napplications. Existing model unlearning methods either rely on access to\npre-trained datasets or focus on coarse-grained unlearning (e.g., entire\nclasses), leaving a critical gap for fine-grained unlearning. In this paper, we\naddress the challenging scenario of selectively forgetting specific portions of\nknowledge within a class, without access to pre-trained data, while preserving\nthe model's overall performance. We propose a novel three-stage approach that\nprogressively unlearns targeted knowledge while mitigating over-forgetting. It\nconsists of (1) a forgetting stage to fine-tune the CLIP on samples to be\nforgotten, (2) a reminding stage to restore performance on retained samples,\nand (3) a restoring stage to recover zero-shot capabilities using model\nsouping. Additionally, we introduce knowledge distillation to handle the\ndistribution disparity between forgetting, retaining samples, and unseen\npre-trained data. Extensive experiments on CIFAR-10, ImageNet-1K, and style\ndatasets demonstrate that our approach effectively unlearns specific subgroups\nwhile maintaining strong zero-shot performance on semantically similar\nsubgroups and other categories, significantly outperforming baseline unlearning\nmethods, which lose effectiveness under the CLIP unlearning setting.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "fine-grained"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03139", "pdf": "https://arxiv.org/pdf/2506.03139", "abs": "https://arxiv.org/abs/2506.03139", "authors": ["Siqi Chen", "Xinyu Dong", "Haolei Xu", "Xingyu Wu", "Fei Tang", "Hang Zhang", "Yuchen Yan", "Linjuan Wu", "Wenqi Zhang", "Guiyang Hou", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages,4 figures, Project page:\n  https://zju-real.github.io/SVGenius, Code:\n  https://github.com/ZJU-REAL/SVGenius-Bench", "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.01980", "pdf": "https://arxiv.org/pdf/2506.01980", "abs": "https://arxiv.org/abs/2506.01980", "authors": ["Lianhao Yin", "Ozanan Meireles", "Guy Rosman", "Daniela Rus"], "title": "Surgical Foundation Model Leveraging Compression and Entropy Maximization for Image-Guided Surgical Assistance", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Real-time video understanding is critical to guide procedures in minimally\ninvasive surgery (MIS). However, supervised learning approaches require large,\nannotated datasets that are scarce due to annotation efforts that are\nprohibitive, e.g., in medical fields. Although self-supervision methods can\naddress such limitations, current self-supervised methods often fail to capture\nstructural and physical information in a form that generalizes across tasks. We\npropose Compress-to-Explore (C2E), a novel self-supervised framework that\nleverages Kolmogorov complexity to learn compact, informative representations\nfrom surgical videos. C2E uses entropy-maximizing decoders to compress images\nwhile preserving clinically relevant details, improving encoder performance\nwithout labeled data. Trained on large-scale unlabeled surgical datasets, C2E\ndemonstrates strong generalization across a variety of surgical ML tasks, such\nas workflow classification, tool-tissue interaction classification,\nsegmentation, and diagnosis tasks, providing improved performance as a surgical\nvisual foundation model. As we further show in the paper, the model's internal\ncompact representation better disentangles features from different structural\nparts of images. The resulting performance improvements highlight the yet\nuntapped potential of self-supervised learning to enhance surgical AI and\nimprove outcomes in MIS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02351", "pdf": "https://arxiv.org/pdf/2506.02351", "abs": "https://arxiv.org/abs/2506.02351", "authors": ["Jeonghun Kang", "Soonmok Kwon", "Joonseok Lee", "Byung-Hak Kim"], "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "To appear in the First REALM (Research on Agent Language Models)\n  workshop at ACL 2025", "summary": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking\nor computer vision-driven event detection -- can identify scoring plays but\noften miss strategic depth, momentum shifts, and storyline progression. Manual\ncuration remains the gold standard but is resource-intensive and not scalable.\nWe introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight\nsummarization that integrates structured sports analytics with natural language\nreasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and\nLeverage Index -- to quantify play importance, while an LLM module enhances\nselection based on contextual narrative value. This hybrid approach ensures\nboth quantitative rigor and qualitative richness, surpassing the limitations of\npurely statistical or vision-based systems. Evaluated on five diverse Korean\nBaseball Organization League games, DIAMOND improves F1-score from 42.9%\n(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.\nThough limited in scale, our results highlight the potential of modular,\ninterpretable agent-based frameworks for event-level summarization in sports\nand beyond.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02380", "pdf": "https://arxiv.org/pdf/2506.02380", "abs": "https://arxiv.org/abs/2506.02380", "authors": ["Zihao Ding", "Cheng-Tse Lee", "Mufeng Zhu", "Tao Guan", "Yuan-Chun Sun", "Cheng-Hsin Hsu", "Yao Liu"], "title": "EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for Real-World 3DGS Scenes in VR", "categories": ["cs.MM", "cs.CV", "cs.GR", "cs.HC"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is an emerging media representation that\nreconstructs real-world 3D scenes in high fidelity, enabling\n6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,\ndeveloping and evaluating 3DGS-enabled applications and optimizing their\nrendering performance, require realistic user navigation data. Such data is\ncurrently unavailable for photorealistic 3DGS reconstructions of real-world\nscenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available\n6-DoF navigation dataset featuring traces from 46 participants exploring twelve\ndiverse, real-world 3DGS scenes. The dataset was collected at two sites, using\nthe Meta Quest Pro headsets, recording the head pose and eye gaze data for each\nrendered frame during free world standing 6-DoF navigation. For each of the\ntwelve scenes, we performed careful scene initialization to correct for scene\ntilt and scale, ensuring a perceptually-comfortable VR experience. We also\nrelease our open-source SIBR viewer software fork with record-and-replay\nfunctionalities and a suite of utility tools for data processing, conversion,\nand visualization. The EyeNavGS dataset and its accompanying software tools\nprovide valuable resources for advancing research in 6-DoF viewport prediction,\nadaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The\nEyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02494", "pdf": "https://arxiv.org/pdf/2506.02494", "abs": "https://arxiv.org/abs/2506.02494", "authors": ["Junzhe Zhang", "Huixuan Zhang", "Xinyu Hu", "Li Lin", "Mingqi Gao", "Shi Qiu", "Xiaojun Wan"], "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Evaluation is important for multimodal generation tasks. With the rapid\nprogress of MLLMs, there is growing interest in applying MLLMs to build general\nevaluation systems. However, existing work overlooks two aspects: (1) the\ndevelopment of evaluation capabilities for text-to-image (T2I) generation task,\nand (2) the incorporation of large-scale human evaluation data. In this paper,\nwe introduce Minos-Corpus, a large-scale multimodal evaluation dataset that\ncombines evaluation data from both human and GPT. The corpus contains\nevaluation data across both image-to-text(I2T) and T2I generation tasks. Based\non this corpus, we propose Data Selection and Balance, Mix-SFT training\nmethods, and apply DPO to develop Minos, a multimodal evaluation model built\nupon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among\nall open-source evaluation models of similar scale on the average of evaluation\nperformance on all tasks, and outperforms all open-source and closed-source\nmodels on evaluation of T2I generation task. Extensive experiments demonstrate\nthe importance of leveraging high-quality human evaluation data and jointly\ntraining on evaluation data from both I2T and T2I generation tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["DPO"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02541", "pdf": "https://arxiv.org/pdf/2506.02541", "abs": "https://arxiv.org/abs/2506.02541", "authors": ["Minsung Kim", "Nakyeong Yang", "Kyomin Jung"], "title": "Rethinking Post-Unlearning Behavior of Large Vision-Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "10 pages, 5 figures", "summary": "Machine unlearning is used to mitigate the privacy risks of Large\nVision-Language Models (LVLMs) arising from training on large-scale web data.\nHowever, existing unlearning methods often fail to carefully select substitute\noutputs for forget targets, resulting in Unlearning Aftermaths-undesirable\nbehaviors such as degenerate, hallucinated, or excessively refused responses.\nWe highlight that, especially for generative LVLMs, it is crucial to consider\nthe quality and informativeness of post-unlearning responses rather than\nrelying solely on naive suppression. To address this, we introduce a new\nunlearning task for LVLMs that requires models to provide privacy-preserving\nyet informative and visually grounded responses. We also propose PUBG, a novel\nunlearning method that explicitly guides post-unlearning behavior toward a\ndesirable output distribution. Experiments show that, while existing methods\nsuffer from Unlearning Aftermaths despite successfully preventing privacy\nviolations, PUBG effectively mitigates these issues, generating visually\ngrounded and informative responses without privacy leakage for forgotten\ntargets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02554", "pdf": "https://arxiv.org/pdf/2506.02554", "abs": "https://arxiv.org/abs/2506.02554", "authors": ["Timo Osterburg", "Franz Albers", "Christopher Diehl", "Rajesh Pushparaj", "Torsten Bertram"], "title": "HiLO: High-Level Object Fusion for Autonomous Driving using Transformers", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "6 pages, accepted at IEEE Intelligent Vehicles Symposium (IV) 2025", "summary": "The fusion of sensor data is essential for a robust perception of the\nenvironment in autonomous driving. Learning-based fusion approaches mainly use\nfeature-level fusion to achieve high performance, but their complexity and\nhardware requirements limit their applicability in near-production vehicles.\nHigh-level fusion methods offer robustness with lower computational\nrequirements. Traditional methods, such as the Kalman filter, dominate this\narea. This paper modifies the Adapted Kalman Filter (AKF) and proposes a novel\ntransformer-based high-level object fusion method called HiLO. Experimental\nresults demonstrate improvements of $25.9$ percentage points in $\\textrm{F}_1$\nscore and $6.1$ percentage points in mean IoU. Evaluation on a new large-scale\nreal-world dataset demonstrates the effectiveness of the proposed approaches.\nTheir generalizability is further validated by cross-domain evaluation between\nurban and highway scenarios. Code, data, and models are available at\nhttps://github.com/rst-tu-dortmund/HiLO .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
