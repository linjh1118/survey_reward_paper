{"id": "2506.17442", "categories": ["cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17442", "abs": "https://arxiv.org/abs/2506.17442", "authors": ["Hao Guan", "David Bates", "Li Zhou"], "title": "Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation", "comment": "15 pages, 5 figures", "summary": "Artificial intelligence (AI) is increasingly integrated into modern\nhealthcare, offering powerful support for clinical decision-making. However, in\nreal-world settings, AI systems may experience performance degradation over\ntime, due to factors such as shifting data distributions, changes in patient\ncharacteristics, evolving clinical protocols, and variations in data quality.\nThese factors can compromise model reliability, posing safety concerns and\nincreasing the likelihood of inaccurate predictions or adverse outcomes. This\nreview presents a forward-looking perspective on monitoring and maintaining the\n\"health\" of AI systems in healthcare. We highlight the urgent need for\ncontinuous performance monitoring, early degradation detection, and effective\nself-correction mechanisms. The paper begins by reviewing common causes of\nperformance degradation at both data and model levels. We then summarize key\ntechniques for detecting data and model drift, followed by an in-depth look at\nroot cause analysis. Correction strategies are further reviewed, ranging from\nmodel retraining to test-time adaptation. Our survey spans both traditional\nmachine learning models and state-of-the-art large language models (LLMs),\noffering insights into their strengths and limitations. Finally, we discuss\nongoing technical challenges and propose future research directions. This work\naims to guide the development of reliable, robust medical AI systems capable of\nsustaining safe, long-term deployment in dynamic clinical settings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation", "self-correction"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "reliability"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17585", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17585", "abs": "https://arxiv.org/abs/2506.17585", "authors": ["Yukun Huang", "Sanxing Chen", "Jian Pei", "Manzil Zaheer", "Bhuwan Dhingra"], "title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models", "comment": null, "summary": "Trustworthy language models should provide both correct and verifiable\nanswers. While language models can sometimes attribute their outputs to\npretraining data, their citations are often unreliable due to hallucination. As\na result, current systems insert citations by querying an external retriever at\ninference time, introducing latency, infrastructure dependence, and\nvulnerability to retrieval noise. We explore whether LLMs can be made to\nreliably attribute to the documents seen during (continual)\npretraining--without test-time retrieval--by revising the training process. To\nevaluate this, we release CitePretrainBench, a benchmark that mixes real-world\ncorpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and\nprobes both short-form (single fact) and long-form (multi-fact) citation tasks.\nOur approach follows a two-stage process: (1) continual pretraining to bind\nfacts to persistent document identifiers, and (2) instruction tuning to elicit\ncitation behavior. We find that simple Passive Indexing, which appends an\nidentifier to each document, helps memorize verbatim text but fails on\nparaphrased or compositional facts. Instead, we propose Active Indexing, which\ncontinually pretrains on synthetic QA pairs that (1) restate each fact in\ndiverse compositional forms, and (2) require bidirectional source-to-fact and\nfact-to-source generation, jointly teaching the model to generate content from\na cited source and to attribute its own answers. Experiments with Qwen2.5-7B\nand 3B show that Active Indexing consistently outperforms Passive Indexing\nacross all tasks and models, with citation precision gains up to 30.2 percent.\nOur ablation studies reveal that performance continues to improve as we scale\nthe amount of augmented data, showing a clear upward trend even at 16 times the\noriginal token count.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference time", "scale"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18810", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18810", "abs": "https://arxiv.org/abs/2506.18810", "authors": ["Siao Tang", "Xinyin Ma", "Gongfan Fang", "Xinchao Wang"], "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation", "comment": "Codes are available at https://github.com/tsa18/ConciseHint", "summary": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and\nOpenAI o1 series have achieved notable performance enhancements on complex\nreasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).\nHowever, an emerging issue is their inclination to produce excessively verbose\nreasoning processes, leading to the inefficiency problem. Existing literature\non improving efficiency mainly adheres to the before-reasoning paradigms such\nas prompting and reasoning or fine-tuning and reasoning, but ignores the\npromising direction of directly encouraging the model to speak concisely by\nintervening during the generation of reasoning. In order to fill the blank, we\npropose a framework dubbed ConciseHint, which continuously encourages the\nreasoning model to speak concisely by injecting the textual hint (manually\ndesigned or trained on the concise data) during the token generation of the\nreasoning process. Besides, ConciseHint is adaptive to the complexity of the\nquery by adaptively adjusting the hint intensity, which ensures it will not\nundermine model performance. Experiments on the state-of-the-art LRMs,\nincluding DeepSeek-R1 and Qwen-3 series, demonstrate that our method can\neffectively produce concise reasoning processes while maintaining performance\nwell. For instance, we achieve a reduction ratio of 65\\% for the reasoning\nlength on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "o1", "reasoning model"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18341", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18341", "abs": "https://arxiv.org/abs/2506.18341", "authors": ["Kang Chen", "Mengdi Zhang", "Yixin Cao"], "title": "Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs", "comment": null, "summary": "This paper explores the challenges of test-time scaling of large language\nmodels (LLMs), regarding both the data and inference efficiency. We highlight\nthe diversity of multi-lingual reasoning based on our pilot studies, and then\nintroduce a novel approach, \\(L^2\\) multi-lingual unification learning with a\ndecoding intervention strategy for further investigation. The basic idea of\n\\(L^2\\) is that the reasoning process varies across different languages, which\nmay be mutually beneficial to enhance both model performance and efficiency. In\nspecific, there are two types of multi-lingual data: the entire long\nchain-of-thought annotations in different languages and the step-wise mixture\nof languages. By further tuning based on them, we show that even small amounts\nof data can significantly improve reasoning capabilities. Our findings suggest\nthat multilingual learning reduces both the required data and the number of\ninference tokens while maintaining a comparable performance. Furthermore,\n\\(L^2\\) is orthogonal to other data efficient methods. Thus, we also emphasize\nthe importance of diverse data selection. The \\(L^2\\) method offers a promising\nsolution to the challenges of data collection and test-time compute efficiency\nin LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "test-time compute"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17585", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17585", "abs": "https://arxiv.org/abs/2506.17585", "authors": ["Yukun Huang", "Sanxing Chen", "Jian Pei", "Manzil Zaheer", "Bhuwan Dhingra"], "title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models", "comment": null, "summary": "Trustworthy language models should provide both correct and verifiable\nanswers. While language models can sometimes attribute their outputs to\npretraining data, their citations are often unreliable due to hallucination. As\na result, current systems insert citations by querying an external retriever at\ninference time, introducing latency, infrastructure dependence, and\nvulnerability to retrieval noise. We explore whether LLMs can be made to\nreliably attribute to the documents seen during (continual)\npretraining--without test-time retrieval--by revising the training process. To\nevaluate this, we release CitePretrainBench, a benchmark that mixes real-world\ncorpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and\nprobes both short-form (single fact) and long-form (multi-fact) citation tasks.\nOur approach follows a two-stage process: (1) continual pretraining to bind\nfacts to persistent document identifiers, and (2) instruction tuning to elicit\ncitation behavior. We find that simple Passive Indexing, which appends an\nidentifier to each document, helps memorize verbatim text but fails on\nparaphrased or compositional facts. Instead, we propose Active Indexing, which\ncontinually pretrains on synthetic QA pairs that (1) restate each fact in\ndiverse compositional forms, and (2) require bidirectional source-to-fact and\nfact-to-source generation, jointly teaching the model to generate content from\na cited source and to attribute its own answers. Experiments with Qwen2.5-7B\nand 3B show that Active Indexing consistently outperforms Passive Indexing\nacross all tasks and models, with citation precision gains up to 30.2 percent.\nOur ablation studies reveal that performance continues to improve as we scale\nthe amount of augmented data, showing a clear upward trend even at 16 times the\noriginal token count.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference time", "scale"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18810", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18810", "abs": "https://arxiv.org/abs/2506.18810", "authors": ["Siao Tang", "Xinyin Ma", "Gongfan Fang", "Xinchao Wang"], "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation", "comment": "Codes are available at https://github.com/tsa18/ConciseHint", "summary": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and\nOpenAI o1 series have achieved notable performance enhancements on complex\nreasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).\nHowever, an emerging issue is their inclination to produce excessively verbose\nreasoning processes, leading to the inefficiency problem. Existing literature\non improving efficiency mainly adheres to the before-reasoning paradigms such\nas prompting and reasoning or fine-tuning and reasoning, but ignores the\npromising direction of directly encouraging the model to speak concisely by\nintervening during the generation of reasoning. In order to fill the blank, we\npropose a framework dubbed ConciseHint, which continuously encourages the\nreasoning model to speak concisely by injecting the textual hint (manually\ndesigned or trained on the concise data) during the token generation of the\nreasoning process. Besides, ConciseHint is adaptive to the complexity of the\nquery by adaptively adjusting the hint intensity, which ensures it will not\nundermine model performance. Experiments on the state-of-the-art LRMs,\nincluding DeepSeek-R1 and Qwen-3 series, demonstrate that our method can\neffectively produce concise reasoning processes while maintaining performance\nwell. For instance, we achieve a reduction ratio of 65\\% for the reasoning\nlength on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "o1", "reasoning model"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18810", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18810", "abs": "https://arxiv.org/abs/2506.18810", "authors": ["Siao Tang", "Xinyin Ma", "Gongfan Fang", "Xinchao Wang"], "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation", "comment": "Codes are available at https://github.com/tsa18/ConciseHint", "summary": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and\nOpenAI o1 series have achieved notable performance enhancements on complex\nreasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).\nHowever, an emerging issue is their inclination to produce excessively verbose\nreasoning processes, leading to the inefficiency problem. Existing literature\non improving efficiency mainly adheres to the before-reasoning paradigms such\nas prompting and reasoning or fine-tuning and reasoning, but ignores the\npromising direction of directly encouraging the model to speak concisely by\nintervening during the generation of reasoning. In order to fill the blank, we\npropose a framework dubbed ConciseHint, which continuously encourages the\nreasoning model to speak concisely by injecting the textual hint (manually\ndesigned or trained on the concise data) during the token generation of the\nreasoning process. Besides, ConciseHint is adaptive to the complexity of the\nquery by adaptively adjusting the hint intensity, which ensures it will not\nundermine model performance. Experiments on the state-of-the-art LRMs,\nincluding DeepSeek-R1 and Qwen-3 series, demonstrate that our method can\neffectively produce concise reasoning processes while maintaining performance\nwell. For instance, we achieve a reduction ratio of 65\\% for the reasoning\nlength on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "o1", "reasoning model"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17636", "categories": ["cs.GR", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.17636", "abs": "https://arxiv.org/abs/2506.17636", "authors": ["Shihan Chen", "Zhaojin Li", "Zeyu Chen", "Qingsong Yan", "Gaoyang Shen", "Ran Duan"], "title": "3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene", "comment": "IROS 2025", "summary": "Recent developments in 3D Gaussian Splatting have made significant advances\nin surface reconstruction. However, scaling these methods to large-scale scenes\nremains challenging due to high computational demands and the complex dynamic\nappearances typical of outdoor environments. These challenges hinder the\napplication in aerial surveying and autonomous driving. This paper proposes a\nnovel solution to reconstruct large-scale surfaces with fine details,\nsupervised by full-sized images. Firstly, we introduce a coarse-to-fine\nstrategy to reconstruct a coarse model efficiently, followed by adaptive scene\npartitioning and sub-scene refining from image segments. Additionally, we\nintegrate a decoupling appearance model to capture global appearance variations\nand a transient mask model to mitigate interference from moving objects.\nFinally, we expand the multi-view constraint and introduce a single-view\nregularization for texture-less areas. Our experiments were conducted on the\npublicly available dataset GauU-Scene V2, which was captured using unmanned\naerial vehicles. To the best of our knowledge, our method outperforms existing\nNeRF-based and Gaussian-based methods, achieving high-fidelity visual results\nand accurate surface from full-size image optimization. Open-source code will\nbe available on GitHub.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18070", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18070", "abs": "https://arxiv.org/abs/2506.18070", "authors": ["Hangzhou He", "Jiachen Tang", "Lei Zhu", "Kaiwen Li", "Yanye Lu"], "title": "Training-free Test-time Improvement for Explainable Medical Image Classification", "comment": "This is the initial version of our work accepted by MICCAI 2025.\n  We'll include a link to the version on SpringerLink after this becomes\n  available", "summary": "Deep learning-based medical image classification techniques are rapidly\nadvancing in medical image analysis, making it crucial to develop accurate and\ntrustworthy models that can be efficiently deployed across diverse clinical\nscenarios. Concept Bottleneck Models (CBMs), which first predict a set of\nexplainable concepts from images and then perform classification based on these\nconcepts, are increasingly being adopted for explainable medical image\nclassification. However, the inherent explainability of CBMs introduces new\nchallenges when deploying trained models to new environments. Variations in\nimaging protocols and staining methods may induce concept-level shifts, such as\nalterations in color distribution and scale. Furthermore, since CBM training\nrequires explicit concept annotations, fine-tuning models solely with\nimage-level labels could compromise concept prediction accuracy and\nfaithfulness - a critical limitation given the high cost of acquiring\nexpert-annotated concept labels in medical domains. To address these\nchallenges, we propose a training-free confusion concept identification\nstrategy. By leveraging minimal new data (e.g., 4 images per class) with only\nimage-level labels, our approach enhances out-of-domain performance without\nsacrificing source domain accuracy through two key operations: masking\nmisactivated confounding concepts and amplifying under-activated discriminative\nconcepts. The efficacy of our method is validated on both skin and white blood\ncell images. Our code is available at:\nhttps://github.com/riverback/TF-TTI-XMed.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18896", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18896", "abs": "https://arxiv.org/abs/2506.18896", "authors": ["Jiaru Zou", "Ling Yang", "Jingwen Gu", "Jiahao Qiu", "Ke Shen", "Jingrui He", "Mengdi Wang"], "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs", "comment": "Codes and Models: https://github.com/Gen-Verse/ReasonFlux", "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18881", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.18881", "abs": "https://arxiv.org/abs/2506.18881", "authors": ["Xinyu Zhang", "Dong Gong", "Zicheng Duan", "Anton van den Hengel", "Lingqiao Liu"], "title": "Let Your Video Listen to Your Music!", "comment": "project page: https://zhangxinyu-xyz.github.io/MVAA/", "summary": "Aligning the rhythm of visual motion in a video with a given music track is a\npractical need in multimedia production, yet remains an underexplored task in\nautonomous video editing. Effective alignment between motion and musical beats\nenhances viewer engagement and visual appeal, particularly in music videos,\npromotional content, and cinematic editing. Existing methods typically depend\non labor-intensive manual cutting, speed adjustments, or heuristic-based\nediting techniques to achieve synchronization. While some generative models\nhandle joint video and music generation, they often entangle the two\nmodalities, limiting flexibility in aligning video to music beats while\npreserving the full visual content. In this paper, we propose a novel and\nefficient framework, termed MVAA (Music-Video Auto-Alignment), that\nautomatically edits video to align with the rhythm of a given music track while\npreserving the original visual content. To enhance flexibility, we modularize\nthe task into a two-step process in our MVAA: aligning motion keyframes with\naudio beats, followed by rhythm-aware video inpainting. Specifically, we first\ninsert keyframes at timestamps aligned with musical beats, then use a\nframe-conditioned diffusion model to generate coherent intermediate frames,\npreserving the original video's semantic content. Since comprehensive test-time\ntraining can be time-consuming, we adopt a two-stage strategy: pretraining the\ninpainting module on a small video set to learn general motion priors, followed\nby rapid inference-time fine-tuning for video-specific adaptation. This hybrid\napproach enables adaptation within 10 minutes with one epoch on a single NVIDIA\n4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show\nthat our approach can achieve high-quality beat alignment and visual\nsmoothness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18890", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18890", "abs": "https://arxiv.org/abs/2506.18890", "authors": ["Ziqiao Ma", "Xuweiyi Chen", "Shoubin Yu", "Sai Bi", "Kai Zhang", "Chen Ziwen", "Sihan Xu", "Jianing Yang", "Zexiang Xu", "Kalyan Sunkavalli", "Mohit Bansal", "Joyce Chai", "Hao Tan"], "title": "4D-LRM: Large Space-Time Reconstruction Model From and To Any View at Any Time", "comment": "Project page: https://4dlrm.github.io/", "summary": "Can we scale 4D pretraining to learn general space-time representations that\nreconstruct an object from a few views at some times to any view at any time?\nWe provide an affirmative answer with 4D-LRM, the first large-scale 4D\nreconstruction model that takes input from unconstrained views and timestamps\nand renders arbitrary novel view-time combinations. Unlike prior 4D approaches,\ne.g., optimization-based, geometry-based, or generative, that struggle with\nefficiency, generalization, or faithfulness, 4D-LRM learns a unified space-time\nrepresentation and directly predicts per-pixel 4D Gaussian primitives from\nposed image tokens across time, enabling fast, high-quality rendering at, in\nprinciple, infinite frame rate. Our results demonstrate that scaling\nspatiotemporal pretraining enables accurate and efficient 4D reconstruction. We\nshow that 4D-LRM generalizes to novel objects, interpolates across time, and\nhandles diverse camera setups. It reconstructs 24-frame sequences in one\nforward pass with less than 1.5 seconds on a single A100 GPU.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17636", "categories": ["cs.GR", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.17636", "abs": "https://arxiv.org/abs/2506.17636", "authors": ["Shihan Chen", "Zhaojin Li", "Zeyu Chen", "Qingsong Yan", "Gaoyang Shen", "Ran Duan"], "title": "3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene", "comment": "IROS 2025", "summary": "Recent developments in 3D Gaussian Splatting have made significant advances\nin surface reconstruction. However, scaling these methods to large-scale scenes\nremains challenging due to high computational demands and the complex dynamic\nappearances typical of outdoor environments. These challenges hinder the\napplication in aerial surveying and autonomous driving. This paper proposes a\nnovel solution to reconstruct large-scale surfaces with fine details,\nsupervised by full-sized images. Firstly, we introduce a coarse-to-fine\nstrategy to reconstruct a coarse model efficiently, followed by adaptive scene\npartitioning and sub-scene refining from image segments. Additionally, we\nintegrate a decoupling appearance model to capture global appearance variations\nand a transient mask model to mitigate interference from moving objects.\nFinally, we expand the multi-view constraint and introduce a single-view\nregularization for texture-less areas. Our experiments were conducted on the\npublicly available dataset GauU-Scene V2, which was captured using unmanned\naerial vehicles. To the best of our knowledge, our method outperforms existing\nNeRF-based and Gaussian-based methods, achieving high-fidelity visual results\nand accurate surface from full-size image optimization. Open-source code will\nbe available on GitHub.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17290", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17290", "abs": "https://arxiv.org/abs/2506.17290", "authors": ["Yuqi Li", "Junhao Dong", "Zeyu Dong", "Chuanguang Yang", "Zhulin An", "Yongjun Xu"], "title": "SRKD: Towards Efficient 3D Point Cloud Segmentation via Structure- and Relation-aware Knowledge Distillation", "comment": "13 pages", "summary": "3D point cloud segmentation faces practical challenges due to the\ncomputational complexity and deployment limitations of large-scale\ntransformer-based models. To address this, we propose a novel Structure- and\nRelation-aware Knowledge Distillation framework, named SRKD, that transfers\nrich geometric and semantic knowledge from a large frozen teacher model (>100M)\nto a lightweight student model (<15M). Specifically, we propose an affinity\nmatrix-based relation alignment module, which distills structural dependencies\nfrom the teacher to the student through point-wise similarity matching,\nenhancing the student's capability to learn contextual interactions. Meanwhile,\nwe introduce a cross-sample mini-batch construction strategy that enables the\nstudent to perceive stable and generalized geometric structure. This aligns\nacross diverse point cloud instances of the teacher, rather than within a\nsingle sample. Additionally, KL divergence is applied to align semantic\ndistributions, and ground-truth supervision further reinforces accurate\nsegmentation. Our method achieves state of the art performance with\nsignificantly reduced model complexity, demonstrating its effectiveness and\nefficiency in real-world deployment scenarios. Our Code is available at\nhttps://github.com/itsnotacie/SRKD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17302", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17302", "abs": "https://arxiv.org/abs/2506.17302", "authors": ["Yijun Lin", "Theresa Chen", "Colby Brungard", "Grunwald Sabine", "Sue Ives", "Matt Macander", "Timm Nawrocki", "Yao-Yi Chiang", "Nic Jelinski"], "title": "Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning", "comment": "12 pages, Submitted to SIGSPATIAL 2025", "summary": "Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and\nlocalized simulations, remains a critical yet underdeveloped task, despite the\nregion's ecological importance and extensive permafrost coverage. As permafrost\nthaw accelerates due to climate change, it threatens infrastructure stability\nand key ecosystem services, such as soil carbon storage. High-resolution soil\nmaps are essential for characterizing permafrost distribution, identifying\nvulnerable areas, and informing adaptation strategies. We present MISO, a\nvision-based machine learning (ML) model to produce statewide fine-scale soil\nmaps for near-surface permafrost and soil taxonomy. The model integrates a\ngeospatial foundation model for visual feature extraction, implicit neural\nrepresentations for continuous spatial prediction, and contrastive learning for\nmultimodal alignment and geo-location awareness. We compare MISO with Random\nForest (RF), a traditional ML model that has been widely used in soil mapping\napplications. Spatial cross-validation and regional analysis across Permafrost\nZones and Major Land Resource Areas (MLRAs) show that MISO generalizes better\nto remote, unseen locations and achieves higher recall than RF, which is\ncritical for monitoring permafrost thaw and related environmental processes.\nThese findings demonstrate the potential of advanced ML approaches for\nfine-scale soil mapping and provide practical guidance for future soil sampling\nand infrastructure planning in permafrost-affected landscapes. The project will\nbe released at https://github.com/knowledge-computing/Peatland-permafrost.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17325", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17325", "abs": "https://arxiv.org/abs/2506.17325", "authors": ["Sina Najafi", "M. Hadi Sepanj", "Fahimeh Jafari"], "title": "RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences", "comment": null, "summary": "Predicting user churn in non-subscription gig platforms, where disengagement\nis implicit, poses unique challenges due to the absence of explicit labels and\nthe dynamic nature of user behavior. Existing methods often rely on aggregated\nsnapshots or static visual representations, which obscure temporal cues\ncritical for early detection. In this work, we propose a temporally-aware\ncomputer vision framework that models user behavioral patterns as a sequence of\nradar chart images, each encoding day-level behavioral features. By integrating\na pretrained CNN encoder with a bidirectional LSTM, our architecture captures\nboth spatial and temporal patterns underlying churn behavior. Extensive\nexperiments on a large real-world dataset demonstrate that our method\noutperforms classical models and ViT-based radar chart baselines, yielding\ngains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with\nimproved interpretability. The framework's modular design, explainability\ntools, and efficient deployment characteristics make it suitable for\nlarge-scale churn modeling in dynamic gig-economy platforms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17332", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17332", "abs": "https://arxiv.org/abs/2506.17332", "authors": ["Haitian Wang", "Yiren Wang", "Xinyu Wang", "Yumeng Miao", "Yuliang Zhang", "Yu Zhang", "Atif Mansoor"], "title": "P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments", "comment": "Accepted to appear in the 2025 IEEE International Workshop on AIoT\n  and Smart Systems (AIoTSys'25). Nominated for Best Paper Award and Best IoT\n  System Implementation Award. Code and pretrained models available at:\n  https://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom", "summary": "By 2050, people aged 65 and over are projected to make up 16 percent of the\nglobal population. As aging is closely associated with increased fall risk,\nparticularly in wet and confined environments such as bathrooms where over 80\npercent of falls occur. Although recent research has increasingly focused on\nnon-intrusive, privacy-preserving approaches that do not rely on wearable\ndevices or video-based monitoring, these efforts have not fully overcome the\nlimitations of existing unimodal systems (e.g., WiFi-, infrared-, or\nmmWave-based), which are prone to reduced accuracy in complex environments.\nThese limitations stem from fundamental constraints in unimodal sensing,\nincluding system bias and environmental interference, such as multipath fading\nin WiFi-based systems and drastic temperature changes in infrared-based\nmethods. To address these challenges, we propose a Privacy-Preserving\nMultimodal Fall Detection System for Elderly People in Bathroom Environments.\nFirst, we develop a sensor evaluation framework to select and fuse\nmillimeter-wave radar with 3D vibration sensing, and use it to construct and\npreprocess a large-scale, privacy-preserving multimodal dataset in real\nbathroom settings, which will be released upon publication. Second, we\nintroduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch\nfor radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch\nfor vibration impact detection. By uniting macro- and micro-scale features,\nP2MFDS delivers significant gains in accuracy and recall over state-of-the-art\napproaches. Code and pretrained models will be made available at:\nhttps://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17286", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17286", "abs": "https://arxiv.org/abs/2506.17286", "authors": ["Luoyang Sun", "Jiwen Jiang", "Cheng Deng", "Xinjian Wu", "Haifeng Zhang", "Lei Chen", "Lionel Ni", "Jun Wang"], "title": "GTA: Grouped-head latenT Attention", "comment": null, "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17434", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17434", "abs": "https://arxiv.org/abs/2506.17434", "authors": ["Sydney Levine", "Matija Franklin", "Tan Zhi-Xuan", "Secil Yanik Guyot", "Lionel Wong", "Daniel Kilov", "Yejin Choi", "Joshua B. Tenenbaum", "Noah Goodman", "Seth Lazar", "Iason Gabriel"], "title": "Resource Rational Contractualism Should Guide AI Alignment", "comment": "24 pages, 10 figures", "summary": "AI systems will soon have to navigate human environments and make decisions\nthat affect people and other AI agents whose goals and values diverge.\nContractualist alignment proposes grounding those decisions in agreements that\ndiverse stakeholders would endorse under the right conditions, yet securing\nsuch agreement at scale remains costly and slow -- even for advanced AI. We\ntherefore propose Resource-Rational Contractualism (RRC): a framework where AI\nsystems approximate the agreements rational parties would form by drawing on a\ntoolbox of normatively-grounded, cognitively-inspired heuristics that trade\neffort for accuracy. An RRC-aligned agent would not only operate efficiently,\nbut also be equipped to dynamically adapt to and interpret the ever-changing\nhuman social world.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["agreement", "accuracy"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17449", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17449", "abs": "https://arxiv.org/abs/2506.17449", "authors": ["Manasa Bharadwaj", "Nikhil Verma", "Kevin Ferreira"], "title": "OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections", "comment": null, "summary": "Efforts to improve Large Language Model (LLM) agent performance on complex\ntasks have largely focused on fine-tuning and iterative self-correction.\nHowever, these approaches often lack generalizable mechanisms for longterm\nlearning and remain inefficient in dynamic environments. We introduce\nOmniReflect, a hierarchical, reflection-driven framework that constructs a\nconstitution, a compact set of guiding principles distilled from task\nexperiences, to enhance the effectiveness and efficiency of an LLM agent.\nOmniReflect operates in two modes: Self-sustaining, where a single agent\nperiodically curates its own reflections during task execution, and\nCo-operative, where a Meta-advisor derives a constitution from a small\ncalibration set to guide another agent. To construct these constitutional\nprinciples, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering\na balance between contextual adaptability and computational efficiency.\nEmpirical results averaged across models show major improvements in task\nsuccess, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3%\non PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative\nmode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion\nbaselines on BabyAI. These findings highlight the robustness and effectiveness\nof OmniReflect across environments and backbones.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17298", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17298", "abs": "https://arxiv.org/abs/2506.17298", "authors": ["Inception Labs", "Samar Khanna", "Siddhant Kharbanda", "Shufan Li", "Harshit Varma", "Eric Wang", "Sawyer Birnbaum", "Ziyang Luo", "Yanis Miraoui", "Akash Palrecha", "Stefano Ermon", "Aditya Grover", "Volodymyr Kuleshov"], "title": "Mercury: Ultra-Fast Language Models Based on Diffusion", "comment": "15 pages; equal core, cross-function, senior authors listed\n  alphabetically", "summary": "We present Mercury, a new generation of commercial-scale large language\nmodels (LLMs) based on diffusion. These models are parameterized via the\nTransformer architecture and trained to predict multiple tokens in parallel. In\nthis report, we detail Mercury Coder, our first set of diffusion LLMs designed\nfor coding applications. Currently, Mercury Coder comes in two sizes: Mini and\nSmall. These models set a new state-of-the-art on the speed-quality frontier.\nBased on independent evaluations conducted by Artificial Analysis, Mercury\nCoder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109\ntokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform\nspeed-optimized frontier models by up to 10x on average while maintaining\ncomparable quality. We discuss additional results on a variety of code\nbenchmarks spanning multiple languages and use-cases as well as real-world\nvalidation by developers on Copilot Arena, where the model currently ranks\nsecond on quality and is the fastest model overall. We also release a public\nAPI at https://platform.inceptionlabs.ai/ and free playground at\nhttps://chat.inceptionlabs.ai", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17484", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17484", "abs": "https://arxiv.org/abs/2506.17484", "authors": ["Yao Zhang", "Zaixi Shang", "Silpan Patel", "Mikel Zuniga"], "title": "From Unstructured Communication to Intelligent RAG: Multi-Agent Automation for Supply Chain Knowledge Bases", "comment": "Accepted In Proceedings of the 1st Workshop on AI for Supply Chain:\n  Today and Future @ 31st ACM SIGKDD Conference on Knowledge Discovery and Data\n  Mining V.2 (KDD 25), August 3, 2025, Toronto, ON, Canada. ACM, New York, NY,\n  USA, 14 pages, 2 figures", "summary": "Supply chain operations generate vast amounts of operational data; however,\ncritical knowledge such as system usage practices, troubleshooting workflows,\nand resolution techniques often remains buried within unstructured\ncommunications like support tickets, emails, and chat logs. While RAG systems\naim to leverage such communications as a knowledge base, their effectiveness is\nlimited by raw data challenges: support tickets are typically noisy,\ninconsistent, and incomplete, making direct retrieval suboptimal. Unlike\nexisting RAG approaches that focus on runtime optimization, we introduce a\nnovel offline-first methodology that transforms these communications into a\nstructured knowledge base. Our key innovation is a LLMs-based multi-agent\nsystem orchestrating three specialized agents: Category Discovery for taxonomy\ncreation, Categorization for ticket grouping, and Knowledge Synthesis for\narticle generation. Applying our methodology to real-world support tickets with\nresolution notes and comments, our system creates a compact knowledge base -\nreducing total volume to just 3.4% of original ticket data while improving\nquality. Experiments demonstrate that our prebuilt knowledge base in RAG\nsystems significantly outperforms traditional RAG implementations (48.74% vs.\n38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses.\nBy automating institutional knowledge capture that typically remains siloed in\nexperts' heads, our solution translates to substantial operational efficiency:\nreducing support workload, accelerating resolution times, and creating\nself-improving systems that automatically resolve approximately 50% of future\nsupply chain tickets. Our approach addresses a key gap in knowledge management\nby transforming transient communications into structured, reusable knowledge\nthrough intelligent offline processing rather than latency-inducing runtime\narchitectures.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["runtime optimization"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17425", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17425", "abs": "https://arxiv.org/abs/2506.17425", "authors": ["Minmin Yang", "Huantao Ren", "Senem Velipasalar"], "title": "Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction", "comment": null, "summary": "Cone-beam computed tomography (CBCT) using only a few X-ray projection views\nenables faster scans with lower radiation dose, but the resulting severe\nunder-sampling causes strong artifacts and poor spatial coverage. We address\nthese challenges in a unified framework. First, we replace conventional\nUNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model.\nConvolutional layers capture local details, while self-attention layers enhance\nglobal context. We adapt TransUNet to CBCT by combining multi-scale features,\nquerying view-specific features per 3D point, and adding a lightweight\nattenuation-prediction head. This yields Trans-CBCT, which surpasses prior\nbaselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views.\nSecond, we introduce a neighbor-aware Point Transformer to enforce volumetric\ncoherence. This module uses 3D positional encoding and attention over k-nearest\nneighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT,\nprovides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on\nLUNA16 and ToothFairy show consistent gains from six to ten views, validating\nthe effectiveness of combining CNN-Transformer features with point-based\ngeometry reasoning for sparse-view CBCT reconstruction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17410", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.17410", "abs": "https://arxiv.org/abs/2506.17410", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Jionghao Lin", "Sanjit Kakarla", "Shambhavi Bhushan", "Erin Gatz", "Shivang Gupta", "Ralph Abboud", "Kenneth R. Koedinger"], "title": "Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study", "comment": "Short research paper accepted at EC-TEL 2025", "summary": "Tutoring improves student achievement, but identifying and studying what\ntutoring actions are most associated with student learning at scale based on\naudio transcriptions is an open research problem. This present study\ninvestigates the feasibility and scalability of using generative AI to identify\nand evaluate specific tutor moves in real-life math tutoring. We analyze 50\nrandomly selected transcripts of college-student remote tutors assisting middle\nschool students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo,\nGemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills:\ndelivering effective praise and responding to student math errors. All models\nreliably detected relevant situations, for example, tutors providing praise to\nstudents (94-98% accuracy) and a student making a math error (82-88% accuracy)\nand effectively evaluated the tutors' adherence to tutoring best practices,\naligning closely with human judgments (83-89% and 73-77%, respectively). We\npropose a cost-effective prompting strategy and discuss practical implications\nfor using large language models to support scalable assessment in authentic\nsettings. This work further contributes LLM prompts to support reproducibility\nand research in AI-supported learning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17667", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17667", "abs": "https://arxiv.org/abs/2506.17667", "authors": ["Lintao Wang", "Encheng Su", "Jiaqi Liu", "Pengze Li", "Peng Xia", "Jiabei Xiao", "Wenlong Zhang", "Xinnan Dai", "Xi Chen", "Yuan Meng", "Mingyu Ding", "Lei Bai", "Wanli Ouyang", "Shixiang Tang", "Aoran Wang", "Xinzhu Ma"], "title": "PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models", "comment": null, "summary": "Physics problem-solving is a challenging domain for large AI models,\nrequiring integration of conceptual understanding, mathematical reasoning, and\ninterpretation of physical diagrams. Current evaluation methodologies show\nnotable limitations in capturing the breadth and complexity of\nundergraduate-level physics, underscoring the need for more rigorous\nassessments. To this end, we present PhysUniBench, a large-scale multimodal\nbenchmark designed to evaluate and improve the reasoning capabilities of\nmultimodal large language models (MLLMs) specifically on undergraduate-level\nphysics problems. PhysUniBench consists of 3,304 physics questions spanning 8\nmajor sub-disciplines of physics, each accompanied by one visual diagrams. The\nbenchmark includes both open-ended and multiple-choice questions,\nsystematically curated and difficulty-rated through an iterative\nmodel-in-the-loop process. The benchmark's construction involved a rigorous\nmulti-stage process, including multiple roll-outs, expert-level evaluation,\nautomated filtering of easily solved problems, and a nuanced difficulty grading\nsystem with five levels. Through extensive experiments, we observe that current\nstate-of-the-art models encounter substantial challenges in physics reasoning.\nFor example, GPT-4o mini achieves only about 34.2\\% accuracy in the proposed\nPhysUniBench. These results highlight that current MLLMs struggle with advanced\nphysics reasoning, especially on multi-step problems and those requiring\nprecise diagram interpretation. By providing a broad and rigorous assessment\ntool, PhysUniBench aims to drive progress in AI for Science, encouraging the\ndevelopment of models with stronger physical reasoning, problem-solving skills,\nand multimodal understanding. The benchmark and evaluation scripts are\navailable at https://prismax-team.github.io/PhysUniBenchmark/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "mathematical reasoning"], "score": 4}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17469", "categories": ["cs.CV", "I.5.4; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.17469", "abs": "https://arxiv.org/abs/2506.17469", "authors": ["Thomas Plante St-Cyr", "Franois Duhaime", "Jean-Sbastien Dub", "Simon Grenier"], "title": "Photogranulometry -- Dataset of soil images with corresponding particle size distributions", "comment": "8 pages, 10 figures, conference", "summary": "Traditional particle size distribution (PSD) analyses create significant\ndowntime and are expensive in labor and maintenance. These drawbacks could be\nalleviated using optical grain size analysis integrated into routine\ngeotechnical laboratory workflow. This paper presents a high-resolution dataset\nof 12,714 images of 321 different soil samples collected in the Montreal,\nQuebec region, alongside their PSD analysis. It is designed to provide a robust\nstarting point for training convolutional neural networks (CNN) in geotechnical\napplications. Soil samples were photographed in a standardized top-view\nposition with a resolution of 45 MP and a minimum scale of 39.4 micrometers per\npixel, both in their moist and dry states. A custom test bench employing 13x9\ninch white aluminum trays, on which the samples are spread in a thin layer, was\nused. For samples exceeding a size limit, a coning and quartering method was\nemployed for mass reduction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17467", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17467", "abs": "https://arxiv.org/abs/2506.17467", "authors": ["Weixin Liang"], "title": "Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems", "comment": "Stanford CS PhD Dissertation", "summary": "Large language models (LLMs) have shown significant potential to change how\nwe write, communicate, and create, leading to rapid adoption across society.\nThis dissertation examines how individuals and institutions are adapting to and\nengaging with this emerging technology through three research directions.\nFirst, I demonstrate how the institutional adoption of AI detectors introduces\nsystematic biases, particularly disadvantaging writers of non-dominant language\nvarieties, highlighting critical equity concerns in AI governance. Second, I\npresent novel population-level algorithmic approaches that measure the\nincreasing adoption of LLMs across writing domains, revealing consistent\npatterns of AI-assisted content in academic peer reviews, scientific\npublications, consumer complaints, corporate communications, job postings, and\ninternational organization press releases. Finally, I investigate LLMs'\ncapability to provide feedback on research manuscripts through a large-scale\nempirical analysis, offering insights into their potential to support\nresearchers who face barriers in accessing timely manuscript feedback,\nparticularly early-career researchers and those from under-resourced settings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17788", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "I.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.17788", "abs": "https://arxiv.org/abs/2506.17788", "authors": ["Shahab Rahimirad", "Guven Gergerli", "Lucia Romero", "Angela Qian", "Matthew Lyle Olson", "Simon Stepputtis", "Joseph Campbell"], "title": "Bayesian Social Deduction with Graph-Informed Language Models", "comment": "32 pages, 10 figures. Under review", "summary": "Social reasoning - inferring unobservable beliefs and intentions from partial\nobservations of other agents - remains a challenging task for large language\nmodels (LLMs). We evaluate the limits of current reasoning language models in\nthe social deduction game Avalon and find that while the largest models\ndemonstrate strong performance, they require extensive test-time inference and\ndegrade sharply when distilled to smaller, real-time-capable variants. To\naddress this, we introduce a hybrid reasoning framework that externalizes\nbelief inference to a structured probabilistic model, while using an LLM for\nlanguage understanding and interaction. Our approach achieves competitive\nperformance with much larger models in Agent-Agent play and, notably, is the\nfirst language agent to defeat human players in a controlled study - achieving\na 67% win rate and receiving higher qualitative ratings than both reasoning\nbaselines and human teammates. We release code, models, and a dataset to\nsupport future work on social reasoning in LLM agents, which can be found at\nhttps://camp-lab-purdue.github.io/bayesian-social-deduction/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17792", "categories": ["cs.AI", "cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17792", "abs": "https://arxiv.org/abs/2506.17792", "authors": ["Alexandros Evangelidis", "Gricel Vzquez", "Simos Gerasimou"], "title": "Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition", "comment": null, "summary": "Software-intensive systems, such as software product lines and robotics,\nutilise Markov decision processes (MDPs) to capture uncertainty and analyse\nsequential decision-making problems. Despite the usefulness of conventional\npolicy synthesis methods, they fail to scale to large state spaces. Our\napproach addresses this issue and accelerates policy synthesis in large MDPs by\ndynamically refining the MDP and iteratively selecting the most fragile MDP\nregions for refinement. This iterative procedure offers a balance between\naccuracy and efficiency, as refinement occurs only when necessary. Through a\ncomprehensive empirical evaluation comprising diverse case studies and MDPs up\nto 1M states, we demonstrate significant performance improvements yielded by\nour approach compared to the leading probabilistic model checker PRISM (up to\n2x), thus offering a very competitive solution for real-world policy synthesis\ntasks in larger MDPs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17533", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17533", "abs": "https://arxiv.org/abs/2506.17533", "authors": ["Yuanhao Wu", "Juntong Song", "Hanning Zhang", "Tong Zhang", "Cheng Niu"], "title": "DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning", "comment": null, "summary": "In this paper, we propose DuaShepherd, a novel reward modeling framework that\nintegrates two complementary reward signals, correctness and potential, to\nenhance the mathematical reasoning capabilities of Large Language Models\n(LLMs). While correctness-based signals emphasize identification of stepwise\nerrors, potential-based signals focus on the likelihood of reaching the correct\nfinal answer. We developed an automated pipeline for constructing large-scale\nreward modeling dataset with both signals. A unified, multi-head architecture\nwas explored to train the two reward models in a multi-task setup,\ndemonstrating benefits from learning both correctness and potential in\nparallel. By combining these two signals into a compound probability, our model\nachieves consistent performance improvements across multiple benchmarks.\nEmpirical evaluations on MATH500 and ProcessBench confirm that this combined\nreward significantly outperforms models trained on either reward type alone,\nachieving state-of-the-art performance under comparable resource constraints.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "mathematical reasoning"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17900", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.17900", "abs": "https://arxiv.org/abs/2506.17900", "authors": ["Cheng Ji", "Huaiying Luo"], "title": "Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms", "comment": "Accepted by 2025 8th International Conference on Advanced Electronic\n  Materials, Computers and Software Engineering (AEMCSE 2025)", "summary": "With the increasing complexity and rapid expansion of the scale of AI systems\nin cloud platforms, the log data generated during system operation is massive,\nunstructured, and semantically ambiguous, which brings great challenges to\nfault location and system self-repair. In order to solve this problem, this\npaper proposes an intelligent log processing and automatic debugging framework\nbased on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This\nmethod is extended on the basis of the existing pre-trained Transformer model,\nand integrates a multi-stage semantic inference mechanism to realize the\ncontext understanding of system logs and the automatic reconstruction of fault\nchains. Firstly, the system log is dynamically structured, and the unsupervised\nclustering and embedding mechanism is used to extract the event template and\nsemantic schema. Subsequently, the fine-tuned LLM combined with the multi-round\nattention mechanism to perform contextual reasoning on the log sequence to\ngenerate potential fault assumptions and root cause paths. Furthermore, this\npaper introduces a reinforcement learning-based policy-guided recovery planner,\nwhich is driven by the remediation strategy generated by LLM to support dynamic\ndecision-making and adaptive debugging in the cloud environment. Compared with\nthe existing rule engine or traditional log analysis system, the proposed model\nhas stronger semantic understanding ability, continuous learning ability and\nheterogeneous environment adaptability. Experiments on the cloud platform log\ndataset show that LLM-ID improves the fault location accuracy by 16.2%, which\nis significantly better than the current mainstream methods", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17611", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17611", "abs": "https://arxiv.org/abs/2506.17611", "authors": ["Jinchuan Tian", "William Chen", "Yifan Peng", "Jiatong Shi", "Siddhant Arora", "Shikhar Bharadwaj", "Takashi Maekaku", "Yusuke Shinohara", "Keita Goto", "Xiang Yue", "Huck Yang", "Shinji Watanabe"], "title": "OpusLM: A Family of Open Unified Speech Language Models", "comment": null, "summary": "This paper presents Open Unified Speech Language Models (OpusLMs), a family\nof open foundational speech language models (SpeechLMs) up to 7B. Initialized\nfrom decoder-only text language models, the OpusLMs are continuously\npre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We\ndemonstrate our OpusLMs achieve comparable (or even superior) performance with\nexisting SpeechLMs in speech recognition, speech synthesis, and text-only\ncapabilities. Technically, this paper articulates our SpeechLM designs on\ntokenization, multi-stream language models, and multi-stage training\nstrategies. We experimentally demonstrate the importance of model size scaling\nand the effect of annealing data selection. The OpusLMs are all built from\npublicly available materials and are fully transparent models. We release our\ncode, data, checkpoints, and training logs to facilitate open SpeechLM research", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17597", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17597", "abs": "https://arxiv.org/abs/2506.17597", "authors": ["Pengyu Kan", "Craig Jones", "Kenichi Oishi"], "title": "OpenMAP-BrainAge: Generalizable and Interpretable Brain Age Predictor", "comment": null, "summary": "Purpose: To develop an age prediction model which is interpretable and robust\nto demographic and technological variances in brain MRI scans. Materials and\nMethods: We propose a transformer-based architecture that leverages\nself-supervised pre-training on large-scale datasets. Our model processes\npseudo-3D T1-weighted MRI scans from three anatomical views and incorporates\nbrain volumetric information. By introducing a stem architecture, we reduce the\nconventional quadratic complexity of transformer models to linear complexity,\nenabling scalability for high-dimensional MRI data. We trained our model on\nADNI2 $\\&$ 3 (N=1348) and OASIS3 (N=716) datasets (age range: 42 - 95) from the\nNorth America, with an 8:1:1 split for train, validation and test. Then, we\nvalidated it on the AIBL dataset (N=768, age range: 60 - 92) from Australia.\nResults: We achieved an MAE of 3.65 years on ADNI2 $\\&$ 3 and OASIS3 test set\nand a high generalizability of MAE of 3.54 years on AIBL. There was a notable\nincrease in brain age gap (BAG) across cognitive groups, with mean of 0.15\nyears (95% CI: [-0.22, 0.51]) in CN, 2.55 years ([2.40, 2.70]) in MCI, 6.12\nyears ([5.82, 6.43]) in AD. Additionally, significant negative correlation\nbetween BAG and cognitive scores was observed, with correlation coefficient of\n-0.185 (p < 0.001) for MoCA and -0.231 (p < 0.001) for MMSE. Gradient-based\nfeature attribution highlighted ventricles and white matter structures as key\nregions influenced by brain aging. Conclusion: Our model effectively fused\ninformation from different views and volumetric information to achieve\nstate-of-the-art brain age prediction accuracy, improved generalizability and\ninterpretability with association to neurodegenerative disorders.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17608", "abs": "https://arxiv.org/abs/2506.17608", "authors": ["Nikitha SR", "Aradhya Neeraj Mathur", "Tarun Ram Menta", "Rishabh Jain", "Mausoom Sarkar"], "title": "HIRE: Lightweight High-Resolution Image Feature Enrichment for Multimodal LLMs", "comment": "Accepted in CVPR 2025 Workshop on What's Next in Multimodal\n  Foundational Models", "summary": "The integration of high-resolution image features in modern multimodal large\nlanguage models has demonstrated significant improvements in fine-grained\nvisual understanding tasks, achieving high performance across multiple\nbenchmarks. Since these features are obtained from large image encoders like\nViT, they come with a significant increase in computational costs due to\nmultiple calls to these encoders. In this work, we first develop an intuition\nfor feature upsampling as a natural extension of high-resolution feature\ngeneration. Through extensive experiments and ablations, we demonstrate how a\nshallow feature enricher can achieve competitive results with tremendous\nreductions in training and inference time as well as computational cost, with\nupto 1.5x saving in FLOPs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17693", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17693", "abs": "https://arxiv.org/abs/2506.17693", "authors": ["Yuzhe Ding", "Kang He", "Bobo Li", "Li Zheng", "Haijun He", "Fei Li", "Chong Teng", "Donghong Ji"], "title": "Zero-Shot Conversational Stance Detection: Dataset and Approaches", "comment": "ACL 2025 (Findings)", "summary": "Stance detection, which aims to identify public opinion towards specific\ntargets using social media data, is an important yet challenging task. With the\nincreasing number of online debates among social media users, conversational\nstance detection has become a crucial research area. However, existing\nconversational stance detection datasets are restricted to a limited set of\nspecific targets, which constrains the effectiveness of stance detection models\nwhen encountering a large number of unseen targets in real-world applications.\nTo bridge this gap, we manually curate a large-scale, high-quality zero-shot\nconversational stance detection dataset, named ZS-CSD, comprising 280 targets\nacross two distinct target types. Leveraging the ZS-CSD dataset, we propose\nSITPCL, a speaker interaction and target-aware prototypical contrastive\nlearning model, and establish the benchmark performance in the zero-shot\nsetting. Experimental results demonstrate that our proposed SITPCL model\nachieves state-of-the-art performance in zero-shot conversational stance\ndetection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,\nhighlighting the persistent challenges in zero-shot conversational stance\ndetection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17679", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17679", "abs": "https://arxiv.org/abs/2506.17679", "authors": ["Wei Haolin"], "title": "CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection", "comment": "15pages, 11figures", "summary": "Convolutional neural networks (CNNs) have long been the cornerstone of target\ndetection, but they are often limited by limited receptive fields, which\nhinders their ability to capture global contextual information. This paper\nbelieves that the effective utilization of extracted features is as important\nas the feature extraction process itself. We critically re-evaluated the\nDETR-inspired header network architecture, questioning the indispensable nature\nof its self-attention mechanism, and discovering significant information\nredundancies. To solve these problems, we introduced the Context-Gated\nScale-Adaptive Detection Network (CSDN), a Transformer-based detection header\ninspired by natural language processing architecture and human visual\nperception. CSDN aims to efficiently utilize the characteristics of the CNN\nbackbone network by replacing the traditional stacked self-attention and\ncross-attention layers with a novel gating mechanism. This mechanism enables\neach region of interest (ROI) to adaptively select and combine feature\ndimensions and scale information from multiple attention patterns. CSDN\nprovides more powerful global context modeling capabilities and can better\nadapt to objects of different sizes and structures. Our proposed detection head\ncan directly replace the native heads of various CNN-based detectors, and only\na few rounds of fine-tuning on the pre-training weights can significantly\nimprove the detection accuracy, thus avoiding the need to achieve small\nimprovements. Various layer modules undergo extensive re-training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18183", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18183", "abs": "https://arxiv.org/abs/2506.18183", "authors": ["Zhiting Mei", "Christina Zhang", "Tenny Yin", "Justin Lidard", "Ola Shorinwa", "Anirudha Majumdar"], "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?", "comment": null, "summary": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17863", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17863", "abs": "https://arxiv.org/abs/2506.17863", "authors": ["Haoran Liu", "Amir Tahmasbi", "Ehtesham Sam Haque", "Purak Jain"], "title": "LLMs for Customized Marketing Content Generation and Evaluation at Scale", "comment": "KDD LLM4ECommerce Workshop 2025", "summary": "Offsite marketing is essential in e-commerce, enabling businesses to reach\ncustomers through external platforms and drive traffic to retail websites.\nHowever, most current offsite marketing content is overly generic,\ntemplate-based, and poorly aligned with landing pages, limiting its\neffectiveness. To address these limitations, we propose MarketingFM, a\nretrieval-augmented system that integrates multiple data sources to generate\nkeyword-specific ad copy with minimal human intervention. We validate\nMarketingFM via offline human and automated evaluations and large-scale online\nA/B tests. In one experiment, keyword-focused ad copy outperformed templates,\nachieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,\ndemonstrating gains in ad ranking and cost efficiency. Despite these gains,\nhuman review of generated ads remains costly. To address this, we propose\nAutoEval-Main, an automated evaluation system that combines rule-based metrics\nwith LLM-as-a-Judge techniques to ensure alignment with marketing principles.\nIn experiments with large-scale human annotations, AutoEval-Main achieved\n89.57% agreement with human reviewers. Building on this, we propose\nAutoEval-Update, a cost-efficient LLM-human collaborative framework to\ndynamically refine evaluation prompts and adapt to shifting criteria with\nminimal human input. By selectively sampling representative ads for human\nreview and using a critic LLM to generate alignment reports, AutoEval-Update\nimproves evaluation consistency while reducing manual effort. Experiments show\nthe critic LLM suggests meaningful refinements, improving LLM-human agreement.\nNonetheless, human oversight remains essential for setting thresholds and\nvalidating refinements before deployment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement", "consistency", "criteria"], "score": 4}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18233", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18233", "abs": "https://arxiv.org/abs/2506.18233", "authors": ["Ruike Zhu", "Hanwen Zhang", "Tianyu Shi", "Chi Wang", "Tianyi Zhou", "Zengyi Qin"], "title": "The 4th Dimension for Scaling Model Size", "comment": null, "summary": "Scaling the size of large language models typically involves three\ndimensions: depth, width, and the number of parameters. In this work, we\nexplore a fourth dimension, virtual logical depth (VLD), which increases the\neffective algorithmic depth without changing the overall parameter count by\nreusing parameters within the model. Although parameter reuse is not a new\nconcept, its potential and characteristics in model scaling have not been\nthoroughly studied. Through carefully designed controlled experiments, we make\nthe following key discoveries regarding VLD scaling:\n  VLD scaling forces the knowledge capacity of the model to remain almost\nconstant, with only minor variations.\n  VLD scaling enables a significant improvement in reasoning capability,\nprovided the scaling method is properly implemented.\n  The number of parameters correlates with knowledge capacity, but not with\nreasoning capability. Under certain conditions, it is not necessary to increase\nthe parameter count to enhance reasoning.\n  These findings are consistent across various model configurations and are\nlikely to be generally valid within the scope of our experiments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17707", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.17707", "abs": "https://arxiv.org/abs/2506.17707", "authors": ["Jihyun Kim", "Junho Park", "Kyeongbo Kong", "Suk-Ju Kang"], "title": "Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models", "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "We present Programmable-Room, a framework which interactively generates and\nedits a 3D room mesh, given natural language instructions. For precise control\nof a room's each attribute, we decompose the challenging task into simpler\nsteps such as creating plausible 3D coordinates for room meshes, generating\npanorama images for the texture, constructing 3D meshes by integrating the\ncoordinates and panorama texture images, and arranging furniture. To support\nthe various decomposed tasks with a unified framework, we incorporate visual\nprogramming (VP). VP is a method that utilizes a large language model (LLM) to\nwrite a Python-like program which is an ordered list of necessary modules for\nthe various tasks given in natural language. We develop most of the modules.\nEspecially, for the texture generating module, we utilize a pretrained\nlarge-scale diffusion model to generate panorama images conditioned on text and\nvisual prompts (i.e., layout, depth, and semantic map) simultaneously.\nSpecifically, we enhance the panorama image generation quality by optimizing\nthe training objective with a 1D representation of a panorama scene obtained\nfrom bidirectional LSTM. We demonstrate Programmable-Room's flexibility in\ngenerating and editing 3D room meshes, and prove our framework's superiority to\nan existing model quantitatively and qualitatively. Project page is available\nin https://jihyun0510.github.io/Programmable_Room_Page/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17864", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17864", "abs": "https://arxiv.org/abs/2506.17864", "authors": ["Taolin Zhang", "Haidong Kang", "Dongyang Li", "Qizhou Chen", "Chengyu Wang Xiaofeng He", "Richang Hong"], "title": "QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs", "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive results\nbut still suffer from hallucinations. Model editing has been proposed to\ncorrect factual inaccuracies in LLMs. A challenging case is sequential model\nediting (SME), which aims to rectify errors continuously rather than treating\nthem as a one-time task. During SME, the general capabilities of LLMs can be\nnegatively affected due to the introduction of new parameters. In this paper,\nwe propose a queue-based self-correction framework (QueueEDIT) that not only\nenhances SME performance by addressing long-sequence dependency but also\nmitigates the impact of parameter bias on the general capabilities of LLMs.\nSpecifically, we first introduce a structural mapping editing loss to map the\ntriplets to the knowledge-sensitive neurons within the Transformer layers of\nLLMs. We then store the located parameters for each piece of edited knowledge\nin a queue and dynamically align previously edited parameters. In each edit, we\nselect queue parameters most relevant to the currently located parameters to\ndetermine whether previous knowledge needs realignment. Irrelevant parameters\nin the queue are frozen, and we update the parameters at the queue head to the\nLLM to ensure they do not harm general abilities. Experiments show that our\nframework significantly outperforms strong baselines across various SME\nsettings and maintains competitiveness in single-turn editing. The resulting\nLLMs also preserve high capabilities in general NLP tasks throughout the SME\nprocess.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17712", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17712", "abs": "https://arxiv.org/abs/2506.17712", "authors": ["Xinyu Xiong", "Wuteng Cao", "Zihuang Wu", "Lei Zhang", "Chong Gao", "Guanbin Li", "Qiyuan Qin"], "title": "PDC-Net: Pattern Divide-and-Conquer Network for Pelvic Radiation Injury Segmentation", "comment": "MICCAI 2025", "summary": "Accurate segmentation of Pelvic Radiation Injury (PRI) from Magnetic\nResonance Images (MRI) is crucial for more precise prognosis assessment and the\ndevelopment of personalized treatment plans. However, automated segmentation\nremains challenging due to factors such as complex organ morphologies and\nconfusing context. To address these challenges, we propose a novel Pattern\nDivide-and-Conquer Network (PDC-Net) for PRI segmentation. The core idea is to\nuse different network modules to \"divide\" various local and global patterns\nand, through flexible feature selection, to \"conquer\" the Regions of Interest\n(ROI) during the decoding phase. Specifically, considering that our ROI often\nmanifests as strip-like or circular-like structures in MR slices, we introduce\na Multi-Direction Aggregation (MDA) module. This module enhances the model's\nability to fit the shape of the organ by applying strip convolutions in four\ndistinct directions. Additionally, to mitigate the challenge of confusing\ncontext, we propose a Memory-Guided Context (MGC) module. This module\nexplicitly maintains a memory parameter to track cross-image patterns at the\ndataset level, thereby enhancing the distinction between global patterns\nassociated with the positive and negative classes. Finally, we design an\nAdaptive Fusion Decoder (AFD) that dynamically selects features from different\npatterns based on the Mixture-of-Experts (MoE) framework, ultimately generating\nthe final segmentation results. We evaluate our method on the first large-scale\npelvic radiation injury dataset, and the results demonstrate the superiority of\nour PDC-Net over existing approaches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17733", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17733", "abs": "https://arxiv.org/abs/2506.17733", "authors": ["Mengqi Lei", "Siqi Li", "Yihong Wu", "Han Hu", "You Zhou", "Xinhu Zheng", "Guiguang Ding", "Shaoyi Du", "Zongze Wu", "Yue Gao"], "title": "YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception", "comment": null, "summary": "The YOLO series models reign supreme in real-time object detection due to\ntheir superior accuracy and computational efficiency. However, both the\nconvolutional architectures of YOLO11 and earlier versions and the area-based\nself-attention mechanism introduced in YOLOv12 are limited to local information\naggregation and pairwise correlation modeling, lacking the capability to\ncapture global multi-to-multi high-order correlations, which limits detection\nperformance in complex scenarios. In this paper, we propose YOLOv13, an\naccurate and lightweight object detector. To address the above-mentioned\nchallenges, we propose a Hypergraph-based Adaptive Correlation Enhancement\n(HyperACE) mechanism that adaptively exploits latent high-order correlations\nand overcomes the limitation of previous methods that are restricted to\npairwise correlation modeling based on hypergraph computation, achieving\nefficient global cross-location and cross-scale feature fusion and enhancement.\nSubsequently, we propose a Full-Pipeline Aggregation-and-Distribution (FullPAD)\nparadigm based on HyperACE, which effectively achieves fine-grained information\nflow and representation synergy within the entire network by distributing\ncorrelation-enhanced features to the full pipeline. Finally, we propose to\nleverage depthwise separable convolutions to replace vanilla large-kernel\nconvolutions, and design a series of blocks that significantly reduce\nparameters and computational complexity without sacrificing performance. We\nconduct extensive experiments on the widely used MS COCO benchmark, and the\nexperimental results demonstrate that our method achieves state-of-the-art\nperformance with fewer parameters and FLOPs. Specifically, our YOLOv13-N\nimproves mAP by 3.0\\% over YOLO11-N and by 1.5\\% over YOLOv12-N. The code and\nmodels of our YOLOv13 model are available at:\nhttps://github.com/iMoonLab/yolov13.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18035", "categories": ["cs.CL", "cs.SD", "eess.AS", "68T50 (Primary)", "I.2.7; I.5.4"], "pdf": "https://arxiv.org/pdf/2506.18035", "abs": "https://arxiv.org/abs/2506.18035", "authors": ["Maxence Lasbordes", "Daniele Falavigna", "Alessio Brutti"], "title": "Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices", "comment": "5 pages, 3 Postscript figures", "summary": "The ability to dynamically adjust the computational load of neural models\nduring inference in a resource aware manner is crucial for on-device processing\nscenarios, characterised by limited and time-varying computational resources.\nEarly-exit architectures represent an elegant and effective solution, since\nthey can process the input with a subset of their layers, exiting at\nintermediate branches (the upmost layers are hence removed from the model).\n  From a different perspective, for automatic speech recognition applications\nthere are memory-efficient neural architectures that apply variable frame rate\nanalysis, through downsampling/upsampling operations in the middle layers,\nreducing the overall number of operations and improving significantly the\nperformance on well established benchmarks. One example is the Zipformer.\nHowever, these architectures lack the modularity necessary to inject early-exit\nbranches.\n  With the aim of improving the performance in early-exit models, we propose\nintroducing parallel layers in the architecture that process downsampled\nversions of their inputs. % in conjunction with standard processing layers. We\nshow that in this way the speech recognition performance on standard benchmarks\nsignificantly improve, at the cost of a small increase in the overall number of\nmodel parameters but without affecting the inference time.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17286", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17286", "abs": "https://arxiv.org/abs/2506.17286", "authors": ["Luoyang Sun", "Jiwen Jiang", "Cheng Deng", "Xinjian Wu", "Haifeng Zhang", "Lei Chen", "Lionel Ni", "Jun Wang"], "title": "GTA: Grouped-head latenT Attention", "comment": null, "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17298", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17298", "abs": "https://arxiv.org/abs/2506.17298", "authors": ["Inception Labs", "Samar Khanna", "Siddhant Kharbanda", "Shufan Li", "Harshit Varma", "Eric Wang", "Sawyer Birnbaum", "Ziyang Luo", "Yanis Miraoui", "Akash Palrecha", "Stefano Ermon", "Aditya Grover", "Volodymyr Kuleshov"], "title": "Mercury: Ultra-Fast Language Models Based on Diffusion", "comment": "15 pages; equal core, cross-function, senior authors listed\n  alphabetically", "summary": "We present Mercury, a new generation of commercial-scale large language\nmodels (LLMs) based on diffusion. These models are parameterized via the\nTransformer architecture and trained to predict multiple tokens in parallel. In\nthis report, we detail Mercury Coder, our first set of diffusion LLMs designed\nfor coding applications. Currently, Mercury Coder comes in two sizes: Mini and\nSmall. These models set a new state-of-the-art on the speed-quality frontier.\nBased on independent evaluations conducted by Artificial Analysis, Mercury\nCoder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109\ntokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform\nspeed-optimized frontier models by up to 10x on average while maintaining\ncomparable quality. We discuss additional results on a variety of code\nbenchmarks spanning multiple languages and use-cases as well as real-world\nvalidation by developers on Copilot Arena, where the model currently ranks\nsecond on quality and is the fastest model overall. We also release a public\nAPI at https://platform.inceptionlabs.ai/ and free playground at\nhttps://chat.inceptionlabs.ai", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17325", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17325", "abs": "https://arxiv.org/abs/2506.17325", "authors": ["Sina Najafi", "M. Hadi Sepanj", "Fahimeh Jafari"], "title": "RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences", "comment": null, "summary": "Predicting user churn in non-subscription gig platforms, where disengagement\nis implicit, poses unique challenges due to the absence of explicit labels and\nthe dynamic nature of user behavior. Existing methods often rely on aggregated\nsnapshots or static visual representations, which obscure temporal cues\ncritical for early detection. In this work, we propose a temporally-aware\ncomputer vision framework that models user behavioral patterns as a sequence of\nradar chart images, each encoding day-level behavioral features. By integrating\na pretrained CNN encoder with a bidirectional LSTM, our architecture captures\nboth spatial and temporal patterns underlying churn behavior. Extensive\nexperiments on a large real-world dataset demonstrate that our method\noutperforms classical models and ViT-based radar chart baselines, yielding\ngains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with\nimproved interpretability. The framework's modular design, explainability\ntools, and efficient deployment characteristics make it suitable for\nlarge-scale churn modeling in dynamic gig-economy platforms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17931", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17931", "abs": "https://arxiv.org/abs/2506.17931", "authors": ["Ravi Kant Gupta", "Shounak Das", "Amit Sethi"], "title": "IDAL: Improved Domain Adaptive Learning for Natural Images Dataset", "comment": "Accepted in ICPR'24 (International Conference on Pattern Recognition)", "summary": "We present a novel approach for unsupervised domain adaptation (UDA) for\nnatural images. A commonly-used objective for UDA schemes is to enhance domain\nalignment in representation space even if there is a domain shift in the input\nspace. Existing adversarial domain adaptation methods may not effectively align\ndifferent domains of multimodal distributions associated with classification\nproblems. Our approach has two main features. Firstly, its neural architecture\nuses the deep structure of ResNet and the effective separation of scales of\nfeature pyramidal network (FPN) to work with both content and style features.\nSecondly, it uses a combination of a novel loss function and judiciously\nselected existing loss functions to train the network architecture. This\ntailored combination is designed to address challenges inherent to natural\nimages, such as scale, noise, and style shifts, that occur on top of a\nmulti-modal (multi-class) distribution. The combined loss function not only\nenhances model accuracy and robustness on the target domain but also speeds up\ntraining convergence. Our proposed UDA scheme generalizes better than\nstate-of-the-art for CNN-based methods on Office-Home, Office-31, and\nVisDA-2017 datasets and comaparable for DomainNet dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17332", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17332", "abs": "https://arxiv.org/abs/2506.17332", "authors": ["Haitian Wang", "Yiren Wang", "Xinyu Wang", "Yumeng Miao", "Yuliang Zhang", "Yu Zhang", "Atif Mansoor"], "title": "P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments", "comment": "Accepted to appear in the 2025 IEEE International Workshop on AIoT\n  and Smart Systems (AIoTSys'25). Nominated for Best Paper Award and Best IoT\n  System Implementation Award. Code and pretrained models available at:\n  https://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom", "summary": "By 2050, people aged 65 and over are projected to make up 16 percent of the\nglobal population. As aging is closely associated with increased fall risk,\nparticularly in wet and confined environments such as bathrooms where over 80\npercent of falls occur. Although recent research has increasingly focused on\nnon-intrusive, privacy-preserving approaches that do not rely on wearable\ndevices or video-based monitoring, these efforts have not fully overcome the\nlimitations of existing unimodal systems (e.g., WiFi-, infrared-, or\nmmWave-based), which are prone to reduced accuracy in complex environments.\nThese limitations stem from fundamental constraints in unimodal sensing,\nincluding system bias and environmental interference, such as multipath fading\nin WiFi-based systems and drastic temperature changes in infrared-based\nmethods. To address these challenges, we propose a Privacy-Preserving\nMultimodal Fall Detection System for Elderly People in Bathroom Environments.\nFirst, we develop a sensor evaluation framework to select and fuse\nmillimeter-wave radar with 3D vibration sensing, and use it to construct and\npreprocess a large-scale, privacy-preserving multimodal dataset in real\nbathroom settings, which will be released upon publication. Second, we\nintroduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch\nfor radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch\nfor vibration impact detection. By uniting macro- and micro-scale features,\nP2MFDS delivers significant gains in accuracy and recall over state-of-the-art\napproaches. Code and pretrained models will be made available at:\nhttps://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17954", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17954", "abs": "https://arxiv.org/abs/2506.17954", "authors": ["Liong Gele", "Tan Chye Cheah"], "title": "Mobile Image Analysis Application for Mantoux Skin Test", "comment": null, "summary": "This paper presents a newly developed mobile application designed to diagnose\nLatent Tuberculosis Infection (LTBI) using the Mantoux Skin Test (TST).\nTraditional TST methods often suffer from low follow-up return rates, patient\ndiscomfort, and subjective manual interpretation, particularly with the\nball-point pen method, leading to misdiagnosis and delayed treatment. Moreover,\nprevious developed mobile applications that used 3D reconstruction, this app\nutilizes scaling stickers as reference objects for induration measurement. This\nmobile application integrates advanced image processing technologies, including\nARCore, and machine learning algorithms such as DeepLabv3 for robust image\nsegmentation and precise measurement of skin indurations indicative of LTBI.\nThe system employs an edge detection algorithm to enhance accuracy. The\napplication was evaluated against standard clinical practices, demonstrating\nsignificant improvements in accuracy and reliability. This innovation is\ncrucial for effective tuberculosis management, especially in resource-limited\nregions. By automating and standardizing TST evaluations, the application\nenhances the accessibility and efficiency of TB di-agnostics. Future work will\nfocus on refining machine learning models, optimizing measurement algorithms,\nexpanding functionalities to include comprehensive patient data management, and\nenhancing ARCore's performance across various lighting conditions and\noperational settings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17425", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17425", "abs": "https://arxiv.org/abs/2506.17425", "authors": ["Minmin Yang", "Huantao Ren", "Senem Velipasalar"], "title": "Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction", "comment": null, "summary": "Cone-beam computed tomography (CBCT) using only a few X-ray projection views\nenables faster scans with lower radiation dose, but the resulting severe\nunder-sampling causes strong artifacts and poor spatial coverage. We address\nthese challenges in a unified framework. First, we replace conventional\nUNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model.\nConvolutional layers capture local details, while self-attention layers enhance\nglobal context. We adapt TransUNet to CBCT by combining multi-scale features,\nquerying view-specific features per 3D point, and adding a lightweight\nattenuation-prediction head. This yields Trans-CBCT, which surpasses prior\nbaselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views.\nSecond, we introduce a neighbor-aware Point Transformer to enforce volumetric\ncoherence. This module uses 3D positional encoding and attention over k-nearest\nneighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT,\nprovides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on\nLUNA16 and ToothFairy show consistent gains from six to ten views, validating\nthe effectiveness of combining CNN-Transformer features with point-based\ngeometry reasoning for sparse-view CBCT reconstruction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17467", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17467", "abs": "https://arxiv.org/abs/2506.17467", "authors": ["Weixin Liang"], "title": "Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems", "comment": "Stanford CS PhD Dissertation", "summary": "Large language models (LLMs) have shown significant potential to change how\nwe write, communicate, and create, leading to rapid adoption across society.\nThis dissertation examines how individuals and institutions are adapting to and\nengaging with this emerging technology through three research directions.\nFirst, I demonstrate how the institutional adoption of AI detectors introduces\nsystematic biases, particularly disadvantaging writers of non-dominant language\nvarieties, highlighting critical equity concerns in AI governance. Second, I\npresent novel population-level algorithmic approaches that measure the\nincreasing adoption of LLMs across writing domains, revealing consistent\npatterns of AI-assisted content in academic peer reviews, scientific\npublications, consumer complaints, corporate communications, job postings, and\ninternational organization press releases. Finally, I investigate LLMs'\ncapability to provide feedback on research manuscripts through a large-scale\nempirical analysis, offering insights into their potential to support\nresearchers who face barriers in accessing timely manuscript feedback,\nparticularly early-career researchers and those from under-resourced settings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18006", "abs": "https://arxiv.org/abs/2506.18006", "authors": ["Shuaiyu Chen", "Fu Wang", "Peng Ren", "Chunbo Luo", "Zeyu Fu"], "title": "OSDMamba: Enhancing Oil Spill Detection from Remote Sensing Images Using Selective State Space Model", "comment": null, "summary": "Semantic segmentation is commonly used for Oil Spill Detection (OSD) in\nremote sensing images. However, the limited availability of labelled oil spill\nsamples and class imbalance present significant challenges that can reduce\ndetection accuracy. Furthermore, most existing methods, which rely on\nconvolutional neural networks (CNNs), struggle to detect small oil spill areas\ndue to their limited receptive fields and inability to effectively capture\nglobal contextual information. This study explores the potential of State-Space\nModels (SSMs), particularly Mamba, to overcome these limitations, building on\ntheir recent success in vision applications. We propose OSDMamba, the first\nMamba-based architecture specifically designed for oil spill detection.\nOSDMamba leverages Mamba's selective scanning mechanism to effectively expand\nthe model's receptive field while preserving critical details. Moreover, we\ndesigned an asymmetric decoder incorporating ConvSSM and deep supervision to\nstrengthen multi-scale feature fusion, thereby enhancing the model's\nsensitivity to minority class samples. Experimental results show that the\nproposed OSDMamba achieves state-of-the-art performance, yielding improvements\nof 8.9% and 11.8% in OSD across two publicly available datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18023", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18023", "abs": "https://arxiv.org/abs/2506.18023", "authors": ["Kui Huang", "Xinrong Chen", "Wenyu Lv", "Jincheng Liao", "Guanzhong Wang", "Yi Liu"], "title": "PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding", "comment": null, "summary": "This report introduces PP-DocBee2, an advanced version of the PP-DocBee,\ndesigned to enhance multimodal document understanding. Built on a large\nmultimodal model architecture, PP-DocBee2 addresses the limitations of its\npredecessor through key technological improvements, including enhanced\nsynthetic data quality, improved visual feature fusion strategy, and optimized\ninference methodologies. These enhancements yield an $11.4\\%$ performance boost\non internal benchmarks for Chinese business documents, and reduce inference\nlatency by $73.0\\%$ to the vanilla version. A key innovation of our work is a\ndata quality optimization strategy for multimodal document tasks. By employing\na large-scale multimodal pre-trained model to evaluate data, we apply a novel\nstatistical criterion to filter outliers, ensuring high-quality training data.\nInspired by insights into underutilized intermediate features in multimodal\nmodels, we enhance the ViT representational capacity by decomposing it into\nlayers and applying a novel feature fusion strategy to improve complex\nreasoning. The source code and pre-trained model are available at\n\\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18582", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18582", "abs": "https://arxiv.org/abs/2506.18582", "authors": ["Haoyi Wu", "Zhihao Teng", "Kewei Tu"], "title": "Parallel Continuous Chain-of-Thought with Jacobi Iteration", "comment": "under review", "summary": "Continuous chain-of-thought has been shown to be effective in saving\nreasoning tokens for large language models. By reasoning with continuous latent\nthought tokens, continuous CoT is able to perform implicit reasoning in a\ncompact manner. However, the sequential dependencies between latent thought\ntokens spoil parallel training, leading to long training time. In this paper,\nwe propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi\niteration on the latent thought tokens, updating them iteratively in parallel\ninstead of sequentially and thus improving both training and inference\nefficiency of continuous CoT. Experiments demonstrate that by choosing the\nproper number of iterations, we are able to achieve comparable or even better\nperformance while saving nearly 50% of the training and inference time.\nMoreover, PCCoT shows better stability and robustness in the training process.\nOur code is available at https://github.com/whyNLP/PCCoT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18028", "abs": "https://arxiv.org/abs/2506.18028", "authors": ["Junjian Li", "Hulin Kuang", "Jin Liu", "Hailin Yue", "Mengshen He", "Jianxin Wang"], "title": "MiCo: Multiple Instance Learning with Context-Aware Clustering for Whole Slide Image Analysis", "comment": "MICCAI 2025", "summary": "Multiple instance learning (MIL) has shown significant promise in\nhistopathology whole slide image (WSI) analysis for cancer diagnosis and\nprognosis. However, the inherent spatial heterogeneity of WSIs presents\ncritical challenges, as morphologically similar tissue types are often\ndispersed across distant anatomical regions. Conventional MIL methods struggle\nto model these scattered tissue distributions and capture cross-regional\nspatial interactions effectively. To address these limitations, we propose a\nnovel Multiple instance learning framework with Context-Aware Clustering\n(MiCo), designed to enhance cross-regional intra-tissue correlations and\nstrengthen inter-tissue semantic associations in WSIs. MiCo begins by\nclustering instances to distill discriminative morphological patterns, with\ncluster centroids serving as semantic anchors. To enhance cross-regional\nintra-tissue correlations, MiCo employs a Cluster Route module, which\ndynamically links instances of the same tissue type across distant regions via\nfeature similarity. These semantic anchors act as contextual hubs, propagating\nsemantic relationships to refine instance-level representations. To eliminate\nsemantic fragmentation and strengthen inter-tissue semantic associations, MiCo\nintegrates a Cluster Reducer module, which consolidates redundant anchors while\nenhancing information exchange between distinct semantic groups. Extensive\nexperiments on two challenging tasks across nine large-scale public cancer\ndatasets demonstrate the effectiveness of MiCo, showcasing its superiority over\nstate-of-the-art methods. The code is available at\nhttps://github.com/junjianli106/MiCo.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17707", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.17707", "abs": "https://arxiv.org/abs/2506.17707", "authors": ["Jihyun Kim", "Junho Park", "Kyeongbo Kong", "Suk-Ju Kang"], "title": "Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models", "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "We present Programmable-Room, a framework which interactively generates and\nedits a 3D room mesh, given natural language instructions. For precise control\nof a room's each attribute, we decompose the challenging task into simpler\nsteps such as creating plausible 3D coordinates for room meshes, generating\npanorama images for the texture, constructing 3D meshes by integrating the\ncoordinates and panorama texture images, and arranging furniture. To support\nthe various decomposed tasks with a unified framework, we incorporate visual\nprogramming (VP). VP is a method that utilizes a large language model (LLM) to\nwrite a Python-like program which is an ordered list of necessary modules for\nthe various tasks given in natural language. We develop most of the modules.\nEspecially, for the texture generating module, we utilize a pretrained\nlarge-scale diffusion model to generate panorama images conditioned on text and\nvisual prompts (i.e., layout, depth, and semantic map) simultaneously.\nSpecifically, we enhance the panorama image generation quality by optimizing\nthe training objective with a 1D representation of a panorama scene obtained\nfrom bidirectional LSTM. We demonstrate Programmable-Room's flexibility in\ngenerating and editing 3D room meshes, and prove our framework's superiority to\nan existing model quantitatively and qualitatively. Project page is available\nin https://jihyun0510.github.io/Programmable_Room_Page/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18071", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18071", "abs": "https://arxiv.org/abs/2506.18071", "authors": ["Jisheng Dang", "Huilin Song", "Junbin Xiao", "Bimei Wang", "Han Peng", "Haoxuan Li", "Xun Yang", "Meng Wang", "Tat-Seng Chua"], "title": "MUPA: Towards Multi-Path Agentic Reasoning for Grounded Video Question Answering", "comment": null, "summary": "Grounded Video Question Answering (Grounded VideoQA) requires aligning\ntextual answers with explicit visual evidence. However, modern multimodal\nmodels often rely on linguistic priors and spurious correlations, resulting in\npoorly grounded predictions. In this work, we propose MUPA, a cooperative\nMUlti-Path Agentic approach that unifies video grounding, question answering,\nanswer reflection and aggregation to tackle Grounded VideoQA. MUPA features\nthree distinct reasoning paths on the interplay of grounding and QA agents in\ndifferent chronological orders, along with a dedicated reflection agent to\njudge and aggregate the multi-path results to accomplish consistent QA and\ngrounding. This design markedly improves grounding fidelity without sacrificing\nanswer accuracy. Despite using only 2B parameters, our method outperforms all\n7B-scale competitors. When scaled to 7B parameters, MUPA establishes new\nstate-of-the-art results, with Acc@GQA of 30.3% and 47.4% on NExT-GQA and\nDeVE-QA respectively, demonstrating MUPA' effectiveness towards trustworthy\nvideo-language understanding. Our code is available in\nhttps://github.com/longmalongma/MUPA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18134", "abs": "https://arxiv.org/abs/2506.18134", "authors": ["Quan Zhou", "Gan Luo", "Qiang Hu", "Qingyong Zhang", "Jinhua Zhang", "Yinjiao Tian", "Qiang Li", "Zhiwei Wang"], "title": "Targeted False Positive Synthesis via Detector-guided Adversarial Diffusion Attacker for Robust Polyp Detection", "comment": "Early Accepted by MICCAI 2025", "summary": "Polyp detection is crucial for colorectal cancer screening, yet existing\nmodels are limited by the scale and diversity of available data. While\ngenerative models show promise for data augmentation, current methods mainly\nfocus on enhancing polyp diversity, often overlooking the critical issue of\nfalse positives. In this paper, we address this gap by proposing an adversarial\ndiffusion framework to synthesize high-value false positives. The extensive\nvariability of negative backgrounds presents a significant challenge in false\npositive synthesis. To overcome this, we introduce two key innovations: First,\nwe design a regional noise matching strategy to construct a negative synthesis\nspace using polyp detection datasets. This strategy trains a negative-centric\ndiffusion model by masking polyp regions, ensuring the model focuses\nexclusively on learning diverse background patterns. Second, we introduce the\nDetector-guided Adversarial Diffusion Attacker (DADA) module, which perturbs\nthe negative synthesis process to disrupt a pre-trained detector's decision,\nguiding the negative-centric diffusion model to generate high-value,\ndetector-confusing false positives instead of low-value, ordinary backgrounds.\nOur approach is the first to apply adversarial diffusion to lesion detection,\nestablishing a new paradigm for targeted false positive synthesis and paving\nthe way for more reliable clinical applications in colorectal cancer screening.\nExtensive results on public and in-house datasets verify the superiority of our\nmethod over the current state-of-the-arts, with our synthesized data improving\nthe detectors by at least 2.6% and 2.7% in F1-score, respectively, over the\nbaselines. Codes are at https://github.com/Huster-Hq/DADA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18173", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18173", "abs": "https://arxiv.org/abs/2506.18173", "authors": ["Sabbir Ahmed", "Md. Bakhtiar Hasan", "Tasnim Ahmed", "Md. Hasanul Kabir"], "title": "DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data", "comment": "Submitted to ACPR Springer, 15 pages, 1 Figure, 7 Tables, and lots of\n  efforts :)", "summary": "While deep learning-based architectures have been widely used for correctly\ndetecting and classifying plant diseases, they require large-scale datasets to\nlearn generalized features and achieve state-of-the-art performance. This poses\na challenge for such models to obtain satisfactory performance in classifying\nleaf diseases with limited samples. This work proposes a few-shot learning\nframework, Domain-adapted Expert Network (DExNet), for plant disease\nclassification that compensates for the lack of sufficient training data by\ncombining observations of a number of expert critics. It starts with extracting\nthe feature embeddings as 'observations' from nine 'critics' that are\nstate-of-the-art pre-trained CNN-based architectures. These critics are 'domain\nadapted' using a publicly available leaf disease dataset having no overlapping\nclasses with the specific downstream task of interest. The observations are\nthen passed to the 'Feature Fusion Block' and finally to a classifier network\nconsisting of Bi-LSTM layers. The proposed pipeline is evaluated on the 10\nclasses of tomato leaf images from the PlantVillage dataset, achieving\npromising accuracies of 89.06%, 92.46%, and 94.07%, respectively, for 5-shot,\n10-shot, and 15-shot classification. Furthermore, an accuracy of 98.09+-0.7%\nhas been achieved in 80-shot classification, which is only 1.2% less than\nstate-of-the-art, allowing a 94.5% reduction in the training data requirement.\nThe proposed pipeline also outperforms existing works on leaf disease\nclassification with limited data in both laboratory and real-life conditions in\nsingle-domain, mixed-domain, and cross-domain scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18204", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18204", "abs": "https://arxiv.org/abs/2506.18204", "authors": ["Youjie Zhou", "Guofeng Mei", "Yiming Wang", "Yi Wan", "Fabio Poiesi"], "title": "Multimodal Fusion SLAM with Fourier Attention", "comment": null, "summary": "Visual SLAM is particularly challenging in environments affected by noise,\nvarying lighting conditions, and darkness. Learning-based optical flow\nalgorithms can leverage multiple modalities to address these challenges, but\ntraditional optical flow-based visual SLAM approaches often require significant\ncomputational resources.To overcome this limitation, we propose FMF-SLAM, an\nefficient multimodal fusion SLAM method that utilizes fast Fourier transform\n(FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel\nFourier-based self-attention and cross-attention mechanism to extract features\nfrom RGB and depth signals. We further enhance the interaction of multimodal\nfeatures by incorporating multi-scale knowledge distillation across modalities.\nWe also demonstrate the practical feasibility of FMF-SLAM in real-world\nscenarios with real time performance by integrating it with a security robot by\nfusing with a global positioning module GNSS-RTK and global Bundle Adjustment.\nOur approach is validated using video sequences from TUM, TartanAir, and our\nreal-world datasets, showcasing state-of-the-art performance under noisy,\nvarying lighting, and dark conditions.Our code and datasets are available at\nhttps://github.com/youjie-zhou/FMF-SLAM.git.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18880", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18880", "abs": "https://arxiv.org/abs/2506.18880", "authors": ["Yiyou Sun", "Shawn Hu", "Georgia Zhou", "Ken Zheng", "Hannaneh Hajishirzi", "Nouha Dziri", "Dawn Song"], "title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization", "comment": null, "summary": "Recent large-scale language models (LLMs) with long Chain-of-Thought\nreasoning-such as DeepSeek-R1-have achieved impressive results on\nOlympiad-level mathematics benchmarks. However, they often rely on a narrow set\nof strategies and struggle with problems that require a novel way of thinking.\nTo systematically investigate these limitations, we introduce\nOMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a\ncontrolled yet diverse benchmark designed to evaluate three axes of\nout-of-distribution generalization, inspired by Boden's typology of creativity:\n(1) Exploratory-applying known problem solving skills to more complex instances\nwithin the same problem domain; (2) Compositional-combining distinct reasoning\nskills, previously learned in isolation, to solve novel problems that require\nintegrating these skills in new and coherent ways; and (3)\nTransformative-adopting novel, often unconventional strategies by moving beyond\nfamiliar approaches to solve problems more effectively. OMEGA consists of\nprogrammatically generated training-test pairs derived from templated problem\ngenerators across geometry, number theory, algebra, combinatorics, logic, and\npuzzles, with solutions verified using symbolic, numerical, or graphical\nmethods. We evaluate frontier (or top-tier) LLMs and observe sharp performance\ndegradation as problem complexity increases. Moreover, we fine-tune the\nQwen-series models across all generalization settings and observe notable\nimprovements in exploratory generalization, while compositional generalization\nremains limited and transformative reasoning shows little to no improvement. By\nisolating and quantifying these fine-grained failures, OMEGA lays the\ngroundwork for advancing LLMs toward genuine mathematical creativity beyond\nmechanical proficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18208", "abs": "https://arxiv.org/abs/2506.18208", "authors": ["Ankit Sanjyal"], "title": "Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction", "comment": "5 pages, 1 table, 2 figures. First submission. Code available at:\n  \\url{https://github.com/ANKITSANJYAL/nerf-few-shot-limitations}", "summary": "Neural Radiance Fields (NeRF) have revolutionized 3D scene reconstruction\nfrom sparse image collections. Recent work has explored integrating pre-trained\nvision features, particularly from DINO, to enhance few-shot reconstruction\ncapabilities. However, the effectiveness of such approaches remains unclear,\nespecially in extreme few-shot scenarios. In this paper, we present a\nsystematic evaluation of DINO-enhanced NeRF models, comparing baseline NeRF,\nfrozen DINO features, LoRA fine-tuned features, and multi-scale feature fusion.\nSurprisingly, our experiments reveal that all DINO variants perform worse than\nthe baseline NeRF, achieving PSNR values around 12.9 to 13.0 compared to the\nbaseline's 14.71. This counterintuitive result suggests that pre-trained vision\nfeatures may not be beneficial for few-shot 3D reconstruction and may even\nintroduce harmful biases. We analyze potential causes including feature-task\nmismatch, overfitting to limited data, and integration challenges. Our findings\nchallenge common assumptions in the field and suggest that simpler\narchitectures focusing on geometric consistency may be more effective for\nfew-shot scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17931", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17931", "abs": "https://arxiv.org/abs/2506.17931", "authors": ["Ravi Kant Gupta", "Shounak Das", "Amit Sethi"], "title": "IDAL: Improved Domain Adaptive Learning for Natural Images Dataset", "comment": "Accepted in ICPR'24 (International Conference on Pattern Recognition)", "summary": "We present a novel approach for unsupervised domain adaptation (UDA) for\nnatural images. A commonly-used objective for UDA schemes is to enhance domain\nalignment in representation space even if there is a domain shift in the input\nspace. Existing adversarial domain adaptation methods may not effectively align\ndifferent domains of multimodal distributions associated with classification\nproblems. Our approach has two main features. Firstly, its neural architecture\nuses the deep structure of ResNet and the effective separation of scales of\nfeature pyramidal network (FPN) to work with both content and style features.\nSecondly, it uses a combination of a novel loss function and judiciously\nselected existing loss functions to train the network architecture. This\ntailored combination is designed to address challenges inherent to natural\nimages, such as scale, noise, and style shifts, that occur on top of a\nmulti-modal (multi-class) distribution. The combined loss function not only\nenhances model accuracy and robustness on the target domain but also speeds up\ntraining convergence. Our proposed UDA scheme generalizes better than\nstate-of-the-art for CNN-based methods on Office-Home, Office-31, and\nVisDA-2017 datasets and comaparable for DomainNet dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.17788", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "I.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.17788", "abs": "https://arxiv.org/abs/2506.17788", "authors": ["Shahab Rahimirad", "Guven Gergerli", "Lucia Romero", "Angela Qian", "Matthew Lyle Olson", "Simon Stepputtis", "Joseph Campbell"], "title": "Bayesian Social Deduction with Graph-Informed Language Models", "comment": "32 pages, 10 figures. Under review", "summary": "Social reasoning - inferring unobservable beliefs and intentions from partial\nobservations of other agents - remains a challenging task for large language\nmodels (LLMs). We evaluate the limits of current reasoning language models in\nthe social deduction game Avalon and find that while the largest models\ndemonstrate strong performance, they require extensive test-time inference and\ndegrade sharply when distilled to smaller, real-time-capable variants. To\naddress this, we introduce a hybrid reasoning framework that externalizes\nbelief inference to a structured probabilistic model, while using an LLM for\nlanguage understanding and interaction. Our approach achieves competitive\nperformance with much larger models in Agent-Agent play and, notably, is the\nfirst language agent to defeat human players in a controlled study - achieving\na 67% win rate and receiving higher qualitative ratings than both reasoning\nbaselines and human teammates. We release code, models, and a dataset to\nsupport future work on social reasoning in LLM agents, which can be found at\nhttps://camp-lab-purdue.github.io/bayesian-social-deduction/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18023", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18023", "abs": "https://arxiv.org/abs/2506.18023", "authors": ["Kui Huang", "Xinrong Chen", "Wenyu Lv", "Jincheng Liao", "Guanzhong Wang", "Yi Liu"], "title": "PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding", "comment": null, "summary": "This report introduces PP-DocBee2, an advanced version of the PP-DocBee,\ndesigned to enhance multimodal document understanding. Built on a large\nmultimodal model architecture, PP-DocBee2 addresses the limitations of its\npredecessor through key technological improvements, including enhanced\nsynthetic data quality, improved visual feature fusion strategy, and optimized\ninference methodologies. These enhancements yield an $11.4\\%$ performance boost\non internal benchmarks for Chinese business documents, and reduce inference\nlatency by $73.0\\%$ to the vanilla version. A key innovation of our work is a\ndata quality optimization strategy for multimodal document tasks. By employing\na large-scale multimodal pre-trained model to evaluate data, we apply a novel\nstatistical criterion to filter outliers, ensuring high-quality training data.\nInspired by insights into underutilized intermediate features in multimodal\nmodels, we enhance the ViT representational capacity by decomposing it into\nlayers and applying a novel feature fusion strategy to improve complex\nreasoning. The source code and pre-trained model are available at\n\\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18234", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18234", "abs": "https://arxiv.org/abs/2506.18234", "authors": ["Yue Li", "Meng Tian", "Dechang Zhu", "Jiangtong Zhu", "Zhenyu Lin", "Zhiwei Xiong", "Xinhai Zhao"], "title": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning", "comment": null, "summary": "Large vision-language models (VLMs) for autonomous driving (AD) are evolving\nbeyond perception and cognition tasks toward motion planning. However, we\nidentify two critical challenges in this direction: (1) VLMs tend to learn\nshortcuts by relying heavily on history input information, achieving seemingly\nstrong planning results without genuinely understanding the visual inputs; and\n(2) the chain-ofthought (COT) reasoning processes are always misaligned with\nthe motion planning outcomes, and how to effectively leverage the complex\nreasoning capability to enhance planning remains largely underexplored. In this\npaper, we start from a small-scale domain-specific VLM and propose Drive-R1\ndesigned to bridges the scenario reasoning and motion planning for AD. Drive-R1\nfirst undergoes the supervised finetuning on a elaborate dataset containing\nboth long and short COT data. Drive-R1 is encouraged to reason step-by-step\nfrom visual input to final planning decisions. Subsequently, Drive-R1 is\ntrained within a reinforcement learning framework that incentivizes the\ndiscovery of reasoning paths that are more informative for planning, guided by\nrewards based on predicted trajectories and meta actions. Experimental\nevaluations on the nuScenes and DriveLM-nuScenes benchmarks demonstrate that\nDrive-R1 achieves superior performance compared to existing state-of-the-art\nVLMs. We believe that Drive-R1 presents a promising direction for bridging\nreasoning and planning in AD, offering methodological insights for future\nresearch and applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18246", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18246", "abs": "https://arxiv.org/abs/2506.18246", "authors": ["Xiangzhao Hao", "Kuan Zhu", "Hongyu Guo", "Haiyun Guo", "Ming Tang", "JinQiao Wang"], "title": "Referring Expression Instance Retrieval and A Strong End-to-End Baseline", "comment": null, "summary": "Natural language querying of visual content underpins many vision-language\ntasks, typically categorized by text granularity and visual search scope.\nText-Image Retrieval (TIR) retrieves whole images using coarse descriptions,\nwhile Referring Expression Comprehension (REC) localizes objects using\nfine-grained expressions within a single image. However, real-world scenarios\noften require both instance-level retrieval and localization across large\ngalleries -- tasks where TIR lacks precision and REC lacks scalability. To\naddress this gap, we propose a new task: Referring Expression Instance\nRetrieval (REIR), which jointly supports instance-level retrieval and\nlocalization. We introduce REIRCOCO, a large-scale benchmark constructed by\nprompting vision-language models to generate fine-grained expressions for\nMSCOCO and RefCOCO instances. We also present a baseline method, CLARE,\nfeaturing a dual-stream architecture with a Mix of Relation Experts (MORE)\nmodule for capturing inter-instance relationships. CLARE integrates object\ndetection and REC pretraining with Contrastive Language-Instance Alignment\n(CLIA) for end-to-end optimization. Experiments show that CLARE achieves\nstate-of-the-art performance on REIR and generalizes well to TIR and REC,\nhighlighting its effectiveness and versatility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18023", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18023", "abs": "https://arxiv.org/abs/2506.18023", "authors": ["Kui Huang", "Xinrong Chen", "Wenyu Lv", "Jincheng Liao", "Guanzhong Wang", "Yi Liu"], "title": "PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding", "comment": null, "summary": "This report introduces PP-DocBee2, an advanced version of the PP-DocBee,\ndesigned to enhance multimodal document understanding. Built on a large\nmultimodal model architecture, PP-DocBee2 addresses the limitations of its\npredecessor through key technological improvements, including enhanced\nsynthetic data quality, improved visual feature fusion strategy, and optimized\ninference methodologies. These enhancements yield an $11.4\\%$ performance boost\non internal benchmarks for Chinese business documents, and reduce inference\nlatency by $73.0\\%$ to the vanilla version. A key innovation of our work is a\ndata quality optimization strategy for multimodal document tasks. By employing\na large-scale multimodal pre-trained model to evaluate data, we apply a novel\nstatistical criterion to filter outliers, ensuring high-quality training data.\nInspired by insights into underutilized intermediate features in multimodal\nmodels, we enhance the ViT representational capacity by decomposing it into\nlayers and applying a novel feature fusion strategy to improve complex\nreasoning. The source code and pre-trained model are available at\n\\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18071", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18071", "abs": "https://arxiv.org/abs/2506.18071", "authors": ["Jisheng Dang", "Huilin Song", "Junbin Xiao", "Bimei Wang", "Han Peng", "Haoxuan Li", "Xun Yang", "Meng Wang", "Tat-Seng Chua"], "title": "MUPA: Towards Multi-Path Agentic Reasoning for Grounded Video Question Answering", "comment": null, "summary": "Grounded Video Question Answering (Grounded VideoQA) requires aligning\ntextual answers with explicit visual evidence. However, modern multimodal\nmodels often rely on linguistic priors and spurious correlations, resulting in\npoorly grounded predictions. In this work, we propose MUPA, a cooperative\nMUlti-Path Agentic approach that unifies video grounding, question answering,\nanswer reflection and aggregation to tackle Grounded VideoQA. MUPA features\nthree distinct reasoning paths on the interplay of grounding and QA agents in\ndifferent chronological orders, along with a dedicated reflection agent to\njudge and aggregate the multi-path results to accomplish consistent QA and\ngrounding. This design markedly improves grounding fidelity without sacrificing\nanswer accuracy. Despite using only 2B parameters, our method outperforms all\n7B-scale competitors. When scaled to 7B parameters, MUPA establishes new\nstate-of-the-art results, with Acc@GQA of 30.3% and 47.4% on NExT-GQA and\nDeVE-QA respectively, demonstrating MUPA' effectiveness towards trustworthy\nvideo-language understanding. Our code is available in\nhttps://github.com/longmalongma/MUPA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18248", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18248", "abs": "https://arxiv.org/abs/2506.18248", "authors": ["Jongoh Jeong", "Hunmin Yang", "Jaeseok Jeong", "Kuk-Jin Yoon"], "title": "Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability", "comment": null, "summary": "Generative adversarial attacks train a perturbation generator on a white-box\nsurrogate model and subsequently apply the crafted perturbations to unseen\nblack-box victim models. In contrast to iterative attacks, these methods\ndeliver superior inference-time efficiency, scalability, and transferability;\nhowever, up until now, existing studies have not fully exploited the\nrepresentational capacity of generative models to preserve and harness semantic\ninformation. Specifically, the intermediate activations of the generator encode\nrich semantic features--object boundaries and coarse shapes--that remain\nunder-exploited, thereby limiting the alignment of perturbations with\nobject-salient regions which are critical for adversarial transferability. To\nremedy this, we introduce a semantic structure-aware attack framework based on\nthe Mean Teacher, which serves as a temporally smoothed feature reference. With\nthis smoothed reference, we further direct semantic consistency between the\nearly-layer activations in the student and those of the semantically rich\nteacher by feature distillation. By anchoring perturbation synthesis to the\nsemantically salient early intermediate blocks within the generator based on\nempirical findings, our method guides progressive adversarial perturbation on\nregions that substantially enhance adversarial transferability. We conduct\nextensive experiments over diverse models, domains and tasks to demonstrate\nconsistent improvements relative to state-of-the-art generative attacks,\ncomprehensively evaluated using conventional metrics and our newly proposed\nAccidental Correction Rate (ACR).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18261", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18261", "abs": "https://arxiv.org/abs/2506.18261", "authors": ["Rui Su", "Dong Xu", "Luping Zhou", "Wanli Ouyang"], "title": "Improving Weakly Supervised Temporal Action Localization by Exploiting Multi-resolution Information in Temporal Domain", "comment": "13 pages", "summary": "Weakly supervised temporal action localization is a challenging task as only\nthe video-level annotation is available during the training process. To address\nthis problem, we propose a two-stage approach to fully exploit multi-resolution\ninformation in the temporal domain and generate high quality frame-level pseudo\nlabels based on both appearance and motion streams. Specifically, in the first\nstage, we generate reliable initial frame-level pseudo labels, and in the\nsecond stage, we iteratively refine the pseudo labels and use a set of selected\nframes with highly confident pseudo labels to train neural networks and better\npredict action class scores at each frame. We fully exploit temporal\ninformation at multiple scales to improve temporal action localization\nperformance. Specifically, in order to obtain reliable initial frame-level\npseudo labels, in the first stage, we propose an Initial Label Generation (ILG)\nmodule, which leverages temporal multi-resolution consistency to generate high\nquality class activation sequences (CASs), which consist of a number of\nsequences with each sequence measuring how likely each video frame belongs to\none specific action class. In the second stage, we propose a Progressive\nTemporal Label Refinement (PTLR) framework. In our PTLR framework, two networks\ncalled Network-OTS and Network-RTS, which are respectively used to generate\nCASs for the original temporal scale and the reduced temporal scales, are used\nas two streams (i.e., the OTS stream and the RTS stream) to refine the pseudo\nlabels in turn. By this way, the multi-resolution information in the temporal\ndomain is exchanged at the pseudo label level, and our work can help improve\neach stream (i.e., the OTS/RTS stream) by exploiting the refined pseudo labels\nfrom another stream (i.e., the RTS/OTS stream).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18183", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18183", "abs": "https://arxiv.org/abs/2506.18183", "authors": ["Zhiting Mei", "Christina Zhang", "Tenny Yin", "Justin Lidard", "Ola Shorinwa", "Anirudha Majumdar"], "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?", "comment": null, "summary": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18266", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18266", "abs": "https://arxiv.org/abs/2506.18266", "authors": ["Haoming Chen", "Lichen Yuan", "TianFang Sun", "Jingyu Gong", "Xin Tan", "Zhizhong Zhang", "Yuan Xie"], "title": "YouTube-Occ: Learning Indoor 3D Semantic Occupancy Prediction from YouTube Videos", "comment": null, "summary": "3D semantic occupancy prediction in the past was considered to require\nprecise geometric relationships in order to enable effective training. However,\nin complex indoor environments, the large-scale and widespread collection of\ndata, along with the necessity for fine-grained annotations, becomes\nimpractical due to the complexity of data acquisition setups and privacy\nconcerns. In this paper, we demonstrate that 3D spatially-accurate training can\nbe achieved using only indoor Internet data, without the need for any\npre-knowledge of intrinsic or extrinsic camera parameters. In our framework, we\ncollect a web dataset, YouTube-Occ, which comprises house tour videos from\nYouTube, providing abundant real house scenes for 3D representation learning.\nUpon on this web dataset, we establish a fully self-supervised model to\nleverage accessible 2D prior knowledge for reaching powerful 3D indoor\nperception. Specifically, we harness the advantages of the prosperous vision\nfoundation models, distilling the 2D region-level knowledge into the occupancy\nnetwork by grouping the similar pixels into superpixels. Experimental results\nshow that our method achieves state-of-the-art zero-shot performance on two\npopular benchmarks (NYUv2 and OccScanNet", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18268", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18268", "abs": "https://arxiv.org/abs/2506.18268", "authors": ["Yu Liu", "Yangtao Meng", "Xianfei Pan", "Jie Jiang", "Changhao Chen"], "title": "ThermalLoc: A Vision Transformer-Based Approach for Robust Thermal Camera Relocalization in Large-Scale Environments", "comment": "8 pages, 3 figures, accepted to IROS 2025", "summary": "Thermal cameras capture environmental data through heat emission, a\nfundamentally different mechanism compared to visible light cameras, which rely\non pinhole imaging. As a result, traditional visual relocalization methods\ndesigned for visible light images are not directly applicable to thermal\nimages. Despite significant advancements in deep learning for camera\nrelocalization, approaches specifically tailored for thermal camera-based\nrelocalization remain underexplored. To address this gap, we introduce\nThermalLoc, a novel end-to-end deep learning method for thermal image\nrelocalization. ThermalLoc effectively extracts both local and global features\nfrom thermal images by integrating EfficientNet with Transformers, and performs\nabsolute pose regression using two MLP networks. We evaluated ThermalLoc on\nboth the publicly available thermal-odometry dataset and our own dataset. The\nresults demonstrate that ThermalLoc outperforms existing representative methods\nemployed for thermal camera relocalization, including AtLoc, MapNet, PoseNet,\nand RobustLoc, achieving superior accuracy and robustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18898", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.18898", "abs": "https://arxiv.org/abs/2506.18898", "authors": ["Jiaming Han", "Hao Chen", "Yang Zhao", "Hanyu Wang", "Qi Zhao", "Ziyan Yang", "Hao He", "Xiangyu Yue", "Lu Jiang"], "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations", "comment": "Project page: https://tar.csuhan.com", "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18204", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18204", "abs": "https://arxiv.org/abs/2506.18204", "authors": ["Youjie Zhou", "Guofeng Mei", "Yiming Wang", "Yi Wan", "Fabio Poiesi"], "title": "Multimodal Fusion SLAM with Fourier Attention", "comment": null, "summary": "Visual SLAM is particularly challenging in environments affected by noise,\nvarying lighting conditions, and darkness. Learning-based optical flow\nalgorithms can leverage multiple modalities to address these challenges, but\ntraditional optical flow-based visual SLAM approaches often require significant\ncomputational resources.To overcome this limitation, we propose FMF-SLAM, an\nefficient multimodal fusion SLAM method that utilizes fast Fourier transform\n(FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel\nFourier-based self-attention and cross-attention mechanism to extract features\nfrom RGB and depth signals. We further enhance the interaction of multimodal\nfeatures by incorporating multi-scale knowledge distillation across modalities.\nWe also demonstrate the practical feasibility of FMF-SLAM in real-world\nscenarios with real time performance by integrating it with a security robot by\nfusing with a global positioning module GNSS-RTK and global Bundle Adjustment.\nOur approach is validated using video sequences from TUM, TartanAir, and our\nreal-world datasets, showcasing state-of-the-art performance under noisy,\nvarying lighting, and dark conditions.Our code and datasets are available at\nhttps://github.com/youjie-zhou/FMF-SLAM.git.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18323", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18323", "abs": "https://arxiv.org/abs/2506.18323", "authors": ["Muhammad Azeem Aslam", "Hassan Khalid", "Nisar Ahmed"], "title": "A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for Low-Light Image Enhancement", "comment": null, "summary": "Low-light image enhancement remains a challenging task, particularly in the\nabsence of paired training data. In this study, we present LucentVisionNet, a\nnovel zero-shot learning framework that addresses the limitations of\ntraditional and deep learning-based enhancement methods. The proposed approach\nintegrates multi-scale spatial attention with a deep curve estimation network,\nenabling fine-grained enhancement while preserving semantic and perceptual\nfidelity. To further improve generalization, we adopt a recurrent enhancement\nstrategy and optimize the model using a composite loss function comprising six\ntailored components, including a novel no-reference image quality loss inspired\nby human visual perception. Extensive experiments on both paired and unpaired\nbenchmark datasets demonstrate that LucentVisionNet consistently outperforms\nstate-of-the-art supervised, unsupervised, and zero-shot methods across\nmultiple full-reference and no-reference image quality metrics. Our framework\nachieves high visual quality, structural consistency, and computational\nefficiency, making it well-suited for deployment in real-world applications\nsuch as mobile photography, surveillance, and autonomous navigation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18248", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18248", "abs": "https://arxiv.org/abs/2506.18248", "authors": ["Jongoh Jeong", "Hunmin Yang", "Jaeseok Jeong", "Kuk-Jin Yoon"], "title": "Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability", "comment": null, "summary": "Generative adversarial attacks train a perturbation generator on a white-box\nsurrogate model and subsequently apply the crafted perturbations to unseen\nblack-box victim models. In contrast to iterative attacks, these methods\ndeliver superior inference-time efficiency, scalability, and transferability;\nhowever, up until now, existing studies have not fully exploited the\nrepresentational capacity of generative models to preserve and harness semantic\ninformation. Specifically, the intermediate activations of the generator encode\nrich semantic features--object boundaries and coarse shapes--that remain\nunder-exploited, thereby limiting the alignment of perturbations with\nobject-salient regions which are critical for adversarial transferability. To\nremedy this, we introduce a semantic structure-aware attack framework based on\nthe Mean Teacher, which serves as a temporally smoothed feature reference. With\nthis smoothed reference, we further direct semantic consistency between the\nearly-layer activations in the student and those of the semantically rich\nteacher by feature distillation. By anchoring perturbation synthesis to the\nsemantically salient early intermediate blocks within the generator based on\nempirical findings, our method guides progressive adversarial perturbation on\nregions that substantially enhance adversarial transferability. We conduct\nextensive experiments over diverse models, domains and tasks to demonstrate\nconsistent improvements relative to state-of-the-art generative attacks,\ncomprehensively evaluated using conventional metrics and our newly proposed\nAccidental Correction Rate (ACR).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18335", "abs": "https://arxiv.org/abs/2506.18335", "authors": ["Saad Wazir", "Daeyoung Kim"], "title": "Rethinking Decoder Design: Improving Biomarker Segmentation Using Depth-to-Space Restoration and Residual Linear Attention", "comment": "Proceedings of the Computer Vision and Pattern Recognition Conference\n  (CVPR), 2025, pp. 30861-30871", "summary": "Segmenting biomarkers in medical images is crucial for various biotech\napplications. Despite advances, Transformer and CNN based methods often\nstruggle with variations in staining and morphology, limiting feature\nextraction. In medical image segmentation, where datasets often have limited\nsample availability, recent state-of-the-art (SOTA) methods achieve higher\naccuracy by leveraging pre-trained encoders, whereas end-to-end methods tend to\nunderperform. This is due to challenges in effectively transferring rich\nmultiscale features from encoders to decoders, as well as limitations in\ndecoder efficiency. To address these issues, we propose an architecture that\ncaptures multi-scale local and global contextual information and a novel\ndecoder design, which effectively integrates features from the encoder,\nemphasizes important channels and regions, and reconstructs spatial dimensions\nto enhance segmentation accuracy. Our method, compatible with various encoders,\noutperforms SOTA methods, as demonstrated by experiments on four datasets and\nablation studies. Specifically, our method achieves absolute performance gains\nof 2.76% on MoNuSeg, 3.12% on DSB, 2.87% on Electron Microscopy, and 4.03% on\nTNBC datasets compared to existing SOTA methods. Code:\nhttps://github.com/saadwazir/MCADS-Decoder", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18323", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18323", "abs": "https://arxiv.org/abs/2506.18323", "authors": ["Muhammad Azeem Aslam", "Hassan Khalid", "Nisar Ahmed"], "title": "A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for Low-Light Image Enhancement", "comment": null, "summary": "Low-light image enhancement remains a challenging task, particularly in the\nabsence of paired training data. In this study, we present LucentVisionNet, a\nnovel zero-shot learning framework that addresses the limitations of\ntraditional and deep learning-based enhancement methods. The proposed approach\nintegrates multi-scale spatial attention with a deep curve estimation network,\nenabling fine-grained enhancement while preserving semantic and perceptual\nfidelity. To further improve generalization, we adopt a recurrent enhancement\nstrategy and optimize the model using a composite loss function comprising six\ntailored components, including a novel no-reference image quality loss inspired\nby human visual perception. Extensive experiments on both paired and unpaired\nbenchmark datasets demonstrate that LucentVisionNet consistently outperforms\nstate-of-the-art supervised, unsupervised, and zero-shot methods across\nmultiple full-reference and no-reference image quality metrics. Our framework\nachieves high visual quality, structural consistency, and computational\nefficiency, making it well-suited for deployment in real-world applications\nsuch as mobile photography, surveillance, and autonomous navigation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18369", "abs": "https://arxiv.org/abs/2506.18369", "authors": ["Yeongtak Oh", "Jisoo Mok", "Dohyun Chung", "Juhyeon Shin", "Sangha Park", "Johan Barthelemy", "Sungroh Yoon"], "title": "RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models", "comment": "Project Page: https://github.com/oyt9306/RePIC", "summary": "Recent multi-modal large language models (MLLMs) often struggle to generate\npersonalized image captions, even when trained on high-quality captions. In\nthis work, we observe that such limitations persist in existing\npost-training-based MLLM personalization methods. Specifically, despite being\npost-tuned with large-scale caption data through supervised fine-tuning (SFT),\nthese models frequently fail to produce faithful descriptions in real-world\nscenarios, such as multi-concept image captioning. However, acquiring\nlarge-scale, high-quality captions for such complex settings is both costly and\ndifficult. To address the data-centric nature of SFT, we propose a\nreinforcement learning (RL)-based post-training framework. To the best of our\nknowledge, this is the first RL-based approach to post-train MLLMs for\npersonalized image captioning. Our method significantly enhances both visual\nrecognition and personalized generation capabilities of MLLMs, and consistently\noutperforms existing SFT-based baselines, especially in the challenging\nmulti-concept image captioning task.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18372", "abs": "https://arxiv.org/abs/2506.18372", "authors": ["Hieu Nguyen", "Phuc-Tan Nguyen", "Thien-Phuc Tran", "Minh-Quang Nguyen", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "OpenEvents V1: Large-Scale Benchmark Dataset for Multimodal Event Grounding", "comment": null, "summary": "We introduce OpenEvents V1, a large-scale benchmark dataset aimed at\nadvancing event-centric vision-language understanding. Unlike conventional\nimage captioning and retrieval datasets that emphasize surface-level\ndescriptions, OpenEvents V1 focuses on contextual and temporal grounding\nthrough two primary tasks: (1) generating rich, event-aware image captions and\n(2) retrieving event-relevant images based on narrative-style textual queries.\nThe dataset contains over 200,000 news articles and 400,000 associated images\nsourced from CNN and The Guardian, spanning diverse domains and time periods.\nWe provide extensive baseline results and standardized evaluation protocols for\nboth tasks. OpenEvents V1 establishes a robust foundation for developing\nmultimodal models capable of deep reasoning over complex real-world events. The\ndataset is available at https://ltnghia.github.io/eventa/openevents-v1", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18385", "abs": "https://arxiv.org/abs/2506.18385", "authors": ["Nianchen Deng", "Lixin Gu", "Shenglong Ye", "Yinan He", "Zhe Chen", "Songze Li", "Haomin Wang", "Xingguang Wei", "Tianshuo Yang", "Min Dou", "Tong He", "Wenqi Shao", "Kaipeng Zhang", "Yi Wang", "Botian Shi", "Yanting Zhang", "Jifeng Dai", "Yu Qiao", "Hongjie Zhang", "Wenhai Wang"], "title": "InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models", "comment": null, "summary": "Recent benchmarks and datasets have been proposed to improve spatial\nreasoning in vision-language models (VLMs), yet existing open resources remain\nlimited in scale, visual diversity, and instruction expressiveness. In this\nwork, we introduce InternSpatial, the largest open-source dataset for spatial\nreasoning in VLMs, along with InternSpatial-Bench, a corresponding evaluation\nbenchmark designed to assess spatial understanding under diverse instruction\nformats. InternSpatial comprises 12 million QA pairs spanning both single-view\nand multi-view settings, drawn from diverse visual environments and supporting\n19 instruction formats that reflect varied query styles. For evaluation, we\npropose InternSpatial-Bench for single-view tasks and expand multi-view\nreasoning by introducing a novel rotation angle prediction task that has not\nbeen explored in prior work. Experimental results show that models trained on\nInternSpatial achieve 12.1% improvement on InternSpatial-Bench and 10.7% on\nVSI-Bench, while maintaining strong performance on general-purpose benchmarks.\nWe hope these resources will support the development of spatially capable VLMs\nin practical applications such as robotics and embodied AI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18434", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18434", "abs": "https://arxiv.org/abs/2506.18434", "authors": ["Filippo Ruffini", "Elena Mulero Ayllon", "Linlin Shen", "Paolo Soda", "Valerio Guarrasi"], "title": "Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging", "comment": null, "summary": "Artificial Intelligence (AI) holds significant promise for improving\nprognosis prediction in medical imaging, yet its effective application remains\nchallenging. In this work, we introduce a structured benchmark explicitly\ndesigned to evaluate and compare the transferability of Convolutional Neural\nNetworks and Foundation Models in predicting clinical outcomes in COVID-19\npatients, leveraging diverse publicly available Chest X-ray datasets. Our\nexperimental methodology extensively explores a wide set of fine-tuning\nstrategies, encompassing traditional approaches such as Full Fine-Tuning and\nLinear Probing, as well as advanced Parameter-Efficient Fine-Tuning methods\nincluding Low-Rank Adaptation, BitFit, VeRA, and IA3. The evaluations were\nconducted across multiple learning paradigms, including both extensive\nfull-data scenarios and more clinically realistic Few-Shot Learning settings,\nwhich are critical for modeling rare disease outcomes and rapidly emerging\nhealth threats. By implementing a large-scale comparative analysis involving a\ndiverse selection of pretrained models, including general-purpose architectures\npretrained on large-scale datasets such as CLIP and DINOv2, to\nbiomedical-specific models like MedCLIP, BioMedCLIP, and PubMedCLIP, we\nrigorously assess each model's capacity to effectively adapt and generalize to\nprognosis tasks, particularly under conditions of severe data scarcity and\npronounced class imbalance. The benchmark was designed to capture critical\nconditions common in prognosis tasks, including variations in dataset size and\nclass distribution, providing detailed insights into the strengths and\nlimitations of each fine-tuning strategy. This extensive and structured\nevaluation aims to inform the practical deployment and adoption of robust,\nefficient, and generalizable AI-driven solutions in real-world clinical\nprognosis prediction workflows.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18434", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18434", "abs": "https://arxiv.org/abs/2506.18434", "authors": ["Filippo Ruffini", "Elena Mulero Ayllon", "Linlin Shen", "Paolo Soda", "Valerio Guarrasi"], "title": "Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging", "comment": null, "summary": "Artificial Intelligence (AI) holds significant promise for improving\nprognosis prediction in medical imaging, yet its effective application remains\nchallenging. In this work, we introduce a structured benchmark explicitly\ndesigned to evaluate and compare the transferability of Convolutional Neural\nNetworks and Foundation Models in predicting clinical outcomes in COVID-19\npatients, leveraging diverse publicly available Chest X-ray datasets. Our\nexperimental methodology extensively explores a wide set of fine-tuning\nstrategies, encompassing traditional approaches such as Full Fine-Tuning and\nLinear Probing, as well as advanced Parameter-Efficient Fine-Tuning methods\nincluding Low-Rank Adaptation, BitFit, VeRA, and IA3. The evaluations were\nconducted across multiple learning paradigms, including both extensive\nfull-data scenarios and more clinically realistic Few-Shot Learning settings,\nwhich are critical for modeling rare disease outcomes and rapidly emerging\nhealth threats. By implementing a large-scale comparative analysis involving a\ndiverse selection of pretrained models, including general-purpose architectures\npretrained on large-scale datasets such as CLIP and DINOv2, to\nbiomedical-specific models like MedCLIP, BioMedCLIP, and PubMedCLIP, we\nrigorously assess each model's capacity to effectively adapt and generalize to\nprognosis tasks, particularly under conditions of severe data scarcity and\npronounced class imbalance. The benchmark was designed to capture critical\nconditions common in prognosis tasks, including variations in dataset size and\nclass distribution, providing detailed insights into the strengths and\nlimitations of each fine-tuning strategy. This extensive and structured\nevaluation aims to inform the practical deployment and adoption of robust,\nefficient, and generalizable AI-driven solutions in real-world clinical\nprognosis prediction workflows.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18504", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18504", "abs": "https://arxiv.org/abs/2506.18504", "authors": ["Xinyao Li", "Jingjing Li", "Fengling Li", "Lei Zhu", "Yang Yang", "Heng Tao Shen"], "title": "Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey", "comment": null, "summary": "Recently, vision-language pretraining has emerged as a transformative\ntechnique that integrates the strengths of both visual and textual modalities,\nresulting in powerful vision-language models (VLMs). Leveraging web-scale\npretraining data, these models exhibit strong zero-shot capabilities. However,\ntheir performance often deteriorates when confronted with domain-specific or\nspecialized generalization tasks. To address this, a growing body of research\nfocuses on transferring or generalizing the rich knowledge embedded in VLMs to\nvarious downstream applications. This survey aims to comprehensively summarize\nthe generalization settings, methodologies, benchmarking and results in VLM\nliteratures. Delving into the typical VLM structures, current literatures are\ncategorized into prompt-based, parameter-based and feature-based methods\naccording to the transferred modules. The differences and characteristics in\neach category are furthered summarized and discussed by revisiting the typical\ntransfer learning (TL) settings, providing novel interpretations for TL in the\nera of VLMs. Popular benchmarks for VLM generalization are further introduced\nwith thorough performance comparisons among the reviewed methods. Following the\nadvances in large-scale generalizable pretraining, this survey also discusses\nthe relations and differences between VLMs and up-to-date multimodal large\nlanguage models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the\nsurging literatures in vision-language research from a novel and practical\ngeneralization prospective, this survey contributes to a clear landscape of\ncurrent and future multimodal researches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18437", "abs": "https://arxiv.org/abs/2506.18437", "authors": ["Sijin He", "Guangfeng Lin", "Tao Li", "Yajun Chen"], "title": "Frequency-Domain Fusion Transformer for Image Inpainting", "comment": null, "summary": "Image inpainting plays a vital role in restoring missing image regions and\nsupporting high-level vision tasks, but traditional methods struggle with\ncomplex textures and large occlusions. Although Transformer-based approaches\nhave demonstrated strong global modeling capabilities, they often fail to\npreserve high-frequency details due to the low-pass nature of self-attention\nand suffer from high computational costs. To address these challenges, this\npaper proposes a Transformer-based image inpainting method incorporating\nfrequency-domain fusion. Specifically, an attention mechanism combining wavelet\ntransform and Gabor filtering is introduced to enhance multi-scale structural\nmodeling and detail preservation. Additionally, a learnable frequency-domain\nfilter based on the fast Fourier transform is designed to replace the\nfeedforward network, enabling adaptive noise suppression and detail retention.\nThe model adopts a four-level encoder-decoder structure and is guided by a\nnovel loss strategy to balance global semantics and fine details. Experimental\nresults demonstrate that the proposed method effectively improves the quality\nof image inpainting by preserving more high-frequency information.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18463", "abs": "https://arxiv.org/abs/2506.18463", "authors": ["Sophia Sirko-Galouchenko", "Spyros Gidaris", "Antonin Vobecky", "Andrei Bursuc", "Nicolas Thome"], "title": "DIP: Unsupervised Dense In-Context Post-training of Visual Representations", "comment": null, "summary": "We introduce DIP, a novel unsupervised post-training method designed to\nenhance dense image representations in large-scale pretrained vision encoders\nfor in-context scene understanding. Unlike prior approaches that rely on\ncomplex self-distillation architectures, our method trains the vision encoder\nusing pseudo-tasks that explicitly simulate downstream in-context scenarios,\ninspired by meta-learning principles. To enable post-training on unlabeled\ndata, we propose an automatic mechanism for generating in-context tasks that\ncombines a pretrained diffusion model and the vision encoder itself. DIP is\nsimple, unsupervised, and computationally efficient, requiring less than 9\nhours on a single A100 GPU. By learning dense representations through pseudo\nin-context tasks, it achieves strong performance across a wide variety of\ndownstream real-world in-context scene understanding tasks. It outperforms both\nthe initial vision encoder and prior methods, offering a practical and\neffective solution for improving dense representations. Code available here:\nhttps://github.com/sirkosophia/DIP", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18668", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18668", "abs": "https://arxiv.org/abs/2506.18668", "authors": ["Pablo Meseguer", "Roco del Amor", "Valery Naranjo"], "title": "Benchmarking histopathology foundation models in a multi-center dataset for skin cancer subtyping", "comment": "Accepeted for oral presentation at Medical Image Understanding and\n  Analysis (MIUA) 2025", "summary": "Pretraining on large-scale, in-domain datasets grants histopathology\nfoundation models (FM) the ability to learn task-agnostic data representations,\nenhancing transfer learning on downstream tasks. In computational pathology,\nautomated whole slide image analysis requires multiple instance learning (MIL)\nframeworks due to the gigapixel scale of the slides. The diversity among\nhistopathology FMs has highlighted the need to design real-world challenges for\nevaluating their effectiveness. To bridge this gap, our work presents a novel\nbenchmark for evaluating histopathology FMs as patch-level feature extractors\nwithin a MIL classification framework. For that purpose, we leverage the\nAI4SkIN dataset, a multi-center cohort encompassing slides with challenging\ncutaneous spindle cell neoplasm subtypes. We also define the Foundation Model -\nSilhouette Index (FM-SI), a novel metric to measure model consistency against\ndistribution shifts. Our experimentation shows that extracting less biased\nfeatures enhances classification performance, especially in similarity-based\nMIL classifiers.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18682", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18682", "abs": "https://arxiv.org/abs/2506.18682", "authors": ["Imad Ali Shah", "Jiarong Li", "Tim Brophy", "Martin Glavin", "Edward Jones", "Enda Ward", "Brian Deegan"], "title": "Multi-Scale Spectral Attention Module-based Hyperspectral Segmentation in Autonomous Driving Scenarios", "comment": null, "summary": "Recent advances in autonomous driving (AD) have highlighted the potential of\nHyperspectral Imaging (HSI) for enhanced environmental perception, particularly\nin challenging weather and lighting conditions. However, efficiently processing\nits high-dimensional spectral data remains a significant challenge. This paper\nintroduces a Multi-scale Spectral Attention Module (MSAM) that enhances\nspectral feature extraction through three parallel 1D convolutions with varying\nkernel sizes between 1 to 11, coupled with an adaptive feature aggregation\nmechanism. By integrating MSAM into UNet's skip connections (UNet-SC), our\nproposed UNet-MSAM achieves significant improvements in semantic segmentation\nperformance across multiple HSI datasets: HyKo-VIS v2, HSI-Drive v2, and\nHyperspectral City v2. Our comprehensive experiments demonstrate that with\nminimal computational overhead (on average 0.02% in parameters and 0.82%\nGFLOPS), UNet-MSAM consistently outperforms UNet-SC, achieving average\nimprovements of 3.61% in mean IoU and 3.80% in mF1 across the three datasets.\nThrough extensive ablation studies, we have established that multi-scale kernel\ncombinations perform better than single-scale configurations. These findings\ndemonstrate the potential of HSI processing for AD and provide valuable\ninsights into designing robust, multi-scale spectral feature extractors for\nreal-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18701", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18701", "abs": "https://arxiv.org/abs/2506.18701", "authors": ["Yifan Zhang", "Chunli Peng", "Boyang Wang", "Puyi Wang", "Qingcheng Zhu", "Fei Kang", "Biao Jiang", "Zedong Gao", "Eric Li", "Yang Liu", "Yahui Zhou"], "title": "Matrix-Game: Interactive World Foundation Model", "comment": "Technical Report", "summary": "We introduce Matrix-Game, an interactive world foundation model for\ncontrollable game world generation. Matrix-Game is trained using a two-stage\npipeline that first performs large-scale unlabeled pretraining for environment\nunderstanding, followed by action-labeled training for interactive video\ngeneration. To support this, we curate Matrix-Game-MC, a comprehensive\nMinecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips\nand over 1,000 hours of high-quality labeled clips with fine-grained keyboard\nand mouse action annotations. Our model adopts a controllable image-to-world\ngeneration paradigm, conditioned on a reference image, motion context, and user\nactions. With over 17 billion parameters, Matrix-Game enables precise control\nover character actions and camera movements, while maintaining high visual\nquality and temporal coherence. To evaluate performance, we develop GameWorld\nScore, a unified benchmark measuring visual quality, temporal quality, action\ncontrollability, and physical rule understanding for Minecraft world\ngeneration. Extensive experiments show that Matrix-Game consistently\noutperforms prior open-source Minecraft world models (including Oasis and\nMineWorld) across all metrics, with particularly strong gains in\ncontrollability and physical consistency. Double-blind human evaluations\nfurther confirm the superiority of Matrix-Game, highlighting its ability to\ngenerate perceptually realistic and precisely controllable videos across\ndiverse game scenarios. To facilitate future research on interactive\nimage-to-world generation, we will open-source the Matrix-Game model weights\nand the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency", "fine-grained"], "score": 4}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18504", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18504", "abs": "https://arxiv.org/abs/2506.18504", "authors": ["Xinyao Li", "Jingjing Li", "Fengling Li", "Lei Zhu", "Yang Yang", "Heng Tao Shen"], "title": "Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey", "comment": null, "summary": "Recently, vision-language pretraining has emerged as a transformative\ntechnique that integrates the strengths of both visual and textual modalities,\nresulting in powerful vision-language models (VLMs). Leveraging web-scale\npretraining data, these models exhibit strong zero-shot capabilities. However,\ntheir performance often deteriorates when confronted with domain-specific or\nspecialized generalization tasks. To address this, a growing body of research\nfocuses on transferring or generalizing the rich knowledge embedded in VLMs to\nvarious downstream applications. This survey aims to comprehensively summarize\nthe generalization settings, methodologies, benchmarking and results in VLM\nliteratures. Delving into the typical VLM structures, current literatures are\ncategorized into prompt-based, parameter-based and feature-based methods\naccording to the transferred modules. The differences and characteristics in\neach category are furthered summarized and discussed by revisiting the typical\ntransfer learning (TL) settings, providing novel interpretations for TL in the\nera of VLMs. Popular benchmarks for VLM generalization are further introduced\nwith thorough performance comparisons among the reviewed methods. Following the\nadvances in large-scale generalizable pretraining, this survey also discusses\nthe relations and differences between VLMs and up-to-date multimodal large\nlanguage models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the\nsurging literatures in vision-language research from a novel and practical\ngeneralization prospective, this survey contributes to a clear landscape of\ncurrent and future multimodal researches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18523", "abs": "https://arxiv.org/abs/2506.18523", "authors": ["Kei Taguchi", "Kazumasa Ohara", "Tatsuya Yokota", "Hiroaki Miyoshi", "Noriaki Hashimoto", "Ichiro Takeuchi", "Hidekata Hontani"], "title": "Multi-Scale Representation of Follicular Lymphoma Pathology Images in a Single Hyperbolic Space", "comment": "10 pages, 3 figures", "summary": "We propose a method for representing malignant lymphoma pathology images,\nfrom high-resolution cell nuclei to low-resolution tissue images, within a\nsingle hyperbolic space using self-supervised learning. To capture\nmorphological changes that occur across scales during disease progression, our\napproach embeds tissue and corresponding nucleus images close to each other\nbased on inclusion relationships. Using the Poincar\\'e ball as the feature\nspace enables effective encoding of this hierarchical structure. The learned\nrepresentations capture both disease state and cell type variations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18564", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18564", "abs": "https://arxiv.org/abs/2506.18564", "authors": ["Xuanyu Zhang", "Weiqi Li", "Shijie Zhao", "Junlin Li", "Li Zhang", "Jian Zhang"], "title": "VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning", "comment": "Technical Report", "summary": "Recent advances in AI-generated content (AIGC) have led to the emergence of\npowerful text-to-video generation models. Despite these successes, evaluating\nthe quality of AIGC-generated videos remains challenging due to limited\ngeneralization, lack of temporal awareness, heavy reliance on large-scale\nannotated datasets, and the lack of effective interaction with generation\nmodels. Most current approaches rely on supervised finetuning of\nvision-language models (VLMs), which often require large-scale annotated\ndatasets and tend to decouple understanding and generation. To address these\nshortcomings, we propose VQ-Insight, a novel reasoning-style VLM framework for\nAIGC video quality assessment. Our approach features: (1) a progressive video\nquality learning scheme that combines image quality warm-up, general\ntask-specific temporal learning, and joint optimization with the video\ngeneration model; (2) the design of multi-dimension scoring rewards, preference\ncomparison rewards, and temporal modeling rewards to enhance both\ngeneralization and specialization in video quality evaluation. Extensive\nexperiments demonstrate that VQ-Insight consistently outperforms\nstate-of-the-art baselines in preference comparison, multi-dimension scoring,\nand natural video scoring, bringing significant improvements for video\ngeneration tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "comparison"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dimension"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18880", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18880", "abs": "https://arxiv.org/abs/2506.18880", "authors": ["Yiyou Sun", "Shawn Hu", "Georgia Zhou", "Ken Zheng", "Hannaneh Hajishirzi", "Nouha Dziri", "Dawn Song"], "title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization", "comment": null, "summary": "Recent large-scale language models (LLMs) with long Chain-of-Thought\nreasoning-such as DeepSeek-R1-have achieved impressive results on\nOlympiad-level mathematics benchmarks. However, they often rely on a narrow set\nof strategies and struggle with problems that require a novel way of thinking.\nTo systematically investigate these limitations, we introduce\nOMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a\ncontrolled yet diverse benchmark designed to evaluate three axes of\nout-of-distribution generalization, inspired by Boden's typology of creativity:\n(1) Exploratory-applying known problem solving skills to more complex instances\nwithin the same problem domain; (2) Compositional-combining distinct reasoning\nskills, previously learned in isolation, to solve novel problems that require\nintegrating these skills in new and coherent ways; and (3)\nTransformative-adopting novel, often unconventional strategies by moving beyond\nfamiliar approaches to solve problems more effectively. OMEGA consists of\nprogrammatically generated training-test pairs derived from templated problem\ngenerators across geometry, number theory, algebra, combinatorics, logic, and\npuzzles, with solutions verified using symbolic, numerical, or graphical\nmethods. We evaluate frontier (or top-tier) LLMs and observe sharp performance\ndegradation as problem complexity increases. Moreover, we fine-tune the\nQwen-series models across all generalization settings and observe notable\nimprovements in exploratory generalization, while compositional generalization\nremains limited and transformative reasoning shows little to no improvement. By\nisolating and quantifying these fine-grained failures, OMEGA lays the\ngroundwork for advancing LLMs toward genuine mathematical creativity beyond\nmechanical proficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18898", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.18898", "abs": "https://arxiv.org/abs/2506.18898", "authors": ["Jiaming Han", "Hao Chen", "Yang Zhao", "Hanyu Wang", "Qi Zhao", "Ziyan Yang", "Hao He", "Xiangyu Yue", "Lu Jiang"], "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations", "comment": "Project page: https://tar.csuhan.com", "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18668", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18668", "abs": "https://arxiv.org/abs/2506.18668", "authors": ["Pablo Meseguer", "Roco del Amor", "Valery Naranjo"], "title": "Benchmarking histopathology foundation models in a multi-center dataset for skin cancer subtyping", "comment": "Accepeted for oral presentation at Medical Image Understanding and\n  Analysis (MIUA) 2025", "summary": "Pretraining on large-scale, in-domain datasets grants histopathology\nfoundation models (FM) the ability to learn task-agnostic data representations,\nenhancing transfer learning on downstream tasks. In computational pathology,\nautomated whole slide image analysis requires multiple instance learning (MIL)\nframeworks due to the gigapixel scale of the slides. The diversity among\nhistopathology FMs has highlighted the need to design real-world challenges for\nevaluating their effectiveness. To bridge this gap, our work presents a novel\nbenchmark for evaluating histopathology FMs as patch-level feature extractors\nwithin a MIL classification framework. For that purpose, we leverage the\nAI4SkIN dataset, a multi-center cohort encompassing slides with challenging\ncutaneous spindle cell neoplasm subtypes. We also define the Foundation Model -\nSilhouette Index (FM-SI), a novel metric to measure model consistency against\ndistribution shifts. Our experimentation shows that extracting less biased\nfeatures enhances classification performance, especially in similarity-based\nMIL classifiers.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18678", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18678", "abs": "https://arxiv.org/abs/2506.18678", "authors": ["Tianchen Deng", "Guole Shen", "Xun Chen", "Shenghai Yuan", "Hongming Shen", "Guohao Peng", "Zhenyu Wu", "Jingchuan Wang", "Lihua Xie", "Danwei Wang", "Hesheng Wang", "Weidong Chen"], "title": "MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation", "comment": null, "summary": "Neural implicit scene representations have recently shown promising results\nin dense visual SLAM. However, existing implicit SLAM algorithms are\nconstrained to single-agent scenarios, and fall difficulties in large-scale\nscenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks\ncannot meet the constraints of communication bandwidth. To this end, we propose\nthe first distributed multi-agent collaborative neural SLAM framework with\nhybrid scene representation, distributed camera tracking, intra-to-inter loop\nclosure, and online distillation for multiple submap fusion. A novel\ntriplane-grid joint scene representation method is proposed to improve scene\nreconstruction. A novel intra-to-inter loop closure method is designed to\nachieve local (single-agent) and global (multi-agent) consistency. We also\ndesign a novel online distillation method to fuse the information of different\nsubmaps to achieve global consistency. Furthermore, to the best of our\nknowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that\nprovides both continuous-time trajectories groundtruth and high-accuracy 3D\nmeshes groundtruth. To this end, we propose the first real-world Dense slam\n(DES) dataset covering both single-agent and multi-agent scenarios, ranging\nfrom small rooms to large-scale outdoor scenes, with high-accuracy ground truth\nfor both 3D mesh and continuous-time camera trajectory. This dataset can\nadvance the development of the research in both SLAM, 3D reconstruction, and\nvisual foundation model. Experiments on various datasets demonstrate the\nsuperiority of the proposed method in both mapping, tracking, and\ncommunication. The dataset and code will open-source on\nhttps://github.com/dtc111111/mcnslam.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18682", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18682", "abs": "https://arxiv.org/abs/2506.18682", "authors": ["Imad Ali Shah", "Jiarong Li", "Tim Brophy", "Martin Glavin", "Edward Jones", "Enda Ward", "Brian Deegan"], "title": "Multi-Scale Spectral Attention Module-based Hyperspectral Segmentation in Autonomous Driving Scenarios", "comment": null, "summary": "Recent advances in autonomous driving (AD) have highlighted the potential of\nHyperspectral Imaging (HSI) for enhanced environmental perception, particularly\nin challenging weather and lighting conditions. However, efficiently processing\nits high-dimensional spectral data remains a significant challenge. This paper\nintroduces a Multi-scale Spectral Attention Module (MSAM) that enhances\nspectral feature extraction through three parallel 1D convolutions with varying\nkernel sizes between 1 to 11, coupled with an adaptive feature aggregation\nmechanism. By integrating MSAM into UNet's skip connections (UNet-SC), our\nproposed UNet-MSAM achieves significant improvements in semantic segmentation\nperformance across multiple HSI datasets: HyKo-VIS v2, HSI-Drive v2, and\nHyperspectral City v2. Our comprehensive experiments demonstrate that with\nminimal computational overhead (on average 0.02% in parameters and 0.82%\nGFLOPS), UNet-MSAM consistently outperforms UNet-SC, achieving average\nimprovements of 3.61% in mean IoU and 3.80% in mF1 across the three datasets.\nThrough extensive ablation studies, we have established that multi-scale kernel\ncombinations perform better than single-scale configurations. These findings\ndemonstrate the potential of HSI processing for AD and provide valuable\ninsights into designing robust, multi-scale spectral feature extractors for\nreal-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18701", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18701", "abs": "https://arxiv.org/abs/2506.18701", "authors": ["Yifan Zhang", "Chunli Peng", "Boyang Wang", "Puyi Wang", "Qingcheng Zhu", "Fei Kang", "Biao Jiang", "Zedong Gao", "Eric Li", "Yang Liu", "Yahui Zhou"], "title": "Matrix-Game: Interactive World Foundation Model", "comment": "Technical Report", "summary": "We introduce Matrix-Game, an interactive world foundation model for\ncontrollable game world generation. Matrix-Game is trained using a two-stage\npipeline that first performs large-scale unlabeled pretraining for environment\nunderstanding, followed by action-labeled training for interactive video\ngeneration. To support this, we curate Matrix-Game-MC, a comprehensive\nMinecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips\nand over 1,000 hours of high-quality labeled clips with fine-grained keyboard\nand mouse action annotations. Our model adopts a controllable image-to-world\ngeneration paradigm, conditioned on a reference image, motion context, and user\nactions. With over 17 billion parameters, Matrix-Game enables precise control\nover character actions and camera movements, while maintaining high visual\nquality and temporal coherence. To evaluate performance, we develop GameWorld\nScore, a unified benchmark measuring visual quality, temporal quality, action\ncontrollability, and physical rule understanding for Minecraft world\ngeneration. Extensive experiments show that Matrix-Game consistently\noutperforms prior open-source Minecraft world models (including Oasis and\nMineWorld) across all metrics, with particularly strong gains in\ncontrollability and physical consistency. Double-blind human evaluations\nfurther confirm the superiority of Matrix-Game, highlighting its ability to\ngenerate perceptually realistic and precisely controllable videos across\ndiverse game scenarios. To facilitate future research on interactive\nimage-to-world generation, we will open-source the Matrix-Game model weights\nand the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency", "fine-grained"], "score": 4}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18787", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18787", "abs": "https://arxiv.org/abs/2506.18787", "authors": ["Dylan Ebert"], "title": "3D Arena: An Open Platform for Generative 3D Evaluation", "comment": "9 pages, 2 figures", "summary": "Evaluating Generative 3D models remains challenging due to misalignment\nbetween automated metrics and human perception of quality. Current benchmarks\nrely on image-based metrics that ignore 3D structure or geometric measures that\nfail to capture perceptual appeal and real-world utility. To address this gap,\nwe present 3D Arena, an open platform for evaluating image-to-3D generation\nmodels through large-scale human preference collection using pairwise\ncomparisons.\n  Since launching in June 2024, the platform has collected 123,243 votes from\n8,096 users across 19 state-of-the-art models, establishing the largest human\npreference evaluation for Generative 3D. We contribute the iso3d dataset of 100\nevaluation prompts and demonstrate quality control achieving 99.75% user\nauthenticity through statistical fraud detection. Our ELO-based ranking system\nprovides reliable model assessment, with the platform becoming an established\nevaluation resource.\n  Through analysis of this preference data, we present insights into human\npreference patterns. Our findings reveal preferences for visual presentation\nfeatures, with Gaussian splat outputs achieving a 16.6 ELO advantage over\nmeshes and textured models receiving a 144.1 ELO advantage over untextured\nmodels. We provide recommendations for improving evaluation methods, including\nmulti-criteria assessment, task-oriented evaluation, and format-aware\ncomparison. The platform's community engagement establishes 3D Arena as a\nbenchmark for the field while advancing understanding of human-centered\nevaluation in Generative 3D.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "comparison", "ranking", "pairwise"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "human preference", "criteria"], "score": 5}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18791", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18791", "abs": "https://arxiv.org/abs/2506.18791", "authors": ["Suyash Gaurav", "Muhammad Farhan Humayun", "Jukka Heikkonen", "Jatin Chaudhary"], "title": "Focus Your Attention: Towards Data-Intuitive Lightweight Vision Transformers", "comment": null, "summary": "The evolution of Vision Transformers has led to their widespread adaptation\nto different domains. Despite large-scale success, there remain significant\nchallenges including their reliance on extensive computational and memory\nresources for pre-training on huge datasets as well as difficulties in\ntask-specific transfer learning. These limitations coupled with energy\ninefficiencies mainly arise due to the computation-intensive self-attention\nmechanism. To address these issues, we propose a novel Super-Pixel Based Patch\nPooling (SPPP) technique that generates context-aware, semantically rich, patch\nembeddings to effectively reduce the architectural complexity and improve\nefficiency. Additionally, we introduce the Light Latent Attention (LLA) module\nin our pipeline by integrating latent tokens into the attention mechanism\nallowing cross-attention operations to significantly reduce the time and space\ncomplexity of the attention module. By leveraging the data-intuitive patch\nembeddings coupled with dynamic positional encodings, our approach adaptively\nmodulates the cross-attention process to focus on informative regions while\nmaintaining the global semantic structure. This targeted attention improves\ntraining efficiency and accelerates convergence. Notably, the SPPP module is\nlightweight and can be easily integrated into existing transformer\narchitectures. Extensive experiments demonstrate that our proposed architecture\nprovides significant improvements in terms of computational efficiency while\nachieving comparable results with the state-of-the-art approaches, highlighting\nits potential for energy-efficient transformers suitable for edge deployment.\n(The code is available on our GitHub repository:\nhttps://github.com/zser092/Focused-Attention-ViT).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18851", "abs": "https://arxiv.org/abs/2506.18851", "authors": ["Zhuowei Chen", "Bingchuan Li", "Tianxiang Ma", "Lijie Liu", "Mingcong Liu", "Yi Zhang", "Gen Li", "Xinghui Li", "Siyu Zhou", "Qian He", "Xinglong Wu"], "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset", "comment": "Project page:https://phantom-video.github.io/Phantom-Data/", "summary": "Subject-to-video generation has witnessed substantial progress in recent\nyears. However, existing models still face significant challenges in faithfully\nfollowing textual instructions. This limitation, commonly known as the\ncopy-paste problem, arises from the widely used in-pair training paradigm. This\napproach inherently entangles subject identity with background and contextual\nattributes by sampling reference images from the same scene as the target\nvideo. To address this issue, we introduce \\textbf{Phantom-Data, the first\ngeneral-purpose cross-pair subject-to-video consistency dataset}, containing\napproximately one million identity-consistent pairs across diverse categories.\nOur dataset is constructed via a three-stage pipeline: (1) a general and\ninput-aligned subject detection module, (2) large-scale cross-context subject\nretrieval from more than 53 million videos and 3 billion images, and (3)\nprior-guided identity verification to ensure visual consistency under\ncontextual variation. Comprehensive experiments show that training with\nPhantom-Data significantly improves prompt alignment and visual quality while\npreserving identity consistency on par with in-pair baselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18883", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18883", "abs": "https://arxiv.org/abs/2506.18883", "authors": ["Zeqian Li", "Shangzhe Di", "Zhonghua Zhai", "Weilin Huang", "Yanfeng Wang", "Weidi Xie"], "title": "Universal Video Temporal Grounding with Generative Multi-modal Large Language Models", "comment": null, "summary": "This paper presents a computational model for universal video temporal\ngrounding, which accurately localizes temporal moments in videos based on\nnatural language queries (e.g., questions or descriptions). Unlike existing\nmethods that are often limited to specific video domains or durations, we\npropose UniTime, a robust and universal video grounding model leveraging the\nstrong vision-language understanding capabilities of generative Multi-modal\nLarge Language Models (MLLMs). Our model effectively handles videos of diverse\nviews, genres, and lengths while comprehending complex language queries. The\nkey contributions include: (i) We consider steering strong MLLMs for temporal\ngrounding in videos. To enable precise timestamp outputs, we incorporate\ntemporal information by interleaving timestamp tokens with video tokens. (ii)\nBy training the model to handle videos with different input granularities\nthrough adaptive frame scaling, our approach achieves robust temporal grounding\nfor both short and long videos. (iii) Comprehensive experiments show that\nUniTime outperforms state-of-the-art approaches in both zero-shot and\ndataset-specific finetuned settings across five public temporal grounding\nbenchmarks. (iv) When employed as a preliminary moment retriever for long-form\nvideo question-answering (VideoQA), UniTime significantly improves VideoQA\naccuracy, highlighting its value for complex video understanding tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18898", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.18898", "abs": "https://arxiv.org/abs/2506.18898", "authors": ["Jiaming Han", "Hao Chen", "Yang Zhao", "Hanyu Wang", "Qi Zhao", "Ziyan Yang", "Hao He", "Xiangyu Yue", "Lu Jiang"], "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations", "comment": "Project page: https://tar.csuhan.com", "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-24.jsonl"}
{"id": "2506.18904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18904", "abs": "https://arxiv.org/abs/2506.18904", "authors": ["Yang Liu", "Chuanchen Luo", "Zimo Tang", "Yingyan Li", "Yuran Yang", "Yuanyong Ning", "Lue Fan", "Junran Peng", "Zhaoxiang Zhang"], "title": "TC-Light: Temporally Consistent Relighting for Dynamic Long Videos", "comment": "Project Page: https://dekuliutesla.github.io/tclight/ Code:\n  https://github.com/Linketic/TC-Light", "summary": "Editing illumination in long videos with complex dynamics has significant\nvalue in various downstream tasks, including visual content creation and\nmanipulation, as well as data scaling up for embodied AI through sim2real and\nreal2real transfer. Nevertheless, existing video relighting techniques are\npredominantly limited to portrait videos or fall into the bottleneck of\ntemporal consistency and computation efficiency. In this paper, we propose\nTC-Light, a novel paradigm characterized by the proposed two-stage post\noptimization mechanism. Starting from the video preliminarily relighted by an\ninflated video relighting model, it optimizes appearance embedding in the first\nstage to align global illumination. Then it optimizes the proposed canonical\nvideo representation, i.e., Unique Video Tensor (UVT), to align fine-grained\ntexture and lighting in the second stage. To comprehensively evaluate\nperformance, we also establish a long and highly dynamic video benchmark.\nExtensive experiments show that our method enables physically plausible\nrelighting results with superior temporal coherence and low computation cost.\nThe code and video demos are available at\nhttps://dekuliutesla.github.io/tclight/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-06-24.jsonl"}
