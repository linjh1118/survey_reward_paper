{"id": "2506.19022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19022", "abs": "https://arxiv.org/abs/2506.19022", "authors": ["Jinlong Li", "Dong Zhao", "Qi Zang", "Zequn Jie", "Lin Ma", "Nicu Sebe"], "title": "Orthogonal Projection Subspace to Aggregate Online Prior-knowledge for Continual Test-time Adaptation", "comment": null, "summary": "Continual Test Time Adaptation (CTTA) is a task that requires a source\npre-trained model to continually adapt to new scenarios with changing target\ndistributions. Existing CTTA methods primarily focus on mitigating the\nchallenges of catastrophic forgetting and error accumulation. Though there have\nbeen emerging methods based on forgetting adaptation with parameter-efficient\nfine-tuning, they still struggle to balance competitive performance and\nefficient model adaptation, particularly in complex tasks like semantic\nsegmentation. In this paper, to tackle the above issues, we propose a novel\npipeline, Orthogonal Projection Subspace to aggregate online Prior-knowledge,\ndubbed OoPk. Specifically, we first project a tuning subspace orthogonally\nwhich allows the model to adapt to new domains while preserving the knowledge\nintegrity of the pre-trained source model to alleviate catastrophic forgetting.\nThen, we elaborate an online prior-knowledge aggregation strategy that employs\nan aggressive yet efficient image masking strategy to mimic potential target\ndynamism, enhancing the student model's domain adaptability. This further\ngradually ameliorates the teacher model's knowledge, ensuring high-quality\npseudo labels and reducing error accumulation. We demonstrate our method with\nextensive experiments that surpass previous CTTA methods and achieve\ncompetitive performances across various continual TTA benchmarks in semantic\nsegmentation tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time", "test-time adaptation", "test time adaptation"], "score": 4}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.18957", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18957", "abs": "https://arxiv.org/abs/2506.18957", "authors": ["Sheraz Khan", "Subha Madhavan", "Kannan Natarajan"], "title": "A Comment On \"The Illusion of Thinking\": Reframing the Reasoning Cliff as an Agentic Gap", "comment": "10 pages, 2 figures, Comment on \"The Illusion of Thinking:\n  Understanding the Strengths and Limitations of Reasoning Models via the Lens\n  of Problem Complexity\" (arXiv:2506.06941v1)", "summary": "The recent work by Shojaee et al. (2025), titled The Illusion of Thinking:\nUnderstanding the Strengths and Limitations of Reasoning Models via the Lens of\nProblem Complexity, presents a compelling empirical finding, a reasoning cliff,\nwhere the performance of Large Reasoning Models (LRMs) collapses beyond a\nspecific complexity threshold, which the authors posit as an intrinsic scaling\nlimitation of Chain-of-Thought (CoT) reasoning. This commentary, while\nacknowledging the study's methodological rigor, contends that this conclusion\nis confounded by experimental artifacts. We argue that the observed failure is\nnot evidence of a fundamental cognitive boundary, but rather a predictable\noutcome of system-level constraints in the static, text-only evaluation\nparadigm, including tool use restrictions, context window recall issues, the\nabsence of crucial cognitive baselines, inadequate statistical reporting, and\noutput generation limits. We reframe this performance collapse through the lens\nof an agentic gap, asserting that the models are not failing at reasoning, but\nat execution within a profoundly restrictive interface. We empirically\nsubstantiate this critique by demonstrating a striking reversal. A model,\ninitially declaring a puzzle impossible when confined to text-only generation,\nnow employs agentic tools to not only solve it but also master variations of\ncomplexity far beyond the reasoning cliff it previously failed to surmount.\nAdditionally, our empirical analysis of tool-enabled models like o4-mini and\nGPT-4o reveals a hierarchy of agentic reasoning, from simple procedural\nexecution to complex meta-cognitive self-correction, which has significant\nimplications for how we define and measure machine intelligence. The illusion\nof thinking attributed to LRMs is less a reasoning deficit and more a\nconsequence of an otherwise capable mind lacking the tools for action.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "self-correction"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19235", "abs": "https://arxiv.org/abs/2506.19235", "authors": ["Yu Xie", "Xingkai Ren", "Ying Qi", "Yao Hu", "Lianlei Shan"], "title": "RecLLM-R1: A Two-Stage Training Paradigm with Reinforcement Learning and Chain-of-Thought v1", "comment": null, "summary": "Traditional recommendation systems often grapple with \"filter bubbles\",\nunderutilization of external knowledge, and a disconnect between model\noptimization and business policy iteration. To address these limitations, this\npaper introduces RecLLM-R1, a novel recommendation framework leveraging Large\nLanguage Models (LLMs) and drawing inspiration from the DeepSeek R1\nmethodology. The framework initiates by transforming user profiles, historical\ninteractions, and multi-faceted item attributes into LLM-interpretable natural\nlanguage prompts through a carefully engineered data construction process.\nSubsequently, a two-stage training paradigm is employed: the initial stage\ninvolves Supervised Fine-Tuning (SFT) to imbue the LLM with fundamental\nrecommendation capabilities. The subsequent stage utilizes Group Relative\nPolicy Optimization (GRPO), a reinforcement learning technique, augmented with\na Chain-of-Thought (CoT) mechanism. This stage guides the model through\nmulti-step reasoning and holistic decision-making via a flexibly defined reward\nfunction, aiming to concurrently optimize recommendation accuracy, diversity,\nand other bespoke business objectives. Empirical evaluations on a real-world\nuser behavior dataset from a large-scale social media platform demonstrate that\nRecLLM-R1 significantly surpasses existing baseline methods across a spectrum\nof evaluation metrics, including accuracy, diversity, and novelty. It\neffectively mitigates the filter bubble effect and presents a promising avenue\nfor the integrated optimization of recommendation models and policies under\nintricate business goals.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "multi-step reasoning"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19290", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19290", "abs": "https://arxiv.org/abs/2506.19290", "authors": ["Liang Zeng", "Yongcong Li", "Yuzhen Xiao", "Changshi Li", "Chris Yuhao Liu", "Rui Yan", "Tianwen Wei", "Jujie He", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs", "comment": null, "summary": "Software engineering (SWE) has recently emerged as a crucial testbed for\nnext-generation LLM agents, demanding inherent capabilities in two critical\ndimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)\nand long-context dependency resolution (e.g., >32k tokens). However, the data\ncuration process in SWE remains notoriously time-consuming, as it heavily\nrelies on manual annotation for code file filtering and the setup of dedicated\nruntime environments to execute and validate unit tests. Consequently, most\nexisting datasets are limited to only a few thousand GitHub-sourced instances.\nTo this end, we propose an incremental, automated data-curation pipeline that\nsystematically scales both the volume and diversity of SWE datasets. Our\ndataset comprises 10,169 real-world Python task instances from 2,531 distinct\nGitHub repositories, each accompanied by a task specified in natural language\nand a dedicated runtime-environment image for automated unit-test validation.\nWe have carefully curated over 8,000 successfully runtime-validated training\ntrajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE\nmodel on these trajectories, we uncover a striking data scaling phenomenon: the\ntrained model's performance for software engineering capabilities in LLMs\ncontinues to improve as the data size increases, showing no signs of\nsaturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on\nthe SWE-bench Verified benchmark without using verifiers or multiple rollouts,\nestablishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based\nLLMs built on the OpenHands agent framework. Furthermore, with the\nincorporation of test-time scaling techniques, the performance further improves\nto 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter\nmodels. We release the Skywork-SWE-32B model checkpoint to accelerate future\nresearch.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "testbed", "annotation", "accuracy"], "score": 5}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19599", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19599", "abs": "https://arxiv.org/abs/2506.19599", "authors": ["Zhenke Duan", "Jiqun Pan", "Jiani Tu", "Xiaoyi Wang", "Yanqing Wang"], "title": "ECCoT: A Framework for Enhancing Effective Cognition via Chain of Thought in Large Language Model", "comment": null, "summary": "In the era of large-scale artificial intelligence, Large Language Models\n(LLMs) have made significant strides in natural language processing. However,\nthey often lack transparency and generate unreliable outputs, raising concerns\nabout their interpretability. To address this, the Chain of Thought (CoT)\nprompting method structures reasoning into step-by-step deductions. Yet, not\nall reasoning chains are valid, and errors can lead to unreliable conclusions.\nWe propose ECCoT, an End-to-End Cognitive Chain of Thought Validation\nFramework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates\nthe Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT\ngeneration and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By\nfiltering ineffective chains using structured ordering statistics, ECCoT\nimproves interpretability, reduces biases, and enhances the trustworthiness of\nLLM-based decision-making. Key contributions include the introduction of ECCoT,\nMRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning\nenhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "chain of thought"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.18957", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18957", "abs": "https://arxiv.org/abs/2506.18957", "authors": ["Sheraz Khan", "Subha Madhavan", "Kannan Natarajan"], "title": "A Comment On \"The Illusion of Thinking\": Reframing the Reasoning Cliff as an Agentic Gap", "comment": "10 pages, 2 figures, Comment on \"The Illusion of Thinking:\n  Understanding the Strengths and Limitations of Reasoning Models via the Lens\n  of Problem Complexity\" (arXiv:2506.06941v1)", "summary": "The recent work by Shojaee et al. (2025), titled The Illusion of Thinking:\nUnderstanding the Strengths and Limitations of Reasoning Models via the Lens of\nProblem Complexity, presents a compelling empirical finding, a reasoning cliff,\nwhere the performance of Large Reasoning Models (LRMs) collapses beyond a\nspecific complexity threshold, which the authors posit as an intrinsic scaling\nlimitation of Chain-of-Thought (CoT) reasoning. This commentary, while\nacknowledging the study's methodological rigor, contends that this conclusion\nis confounded by experimental artifacts. We argue that the observed failure is\nnot evidence of a fundamental cognitive boundary, but rather a predictable\noutcome of system-level constraints in the static, text-only evaluation\nparadigm, including tool use restrictions, context window recall issues, the\nabsence of crucial cognitive baselines, inadequate statistical reporting, and\noutput generation limits. We reframe this performance collapse through the lens\nof an agentic gap, asserting that the models are not failing at reasoning, but\nat execution within a profoundly restrictive interface. We empirically\nsubstantiate this critique by demonstrating a striking reversal. A model,\ninitially declaring a puzzle impossible when confined to text-only generation,\nnow employs agentic tools to not only solve it but also master variations of\ncomplexity far beyond the reasoning cliff it previously failed to surmount.\nAdditionally, our empirical analysis of tool-enabled models like o4-mini and\nGPT-4o reveals a hierarchy of agentic reasoning, from simple procedural\nexecution to complex meta-cognitive self-correction, which has significant\nimplications for how we define and measure machine intelligence. The illusion\nof thinking attributed to LRMs is less a reasoning deficit and more a\nconsequence of an otherwise capable mind lacking the tools for action.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "self-correction"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19290", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19290", "abs": "https://arxiv.org/abs/2506.19290", "authors": ["Liang Zeng", "Yongcong Li", "Yuzhen Xiao", "Changshi Li", "Chris Yuhao Liu", "Rui Yan", "Tianwen Wei", "Jujie He", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs", "comment": null, "summary": "Software engineering (SWE) has recently emerged as a crucial testbed for\nnext-generation LLM agents, demanding inherent capabilities in two critical\ndimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)\nand long-context dependency resolution (e.g., >32k tokens). However, the data\ncuration process in SWE remains notoriously time-consuming, as it heavily\nrelies on manual annotation for code file filtering and the setup of dedicated\nruntime environments to execute and validate unit tests. Consequently, most\nexisting datasets are limited to only a few thousand GitHub-sourced instances.\nTo this end, we propose an incremental, automated data-curation pipeline that\nsystematically scales both the volume and diversity of SWE datasets. Our\ndataset comprises 10,169 real-world Python task instances from 2,531 distinct\nGitHub repositories, each accompanied by a task specified in natural language\nand a dedicated runtime-environment image for automated unit-test validation.\nWe have carefully curated over 8,000 successfully runtime-validated training\ntrajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE\nmodel on these trajectories, we uncover a striking data scaling phenomenon: the\ntrained model's performance for software engineering capabilities in LLMs\ncontinues to improve as the data size increases, showing no signs of\nsaturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on\nthe SWE-bench Verified benchmark without using verifiers or multiple rollouts,\nestablishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based\nLLMs built on the OpenHands agent framework. Furthermore, with the\nincorporation of test-time scaling techniques, the performance further improves\nto 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter\nmodels. We release the Skywork-SWE-32B model checkpoint to accelerate future\nresearch.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "testbed", "annotation", "accuracy"], "score": 5}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19552", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.4"], "pdf": "https://arxiv.org/pdf/2506.19552", "abs": "https://arxiv.org/abs/2506.19552", "authors": ["Jakob Ambsdorf", "Asbjørn Munk", "Sebastian Llambias", "Anders Nymark Christensen", "Kamil Mikolaj", "Randall Balestriero", "Martin Tolsgaard", "Aasa Feragen", "Mads Nielsen"], "title": "General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound", "comment": "Submitted version of paper accepted at MICCAI 2025", "summary": "With access to large-scale, unlabeled medical datasets, researchers are\nconfronted with two questions: Should they attempt to pretrain a custom\nfoundation model on this medical data, or use transfer-learning from an\nexisting generalist model? And, if a custom model is pretrained, are novel\nmethods required? In this paper we explore these questions by conducting a\ncase-study, in which we train a foundation model on a large regional fetal\nultrasound dataset of 2M images. By selecting the well-established DINOv2\nmethod for pretraining, we achieve state-of-the-art results on three fetal\nultrasound datasets, covering data from different countries, classification,\nsegmentation, and few-shot tasks. We compare against a series of models\npretrained on natural images, ultrasound images, and supervised baselines. Our\nresults demonstrate two key insights: (i) Pretraining on custom data is worth\nit, even if smaller models are trained on less data, as scaling in natural\nimage pretraining does not translate to ultrasound performance. (ii) Well-tuned\nmethods from computer vision are making it feasible to train custom foundation\nmodels for a given medical domain, requiring no hyperparameter tuning and\nlittle methodological adaptation. Given these findings, we argue that a bias\ntowards methodological innovation should be avoided when developing domain\nspecific foundation models under common computational resource constraints.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19552", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.4"], "pdf": "https://arxiv.org/pdf/2506.19552", "abs": "https://arxiv.org/abs/2506.19552", "authors": ["Jakob Ambsdorf", "Asbjørn Munk", "Sebastian Llambias", "Anders Nymark Christensen", "Kamil Mikolaj", "Randall Balestriero", "Martin Tolsgaard", "Aasa Feragen", "Mads Nielsen"], "title": "General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound", "comment": "Submitted version of paper accepted at MICCAI 2025", "summary": "With access to large-scale, unlabeled medical datasets, researchers are\nconfronted with two questions: Should they attempt to pretrain a custom\nfoundation model on this medical data, or use transfer-learning from an\nexisting generalist model? And, if a custom model is pretrained, are novel\nmethods required? In this paper we explore these questions by conducting a\ncase-study, in which we train a foundation model on a large regional fetal\nultrasound dataset of 2M images. By selecting the well-established DINOv2\nmethod for pretraining, we achieve state-of-the-art results on three fetal\nultrasound datasets, covering data from different countries, classification,\nsegmentation, and few-shot tasks. We compare against a series of models\npretrained on natural images, ultrasound images, and supervised baselines. Our\nresults demonstrate two key insights: (i) Pretraining on custom data is worth\nit, even if smaller models are trained on less data, as scaling in natural\nimage pretraining does not translate to ultrasound performance. (ii) Well-tuned\nmethods from computer vision are making it feasible to train custom foundation\nmodels for a given medical domain, requiring no hyperparameter tuning and\nlittle methodological adaptation. Given these findings, we argue that a bias\ntowards methodological innovation should be avoided when developing domain\nspecific foundation models under common computational resource constraints.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19599", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19599", "abs": "https://arxiv.org/abs/2506.19599", "authors": ["Zhenke Duan", "Jiqun Pan", "Jiani Tu", "Xiaoyi Wang", "Yanqing Wang"], "title": "ECCoT: A Framework for Enhancing Effective Cognition via Chain of Thought in Large Language Model", "comment": null, "summary": "In the era of large-scale artificial intelligence, Large Language Models\n(LLMs) have made significant strides in natural language processing. However,\nthey often lack transparency and generate unreliable outputs, raising concerns\nabout their interpretability. To address this, the Chain of Thought (CoT)\nprompting method structures reasoning into step-by-step deductions. Yet, not\nall reasoning chains are valid, and errors can lead to unreliable conclusions.\nWe propose ECCoT, an End-to-End Cognitive Chain of Thought Validation\nFramework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates\nthe Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT\ngeneration and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By\nfiltering ineffective chains using structured ordering statistics, ECCoT\nimproves interpretability, reduces biases, and enhances the trustworthiness of\nLLM-based decision-making. Key contributions include the introduction of ECCoT,\nMRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning\nenhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "chain of thought"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.18919", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18919", "abs": "https://arxiv.org/abs/2506.18919", "authors": ["Hexiang Gu", "Qifan Yu", "Saihui Hou", "Zhiqin Fang", "Huijia Wu", "Zhaofeng He"], "title": "MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection", "comment": null, "summary": "The rapid development of social media has intensified the spread of harmful\ncontent. Harmful memes, which integrate both images and text, pose significant\nchallenges for automated detection due to their implicit semantics and complex\nmultimodal interactions. Although existing research has made progress in\ndetection accuracy and interpretability, the lack of a systematic, large-scale,\ndiverse, and highly explainable dataset continues to hinder further advancement\nin this field. To address this gap, we introduce MemeMind, a novel dataset\nfeaturing scientifically rigorous standards, large scale, diversity, bilingual\nsupport (Chinese and English), and detailed Chain-of-Thought (CoT) annotations.\nMemeMind fills critical gaps in current datasets by offering comprehensive\nlabeling and explicit reasoning traces, thereby providing a solid foundation\nfor enhancing harmful meme detection. In addition, we propose an innovative\ndetection framework, MemeGuard, which effectively integrates multimodal\ninformation with reasoning process modeling, significantly improving models'\nability to understand and identify harmful memes. Extensive experiments\nconducted on the MemeMind dataset demonstrate that MemeGuard consistently\noutperforms existing state-of-the-art methods in harmful meme detection tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19004", "abs": "https://arxiv.org/abs/2506.19004", "authors": ["Brian Siyuan Zheng", "Alisa Liu", "Orevaoghene Ahia", "Jonathan Hayase", "Yejin Choi", "Noah A. Smith"], "title": "Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations", "comment": "preprint", "summary": "Modern tokenizers employ deterministic algorithms to map text into a single\n\"canonical\" token sequence, yet the same string can be encoded as many\nnon-canonical tokenizations using the tokenizer vocabulary. In this work, we\ninvestigate the robustness of LMs to text encoded with non-canonical\ntokenizations entirely unseen during training. Surprisingly, when evaluated\nacross 20 benchmarks, we find that instruction-tuned models retain up to 93.4%\nof their original performance when given a randomly sampled tokenization, and\n90.8% with character-level tokenization. We see that overall stronger models\ntend to be more robust, and robustness diminishes as the tokenization departs\nfarther from the canonical form. Motivated by these results, we then identify\nsettings where non-canonical tokenization schemes can *improve* performance,\nfinding that character-level segmentation improves string manipulation and code\nunderstanding tasks by up to +14%, and right-aligned digit grouping enhances\nlarge-number arithmetic by +33%. Finally, we investigate the source of this\nrobustness, finding that it arises in the instruction-tuning phase. We show\nthat while both base and post-trained models grasp the semantics of\nnon-canonical tokenizations (perceiving them as containing misspellings), base\nmodels try to mimic the imagined mistakes and degenerate into nonsensical\noutput, while post-trained models are committed to fluent responses. Overall,\nour findings suggest that models are less tied to their tokenizer than\npreviously believed, and demonstrate the promise of intervening on tokenization\nat inference time to boost performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19037", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.NE", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.19037", "abs": "https://arxiv.org/abs/2506.19037", "authors": ["Omer Luxembourg", "Haim Permuter", "Eliya Nachmani"], "title": "Plan for Speed -- Dilated Scheduling for Masked Diffusion Language Models", "comment": null, "summary": "Masked diffusion language models (MDLM) have shown strong promise for\nnon-autoregressive text generation, yet existing samplers act as implicit\nplanners, selecting tokens to unmask via denoiser confidence or entropy scores.\nSuch heuristics falter under parallel unmasking - they ignore pairwise\ninteractions between tokens and cannot account for dependencies when unmasking\nmultiple positions at once, limiting their inference time to traditional\nauto-regressive (AR) models. We introduce the Dilated-scheduled Unmasking\nStrategy (DUS), an inference-only, planner-model-free method that requires no\nadditional training. DUS leverages a first-order Markov assumption to partition\nsequence positions into dilation-based groups of non-adjacent tokens, enabling\nindependent, parallel unmasking steps that respect local context that minimizes\nthe joint entropy of each iteration step. Unlike semi-AR block approaches\n(e.g., LLADA and Dream) that still invoke the denoiser per block, DUS reduces\nthe number of denoiser calls to O(log B) per generation block - yielding\nsubstantial speedup over the O(B) run time of state-of-the-art diffusion\nmodels, where B is the block size in the semi-AR inference process. In\nexperiments on math (GSM8K) and code completion (Humaneval, MBPP) benchmarks -\ndomains suited to non-ordinal generation - DUS improves scores over parallel\nconfidence-based planner, without modifying the underlying denoiser. DUS offers\na lightweight, budget-aware approach to efficient, high-quality text\ngeneration, paving the way to unlock the true capabilities of MDLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19139", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19139", "abs": "https://arxiv.org/abs/2506.19139", "authors": ["Lukas Radl", "Felix Windisch", "Thomas Deixelberger", "Jozef Hladky", "Michael Steiner", "Dieter Schmalstieg", "Markus Steinberger"], "title": "SOF: Sorted Opacity Fields for Fast Unbounded Surface Reconstruction", "comment": null, "summary": "Recent advances in 3D Gaussian representations have significantly improved\nthe quality and efficiency of image-based scene reconstruction. Their explicit\nnature facilitates real-time rendering and fast optimization, yet extracting\naccurate surfaces - particularly in large-scale, unbounded environments -\nremains a difficult task. Many existing methods rely on approximate depth\nestimates and global sorting heuristics, which can introduce artifacts and\nlimit the fidelity of the reconstructed mesh. In this paper, we present Sorted\nOpacity Fields (SOF), a method designed to recover detailed surfaces from 3D\nGaussians with both speed and precision. Our approach improves upon prior work\nby introducing hierarchical resorting and a robust formulation of Gaussian\ndepth, which better aligns with the level-set. To enhance mesh quality, we\nincorporate a level-set regularizer operating on the opacity field and\nintroduce losses that encourage geometrically-consistent primitive shapes. In\naddition, we develop a parallelized Marching Tetrahedra algorithm tailored to\nour opacity formulation, reducing meshing time by up to an order of magnitude.\nAs demonstrated by our quantitative evaluation, SOF achieves higher\nreconstruction accuracy while cutting total processing time by more than a\nfactor of three. These results mark a step forward in turning efficient\nGaussian-based rendering into equally efficient geometry extraction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.18925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18925", "abs": "https://arxiv.org/abs/2506.18925", "authors": ["Tahereh Zarrat Ehsan", "Michael Tangermann", "Yağmur Güçlütürk", "Bastiaan R. Bloem", "Luc J. W. Evers"], "title": "Interpretable and Granular Video-Based Quantification of Motor Characteristics from the Finger Tapping Test in Parkinson Disease", "comment": null, "summary": "Accurately quantifying motor characteristics in Parkinson disease (PD) is\ncrucial for monitoring disease progression and optimizing treatment strategies.\nThe finger-tapping test is a standard motor assessment. Clinicians visually\nevaluate a patient's tapping performance and assign an overall severity score\nbased on tapping amplitude, speed, and irregularity. However, this subjective\nevaluation is prone to inter- and intra-rater variability, and does not offer\ninsights into individual motor characteristics captured during this test. This\npaper introduces a granular computer vision-based method for quantifying PD\nmotor characteristics from video recordings. Four sets of clinically relevant\nfeatures are proposed to characterize hypokinesia, bradykinesia, sequence\neffect, and hesitation-halts. We evaluate our approach on video recordings and\nclinical evaluations of 74 PD patients from the Personalized Parkinson Project.\nPrincipal component analysis with varimax rotation shows that the video-based\nfeatures corresponded to the four deficits. Additionally, video-based analysis\nhas allowed us to identify further granular distinctions within sequence effect\nand hesitation-halts deficits. In the following, we have used these features to\ntrain machine learning classifiers to estimate the Movement Disorder Society\nUnified Parkinson Disease Rating Scale (MDS-UPDRS) finger-tapping score.\nCompared to state-of-the-art approaches, our method achieves a higher accuracy\nin MDS-UPDRS score prediction, while still providing an interpretable\nquantification of individual finger-tapping motor characteristics. In summary,\nthe proposed framework provides a practical solution for the objective\nassessment of PD motor characteristics, that can potentially be applied in both\nclinical and remote settings. Future work is needed to assess its\nresponsiveness to symptomatic treatment and disease progression.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19046", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19046", "abs": "https://arxiv.org/abs/2506.19046", "authors": ["Filip Sabo", "Michele Meroni", "Maria Piles", "Martin Claverie", "Fanie Ferreira", "Elna Van Den Berg", "Francesco Collivignarelli", "Felix Rembold"], "title": "From Rows to Yields: How Foundation Models for Tabular Data Simplify Crop Yield Prediction", "comment": null, "summary": "We present an application of a foundation model for small- to medium-sized\ntabular data (TabPFN), to sub-national yield forecasting task in South Africa.\nTabPFN has recently demonstrated superior performance compared to traditional\nmachine learning (ML) models in various regression and classification tasks. We\nused the dekadal (10-days) time series of Earth Observation (EO; FAPAR and soil\nmoisture) and gridded weather data (air temperature, precipitation and\nradiation) to forecast the yield of summer crops at the sub-national level. The\ncrop yield data was available for 23 years and for up to 8 provinces. Covariate\nvariables for TabPFN (i.e., EO and weather) were extracted by region and\naggregated at a monthly scale. We benchmarked the results of the TabPFN against\nsix ML models and three baseline models. Leave-one-year-out cross-validation\nexperiment setting was used in order to ensure the assessment of the models\ncapacity to forecast an unseen year. Results showed that TabPFN and ML models\nexhibit comparable accuracy, outperforming the baselines. Nonetheless, TabPFN\ndemonstrated superior practical utility due to its significantly faster tuning\ntime and reduced requirement for feature engineering. This renders TabPFN a\nmore viable option for real-world operation yield forecasting applications,\nwhere efficiency and ease of implementation are paramount.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19415", "categories": ["cs.GR", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.19415", "abs": "https://arxiv.org/abs/2506.19415", "authors": ["Jonathan Haberl", "Philipp Fleck", "Clemens Arth"], "title": "Virtual Memory for 3D Gaussian Splatting", "comment": "Based on the Master Thesis from Jonathan Haberl from 2024, Submitted\n  to TVCG in Feb. 2025;", "summary": "3D Gaussian Splatting represents a breakthrough in the field of novel view\nsynthesis. It establishes Gaussians as core rendering primitives for highly\naccurate real-world environment reconstruction. Recent advances have\ndrastically increased the size of scenes that can be created. In this work, we\npresent a method for rendering large and complex 3D Gaussian Splatting scenes\nusing virtual memory. By leveraging well-established virtual memory and virtual\ntexturing techniques, our approach efficiently identifies visible Gaussians and\ndynamically streams them to the GPU just in time for real-time rendering.\nSelecting only the necessary Gaussians for both storage and rendering results\nin reduced memory usage and effectively accelerates rendering, especially for\nhighly complex scenes. Furthermore, we demonstrate how level of detail can be\nintegrated into our proposed method to further enhance rendering speed for\nlarge-scale scenes. With an optimized implementation, we highlight key\npractical considerations and thoroughly evaluate the proposed technique and its\nimpact on desktop and mobile devices.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19708", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19708", "abs": "https://arxiv.org/abs/2506.19708", "authors": ["Matyas Bohacek", "Thomas Fel", "Maneesh Agrawala", "Ekdeep Singh Lubana"], "title": "Uncovering Conceptual Blindspots in Generative Image Models Using Sparse Autoencoders", "comment": null, "summary": "Despite their impressive performance, generative image models trained on\nlarge-scale datasets frequently fail to produce images with seemingly simple\nconcepts -- e.g., human hands or objects appearing in groups of four -- that\nare reasonably expected to appear in the training data. These failure modes\nhave largely been documented anecdotally, leaving open the question of whether\nthey reflect idiosyncratic anomalies or more structural limitations of these\nmodels. To address this, we introduce a systematic approach for identifying and\ncharacterizing \"conceptual blindspots\" -- concepts present in the training data\nbut absent or misrepresented in a model's generations. Our method leverages\nsparse autoencoders (SAEs) to extract interpretable concept embeddings,\nenabling a quantitative comparison of concept prevalence between real and\ngenerated images. We train an archetypal SAE (RA-SAE) on DINOv2 features with\n32,000 concepts -- the largest such SAE to date -- enabling fine-grained\nanalysis of conceptual disparities. Applied to four popular generative models\n(Stable Diffusion 1.5/2.1, PixArt, and Kandinsky), our approach reveals\nspecific suppressed blindspots (e.g., bird feeders, DVD discs, and whitespaces\non documents) and exaggerated blindspots (e.g., wood background texture and\npalm trees). At the individual datapoint level, we further isolate memorization\nartifacts -- instances where models reproduce highly specific visual templates\nseen during training. Overall, we propose a theoretically grounded framework\nfor systematically identifying conceptual blindspots in generative models by\nassessing their conceptual fidelity with respect to the underlying\ndata-generating process.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.18943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18943", "abs": "https://arxiv.org/abs/2506.18943", "authors": ["Andrew Kiruluta", "Priscilla Burity"], "title": "From Pixels and Words to Waves: A Unified Framework for Spectral Dictionary vLLMs", "comment": null, "summary": "Vision-language models (VLMs) unify computer vision and natural language\nprocessing in a single architecture capable of interpreting and describing\nimages. Most state-of-the-art systems rely on two computationally intensive\ncomponents: convolutions in the vision encoder and quadratic self-attention for\nmultimodal fusion. This work removes both by introducing a spectral dictionary\ntoken mixer, which represents each image patch or wordpiece as a sparse\ncombination of learnable frequency atoms. Our 1.1B-parameter prototype,\nSDict-VLM, achieves BLEU-4 of 39.2, CIDEr of 127.5, and SPICE of 27.0 on\nMS-COCO captioning, along with 50.3 percent accuracy on VQAv2. These results\nclose approximately 85 percent of the performance gap to BLIP-2 while using 60\npercent fewer parameters, 2.3 times less peak GPU memory, and 2.2 times faster\ninference than PaLI-3. To our knowledge, this is the first VLM to eliminate\nboth convolutions and self-attention while matching mid-scale transformer\nbaselines. In addition to its O(L log L) complexity, the shared frequency\ndictionary enables transparent cross-modal alignment and offers a tunable\ntrade-off between accuracy and compute, paving the way for efficient and\ninterpretable VLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.18946", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18946", "abs": "https://arxiv.org/abs/2506.18946", "authors": ["Zhe Dong", "Yuzhe Sun", "Tianzhu Liu", "Yanfeng Gu"], "title": "DiffRIS: Enhancing Referring Remote Sensing Image Segmentation with Pre-trained Text-to-Image Diffusion Models", "comment": null, "summary": "Referring remote sensing image segmentation (RRSIS) enables the precise\ndelineation of regions within remote sensing imagery through natural language\ndescriptions, serving critical applications in disaster response, urban\ndevelopment, and environmental monitoring. Despite recent advances, current\napproaches face significant challenges in processing aerial imagery due to\ncomplex object characteristics including scale variations, diverse\norientations, and semantic ambiguities inherent to the overhead perspective. To\naddress these limitations, we propose DiffRIS, a novel framework that harnesses\nthe semantic understanding capabilities of pre-trained text-to-image diffusion\nmodels for enhanced cross-modal alignment in RRSIS tasks. Our framework\nintroduces two key innovations: a context perception adapter (CP-adapter) that\ndynamically refines linguistic features through global context modeling and\nobject-aware reasoning, and a progressive cross-modal reasoning decoder (PCMRD)\nthat iteratively aligns textual descriptions with visual regions for precise\nsegmentation. The CP-adapter bridges the domain gap between general\nvision-language understanding and remote sensing applications, while PCMRD\nenables fine-grained semantic alignment through multi-scale feature\ninteraction. Comprehensive experiments on three benchmark datasets-RRSIS-D,\nRefSegRS, and RISBench-demonstrate that DiffRIS consistently outperforms\nexisting methods across all standard metrics, establishing a new\nstate-of-the-art for RRSIS tasks. The significant performance improvements\nvalidate the effectiveness of leveraging pre-trained diffusion models for\nremote sensing applications through our proposed adaptive framework.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19065", "categories": ["cs.CV", "cs.DL"], "pdf": "https://arxiv.org/pdf/2506.19065", "abs": "https://arxiv.org/abs/2506.19065", "authors": ["Guang Yang", "Victoria Ebert", "Nazif Tamer", "Luiza Pozzobon", "Noah A. Smith"], "title": "LEGATO: Large-scale End-to-end Generalizable Approach to Typeset OMR", "comment": null, "summary": "We propose Legato, a new end-to-end transformer model for optical music\nrecognition (OMR). Legato is the first large-scale pretrained OMR model capable\nof recognizing full-page or multi-page typeset music scores and the first to\ngenerate documents in ABC notation, a concise, human-readable format for\nsymbolic music. Bringing together a pretrained vision encoder with an ABC\ndecoder trained on a dataset of more than 214K images, our model exhibits the\nstrong ability to generalize across various typeset scores. We conduct\nexperiments on a range of datasets and demonstrate that our model achieves\nstate-of-the-art performance. Given the lack of a standardized evaluation for\nend-to-end OMR, we comprehensively compare our model against the previous state\nof the art using a diverse set of metrics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19325", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19325", "abs": "https://arxiv.org/abs/2506.19325", "authors": ["Hyein Seo", "Taewook Hwang", "Yohan Lee", "sangkeun Jung"], "title": "FEAT: A Preference Feedback Dataset through a Cost-Effective Auto-Generation and Labeling Framework for English AI Tutoring", "comment": "ACL 2025 (Short)", "summary": "In English education tutoring, teacher feedback is essential for guiding\nstudents. Recently, AI-based tutoring systems have emerged to assist teachers;\nhowever, these systems require high-quality and large-scale teacher feedback\ndata, which is both time-consuming and costly to generate manually. In this\nstudy, we propose FEAT, a cost-effective framework for generating teacher\nfeedback, and have constructed three complementary datasets: (1) DIRECT-Manual\n(DM), where both humans and large language models (LLMs) collaboratively\ngenerate high-quality teacher feedback, albeit at a higher cost; (2)\nDIRECT-Generated (DG), an LLM-only generated, cost-effective dataset with lower\nquality;, and (3) DIRECT-Augmented (DA), primarily based on DG with a small\nportion of DM added to enhance quality while maintaining cost-efficiency.\nExperimental results showed that incorporating a small portion of DM (5-10%)\ninto DG leads to superior performance compared to using 100% DM alone.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19079", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.19079", "abs": "https://arxiv.org/abs/2506.19079", "authors": ["Iosif Tsangko", "Andreas Triantafyllopoulos", "Adem Abdelmoula", "Adria Mallol-Ragolta", "Bjoern W. Schuller"], "title": "Reading Smiles: Proxy Bias in Foundation Models for Facial Emotion Recognition", "comment": null, "summary": "Foundation Models (FMs) are rapidly transforming Affective Computing (AC),\nwith Vision Language Models (VLMs) now capable of recognising emotions in zero\nshot settings. This paper probes a critical but underexplored question: what\nvisual cues do these models rely on to infer affect, and are these cues\npsychologically grounded or superficially learnt? We benchmark varying scale\nVLMs on a teeth annotated subset of AffectNet dataset and find consistent\nperformance shifts depending on the presence of visible teeth. Through\nstructured introspection of, the best-performing model, i.e., GPT-4o, we show\nthat facial attributes like eyebrow position drive much of its affective\nreasoning, revealing a high degree of internal consistency in its\nvalence-arousal predictions. These patterns highlight the emergent nature of\nFMs behaviour, but also reveal risks: shortcut learning, bias, and fairness\nissues especially in sensitive domains like mental health and education.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19382", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19382", "abs": "https://arxiv.org/abs/2506.19382", "authors": ["Ruben Härle", "Felix Friedrich", "Manuel Brack", "Stephan Wäldchen", "Björn Deiseroth", "Patrick Schramowski", "Kristian Kersting"], "title": "Measuring and Guiding Monosemanticity", "comment": null, "summary": "There is growing interest in leveraging mechanistic interpretability and\ncontrollability to better understand and influence the internal dynamics of\nlarge language models (LLMs). However, current methods face fundamental\nchallenges in reliably localizing and manipulating feature representations.\nSparse Autoencoders (SAEs) have recently emerged as a promising direction for\nfeature extraction at scale, yet they, too, are limited by incomplete feature\nisolation and unreliable monosemanticity. To systematically quantify these\nlimitations, we introduce Feature Monosemanticity Score (FMS), a novel metric\nto quantify feature monosemanticity in latent representation. Building on these\ninsights, we propose Guided Sparse Autoencoders (G-SAE), a method that\nconditions latent representations on labeled concepts during training. We\ndemonstrate that reliable localization and disentanglement of target concepts\nwithin the latent space improve interpretability, detection of behavior, and\ncontrol. Specifically, our evaluations on toxicity detection, writing style\nidentification, and privacy attribute recognition show that G-SAE not only\nenhances monosemanticity but also enables more effective and fine-grained\nsteering with less quality degradation. Our findings provide actionable\nguidelines for measuring and advancing mechanistic interpretability and control\nof LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19087", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19087", "abs": "https://arxiv.org/abs/2506.19087", "authors": ["Bowen Zhang", "Jesse T. Boulerice", "Nikhil Kuniyil", "Charvi Mendiratta", "Satish Kumar", "Hila Shamon", "B. S. Manjunath"], "title": "RareSpot: Spotting Small and Rare Wildlife in Aerial Imagery with Multi-Scale Consistency and Context-Aware Augmentation", "comment": "Accepted to the CVPR 2025 Workshop on Computer Vision for Animal\n  Behavior Tracking and Modeling (CV4Animals)", "summary": "Automated detection of small and rare wildlife in aerial imagery is crucial\nfor effective conservation, yet remains a significant technical challenge.\nPrairie dogs exemplify this issue: their ecological importance as keystone\nspecies contrasts sharply with their elusive presence--marked by small size,\nsparse distribution, and subtle visual features--which undermines existing\ndetection approaches. To address these challenges, we propose RareSpot, a\nrobust detection framework integrating multi-scale consistency learning and\ncontext-aware augmentation. Our multi-scale consistency approach leverages\nstructured alignment across feature pyramids, enhancing fine-grained object\nrepresentation and mitigating scale-related feature loss. Complementarily,\ncontext-aware augmentation strategically synthesizes challenging training\ninstances by embedding difficult-to-detect samples into realistic environmental\ncontexts, significantly boosting model precision and recall. Evaluated on an\nexpert-annotated prairie dog drone imagery benchmark, our method achieves\nstate-of-the-art performance, improving detection accuracy by over 35% compared\nto baseline methods. Importantly, it generalizes effectively across additional\nwildlife datasets, demonstrating broad applicability. The RareSpot benchmark\nand approach not only support critical ecological monitoring but also establish\na new foundation for detecting small, rare species in complex aerial scenes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19117", "abs": "https://arxiv.org/abs/2506.19117", "authors": ["Christina Ourania Tze", "Daniel Dauner", "Yiyi Liao", "Dzmitry Tsishkou", "Andreas Geiger"], "title": "PrITTI: Primitive-based Generation of Controllable and Editable 3D Semantic Scenes", "comment": "Project page: https://raniatze.github.io/pritti/", "summary": "Large-scale 3D semantic scene generation has predominantly relied on\nvoxel-based representations, which are memory-intensive, bound by fixed\nresolutions, and challenging to edit. In contrast, primitives represent\nsemantic entities using compact, coarse 3D structures that are easy to\nmanipulate and compose, making them an ideal representation for this task. In\nthis paper, we introduce PrITTI, a latent diffusion-based framework that\nleverages primitives as the main foundational elements for generating\ncompositional, controllable, and editable 3D semantic scene layouts. Our method\nadopts a hybrid representation, modeling ground surfaces in a rasterized format\nwhile encoding objects as vectorized 3D primitives. This decomposition is also\nreflected in a structured latent representation that enables flexible scene\nmanipulation of ground and object components. To overcome the orientation\nambiguities in conventional encoding methods, we introduce a stable\nCholesky-based parameterization that jointly encodes object size and\norientation. Experiments on the KITTI-360 dataset show that PrITTI outperforms\na voxel-based baseline in generation quality, while reducing memory\nrequirements by up to $3\\times$. In addition, PrITTI enables direct\ninstance-level manipulation of objects in the scene and supports a range of\ndownstream applications, including scene inpainting, outpainting, and\nphoto-realistic street-view synthesis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19208", "abs": "https://arxiv.org/abs/2506.19208", "authors": ["Xiaolei Diao", "Rite Bo", "Yanling Xiao", "Lida Shi", "Zhihan Zhou", "Hao Xu", "Chuntao Li", "Xiongfeng Tang", "Massimo Poesio", "Cédric M. John", "Daqian Shi"], "title": "Ancient Script Image Recognition and Processing: A Review", "comment": null, "summary": "Ancient scripts, e.g., Egyptian hieroglyphs, Oracle Bone Inscriptions, and\nAncient Greek inscriptions, serve as vital carriers of human civilization,\nembedding invaluable historical and cultural information. Automating ancient\nscript image recognition has gained importance, enabling large-scale\ninterpretation and advancing research in archaeology and digital humanities.\nWith the rise of deep learning, this field has progressed rapidly, with\nnumerous script-specific datasets and models proposed. While these scripts vary\nwidely, spanning phonographic systems with limited glyphs to logographic\nsystems with thousands of complex symbols, they share common challenges and\nmethodological overlaps. Moreover, ancient scripts face unique challenges,\nincluding imbalanced data distribution and image degradation, which have driven\nthe development of various dedicated methods. This survey provides a\ncomprehensive review of ancient script image recognition methods. We begin by\ncategorizing existing studies based on script types and analyzing respective\nrecognition methods, highlighting both their differences and shared strategies.\nWe then focus on challenges unique to ancient scripts, systematically examining\ntheir impact and reviewing recent solutions, including few-shot learning and\nnoise-robust techniques. Finally, we summarize current limitations and outline\npromising future directions. Our goal is to offer a structured, forward-looking\nperspective to support ongoing advancements in the recognition, interpretation,\nand decipherment of ancient scripts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19283", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19283", "abs": "https://arxiv.org/abs/2506.19283", "authors": ["Xiangbo Gao", "Yuheng Wu", "Xuewen Luo", "Keshu Wu", "Xinghao Chen", "Yuping Wang", "Chenxi Liu", "Yang Zhou", "Zhengzhong Tu"], "title": "AirV2X: Unified Air-Ground Vehicle-to-Everything Collaboration", "comment": null, "summary": "While multi-vehicular collaborative driving demonstrates clear advantages\nover single-vehicle autonomy, traditional infrastructure-based V2X systems\nremain constrained by substantial deployment costs and the creation of\n\"uncovered danger zones\" in rural and suburban areas. We present\nAirV2X-Perception, a large-scale dataset that leverages Unmanned Aerial\nVehicles (UAVs) as a flexible alternative or complement to fixed Road-Side\nUnits (RSUs). Drones offer unique advantages over ground-based perception:\ncomplementary bird's-eye-views that reduce occlusions, dynamic positioning\ncapabilities that enable hovering, patrolling, and escorting navigation rules,\nand significantly lower deployment costs compared to fixed infrastructure. Our\ndataset comprises 6.73 hours of drone-assisted driving scenarios across urban,\nsuburban, and rural environments with varied weather and lighting conditions.\nThe AirV2X-Perception dataset facilitates the development and standardized\nevaluation of Vehicle-to-Drone (V2D) algorithms, addressing a critical gap in\nthe rapidly expanding field of aerial-assisted autonomous driving systems. The\ndataset and development kits are open-sourced at\nhttps://github.com/taco-group/AirV2X-Perception.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19288", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19288", "abs": "https://arxiv.org/abs/2506.19288", "authors": ["Runwei Guan", "Ningwei Ouyang", "Tianhao Xu", "Shaofeng Liang", "Wei Dai", "Yafeng Sun", "Shang Gao", "Songning Lai", "Shanliang Yao", "Xuming Hu", "Ryan Wen Liu", "Yutao Yue", "Hui Xiong"], "title": "Da Yu: Towards USV-Based Image Captioning for Waterway Surveillance and Scene Understanding", "comment": "14 pages, 13 figures", "summary": "Automated waterway environment perception is crucial for enabling unmanned\nsurface vessels (USVs) to understand their surroundings and make informed\ndecisions. Most existing waterway perception models primarily focus on\ninstance-level object perception paradigms (e.g., detection, segmentation).\nHowever, due to the complexity of waterway environments, current perception\ndatasets and models fail to achieve global semantic understanding of waterways,\nlimiting large-scale monitoring and structured log generation. With the\nadvancement of vision-language models (VLMs), we leverage image captioning to\nintroduce WaterCaption, the first captioning dataset specifically designed for\nwaterway environments. WaterCaption focuses on fine-grained, multi-region\nlong-text descriptions, providing a new research direction for visual\ngeo-understanding and spatial scene cognition. Exactly, it includes 20.2k\nimage-text pair data with 1.8 million vocabulary size. Additionally, we propose\nDa Yu, an edge-deployable multi-modal large language model for USVs, where we\npropose a novel vision-to-language projector called Nano Transformer Adaptor\n(NTA). NTA effectively balances computational efficiency with the capacity for\nboth global and fine-grained local modeling of visual features, thereby\nsignificantly enhancing the model's ability to generate long-form textual\noutputs. Da Yu achieves an optimal balance between performance and efficiency,\nsurpassing state-of-the-art models on WaterCaption and several other captioning\nbenchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19291", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19291", "abs": "https://arxiv.org/abs/2506.19291", "authors": ["Xiaoyuan Wang", "Yizhou Zhao", "Botao Ye", "Xiaojun Shan", "Weijie Lyu", "Lu Qi", "Kelvin C. K. Chan", "Yinxiao Li", "Ming-Hsuan Yang"], "title": "HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis", "comment": null, "summary": "We propose HoliGS, a novel deformable Gaussian splatting framework that\naddresses embodied view synthesis from long monocular RGB videos. Unlike prior\n4D Gaussian splatting and dynamic NeRF pipelines, which struggle with training\noverhead in minute-long captures, our method leverages invertible Gaussian\nSplatting deformation networks to reconstruct large-scale, dynamic environments\naccurately. Specifically, we decompose each scene into a static background plus\ntime-varying objects, each represented by learned Gaussian primitives\nundergoing global rigid transformations, skeleton-driven articulation, and\nsubtle non-rigid deformations via an invertible neural flow. This hierarchical\nwarping strategy enables robust free-viewpoint novel-view rendering from\nvarious embodied camera trajectories by attaching Gaussians to a complete\ncanonical foreground shape (\\eg, egocentric or third-person follow), which may\ninvolve substantial viewpoint changes and interactions between multiple actors.\nOur experiments demonstrate that \\ourmethod~ achieves superior reconstruction\nquality on challenging datasets while significantly reducing both training and\nrendering time compared to state-of-the-art monocular deformable NeRFs. These\nresults highlight a practical and scalable solution for EVS in real-world\nscenarios. The source code will be released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19783", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19783", "abs": "https://arxiv.org/abs/2506.19783", "authors": ["Teng Wang", "Hailei Gong", "Changwang Zhang", "Jun Wang"], "title": "SAGE: Strategy-Adaptive Generation Engine for Query Rewriting", "comment": null, "summary": "Query rewriting is pivotal for enhancing dense retrieval, yet current methods\ndemand large-scale supervised data or suffer from inefficient reinforcement\nlearning (RL) exploration. In this work, we first establish that guiding Large\nLanguage Models (LLMs) with a concise set of expert-crafted strategies, such as\nsemantic expansion and entity disambiguation, substantially improves retrieval\neffectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus,\nand SciFact. Building on this insight, we introduce the Strategy-Adaptive\nGeneration Engine (SAGE), which operationalizes these strategies in an RL\nframework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit\nShaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative\nlearning signals. This strategy-guided approach not only achieves new\nstate-of-the-art NDCG@10 results, but also uncovers a compelling emergent\nbehavior: the agent learns to select optimal strategies, reduces unnecessary\nexploration, and generates concise rewrites, lowering inference cost without\nsacrificing performance. Our findings demonstrate that strategy-guided RL,\nenhanced with nuanced reward shaping, offers a scalable, efficient, and more\ninterpretable paradigm for developing the next generation of robust information\nretrieval systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.04689", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.04689", "abs": "https://arxiv.org/abs/2506.04689", "authors": ["Thao Nguyen", "Yang Li", "Olga Golovneva", "Luke Zettlemoyer", "Sewoong Oh", "Ludwig Schmidt", "Xian Li"], "title": "Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models", "comment": null, "summary": "Scaling laws predict that the performance of large language models improves\nwith increasing model size and data size. In practice, pre-training has been\nrelying on massive web crawls, using almost all data sources publicly available\non the internet so far. However, this pool of natural data does not grow at the\nsame rate as the compute supply. Furthermore, the availability of high-quality\ntexts is even more limited: data filtering pipelines often remove up to 99% of\nthe initial web scrapes to achieve state-of-the-art. To address the \"data wall\"\nof pre-training scaling, our work explores ways to transform and recycle data\ndiscarded in existing filtering processes. We propose REWIRE, REcycling the Web\nwith guIded REwrite, a method to enrich low-quality documents so that they\ncould become useful for training. This in turn allows us to increase the\nrepresentation of synthetic data in the final pre-training set. Experiments at\n1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw\ntexts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points\nimprovement respectively across 22 diverse tasks, compared to training on only\nfiltered web data. Training on the raw-synthetic data mix is also more\neffective than having access to 2x web data. Through further analysis, we\ndemonstrate that about 82% of the mixed in texts come from transforming\nlower-quality documents that would otherwise be discarded. REWIRE also\noutperforms related approaches of generating synthetic data, including\nWikipedia-style paraphrasing, question-answer synthesizing and knowledge\nextraction. These results suggest that recycling web texts holds the potential\nfor being a simple and effective approach for scaling pre-training data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19331", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19331", "abs": "https://arxiv.org/abs/2506.19331", "authors": ["Hongyu Wu", "Pengwan Yang", "Yuki M. Asano", "Cees G. M. Snoek"], "title": "Segment Any 3D-Part in a Scene from a Sentence", "comment": null, "summary": "This paper aims to achieve the segmentation of any 3D part in a scene based\non natural language descriptions, extending beyond traditional object-level 3D\nscene understanding and addressing both data and methodological challenges. Due\nto the expensive acquisition and annotation burden, existing datasets and\nmethods are predominantly limited to object-level comprehension. To overcome\nthe limitations of data and annotation availability, we introduce the 3D-PU\ndataset, the first large-scale 3D dataset with dense part annotations, created\nthrough an innovative and cost-effective method for constructing synthetic 3D\nscenes with fine-grained part-level annotations, paving the way for advanced\n3D-part scene understanding. On the methodological side, we propose OpenPart3D,\na 3D-input-only framework to effectively tackle the challenges of part-level\nsegmentation. Extensive experiments demonstrate the superiority of our approach\nin open-vocabulary 3D scene understanding tasks at the part level, with strong\ngeneralization capabilities across various 3D scene datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "fine-grained"], "score": 3}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.18919", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18919", "abs": "https://arxiv.org/abs/2506.18919", "authors": ["Hexiang Gu", "Qifan Yu", "Saihui Hou", "Zhiqin Fang", "Huijia Wu", "Zhaofeng He"], "title": "MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection", "comment": null, "summary": "The rapid development of social media has intensified the spread of harmful\ncontent. Harmful memes, which integrate both images and text, pose significant\nchallenges for automated detection due to their implicit semantics and complex\nmultimodal interactions. Although existing research has made progress in\ndetection accuracy and interpretability, the lack of a systematic, large-scale,\ndiverse, and highly explainable dataset continues to hinder further advancement\nin this field. To address this gap, we introduce MemeMind, a novel dataset\nfeaturing scientifically rigorous standards, large scale, diversity, bilingual\nsupport (Chinese and English), and detailed Chain-of-Thought (CoT) annotations.\nMemeMind fills critical gaps in current datasets by offering comprehensive\nlabeling and explicit reasoning traces, thereby providing a solid foundation\nfor enhancing harmful meme detection. In addition, we propose an innovative\ndetection framework, MemeGuard, which effectively integrates multimodal\ninformation with reasoning process modeling, significantly improving models'\nability to understand and identify harmful memes. Extensive experiments\nconducted on the MemeMind dataset demonstrate that MemeGuard consistently\noutperforms existing state-of-the-art methods in harmful meme detection tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.18925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18925", "abs": "https://arxiv.org/abs/2506.18925", "authors": ["Tahereh Zarrat Ehsan", "Michael Tangermann", "Yağmur Güçlütürk", "Bastiaan R. Bloem", "Luc J. W. Evers"], "title": "Interpretable and Granular Video-Based Quantification of Motor Characteristics from the Finger Tapping Test in Parkinson Disease", "comment": null, "summary": "Accurately quantifying motor characteristics in Parkinson disease (PD) is\ncrucial for monitoring disease progression and optimizing treatment strategies.\nThe finger-tapping test is a standard motor assessment. Clinicians visually\nevaluate a patient's tapping performance and assign an overall severity score\nbased on tapping amplitude, speed, and irregularity. However, this subjective\nevaluation is prone to inter- and intra-rater variability, and does not offer\ninsights into individual motor characteristics captured during this test. This\npaper introduces a granular computer vision-based method for quantifying PD\nmotor characteristics from video recordings. Four sets of clinically relevant\nfeatures are proposed to characterize hypokinesia, bradykinesia, sequence\neffect, and hesitation-halts. We evaluate our approach on video recordings and\nclinical evaluations of 74 PD patients from the Personalized Parkinson Project.\nPrincipal component analysis with varimax rotation shows that the video-based\nfeatures corresponded to the four deficits. Additionally, video-based analysis\nhas allowed us to identify further granular distinctions within sequence effect\nand hesitation-halts deficits. In the following, we have used these features to\ntrain machine learning classifiers to estimate the Movement Disorder Society\nUnified Parkinson Disease Rating Scale (MDS-UPDRS) finger-tapping score.\nCompared to state-of-the-art approaches, our method achieves a higher accuracy\nin MDS-UPDRS score prediction, while still providing an interpretable\nquantification of individual finger-tapping motor characteristics. In summary,\nthe proposed framework provides a practical solution for the objective\nassessment of PD motor characteristics, that can potentially be applied in both\nclinical and remote settings. Future work is needed to assess its\nresponsiveness to symptomatic treatment and disease progression.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19348", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19348", "abs": "https://arxiv.org/abs/2506.19348", "authors": ["Jintao Rong", "Xin Xie", "Xinyi Yu", "Linlin Ou", "Xinyu Zhang", "Chunhua Shen", "Dong Gong"], "title": "Training-Free Motion Customization for Distilled Video Generators with Adaptive Test-Time Distillation", "comment": null, "summary": "Distilled video generation models offer fast and efficient synthesis but\nstruggle with motion customization when guided by reference videos, especially\nunder training-free settings. Existing training-free methods, originally\ndesigned for standard diffusion models, fail to generalize due to the\naccelerated generative process and large denoising steps in distilled models.\nTo address this, we propose MotionEcho, a novel training-free test-time\ndistillation framework that enables motion customization by leveraging\ndiffusion teacher forcing. Our approach uses high-quality, slow teacher models\nto guide the inference of fast student models through endpoint prediction and\ninterpolation. To maintain efficiency, we dynamically allocate computation\nacross timesteps according to guidance needs. Extensive experiments across\nvarious distilled video generation models and benchmark datasets demonstrate\nthat our method significantly improves motion fidelity and generation quality\nwhile preserving high efficiency. Project page:\nhttps://euminds.github.io/motionecho/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.18946", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18946", "abs": "https://arxiv.org/abs/2506.18946", "authors": ["Zhe Dong", "Yuzhe Sun", "Tianzhu Liu", "Yanfeng Gu"], "title": "DiffRIS: Enhancing Referring Remote Sensing Image Segmentation with Pre-trained Text-to-Image Diffusion Models", "comment": null, "summary": "Referring remote sensing image segmentation (RRSIS) enables the precise\ndelineation of regions within remote sensing imagery through natural language\ndescriptions, serving critical applications in disaster response, urban\ndevelopment, and environmental monitoring. Despite recent advances, current\napproaches face significant challenges in processing aerial imagery due to\ncomplex object characteristics including scale variations, diverse\norientations, and semantic ambiguities inherent to the overhead perspective. To\naddress these limitations, we propose DiffRIS, a novel framework that harnesses\nthe semantic understanding capabilities of pre-trained text-to-image diffusion\nmodels for enhanced cross-modal alignment in RRSIS tasks. Our framework\nintroduces two key innovations: a context perception adapter (CP-adapter) that\ndynamically refines linguistic features through global context modeling and\nobject-aware reasoning, and a progressive cross-modal reasoning decoder (PCMRD)\nthat iteratively aligns textual descriptions with visual regions for precise\nsegmentation. The CP-adapter bridges the domain gap between general\nvision-language understanding and remote sensing applications, while PCMRD\nenables fine-grained semantic alignment through multi-scale feature\ninteraction. Comprehensive experiments on three benchmark datasets-RRSIS-D,\nRefSegRS, and RISBench-demonstrate that DiffRIS consistently outperforms\nexisting methods across all standard metrics, establishing a new\nstate-of-the-art for RRSIS tasks. The significant performance improvements\nvalidate the effectiveness of leveraging pre-trained diffusion models for\nremote sensing applications through our proposed adaptive framework.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19433", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19433", "abs": "https://arxiv.org/abs/2506.19433", "authors": ["Lixuan He", "Haoyu Dong", "Zhenxing Chen", "Yangcheng Yu", "Jie Feng", "Yong Li"], "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System", "comment": null, "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19406", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19406", "abs": "https://arxiv.org/abs/2506.19406", "authors": ["Chen Yi", "Shan LianLei"], "title": "A Global-Local Cross-Attention Network for Ultra-high Resolution Remote Sensing Image Semantic Segmentation", "comment": null, "summary": "With the rapid development of ultra-high resolution (UHR) remote sensing\ntechnology, the demand for accurate and efficient semantic segmentation has\nincreased significantly. However, existing methods face challenges in\ncomputational efficiency and multi-scale feature fusion. To address these\nissues, we propose GLCANet (Global-Local Cross-Attention Network), a\nlightweight segmentation framework designed for UHR remote sensing\nimagery.GLCANet employs a dual-stream architecture to efficiently fuse global\nsemantics and local details while minimizing GPU usage. A self-attention\nmechanism enhances long-range dependencies, refines global features, and\npreserves local details for better semantic consistency. A masked\ncross-attention mechanism also adaptively fuses global-local features,\nselectively enhancing fine-grained details while exploiting global context to\nimprove segmentation accuracy. Experimental results show that GLCANet\noutperforms state-of-the-art methods regarding accuracy and computational\nefficiency. The model effectively processes large, high-resolution images with\na small memory footprint, providing a promising solution for real-world remote\nsensing applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19037", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.NE", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.19037", "abs": "https://arxiv.org/abs/2506.19037", "authors": ["Omer Luxembourg", "Haim Permuter", "Eliya Nachmani"], "title": "Plan for Speed -- Dilated Scheduling for Masked Diffusion Language Models", "comment": null, "summary": "Masked diffusion language models (MDLM) have shown strong promise for\nnon-autoregressive text generation, yet existing samplers act as implicit\nplanners, selecting tokens to unmask via denoiser confidence or entropy scores.\nSuch heuristics falter under parallel unmasking - they ignore pairwise\ninteractions between tokens and cannot account for dependencies when unmasking\nmultiple positions at once, limiting their inference time to traditional\nauto-regressive (AR) models. We introduce the Dilated-scheduled Unmasking\nStrategy (DUS), an inference-only, planner-model-free method that requires no\nadditional training. DUS leverages a first-order Markov assumption to partition\nsequence positions into dilation-based groups of non-adjacent tokens, enabling\nindependent, parallel unmasking steps that respect local context that minimizes\nthe joint entropy of each iteration step. Unlike semi-AR block approaches\n(e.g., LLADA and Dream) that still invoke the denoiser per block, DUS reduces\nthe number of denoiser calls to O(log B) per generation block - yielding\nsubstantial speedup over the O(B) run time of state-of-the-art diffusion\nmodels, where B is the block size in the semi-AR inference process. In\nexperiments on math (GSM8K) and code completion (Humaneval, MBPP) benchmarks -\ndomains suited to non-ordinal generation - DUS improves scores over parallel\nconfidence-based planner, without modifying the underlying denoiser. DUS offers\na lightweight, budget-aware approach to efficient, high-quality text\ngeneration, paving the way to unlock the true capabilities of MDLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19433", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19433", "abs": "https://arxiv.org/abs/2506.19433", "authors": ["Lixuan He", "Haoyu Dong", "Zhenxing Chen", "Yangcheng Yu", "Jie Feng", "Yong Li"], "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System", "comment": null, "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19079", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.19079", "abs": "https://arxiv.org/abs/2506.19079", "authors": ["Iosif Tsangko", "Andreas Triantafyllopoulos", "Adem Abdelmoula", "Adria Mallol-Ragolta", "Bjoern W. Schuller"], "title": "Reading Smiles: Proxy Bias in Foundation Models for Facial Emotion Recognition", "comment": null, "summary": "Foundation Models (FMs) are rapidly transforming Affective Computing (AC),\nwith Vision Language Models (VLMs) now capable of recognising emotions in zero\nshot settings. This paper probes a critical but underexplored question: what\nvisual cues do these models rely on to infer affect, and are these cues\npsychologically grounded or superficially learnt? We benchmark varying scale\nVLMs on a teeth annotated subset of AffectNet dataset and find consistent\nperformance shifts depending on the presence of visible teeth. Through\nstructured introspection of, the best-performing model, i.e., GPT-4o, we show\nthat facial attributes like eyebrow position drive much of its affective\nreasoning, revealing a high degree of internal consistency in its\nvalence-arousal predictions. These patterns highlight the emergent nature of\nFMs behaviour, but also reveal risks: shortcut learning, bias, and fairness\nissues especially in sensitive domains like mental health and education.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19442", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19442", "abs": "https://arxiv.org/abs/2506.19442", "authors": ["Róisín Luo", "James McDermott", "Colm O'Riordan"], "title": "Sampling Matters in Explanations: Towards Trustworthy Attribution Analysis Building Block in Visual Models through Maximizing Explanation Certainty", "comment": "Code:\n  https://anonymous.4open.science/r/sampling_matters_reproducibility-BB60/", "summary": "Image attribution analysis seeks to highlight the feature representations\nlearned by visual models such that the highlighted feature maps can reflect the\npixel-wise importance of inputs. Gradient integration is a building block in\nthe attribution analysis by integrating the gradients from multiple derived\nsamples to highlight the semantic features relevant to inferences. Such a\nbuilding block often combines with other information from visual models such as\nactivation or attention maps to form ultimate explanations. Yet, our\ntheoretical analysis demonstrates that the extent to the alignment of the\nsample distribution in gradient integration with respect to natural image\ndistribution gives a lower bound of explanation certainty. Prior works add\nnoise into images as samples and the noise distributions can lead to low\nexplanation certainty. Counter-intuitively, our experiment shows that extra\ninformation can saturate neural networks. To this end, building trustworthy\nattribution analysis needs to settle the sample distribution misalignment\nproblem. Instead of adding extra information into input images, we present a\nsemi-optimal sampling approach by suppressing features from inputs. The sample\ndistribution by suppressing features is approximately identical to the\ndistribution of natural images. Our extensive quantitative evaluation on large\nscale dataset ImageNet affirms that our approach is effective and able to yield\nmore satisfactory explanations against state-of-the-art baselines throughout\nall experimental models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19087", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19087", "abs": "https://arxiv.org/abs/2506.19087", "authors": ["Bowen Zhang", "Jesse T. Boulerice", "Nikhil Kuniyil", "Charvi Mendiratta", "Satish Kumar", "Hila Shamon", "B. S. Manjunath"], "title": "RareSpot: Spotting Small and Rare Wildlife in Aerial Imagery with Multi-Scale Consistency and Context-Aware Augmentation", "comment": "Accepted to the CVPR 2025 Workshop on Computer Vision for Animal\n  Behavior Tracking and Modeling (CV4Animals)", "summary": "Automated detection of small and rare wildlife in aerial imagery is crucial\nfor effective conservation, yet remains a significant technical challenge.\nPrairie dogs exemplify this issue: their ecological importance as keystone\nspecies contrasts sharply with their elusive presence--marked by small size,\nsparse distribution, and subtle visual features--which undermines existing\ndetection approaches. To address these challenges, we propose RareSpot, a\nrobust detection framework integrating multi-scale consistency learning and\ncontext-aware augmentation. Our multi-scale consistency approach leverages\nstructured alignment across feature pyramids, enhancing fine-grained object\nrepresentation and mitigating scale-related feature loss. Complementarily,\ncontext-aware augmentation strategically synthesizes challenging training\ninstances by embedding difficult-to-detect samples into realistic environmental\ncontexts, significantly boosting model precision and recall. Evaluated on an\nexpert-annotated prairie dog drone imagery benchmark, our method achieves\nstate-of-the-art performance, improving detection accuracy by over 35% compared\nto baseline methods. Importantly, it generalizes effectively across additional\nwildlife datasets, demonstrating broad applicability. The RareSpot benchmark\nand approach not only support critical ecological monitoring but also establish\na new foundation for detecting small, rare species in complex aerial scenes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19848", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19848", "abs": "https://arxiv.org/abs/2506.19848", "authors": ["Long Xing", "Qidong Huang", "Xiaoyi Dong", "Pan Zhang", "Yuhang Zang", "Yuhang Cao", "Jinsong Li", "Shuangrui Ding", "Weiming Zhang", "Nenghai Yu", "Jiaqi Wang", "Feng Wu", "Dahua Lin"], "title": "ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing", "comment": "Code is available at https://github.com/Cooperx521/ScaleCap", "summary": "This paper presents ScaleCap, an inference-time scalable image captioning\nstrategy that generates comprehensive and detailed image captions. The key\nchallenges of high-quality image captioning lie in the inherent biases of\nLVLMs: multimodal bias resulting in imbalanced descriptive granularity,\noffering detailed accounts of some elements while merely skimming over others;\nlinguistic bias leading to hallucinated descriptions of non-existent objects.\nTo address these issues, we propose a scalable debiased captioning strategy,\nwhich continuously enriches and calibrates the caption with increased inference\nbudget. Specifically, we propose two novel components: heuristic question\nanswering and contrastive sentence rating. The former generates\ncontent-specific questions based on the image and answers them to progressively\ninject relevant information into the caption. The latter employs sentence-level\noffline contrastive decoding to effectively identify and eliminate\nhallucinations caused by linguistic biases. With increased inference cost, more\nheuristic questions are raised by ScaleCap to progressively capture additional\nvisual details, generating captions that are more accurate, balanced, and\ninformative. Extensive modality alignment experiments demonstrate the\neffectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them\nfor LVLM pretraining leads to consistent performance gains across 11 widely\nused benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity\nof generated captions with two additional tasks: replacing images with captions\nin VQA task, and reconstructing images from captions to assess semantic\ncoverage. Code is available at https://github.com/Cooperx521/ScaleCap.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19472", "categories": ["cs.CV", "I.4.6"], "pdf": "https://arxiv.org/pdf/2506.19472", "abs": "https://arxiv.org/abs/2506.19472", "authors": ["Lin Hong", "Xin Wang", "Yihao Li", "Xia Wang"], "title": "USIS16K: High-Quality Dataset for Underwater Salient Instance Segmentation", "comment": "8 pages 10 figures", "summary": "Inspired by the biological visual system that selectively allocates attention\nto efficiently identify salient objects or regions, underwater salient instance\nsegmentation (USIS) aims to jointly address the problems of where to look\n(saliency prediction) and what is there (instance segmentation) in underwater\nscenarios. However, USIS remains an underexplored challenge due to the\ninaccessibility and dynamic nature of underwater environments, as well as the\nscarcity of large-scale, high-quality annotated datasets. In this paper, we\nintroduce USIS16K, a large-scale dataset comprising 16,151 high-resolution\nunderwater images collected from diverse environmental settings and covering\n158 categories of underwater objects. Each image is annotated with high-quality\ninstance-level salient object masks, representing a significant advance in\nterms of diversity, complexity, and scalability. Furthermore, we provide\nbenchmark evaluations on underwater object detection and USIS tasks using\nUSIS16K. To facilitate future research in this domain, the dataset and\nbenchmark models are publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19283", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19283", "abs": "https://arxiv.org/abs/2506.19283", "authors": ["Xiangbo Gao", "Yuheng Wu", "Xuewen Luo", "Keshu Wu", "Xinghao Chen", "Yuping Wang", "Chenxi Liu", "Yang Zhou", "Zhengzhong Tu"], "title": "AirV2X: Unified Air-Ground Vehicle-to-Everything Collaboration", "comment": null, "summary": "While multi-vehicular collaborative driving demonstrates clear advantages\nover single-vehicle autonomy, traditional infrastructure-based V2X systems\nremain constrained by substantial deployment costs and the creation of\n\"uncovered danger zones\" in rural and suburban areas. We present\nAirV2X-Perception, a large-scale dataset that leverages Unmanned Aerial\nVehicles (UAVs) as a flexible alternative or complement to fixed Road-Side\nUnits (RSUs). Drones offer unique advantages over ground-based perception:\ncomplementary bird's-eye-views that reduce occlusions, dynamic positioning\ncapabilities that enable hovering, patrolling, and escorting navigation rules,\nand significantly lower deployment costs compared to fixed infrastructure. Our\ndataset comprises 6.73 hours of drone-assisted driving scenarios across urban,\nsuburban, and rural environments with varied weather and lighting conditions.\nThe AirV2X-Perception dataset facilitates the development and standardized\nevaluation of Vehicle-to-Drone (V2D) algorithms, addressing a critical gap in\nthe rapidly expanding field of aerial-assisted autonomous driving systems. The\ndataset and development kits are open-sourced at\nhttps://github.com/taco-group/AirV2X-Perception.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19474", "abs": "https://arxiv.org/abs/2506.19474", "authors": ["Xin Zhang", "Liangxiu Han", "Yue Shi", "Yanlin Zheng", "Alam Uazman", "Maryam Ferdousi", "Rayaz Malik"], "title": "HMSViT: A Hierarchical Masked Self-Supervised Vision Transformer for Corneal Nerve Segmentation and Diabetic Neuropathy Diagnosis", "comment": null, "summary": "Diabetic Peripheral Neuropathy (DPN) affects nearly half of diabetes\npatients, requiring early detection. Corneal Confocal Microscopy (CCM) enables\nnon-invasive diagnosis, but automated methods suffer from inefficient feature\nextraction, reliance on handcrafted priors, and data limitations. We propose\nHMSViT, a novel Hierarchical Masked Self-Supervised Vision Transformer (HMSViT)\ndesigned for corneal nerve segmentation and DPN diagnosis. Unlike existing\nmethods, HMSViT employs pooling-based hierarchical and dual attention\nmechanisms with absolute positional encoding, enabling efficient multi-scale\nfeature extraction by capturing fine-grained local details in early layers and\nintegrating global context in deeper layers, all at a lower computational cost.\nA block-masked self supervised learning framework is designed for the HMSViT\nthat reduces reliance on labelled data, enhancing feature robustness, while a\nmulti-scale decoder is used for segmentation and classification by fusing\nhierarchical features. Experiments on clinical CCM datasets showed HMSViT\nachieves state-of-the-art performance, with 61.34% mIoU for nerve segmentation\nand 70.40% diagnostic accuracy, outperforming leading hierarchical models like\nthe Swin Transformer and HiViT by margins of up to 6.39% in segmentation\naccuracy while using fewer parameters. Detailed ablation studies further reveal\nthat integrating block-masked SSL with hierarchical multi-scale feature\nextraction substantially enhances performance compared to conventional\nsupervised training. Overall, these comprehensive experiments confirm that\nHMSViT delivers excellent, robust, and clinically viable results, demonstrating\nits potential for scalable deployment in real-world diagnostic applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19406", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19406", "abs": "https://arxiv.org/abs/2506.19406", "authors": ["Chen Yi", "Shan LianLei"], "title": "A Global-Local Cross-Attention Network for Ultra-high Resolution Remote Sensing Image Semantic Segmentation", "comment": null, "summary": "With the rapid development of ultra-high resolution (UHR) remote sensing\ntechnology, the demand for accurate and efficient semantic segmentation has\nincreased significantly. However, existing methods face challenges in\ncomputational efficiency and multi-scale feature fusion. To address these\nissues, we propose GLCANet (Global-Local Cross-Attention Network), a\nlightweight segmentation framework designed for UHR remote sensing\nimagery.GLCANet employs a dual-stream architecture to efficiently fuse global\nsemantics and local details while minimizing GPU usage. A self-attention\nmechanism enhances long-range dependencies, refines global features, and\npreserves local details for better semantic consistency. A masked\ncross-attention mechanism also adaptively fuses global-local features,\nselectively enhancing fine-grained details while exploiting global context to\nimprove segmentation accuracy. Experimental results show that GLCANet\noutperforms state-of-the-art methods regarding accuracy and computational\nefficiency. The model effectively processes large, high-resolution images with\na small memory footprint, providing a promising solution for real-world remote\nsensing applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19433", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19433", "abs": "https://arxiv.org/abs/2506.19433", "authors": ["Lixuan He", "Haoyu Dong", "Zhenxing Chen", "Yangcheng Yu", "Jie Feng", "Yong Li"], "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System", "comment": null, "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19656", "categories": ["cs.CV", "cs.DL", "eess.IV", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2506.19656", "abs": "https://arxiv.org/abs/2506.19656", "authors": ["Oscar J. Pellicer-Valero", "Cesar Aybar", "Gustau Camps Valls"], "title": "Video Compression for Spatiotemporal Earth System Data", "comment": null, "summary": "Large-scale Earth system datasets, from high-resolution remote sensing\nimagery to spatiotemporal climate model outputs, exhibit characteristics\nanalogous to those of standard videos. Their inherent spatial, temporal, and\nspectral redundancies can thus be readily exploited by established video\ncompression techniques. Here, we present xarrayvideo, a Python library for\ncompressing multichannel spatiotemporal datasets by encoding them as videos.\nOur approach achieves compression ratios of up to 250x while maintaining high\nfidelity by leveraging standard, well-optimized video codecs through ffmpeg. We\ndemonstrate the library's effectiveness on four real-world multichannel\nspatiotemporal datasets: DynamicEarthNet (very high resolution Planet images),\nDeepExtremeCubes (high resolution Sentinel-2 images), ERA5 (weather reanalysis\ndata), and the SimpleS2 dataset (high resolution multichannel Sentinel-2\nimages), achieving Peak Signal-to-Noise Ratios (PSNRs) of 55.86, 40.60, 46.58,\nand 43.23 dB at 0.1 bits per pixel per band (bpppb) and 65.91, 54.28, 62.90,\nand 55.04 dB at 1 bpppb. We are redistributing two of these datasets,\nDeepExtremeCubes (2.3 Tb) and DynamicEarthNet (525 Gb), in the\nmachine-learning-ready and cloud-ready TACO format through HuggingFace at\nsignificantly reduced sizes (270 Gb and 8.5 Gb, respectively) without\ncompromising quality (PSNR 55.77-56.65 and 60.15). No performance loss is\nobserved when the compressed versions of these datasets are used in their\nrespective deep learning-based downstream tasks (next step reflectance\nprediction and landcover segmentation). In conclusion, xarrayvideo presents an\nefficient solution for handling the rapidly growing size of Earth observation\ndatasets, making advanced compression techniques accessible and practical to\nthe Earth science community. The library is available for use at\nhttps://github.com/IPL-UV/xarrayvideo", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19694", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19694", "abs": "https://arxiv.org/abs/2506.19694", "authors": ["Yue Zhou", "Yuan Bi", "Wenjuan Tong", "Wei Wang", "Nassir Navab", "Zhongliang Jiang"], "title": "UltraAD: Fine-Grained Ultrasound Anomaly Classification via Few-Shot CLIP Adaptation", "comment": null, "summary": "Precise anomaly detection in medical images is critical for clinical\ndecision-making. While recent unsupervised or semi-supervised anomaly detection\nmethods trained on large-scale normal data show promising results, they lack\nfine-grained differentiation, such as benign vs. malignant tumors.\nAdditionally, ultrasound (US) imaging is highly sensitive to devices and\nacquisition parameter variations, creating significant domain gaps in the\nresulting US images. To address these challenges, we propose UltraAD, a\nvision-language model (VLM)-based approach that leverages few-shot US examples\nfor generalized anomaly localization and fine-grained classification. To\nenhance localization performance, the image-level token of query visual\nprototypes is first fused with learnable text embeddings. This image-informed\nprompt feature is then further integrated with patch-level tokens, refining\nlocal representations for improved accuracy. For fine-grained classification, a\nmemory bank is constructed from few-shot image samples and corresponding text\ndescriptions that capture anatomical and abnormality-specific features. During\ntraining, the stored text embeddings remain frozen, while image features are\nadapted to better align with medical data. UltraAD has been extensively\nevaluated on three breast US datasets, outperforming state-of-the-art methods\nin both lesion localization and fine-grained medical classification. The code\nwill be released upon acceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19708", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19708", "abs": "https://arxiv.org/abs/2506.19708", "authors": ["Matyas Bohacek", "Thomas Fel", "Maneesh Agrawala", "Ekdeep Singh Lubana"], "title": "Uncovering Conceptual Blindspots in Generative Image Models Using Sparse Autoencoders", "comment": null, "summary": "Despite their impressive performance, generative image models trained on\nlarge-scale datasets frequently fail to produce images with seemingly simple\nconcepts -- e.g., human hands or objects appearing in groups of four -- that\nare reasonably expected to appear in the training data. These failure modes\nhave largely been documented anecdotally, leaving open the question of whether\nthey reflect idiosyncratic anomalies or more structural limitations of these\nmodels. To address this, we introduce a systematic approach for identifying and\ncharacterizing \"conceptual blindspots\" -- concepts present in the training data\nbut absent or misrepresented in a model's generations. Our method leverages\nsparse autoencoders (SAEs) to extract interpretable concept embeddings,\nenabling a quantitative comparison of concept prevalence between real and\ngenerated images. We train an archetypal SAE (RA-SAE) on DINOv2 features with\n32,000 concepts -- the largest such SAE to date -- enabling fine-grained\nanalysis of conceptual disparities. Applied to four popular generative models\n(Stable Diffusion 1.5/2.1, PixArt, and Kandinsky), our approach reveals\nspecific suppressed blindspots (e.g., bird feeders, DVD discs, and whitespaces\non documents) and exaggerated blindspots (e.g., wood background texture and\npalm trees). At the individual datapoint level, we further isolate memorization\nartifacts -- instances where models reproduce highly specific visual templates\nseen during training. Overall, we propose a theoretically grounded framework\nfor systematically identifying conceptual blindspots in generative models by\nassessing their conceptual fidelity with respect to the underlying\ndata-generating process.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19838", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19838", "abs": "https://arxiv.org/abs/2506.19838", "authors": ["Liangbin Xie", "Yu Li", "Shian Du", "Menghan Xia", "Xintao Wang", "Fanghua Yu", "Ziyan Chen", "Pengfei Wan", "Jiantao Zhou", "Chao Dong"], "title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution", "comment": "Project webpage available at https://simplegvr.github.io/", "summary": "Latent diffusion models have emerged as a leading paradigm for efficient\nvideo generation. However, as user expectations shift toward higher-resolution\noutputs, relying solely on latent computation becomes inadequate. A promising\napproach involves decoupling the process into two stages: semantic content\ngeneration and detail synthesis. The former employs a computationally intensive\nbase model at lower resolutions, while the latter leverages a lightweight\ncascaded video super-resolution (VSR) model to achieve high-resolution output.\nIn this work, we focus on studying key design principles for latter cascaded\nVSR models, which are underexplored currently. First, we propose two\ndegradation strategies to generate training pairs that better mimic the output\ncharacteristics of the base model, ensuring alignment between the VSR model and\nits upstream generator. Second, we provide critical insights into VSR model\nbehavior through systematic analysis of (1) timestep sampling strategies, (2)\nnoise augmentation effects on low-resolution (LR) inputs. These findings\ndirectly inform our architectural and training innovations. Finally, we\nintroduce interleaving temporal unit and sparse local attention to achieve\nefficient training and inference, drastically reducing computational overhead.\nExtensive experiments demonstrate the superiority of our framework over\nexisting methods, with ablation studies confirming the efficacy of each design\nchoice. Our work establishes a simple yet effective baseline for cascaded video\nsuper-resolution generation, offering practical insights to guide future\nadvancements in efficient cascaded synthesis systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19839", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19839", "abs": "https://arxiv.org/abs/2506.19839", "authors": ["Moayed Haji-Ali", "Willi Menapace", "Ivan Skorokhodov", "Arpit Sahni", "Sergey Tulyakov", "Vicente Ordonez", "Aliaksandr Siarohin"], "title": "Improving Progressive Generation with Decomposable Flow Matching", "comment": "Project Webpage: https://snap-research.github.io/dfm/", "summary": "Generating high-dimensional visual modalities is a computationally intensive\ntask. A common solution is progressive generation, where the outputs are\nsynthesized in a coarse-to-fine spectral autoregressive manner. While diffusion\nmodels benefit from the coarse-to-fine nature of denoising, explicit\nmulti-stage architectures are rarely adopted. These architectures have\nincreased the complexity of the overall approach, introducing the need for a\ncustom diffusion formulation, decomposition-dependent stage transitions,\nadd-hoc samplers, or a model cascade. Our contribution, Decomposable Flow\nMatching (DFM), is a simple and effective framework for the progressive\ngeneration of visual media. DFM applies Flow Matching independently at each\nlevel of a user-defined multi-scale representation (such as Laplacian pyramid).\nAs shown by our experiments, our approach improves visual quality for both\nimages and videos, featuring superior results compared to prior multistage\nframeworks. On Imagenet-1k 512px, DFM achieves 35.2% improvements in FDD scores\nover the base architecture and 26.4% over the best-performing baseline, under\nthe same training compute. When applied to finetuning of large models, such as\nFLUX, DFM shows faster convergence speed to the training distribution.\nCrucially, all these advantages are achieved with a single model, architectural\nsimplicity, and minimal modifications to existing training pipelines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19840", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19840", "abs": "https://arxiv.org/abs/2506.19840", "authors": ["Zekun Li", "Rui Zhou", "Rahul Sajnani", "Xiaoyan Cong", "Daniel Ritchie", "Srinath Sridhar"], "title": "GenHSI: Controllable Generation of Human-Scene Interaction Videos", "comment": null, "summary": "Large-scale pre-trained video diffusion models have exhibited remarkable\ncapabilities in diverse video generation. However, existing solutions face\nseveral challenges in using these models to generate long movie-like videos\nwith rich human-object interactions that include unrealistic human-scene\ninteraction, lack of subject identity preservation, and require expensive\ntraining. We propose GenHSI, a training-free method for controllable generation\nof long human-scene interaction videos (HSI). Taking inspiration from movie\nanimation, our key insight is to overcome the limitations of previous work by\nsubdividing the long video generation task into three stages: (1) script\nwriting, (2) pre-visualization, and (3) animation. Given an image of a scene, a\nuser description, and multiple images of a person, we use these three stages to\ngenerate long-videos that preserve human-identity and provide rich human-scene\ninteractions. Script writing converts complex human tasks into simple atomic\ntasks that are used in the pre-visualization stage to generate 3D keyframes\n(storyboards). These 3D keyframes are rendered and animated by off-the-shelf\nvideo diffusion models for consistent long video generation with rich contacts\nin a 3D-aware manner. A key advantage of our work is that we alleviate the need\nfor scanned, accurate scenes and create 3D keyframes from single-view images.\nWe are the first to generate a long video sequence with a consistent camera\npose that contains arbitrary numbers of character actions without training.\nExperiments demonstrate that our method can generate long videos that\neffectively preserve scene content and character identity with plausible\nhuman-scene interaction from a single image scene. Visit our project homepage\nhttps://kunkun0w0.github.io/project/GenHSI/ for more information.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19839", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19839", "abs": "https://arxiv.org/abs/2506.19839", "authors": ["Moayed Haji-Ali", "Willi Menapace", "Ivan Skorokhodov", "Arpit Sahni", "Sergey Tulyakov", "Vicente Ordonez", "Aliaksandr Siarohin"], "title": "Improving Progressive Generation with Decomposable Flow Matching", "comment": "Project Webpage: https://snap-research.github.io/dfm/", "summary": "Generating high-dimensional visual modalities is a computationally intensive\ntask. A common solution is progressive generation, where the outputs are\nsynthesized in a coarse-to-fine spectral autoregressive manner. While diffusion\nmodels benefit from the coarse-to-fine nature of denoising, explicit\nmulti-stage architectures are rarely adopted. These architectures have\nincreased the complexity of the overall approach, introducing the need for a\ncustom diffusion formulation, decomposition-dependent stage transitions,\nadd-hoc samplers, or a model cascade. Our contribution, Decomposable Flow\nMatching (DFM), is a simple and effective framework for the progressive\ngeneration of visual media. DFM applies Flow Matching independently at each\nlevel of a user-defined multi-scale representation (such as Laplacian pyramid).\nAs shown by our experiments, our approach improves visual quality for both\nimages and videos, featuring superior results compared to prior multistage\nframeworks. On Imagenet-1k 512px, DFM achieves 35.2% improvements in FDD scores\nover the base architecture and 26.4% over the best-performing baseline, under\nthe same training compute. When applied to finetuning of large models, such as\nFLUX, DFM shows faster convergence speed to the training distribution.\nCrucially, all these advantages are achieved with a single model, architectural\nsimplicity, and minimal modifications to existing training pipelines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19848", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19848", "abs": "https://arxiv.org/abs/2506.19848", "authors": ["Long Xing", "Qidong Huang", "Xiaoyi Dong", "Pan Zhang", "Yuhang Zang", "Yuhang Cao", "Jinsong Li", "Shuangrui Ding", "Weiming Zhang", "Nenghai Yu", "Jiaqi Wang", "Feng Wu", "Dahua Lin"], "title": "ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing", "comment": "Code is available at https://github.com/Cooperx521/ScaleCap", "summary": "This paper presents ScaleCap, an inference-time scalable image captioning\nstrategy that generates comprehensive and detailed image captions. The key\nchallenges of high-quality image captioning lie in the inherent biases of\nLVLMs: multimodal bias resulting in imbalanced descriptive granularity,\noffering detailed accounts of some elements while merely skimming over others;\nlinguistic bias leading to hallucinated descriptions of non-existent objects.\nTo address these issues, we propose a scalable debiased captioning strategy,\nwhich continuously enriches and calibrates the caption with increased inference\nbudget. Specifically, we propose two novel components: heuristic question\nanswering and contrastive sentence rating. The former generates\ncontent-specific questions based on the image and answers them to progressively\ninject relevant information into the caption. The latter employs sentence-level\noffline contrastive decoding to effectively identify and eliminate\nhallucinations caused by linguistic biases. With increased inference cost, more\nheuristic questions are raised by ScaleCap to progressively capture additional\nvisual details, generating captions that are more accurate, balanced, and\ninformative. Extensive modality alignment experiments demonstrate the\neffectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them\nfor LVLM pretraining leads to consistent performance gains across 11 widely\nused benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity\nof generated captions with two additional tasks: replacing images with captions\nin VQA task, and reconstructing images from captions to assess semantic\ncoverage. Code is available at https://github.com/Cooperx521/ScaleCap.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19850", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19850", "abs": "https://arxiv.org/abs/2506.19850", "authors": ["Yuqi Wang", "Xinghang Li", "Wenxuan Wang", "Junbo Zhang", "Yingyan Li", "Yuntao Chen", "Xinlong Wang", "Zhaoxiang Zhang"], "title": "Unified Vision-Language-Action Model", "comment": "technical report", "summary": "Vision-language-action models (VLAs) have garnered significant attention for\ntheir potential in advancing robotic manipulation. However, previous approaches\npredominantly rely on the general comprehension capabilities of vision-language\nmodels (VLMs) to generate action signals, often overlooking the rich temporal\nand causal structure embedded in visual observations. In this paper, we present\nUniVLA, a unified and native multimodal VLA model that autoregressively models\nvision, language, and action signals as discrete token sequences. This\nformulation enables flexible multimodal tasks learning, particularly from\nlarge-scale video data. By incorporating world modeling during post-training,\nUniVLA captures causal dynamics from videos, facilitating effective transfer to\ndownstream policy learning--especially for long-horizon tasks. Our approach\nsets new state-of-the-art results across several widely used simulation\nbenchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly\nsurpassing previous methods. For example, UniVLA achieves 95.5% average success\nrate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate\nits broad applicability on real-world ALOHA manipulation and autonomous\ndriving.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.18919", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18919", "abs": "https://arxiv.org/abs/2506.18919", "authors": ["Hexiang Gu", "Qifan Yu", "Saihui Hou", "Zhiqin Fang", "Huijia Wu", "Zhaofeng He"], "title": "MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection", "comment": null, "summary": "The rapid development of social media has intensified the spread of harmful\ncontent. Harmful memes, which integrate both images and text, pose significant\nchallenges for automated detection due to their implicit semantics and complex\nmultimodal interactions. Although existing research has made progress in\ndetection accuracy and interpretability, the lack of a systematic, large-scale,\ndiverse, and highly explainable dataset continues to hinder further advancement\nin this field. To address this gap, we introduce MemeMind, a novel dataset\nfeaturing scientifically rigorous standards, large scale, diversity, bilingual\nsupport (Chinese and English), and detailed Chain-of-Thought (CoT) annotations.\nMemeMind fills critical gaps in current datasets by offering comprehensive\nlabeling and explicit reasoning traces, thereby providing a solid foundation\nfor enhancing harmful meme detection. In addition, we propose an innovative\ndetection framework, MemeGuard, which effectively integrates multimodal\ninformation with reasoning process modeling, significantly improving models'\nability to understand and identify harmful memes. Extensive experiments\nconducted on the MemeMind dataset demonstrate that MemeGuard consistently\noutperforms existing state-of-the-art methods in harmful meme detection tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19139", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19139", "abs": "https://arxiv.org/abs/2506.19139", "authors": ["Lukas Radl", "Felix Windisch", "Thomas Deixelberger", "Jozef Hladky", "Michael Steiner", "Dieter Schmalstieg", "Markus Steinberger"], "title": "SOF: Sorted Opacity Fields for Fast Unbounded Surface Reconstruction", "comment": null, "summary": "Recent advances in 3D Gaussian representations have significantly improved\nthe quality and efficiency of image-based scene reconstruction. Their explicit\nnature facilitates real-time rendering and fast optimization, yet extracting\naccurate surfaces - particularly in large-scale, unbounded environments -\nremains a difficult task. Many existing methods rely on approximate depth\nestimates and global sorting heuristics, which can introduce artifacts and\nlimit the fidelity of the reconstructed mesh. In this paper, we present Sorted\nOpacity Fields (SOF), a method designed to recover detailed surfaces from 3D\nGaussians with both speed and precision. Our approach improves upon prior work\nby introducing hierarchical resorting and a robust formulation of Gaussian\ndepth, which better aligns with the level-set. To enhance mesh quality, we\nincorporate a level-set regularizer operating on the opacity field and\nintroduce losses that encourage geometrically-consistent primitive shapes. In\naddition, we develop a parallelized Marching Tetrahedra algorithm tailored to\nour opacity formulation, reducing meshing time by up to an order of magnitude.\nAs demonstrated by our quantitative evaluation, SOF achieves higher\nreconstruction accuracy while cutting total processing time by more than a\nfactor of three. These results mark a step forward in turning efficient\nGaussian-based rendering into equally efficient geometry extraction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19415", "categories": ["cs.GR", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.19415", "abs": "https://arxiv.org/abs/2506.19415", "authors": ["Jonathan Haberl", "Philipp Fleck", "Clemens Arth"], "title": "Virtual Memory for 3D Gaussian Splatting", "comment": "Based on the Master Thesis from Jonathan Haberl from 2024, Submitted\n  to TVCG in Feb. 2025;", "summary": "3D Gaussian Splatting represents a breakthrough in the field of novel view\nsynthesis. It establishes Gaussians as core rendering primitives for highly\naccurate real-world environment reconstruction. Recent advances have\ndrastically increased the size of scenes that can be created. In this work, we\npresent a method for rendering large and complex 3D Gaussian Splatting scenes\nusing virtual memory. By leveraging well-established virtual memory and virtual\ntexturing techniques, our approach efficiently identifies visible Gaussians and\ndynamically streams them to the GPU just in time for real-time rendering.\nSelecting only the necessary Gaussians for both storage and rendering results\nin reduced memory usage and effectively accelerates rendering, especially for\nhighly complex scenes. Furthermore, we demonstrate how level of detail can be\nintegrated into our proposed method to further enhance rendering speed for\nlarge-scale scenes. With an optimized implementation, we highlight key\npractical considerations and thoroughly evaluate the proposed technique and its\nimpact on desktop and mobile devices.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
{"id": "2506.19708", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19708", "abs": "https://arxiv.org/abs/2506.19708", "authors": ["Matyas Bohacek", "Thomas Fel", "Maneesh Agrawala", "Ekdeep Singh Lubana"], "title": "Uncovering Conceptual Blindspots in Generative Image Models Using Sparse Autoencoders", "comment": null, "summary": "Despite their impressive performance, generative image models trained on\nlarge-scale datasets frequently fail to produce images with seemingly simple\nconcepts -- e.g., human hands or objects appearing in groups of four -- that\nare reasonably expected to appear in the training data. These failure modes\nhave largely been documented anecdotally, leaving open the question of whether\nthey reflect idiosyncratic anomalies or more structural limitations of these\nmodels. To address this, we introduce a systematic approach for identifying and\ncharacterizing \"conceptual blindspots\" -- concepts present in the training data\nbut absent or misrepresented in a model's generations. Our method leverages\nsparse autoencoders (SAEs) to extract interpretable concept embeddings,\nenabling a quantitative comparison of concept prevalence between real and\ngenerated images. We train an archetypal SAE (RA-SAE) on DINOv2 features with\n32,000 concepts -- the largest such SAE to date -- enabling fine-grained\nanalysis of conceptual disparities. Applied to four popular generative models\n(Stable Diffusion 1.5/2.1, PixArt, and Kandinsky), our approach reveals\nspecific suppressed blindspots (e.g., bird feeders, DVD discs, and whitespaces\non documents) and exaggerated blindspots (e.g., wood background texture and\npalm trees). At the individual datapoint level, we further isolate memorization\nartifacts -- instances where models reproduce highly specific visual templates\nseen during training. Overall, we propose a theoretically grounded framework\nfor systematically identifying conceptual blindspots in generative models by\nassessing their conceptual fidelity with respect to the underlying\ndata-generating process.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-25.jsonl"}
