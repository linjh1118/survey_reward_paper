{"id": "2507.07424", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07424", "abs": "https://arxiv.org/abs/2507.07424", "authors": ["Jingjing Jiang", "Chao Ma", "Xurui Song", "Hanwang Zhang", "Jun Luo"], "title": "Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning", "comment": "ICCV 2025", "summary": "Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated exceptional performance in multimodal perception and\nunderstanding. However, leading open-source MLLMs exhibit significant\nlimitations in complex and structured reasoning, particularly in tasks\nrequiring deep reasoning for decision-making and problem-solving. In this work,\nwe present Corvid, an MLLM with enhanced chain-of-thought (CoT) reasoning\ncapabilities. Architecturally, Corvid incorporates a hybrid vision encoder for\ninformative visual representation and a meticulously designed connector\n(GateMixer) to facilitate cross-modal alignment. To enhance Corvid's CoT\nreasoning capabilities, we introduce MCoT-Instruct-287K, a high-quality\nmultimodal CoT instruction-following dataset, refined and standardized from\ndiverse public reasoning sources. Leveraging this dataset, we fine-tune Corvid\nwith a two-stage CoT-formatted training approach to progressively enhance its\nstep-by-step reasoning abilities. Furthermore, we propose an effective\ninference-time scaling strategy that enables Corvid to mitigate over-reasoning\nand under-reasoning through self-verification. Extensive experiments\ndemonstrate that Corvid outperforms existing o1-like MLLMs and state-of-the-art\nMLLMs with similar parameter scales, with notable strengths in mathematical\nreasoning and science problem-solving. Project page:\nhttps://mm-vl.github.io/corvid.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "o1", "self-verification"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07544", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07544", "abs": "https://arxiv.org/abs/2507.07544", "authors": ["Oliver Eberle", "Thomas McGee", "Hamza Giaffar", "Taylor Webb", "Ida Momennejad"], "title": "Position: We Need An Algorithmic Understanding of Generative AI", "comment": "Accepted at ICML 2025 as a Spotlight Position Paper", "summary": "What algorithms do LLMs actually learn and use to solve problems? Studies\naddressing this question are sparse, as research priorities are focused on\nimproving performance through scale, leaving a theoretical and empirical gap in\nunderstanding emergent algorithms. This position paper proposes AlgEval: a\nframework for systematic research into the algorithms that LLMs learn and use.\nAlgEval aims to uncover algorithmic primitives, reflected in latent\nrepresentations, attention, and inference-time compute, and their algorithmic\ncomposition to solve task-specific problems. We highlight potential\nmethodological paths and a case study toward this goal, focusing on emergent\nsearch algorithms. Our case study illustrates both the formation of top-down\nhypotheses about candidate algorithms, and bottom-up tests of these hypotheses\nvia circuit-level analysis of attention patterns and hidden states. The\nrigorous, systematic evaluation of how LLMs actually solve tasks provides an\nalternative to resource-intensive scaling, reorienting the field toward a\nprincipled understanding of underlying computations. Such algorithmic\nexplanations offer a pathway to human-understandable interpretability, enabling\ncomprehension of the model's internal reasoning performance measures. This can\nin turn lead to more sample-efficient methods for training and improving\nperformance, as well as novel architectures for end-to-end and multi-agent\nsystems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "scale"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07931", "categories": ["cs.AI", "cs.CY", "I.2.0; K.4.1"], "pdf": "https://arxiv.org/pdf/2507.07931", "abs": "https://arxiv.org/abs/2507.07931", "authors": ["Hans Gundlach", "Jayson Lynch", "Neil Thompson"], "title": "Meek Models Shall Inherit the Earth", "comment": "13 pages, 9 figures, longer version of the paper presented at TAIG\n  ICML 2025", "summary": "The past decade has seen incredible scaling of AI systems by a few companies,\nleading to inequality in AI model performance. This paper argues that, contrary\nto prevailing intuition, the diminishing returns to compute scaling will lead\nto a convergence of AI model capabilities. In other words, meek models (those\nwith limited computation budget) shall inherit the earth, approaching the\nperformance level of the best models overall. We develop a model illustrating\nthat under a fixed-distribution next-token objective, the marginal capability\nreturns to raw compute shrink substantially. Given current scaling practices,\nwe argue that these diminishing returns are strong enough that even companies\nthat can scale their models exponentially faster than other organizations will\neventually have little advantage in capabilities. As part of our argument, we\ngive several reasons that proxies like training loss differences capture\nimportant capability measures using evidence from benchmark data and\ntheoretical performance models. In addition, we analyze empirical data on the\ncapability difference of AI models over time. Finally, in light of the\nincreasing ability of meek models, we argue that AI strategy and policy require\nreexamination, and we outline the areas this shift will affect.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale", "compute scaling"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07820", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07820", "abs": "https://arxiv.org/abs/2507.07820", "authors": ["Eunsu Baek", "Keondo Park", "Jeonggil Ko", "Min-hwan Oh", "Taesik Gong", "Hyung-Sin Kim"], "title": "AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift", "comment": null, "summary": "Current AI advances largely rely on scaling neural models and expanding\ntraining datasets to achieve generalization and robustness. Despite notable\nsuccesses, this paradigm incurs significant environmental, economic, and\nethical costs, limiting sustainability and equitable access. Inspired by\nbiological sensory systems, where adaptation occurs dynamically at the input\n(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive\nsensing as a necessary and foundational shift. Adaptive sensing proactively\nmodulates sensor parameters (e.g., exposure, sensitivity, multimodal\nconfigurations) at the input level, significantly mitigating covariate shifts\nand improving efficiency. Empirical evidence from recent studies demonstrates\nthat adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass\nsubstantially larger models (e.g., OpenCLIP-H) trained with significantly more\ndata and compute. We (i) outline a roadmap for broadly integrating adaptive\nsensing into real-world applications spanning humanoid, healthcare, autonomous\nsystems, agriculture, and environmental monitoring, (ii) critically assess\ntechnical and ethical integration challenges, and (iii) propose targeted\nresearch directions, such as standardized benchmarks, real-time adaptive\nalgorithms, multimodal integration, and privacy-preserving methods.\nCollectively, these efforts aim to transition the AI community toward\nsustainable, robust, and equitable artificial intelligence systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07634", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07634", "abs": "https://arxiv.org/abs/2507.07634", "authors": ["Abhinav Java", "Srivathsan Koundinyan", "Nagarajan Natarajan", "Amit Sharma"], "title": "FrugalRAG: Learning to retrieve and reason for multi-hop QA", "comment": "Accepted at ICML Workshop: Efficient Systems for Foundation Models", "summary": "We consider the problem of answering complex questions, given access to a\nlarge unstructured document corpus. The de facto approach to solving the\nproblem is to leverage language models that (iteratively) retrieve and reason\nthrough the retrieved documents, until the model has sufficient information to\ngenerate an answer. Attempts at improving this approach focus on\nretrieval-augmented generation (RAG) metrics such as accuracy and recall and\ncan be categorized into two types: (a) fine-tuning on large question answering\n(QA) datasets augmented with chain-of-thought traces, and (b) leveraging\nRL-based fine-tuning techniques that rely on question-document relevance\nsignals. However, efficiency in the number of retrieval searches is an equally\nimportant metric, which has received less attention. In this work, we show\nthat: (1) Large-scale fine-tuning is not needed to improve RAG metrics,\ncontrary to popular claims in recent literature. Specifically, a standard ReAct\npipeline with improved prompts can outperform state-of-the-art methods on\nbenchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help\nRAG from the perspective of frugality, i.e., the latency due to number of\nsearches at inference time. For example, we show that we can achieve\ncompetitive RAG metrics at nearly half the cost (in terms of number of\nsearches) on popular RAG benchmarks, using the same base model, and at a small\ntraining cost (1000 examples).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07966", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07966", "abs": "https://arxiv.org/abs/2507.07966", "authors": ["Yukang Chen", "Wei Huang", "Baifeng Shi", "Qinghao Hu", "Hanrong Ye", "Ligeng Zhu", "Zhijian Liu", "Pavlo Molchanov", "Jan Kautz", "Xiaojuan Qi", "Sifei Liu", "Hongxu Yin", "Yao Lu", "Song Han"], "title": "Scaling RL to Long Videos", "comment": "Code and models are available at https://github.com/NVlabs/Long-RL", "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07966", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07966", "abs": "https://arxiv.org/abs/2507.07966", "authors": ["Yukang Chen", "Wei Huang", "Baifeng Shi", "Qinghao Hu", "Hanrong Ye", "Ligeng Zhu", "Zhijian Liu", "Pavlo Molchanov", "Jan Kautz", "Xiaojuan Qi", "Sifei Liu", "Hongxu Yin", "Yao Lu", "Song Han"], "title": "Scaling RL to Long Videos", "comment": "Code and models are available at https://github.com/NVlabs/Long-RL", "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07995", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07995", "abs": "https://arxiv.org/abs/2507.07995", "authors": ["Shivam Duggal", "Sanghyun Byun", "William T. Freeman", "Antonio Torralba", "Phillip Isola"], "title": "Single-pass Adaptive Image Tokenization for Minimum Program Search", "comment": "Code at: https://github.com/ShivamDuggal4/karl Keywords:\n  Representation Learning, Adaptive Tokenization, Compression, Algorithmic\n  Information Theory, Kolmogorov Complexity, Upside-Down RL", "summary": "According to Algorithmic Information Theory (AIT) -- Intelligent\nrepresentations compress data into the shortest possible program that can\nreconstruct its content, exhibiting low Kolmogorov Complexity (KC). In\ncontrast, most visual representation learning systems use fixed-length\nrepresentations for all inputs, ignoring variations in complexity or\nfamiliarity. Recent adaptive tokenization methods address this by allocating\nvariable-length representations but typically require test-time search over\nmultiple encodings to find the most predictive one. Inspired by Kolmogorov\nComplexity principles, we propose a single-pass adaptive tokenizer, KARL, which\npredicts the appropriate number of tokens for an image in a single forward\npass, halting once its approximate KC is reached. The token count serves as a\nproxy for the minimum description length. KARL's training procedure closely\nresembles the Upside-Down Reinforcement Learning paradigm, as it learns to\nconditionally predict token halting based on a desired reconstruction quality.\nKARL matches the performance of recent adaptive tokenizers while operating in a\nsingle pass. We present scaling laws for KARL, analyzing the role of\nencoder/decoder size, continuous vs. discrete tokenization and more.\nAdditionally, we offer a conceptual study drawing an analogy between Adaptive\nImage Tokenization and Algorithmic Information Theory, examining the predicted\nimage complexity (KC) across axes such as structure vs. noise and in- vs.\nout-of-distribution familiarity -- revealing alignment with human intuition.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07802", "abs": "https://arxiv.org/abs/2507.07802", "authors": ["Zhihui Zhang", "Luanyuan Dai", "Qika Lin", "Yunfeng Diao", "Guangyin Jin", "Yufei Guo", "Jing Zhang", "Xiaoshuai Hao"], "title": "Synergistic Prompting for Robust Visual Recognition with Missing Modalities", "comment": null, "summary": "Large-scale multi-modal models have demonstrated remarkable performance\nacross various visual recognition tasks by leveraging extensive paired\nmulti-modal training data. However, in real-world applications, the presence of\nmissing or incomplete modality inputs often leads to significant performance\ndegradation. Recent research has focused on prompt-based strategies to tackle\nthis issue; however, existing methods are hindered by two major limitations:\n(1) static prompts lack the flexibility to adapt to varying missing-data\nconditions, and (2) basic prompt-tuning methods struggle to ensure reliable\nperformance when critical modalities are missing.To address these challenges,\nwe propose a novel Synergistic Prompting (SyP) framework for robust visual\nrecognition with missing modalities. The proposed SyP introduces two key\ninnovations: (I) a Dynamic Adapter, which computes adaptive scaling factors to\ndynamically generate prompts, replacing static parameters for flexible\nmulti-modal adaptation, and (II) a Synergistic Prompting Strategy, which\ncombines static and dynamic prompts to balance information across modalities,\nensuring robust reasoning even when key modalities are missing. The proposed\nSyP achieves significant performance improvements over existing approaches\nacross three widely-used visual recognition datasets, demonstrating robustness\nunder diverse missing rates and conditions. Extensive experiments and ablation\nstudies validate its effectiveness in handling missing modalities, highlighting\nits superior adaptability and reliability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07908", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07908", "abs": "https://arxiv.org/abs/2507.07908", "authors": ["Xiao Yang", "Yuxuan Fan", "Can Liu", "Houcheng Su", "Weichen Guo", "Jiyao Wang", "Dengbo He"], "title": "Not Only Consistency: Enhance Test-Time Adaptation with Spatio-temporal Inconsistency for Remote Physiological Measurement", "comment": null, "summary": "Remote photoplethysmography (rPPG) has emerged as a promising non-invasive\nmethod for monitoring physiological signals using the camera. Although various\ndomain adaptation and generalization methods were proposed to promote the\nadaptability of deep-based rPPG models in unseen deployment environments,\nconsiderations in aspects like privacy concerns and real-time adaptation\nrestrict their application in real-world deployment. Thus, we aim to propose a\nnovel fully Test-Time Adaptation (TTA) strategy tailored for rPPG tasks in this\nwork. Specifically, based on prior knowledge in physiology and our\nobservations, we noticed not only there is spatio-temporal consistency in the\nfrequency domain of rPPG signals, but also that inconsistency in the time\ndomain was significant. Given this, by leveraging both consistency and\ninconsistency priors, we introduce an innovative expert knowledge-based\nself-supervised\n\\textbf{C}onsistency-\\textbf{i}n\\textbf{C}onsistency-\\textbf{i}ntegration\n(\\textbf{CiCi}) framework to enhances model adaptation during inference.\nBesides, our approach further incorporates a gradient dynamic control mechanism\nto mitigate potential conflicts between priors, ensuring stable adaptation\nacross instances. Through extensive experiments on five diverse datasets under\nthe TTA protocol, our method consistently outperforms existing techniques,\npresenting state-of-the-art performance in real-time self-supervised adaptation\nwithout accessing source data. The code will be released later.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07966", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07966", "abs": "https://arxiv.org/abs/2507.07966", "authors": ["Yukang Chen", "Wei Huang", "Baifeng Shi", "Qinghao Hu", "Hanrong Ye", "Ligeng Zhu", "Zhijian Liu", "Pavlo Molchanov", "Jan Kautz", "Xiaojuan Qi", "Sifei Liu", "Hongxu Yin", "Yao Lu", "Song Han"], "title": "Scaling RL to Long Videos", "comment": "Code and models are available at https://github.com/NVlabs/Long-RL", "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07995", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07995", "abs": "https://arxiv.org/abs/2507.07995", "authors": ["Shivam Duggal", "Sanghyun Byun", "William T. Freeman", "Antonio Torralba", "Phillip Isola"], "title": "Single-pass Adaptive Image Tokenization for Minimum Program Search", "comment": "Code at: https://github.com/ShivamDuggal4/karl Keywords:\n  Representation Learning, Adaptive Tokenization, Compression, Algorithmic\n  Information Theory, Kolmogorov Complexity, Upside-Down RL", "summary": "According to Algorithmic Information Theory (AIT) -- Intelligent\nrepresentations compress data into the shortest possible program that can\nreconstruct its content, exhibiting low Kolmogorov Complexity (KC). In\ncontrast, most visual representation learning systems use fixed-length\nrepresentations for all inputs, ignoring variations in complexity or\nfamiliarity. Recent adaptive tokenization methods address this by allocating\nvariable-length representations but typically require test-time search over\nmultiple encodings to find the most predictive one. Inspired by Kolmogorov\nComplexity principles, we propose a single-pass adaptive tokenizer, KARL, which\npredicts the appropriate number of tokens for an image in a single forward\npass, halting once its approximate KC is reached. The token count serves as a\nproxy for the minimum description length. KARL's training procedure closely\nresembles the Upside-Down Reinforcement Learning paradigm, as it learns to\nconditionally predict token halting based on a desired reconstruction quality.\nKARL matches the performance of recent adaptive tokenizers while operating in a\nsingle pass. We present scaling laws for KARL, analyzing the role of\nencoder/decoder size, continuous vs. discrete tokenization and more.\nAdditionally, we offer a conceptual study drawing an analogy between Adaptive\nImage Tokenization and Algorithmic Information Theory, examining the predicted\nimage complexity (KC) across axes such as structure vs. noise and in- vs.\nout-of-distribution familiarity -- revealing alignment with human intuition.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07125", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.07125", "abs": "https://arxiv.org/abs/2507.07125", "authors": ["Cristina Mata", "Kanchana Ranasinghe", "Michael S. Ryoo"], "title": "CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic Text Embeddings", "comment": "ECCV 2024", "summary": "Unsupervised domain adaptation (UDA) involves learning class semantics from\nlabeled data within a source domain that generalize to an unseen target domain.\nUDA methods are particularly impactful for semantic segmentation, where\nannotations are more difficult to collect than in image classification. Despite\nrecent advances in large-scale vision-language representation learning, UDA\nmethods for segmentation have not taken advantage of the domain-agnostic\nproperties of text. To address this, we present a novel Covariance-based\nPixel-Text loss, CoPT, that uses domain-agnostic text embeddings to learn\ndomain-invariant features in an image segmentation encoder. The text embeddings\nare generated through our LLM Domain Template process, where an LLM is used to\ngenerate source and target domain descriptions that are fed to a frozen CLIP\nmodel and combined. In experiments on four benchmarks we show that a model\ntrained using CoPT achieves the new state of the art performance on UDA for\nsegmentation. The code can be found at https://github.com/cfmata/CoPT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07134", "categories": ["cs.AI", "cs.LG", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.07134", "abs": "https://arxiv.org/abs/2507.07134", "authors": ["Mridula Vijendran", "Shuang Chen", "Jingjing Deng", "Hubert P. H. Shum"], "title": "BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks", "comment": "18 pages, 7 figures, 3 tables", "summary": "The pervasive issue of bias in AI presents a significant challenge to\npainting classification, and is getting more serious as these systems become\nincreasingly integrated into tasks like art curation and restoration. Biases,\noften arising from imbalanced datasets where certain artistic styles dominate,\ncompromise the fairness and accuracy of model predictions, i.e., classifiers\nare less accurate on rarely seen paintings. While prior research has made\nstrides in improving classification performance, it has largely overlooked the\ncritical need to address these underlying biases, that is, when dealing with\nout-of-distribution (OOD) data. Our insight highlights the necessity of a more\nrobust approach to bias mitigation in AI models for art classification on\nbiased training data. We propose a novel OOD-informed model bias adaptive\nsampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It\naddresses these challenges by dynamically adjusting temperature scaling and\nsampling probabilities, thereby promoting a more equitable representation of\nall classes. We evaluate our proposed approach to the KaoKore and PACS\ndatasets, focusing on the model's ability to reduce class-wise bias. We further\npropose a new metric, Same-Dataset OOD Detection Score (SODC), designed to\nassess class-wise separation and per-class bias reduction. Our method\ndemonstrates the ability to balance high performance with fairness, making it a\nrobust solution for unbiasing AI models in the art domain.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07154", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07154", "abs": "https://arxiv.org/abs/2507.07154", "authors": ["Desheng Li", "Chaoliang Liu", "Zhiyong Xiao"], "title": "CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp Segmentation", "comment": null, "summary": "Accurate segmentation of polyps from colonoscopy images is crucial for the\nearly diagnosis and treatment of colorectal cancer. Most existing deep\nlearning-based polyp segmentation methods adopt an Encoder-Decoder\narchitecture, and some utilize multi-task frameworks that incorporate auxiliary\ntasks such as classification to enhance segmentation performance. However,\nthese approaches often require additional labeled data and rely on task\nsimilarity, which can limit their generalizability. To address these\nchallenges, we propose CL-Polyp, a contrastive learning-enhanced polyp\nsegmentation network. Our method leverages contrastive learning to improve the\nencoder's ability to extract discriminative features by contrasting positive\nand negative sample pairs derived from polyp images. This self-supervised\nstrategy enhances visual representation without requiring additional\nannotations. In addition, we introduce two lightweight and effective modules:\nthe Modified Atrous Spatial Pyramid Pooling (MASPP) module for better\nmulti-scale feature fusion, and the Channel Concatenate and Element Add (CA)\nmodule to fuse low-level and upsampled features for improved boundary\nreconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,\nCVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-demonstrate that CL-Polyp\nconsistently outperforms state-of-the-art methods. Specifically, it improves\nthe IoU metric by 0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets,\nrespectively, validating its effectiveness in clinical polyp segmentation\ntasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07426", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07426", "abs": "https://arxiv.org/abs/2507.07426", "authors": ["Zerui Yang", "Yuwei Wan", "Yinqiao Li", "Yudai Matsuda", "Tong Xie", "Linqi Song"], "title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search", "comment": null, "summary": "Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug discovery. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repurposing. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Without requiring domain-specific\nfine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by\nover 20\\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate\nthat DrugMCTS achieves substantially higher recall and robustness compared to\nboth general-purpose LLMs and deep learning baselines. Our results highlight\nthe importance of structured reasoning, agent-based collaboration, and\nfeedback-driven search mechanisms in advancing LLM applications for drug\ndiscovery.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07484", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07484", "abs": "https://arxiv.org/abs/2507.07484", "authors": ["Kaiqu Liang", "Haimin Hu", "Xuandong Zhao", "Dawn Song", "Thomas L. Griffiths", "Jaime Fernández Fisac"], "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models", "comment": "Project page, code & data: https://machine-bullshit.github.io", "summary": "Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to\nstatements made without regard to their truth value. While previous work has\nexplored large language model (LLM) hallucination and sycophancy, we propose\nmachine bullshit as an overarching conceptual framework that can allow\nresearchers to characterize the broader phenomenon of emergent loss of\ntruthfulness in LLMs and shed light on its underlying mechanisms. We introduce\nthe Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and\npropose a complementary taxonomy analyzing four qualitative forms of bullshit:\nempty rhetoric, paltering, weasel words, and unverified claims. We conduct\nempirical evaluations on the Marketplace dataset, the Political Neutrality\ndataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI\nassistants) explicitly designed to evaluate machine bullshit. Our results\ndemonstrate that model fine-tuning with reinforcement learning from human\nfeedback (RLHF) significantly exacerbates bullshit and inference-time\nchain-of-thought (CoT) prompting notably amplify specific bullshit forms,\nparticularly empty rhetoric and paltering. We also observe prevalent machine\nbullshit in political contexts, with weasel words as the dominant strategy. Our\nfindings highlight systematic challenges in AI alignment and provide new\ninsights toward more truthful LLM behavior.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "truthfulness"], "score": 3}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07495", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07495", "abs": "https://arxiv.org/abs/2507.07495", "authors": ["Mihir Parmar", "Palash Goyal", "Xin Liu", "Yiwen Song", "Mingyang Ling", "Chitta Baral", "Hamid Palangi", "Tomas Pfister"], "title": "PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving", "comment": "15 Pages", "summary": "Recently, decomposing complex problems into simple subtasks--a crucial part\nof human-like natural planning--to solve the given problem has significantly\nboosted the performance of large language models (LLMs). However, leveraging\nsuch planning structures during post-training to boost the performance of\nsmaller open-source LLMs remains underexplored. Motivated by this, we introduce\nPLAN-TUNING, a unified post-training framework that (i) distills synthetic task\ndecompositions (termed \"planning trajectories\") from large-scale LLMs and (ii)\nfine-tunes smaller models via supervised and reinforcement-learning objectives\ndesigned to mimic these planning processes to improve complex reasoning. On\nGSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by\nan average $\\sim7\\%$. Furthermore, plan-tuned models show better generalization\ncapabilities on out-of-domain datasets, with average $\\sim10\\%$ and $\\sim12\\%$\nperformance improvements on OlympiadBench and AIME 2024, respectively. Our\ndetailed analysis demonstrates how planning trajectories improves complex\nreasoning capabilities, showing that PLAN-TUNING is an effective strategy for\nimproving task-specific performance of smaller LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07297", "abs": "https://arxiv.org/abs/2507.07297", "authors": ["Chengfei Wu", "Ronald Seoh", "Bingxuan Li", "Liqiang Zhang", "Fengrong Han", "Dan Goldwasser"], "title": "MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning", "comment": null, "summary": "Recent advances in large vision-language models have led to impressive\nperformance in visual question answering and multimodal reasoning. However, it\nremains unclear whether these models genuinely perform grounded visual\nreasoning or rely on superficial patterns and dataset biases. In this work, we\nintroduce MagiC, a comprehensive benchmark designed to evaluate grounded\nmultimodal cognition, assessing not only answer accuracy but also the quality\nof step-by-step reasoning and its alignment with relevant visual evidence. Our\nbenchmark includes approximately 5,500 weakly supervised QA examples generated\nfrom strong model outputs and 900 human-curated examples with fine-grained\nannotations, including answers, rationales, and bounding box groundings. We\nevaluate 15 vision-language models ranging from 7B to 70B parameters across\nfour dimensions: final answer correctness, reasoning validity, grounding\nfidelity, and self-correction ability. MagiC further includes diagnostic\nsettings to probe model robustness under adversarial visual cues and assess\ntheir capacity for introspective error correction. We introduce new metrics\nsuch as MagiScore and StepSense, and provide comprehensive analyses that reveal\nkey limitations and opportunities in current approaches to grounded visual\nreasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "question answering", "fine-grained"], "score": 5}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07498", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07498", "abs": "https://arxiv.org/abs/2507.07498", "authors": ["Keqin Bao", "Nuo Chen", "Xiaoyuan Li", "Binyuan Hui", "Bowen Yu", "Fuli Feng", "Junyang Lin", "Xiangnan He", "Dayiheng Liu"], "title": "Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code", "comment": null, "summary": "Enhancing reasoning capabilities remains a central focus in the LLM reasearch\ncommunity. A promising direction involves requiring models to simulate code\nexecution step-by-step to derive outputs for given inputs. However, as code is\noften designed for large-scale systems, direct application leads to\nover-reliance on complex data structures and algorithms, even for simple cases,\nresulting in overfitting to algorithmic patterns rather than core reasoning\nstructures. To address this, we propose TeaR, which aims at teaching LLMs to\nreason better. TeaR leverages careful data curation and reinforcement learning\nto guide models in discovering optimal reasoning paths through code-related\ntasks, thereby improving general reasoning abilities. We conduct extensive\nexperiments using two base models and three long-CoT distillation models, with\nmodel sizes ranging from 1.5 billion to 32 billion parameters, and across 17\nbenchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results\nconsistently show significant performance improvements. Notably, TeaR achieves\na 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07317", "abs": "https://arxiv.org/abs/2507.07317", "authors": ["Sherry X. Chen", "Yi Wei", "Luowei Zhou", "Suren Kumar"], "title": "ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation", "comment": "International Conference on Computer Vision (ICCV) 2025", "summary": "Recent advances in instruction-guided image editing underscore the need for\neffective automated evaluation. While Vision-Language Models (VLMs) have been\nexplored as judges, open-source models struggle with alignment, and proprietary\nmodels lack transparency and cost efficiency. Additionally, no public training\ndatasets exist to fine-tune open-source VLMs, only small benchmarks with\ndiverse evaluation schemes. To address this, we introduce ADIEE, an automated\ndataset creation approach which is then used to train a scoring model for\ninstruction-guided image editing evaluation. We generate a large-scale dataset\nwith over 100K samples and use it to fine-tune a LLaVA-NeXT-8B model modified\nto decode a numeric score from a custom token. The resulting scorer outperforms\nall open-source VLMs and Gemini-Pro 1.5 across all benchmarks, achieving a\n0.0696 (+17.24%) gain in score correlation with human ratings on AURORA-Bench,\nand improving pair-wise comparison accuracy by 4.03% (+7.21%) on GenAI-Bench\nand 4.75% (+9.35%) on AURORA-Bench, respectively, compared to the\nstate-of-the-art. The scorer can act as a reward model, enabling automated best\nedit selection and model fine-tuning. Notably, the proposed scorer can boost\nMagicBrush model's average evaluation score on ImagenHub from 5.90 to 6.43\n(+8.98%).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "comparison", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation", "accuracy"], "score": 4}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07509", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.07509", "abs": "https://arxiv.org/abs/2507.07509", "authors": ["Yuanchen Shi", "Longyin Zhang", "Fang Kong"], "title": "Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System", "comment": "10pages,8 figures", "summary": "The growing need for psychological support due to increasing pressures has\nexposed the scarcity of relevant datasets, particularly in non-English\nlanguages. To address this, we propose a framework that leverages limited\nreal-world data and expert knowledge to fine-tune two large language models:\nDialog Generator and Dialog Modifier. The Generator creates large-scale\npsychological counseling dialogues based on predefined paths, which guide\nsystem response strategies and user interactions, forming the basis for\neffective support. The Modifier refines these dialogues to align with\nreal-world data quality. Through both automated and manual review, we construct\nthe Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K\ndialogues across 13 groups, 16 psychological problems, 13 causes, and 12\nsupport focuses. Additionally, we introduce the Comprehensive Agent Dialogue\nSupport System (CADSS), where a Profiler analyzes user characteristics, a\nSummarizer condenses dialogue history, a Planner selects strategies, and a\nSupporter generates empathetic responses. The experimental results of the\nStrategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate\nthat CADSS achieves state-of-the-art performance on both CPsDD and ESConv\ndatasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07374", "abs": "https://arxiv.org/abs/2507.07374", "authors": ["Haotian Wang", "Aoran Xiao", "Xiaoqin Zhang", "Meng Yang", "Shijian Lu"], "title": "PacGDC: Label-Efficient Generalizable Depth Completion with Projection Ambiguity and Consistency", "comment": "Accepted to ICCV 2025", "summary": "Generalizable depth completion enables the acquisition of dense metric depth\nmaps for unseen environments, offering robust perception capabilities for\nvarious downstream tasks. However, training such models typically requires\nlarge-scale datasets with metric depth labels, which are often labor-intensive\nto collect. This paper presents PacGDC, a label-efficient technique that\nenhances data diversity with minimal annotation effort for generalizable depth\ncompletion. PacGDC builds on novel insights into inherent ambiguities and\nconsistencies in object shapes and positions during 2D-to-3D projection,\nallowing the synthesis of numerous pseudo geometries for the same visual scene.\nThis process greatly broadens available geometries by manipulating scene scales\nof the corresponding depth maps. To leverage this property, we propose a new\ndata synthesis pipeline that uses multiple depth foundation models as scale\nmanipulators. These models robustly provide pseudo depth labels with varied\nscene scales, affecting both local objects and global layouts, while ensuring\nprojection consistency that supports generalization. To further diversify\ngeometries, we incorporate interpolation and relocation strategies, as well as\nunlabeled images, extending the data coverage beyond the individual use of\nfoundation models. Extensive experiments show that PacGDC achieves remarkable\ngeneralizability across multiple benchmarks, excelling in diverse scene\nsemantics/scales and depth sparsity/patterns under both zero-shot and few-shot\nsettings. Code: https://github.com/Wang-xjtu/PacGDC.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07381", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07381", "abs": "https://arxiv.org/abs/2507.07381", "authors": ["Hao Xu", "Arbind Agrahari Baniya", "Sam Wells", "Mohamed Reda Bouadjenek", "Richard Dazeley", "Sunil Aryal"], "title": "Multi-Scale Attention and Gated Shifting for Fine-Grained Event Spotting in Videos", "comment": null, "summary": "Precise Event Spotting (PES) in sports videos requires frame-level\nrecognition of fine-grained actions from single-camera footage. Existing PES\nmodels typically incorporate lightweight temporal modules such as Gate Shift\nModule (GSM) or Gate Shift Fuse (GSF) to enrich 2D CNN feature extractors with\ntemporal context. However, these modules are limited in both temporal receptive\nfield and spatial adaptability. We propose a Multi-Scale Attention Gate Shift\nModule (MSAGSM) that enhances GSM with multi-scale temporal dilations and\nmulti-head spatial attention, enabling efficient modeling of both short- and\nlong-term dependencies while focusing on salient regions. MSAGSM is a\nlightweight plug-and-play module that can be easily integrated with various 2D\nbackbones. To further advance the field, we introduce the Table Tennis\nAustralia (TTA) dataset-the first PES benchmark for table tennis-containing\nover 4800 precisely annotated events. Extensive experiments across five PES\nbenchmarks demonstrate that MSAGSM consistently improves performance with\nminimal overhead, setting new state-of-the-art results.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07818", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07818", "abs": "https://arxiv.org/abs/2507.07818", "authors": ["Lu Xu", "Jiaqian Yu", "Xiongfeng Peng", "Yiwei Chen", "Weiming Li", "Jaewook Yoo", "Sunghyun Chunag", "Dongwook Lee", "Daehyun Ji", "Chao Zhang"], "title": "MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving", "comment": null, "summary": "Recent studies show large language models (LLMs) and vision language models\n(VLMs) trained using web-scale data can empower end-to-end autonomous driving\nsystems for a better generalization and interpretation. Specifically, by\ndynamically routing inputs to specialized subsets of parameters, the\nMixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve\nsubstantial performance improvements while maintaining computational\nefficiency. However, general MoE models usually demands extensive training data\nand complex optimization. In this work, inspired by the learning process of\nhuman drivers, we propose a skill-oriented MoE, called MoSE, which mimics human\ndrivers' learning process and reasoning process, skill-by-skill and\nstep-by-step. We propose a skill-oriented routing mechanism that begins with\ndefining and annotating specific skills, enabling experts to identify the\nnecessary driving competencies for various scenarios and reasoning tasks,\nthereby facilitating skill-by-skill learning. Further align the driving process\nto multi-step planning in human reasoning and end-to-end driving models, we\nbuild a hierarchical skill dataset and pretrain the router to encourage the\nmodel to think step-by-step. Unlike multi-round dialogs, MoSE integrates\nvaluable auxiliary tasks (e.g.\\ description, reasoning, planning) in one single\nforward process without introducing any extra computational cost. With less\nthan 3B sparsely activated parameters, our model outperforms several 8B+\nparameters on CODA AD corner case reasoning task. Compared to existing methods\nbased on open-source models and data, our approach achieves state-of-the-art\nperformance with significantly reduced activated model size (at least by\n$62.5\\%$) with a single-turn conversation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07572", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07572", "abs": "https://arxiv.org/abs/2507.07572", "authors": ["Yupu Liang", "Yaping Zhang", "Zhiyang Zhang", "Yang Zhao", "Lu Xiang", "Chengqing Zong", "Yu Zhou"], "title": "Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation", "comment": "Accepted by ACL 2025 Main", "summary": "Document Image Machine Translation (DIMT) aims to translate text within\ndocument images, facing generalization challenges due to limited training data\nand the complex interplay between visual and textual information. To address\nthese challenges, we introduce M4Doc, a novel single-to-mix modality alignment\nframework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an\nimage-only encoder with the multimodal representations of an MLLM, pre-trained\non large-scale document image datasets. This alignment enables a lightweight\nDIMT model to learn crucial visual-textual correlations during training. During\ninference, M4Doc bypasses the MLLM, maintaining computational efficiency while\nbenefiting from its multimodal knowledge. Comprehensive experiments demonstrate\nsubstantial improvements in translation quality, especially in cross-domain\ngeneralization and challenging document image scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07586", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07586", "abs": "https://arxiv.org/abs/2507.07586", "authors": ["Cooper Doyle"], "title": "Bayesian Discrete Diffusion Beats Autoregressive Perplexity", "comment": "12 pages, 2 figures, 2 tables", "summary": "We reveal a hidden Bayesian core of discrete-diffusion language models by\nshowing that the expected denoiser output under the forward masking\ndistribution recovers the exact posterior over clean tokens. Under minimal\nassumptions, Monte Carlo marginalization over K independent corruptions\nconverges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of\nconsistency and finite-sample error bounds. Building on this insight, we\nintroduce a lightweight inference-time ensemble that averages K\nmask-and-denoise passes to obtain posterior-aware token probabilities and\nuncertainty estimates at no extra training cost. On WikiText-2, our method\nachieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite\nusing a model of comparable size. Code is available at\nhttps://github.com/mercury0100/bayesradd.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07415", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07415", "abs": "https://arxiv.org/abs/2507.07415", "authors": ["Xinyao Yu", "Hao Sun", "Zeyu Ling", "Ziwei Niu", "Zhenjia Bai", "Rui Qin", "Yen-Wei Chen", "Lanfen Lin"], "title": "EPIC: Efficient Prompt Interaction for Text-Image Classification", "comment": "arXiv admin note: substantial text overlap with arXiv:2401.14856", "summary": "In recent years, large-scale pre-trained multimodal models (LMMs) generally\nemerge to integrate the vision and language modalities, achieving considerable\nsuccess in multimodal tasks, such as text-image classification. The growing\nsize of LMMs, however, results in a significant computational cost for\nfine-tuning these models for downstream tasks. Hence, prompt-based interaction\nstrategy is studied to align modalities more efficiently. In this context, we\npropose a novel efficient prompt-based multimodal interaction strategy, namely\nEfficient Prompt Interaction for text-image Classification (EPIC).\nSpecifically, we utilize temporal prompts on intermediate layers, and integrate\ndifferent modalities with similarity-based prompt interaction, to leverage\nsufficient information exchange between modalities. Utilizing this approach,\nour method achieves reduced computational resource consumption and fewer\ntrainable parameters (about 1\\% of the foundation model) compared to other\nfine-tuning strategies. Furthermore, it demonstrates superior performance on\nthe UPMC-Food101 and SNLI-VE datasets, while achieving comparable performance\non the MM-IMDB dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07435", "abs": "https://arxiv.org/abs/2507.07435", "authors": ["Yuqi Cheng", "Yihan Sun", "Hui Zhang", "Weiming Shen", "Yunkang Cao"], "title": "Towards High-Resolution 3D Anomaly Detection: A Scalable Dataset and Real-Time Framework for Subtle Industrial Defects", "comment": "14 pages, 8figures", "summary": "In industrial point cloud analysis, detecting subtle anomalies demands\nhigh-resolution spatial data, yet prevailing benchmarks emphasize\nlow-resolution inputs. To address this disparity, we propose a scalable\npipeline for generating realistic and subtle 3D anomalies. Employing this\npipeline, we developed MiniShift, the inaugural high-resolution 3D anomaly\ndetection dataset, encompassing 2,577 point clouds, each with 500,000 points\nand anomalies occupying less than 1\\% of the total. We further introduce\nSimple3D, an efficient framework integrating Multi-scale Neighborhood\nDescriptors (MSND) and Local Feature Spatial Aggregation (LFSA) to capture\nintricate geometric details with minimal computational overhead, achieving\nreal-time inference exceeding 20 fps. Extensive evaluations on MiniShift and\nestablished benchmarks demonstrate that Simple3D surpasses state-of-the-art\nmethods in both accuracy and speed, highlighting the pivotal role of\nhigh-resolution data and effective feature aggregation in advancing practical\n3D anomaly detection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07483", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07483", "abs": "https://arxiv.org/abs/2507.07483", "authors": ["Qiangqiang Wu", "Yi Yu", "Chenqi Kong", "Ziquan Liu", "Jia Wan", "Haoliang Li", "Alex C. Kot", "Antoni B. Chan"], "title": "Temporal Unlearnable Examples: Preventing Personal Video Data from Unauthorized Exploitation by Object Tracking", "comment": "Accepted by ICCV 2025", "summary": "With the rise of social media, vast amounts of user-uploaded videos (e.g.,\nYouTube) are utilized as training data for Visual Object Tracking (VOT).\nHowever, the VOT community has largely overlooked video data-privacy issues, as\nmany private videos have been collected and used for training commercial models\nwithout authorization. To alleviate these issues, this paper presents the first\ninvestigation on preventing personal video data from unauthorized exploitation\nby deep trackers. Existing methods for preventing unauthorized data use\nprimarily focus on image-based tasks (e.g., image classification), directly\napplying them to videos reveals several limitations, including inefficiency,\nlimited effectiveness, and poor generalizability. To address these issues, we\npropose a novel generative framework for generating Temporal Unlearnable\nExamples (TUEs), and whose efficient computation makes it scalable for usage on\nlarge-scale video datasets. The trackers trained w/ TUEs heavily rely on\nunlearnable noises for temporal matching, ignoring the original data structure\nand thus ensuring training video data-privacy. To enhance the effectiveness of\nTUEs, we introduce a temporal contrastive loss, which further corrupts the\nlearning of existing trackers when using our TUEs for training. Extensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\nin video data-privacy protection, with strong transferability across VOT\nmodels, datasets, and temporal matching tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07817", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07817", "abs": "https://arxiv.org/abs/2507.07817", "authors": ["Anwoy Chatterjee", "H S V N S Kowndinya Renduchintala", "Sumit Bhatia", "Tanmoy Chakraborty"], "title": "On the Effect of Instruction Tuning Loss on Generalization", "comment": "Transactions of the Association for Computational Linguistics (TACL)", "summary": "Instruction Tuning has emerged as a pivotal post-training paradigm that\nenables pre-trained language models to better follow user instructions. Despite\nits significance, little attention has been given to optimizing the loss\nfunction used. A fundamental, yet often overlooked, question is whether the\nconventional auto-regressive objective - where loss is computed only on\nresponse tokens, excluding prompt tokens - is truly optimal for instruction\ntuning. In this work, we systematically investigate the impact of\ndifferentially weighting prompt and response tokens in instruction tuning loss,\nand propose Weighted Instruction Tuning (WIT) as a better alternative to\nconventional instruction tuning. Through extensive experiments on five language\nmodels of different families and scale, three finetuning datasets of different\nsizes, and five diverse evaluation benchmarks, we show that the standard\ninstruction tuning loss often yields suboptimal performance and limited\nrobustness to input prompt variations. We find that a low-to-moderate weight\nfor prompt tokens coupled with a moderate-to-high weight for response tokens\nyields the best-performing models across settings and also serve as better\nstarting points for the subsequent preference alignment training. These\nfindings highlight the need to reconsider instruction tuning loss and offer\nactionable insights for developing more robust and generalizable models. Our\ncode is open-sourced at https://github.com/kowndinya-renduchintala/WIT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07824", "abs": "https://arxiv.org/abs/2507.07824", "authors": ["Gianluca Vico", "Jindřinch Libovický"], "title": "Conditional Unigram Tokenization with Parallel Data", "comment": "21 pages, 4 figures, submitted to Tokenization Workshop (TokShop) at\n  ICML 2025", "summary": "We introduce conditional unigram tokenization, a novel approach that extends\nunigram tokenization by conditioning target token probabilities on\nsource-language tokens from parallel data. Given a fixed source tokenizer, our\nmethod learns a target tokenizer that maximizes cross-lingual semantic\nalignment. We evaluate our tokenizer on four language pairs across different\nfamilies and resource levels, examining intrinsic properties and downstream\nperformance on machine translation and language modeling. While our conditional\ntokenizer maintains comparable statistical properties to standard unigram\ntokenizers, results are mixed: we observe no improvements in machine\ntranslation quality, but find consistent perplexity reductions in language\nmodeling. We hypothesize that quadratic scaling of conditional probability\nestimation with respect to the vocabulary size creates a data efficiency\nbottleneck. Our findings suggest that alternative parameterizations may be\nnecessary for practical cross-lingual tokenization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07527", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07527", "abs": "https://arxiv.org/abs/2507.07527", "authors": ["Joelle Hanna", "Linus Scheibenreif", "Damian Borth"], "title": "MAPEX: Modality-Aware Pruning of Experts for Remote Sensing Foundation Models", "comment": null, "summary": "Remote sensing data is commonly used for tasks such as flood mapping,\nwildfire detection, or land-use studies. For each task, scientists carefully\nchoose appropriate modalities or leverage data from purpose-built instruments.\nRecent work on remote sensing foundation models pre-trains computer vision\nmodels on large amounts of remote sensing data. These large-scale models tend\nto focus on specific modalities, often optical RGB or multispectral data. For\nmany important applications, this introduces a mismatch between the application\nmodalities and the pre-training data. Moreover, the large size of foundation\nmodels makes them expensive and difficult to fine-tune on typically small\ndatasets for each task. We address this mismatch with MAPEX, a remote sensing\nfoundation model based on mixture-of-modality experts. MAPEX is pre-trained on\nmulti-modal remote sensing data with a novel modality-conditioned token routing\nmechanism that elicits modality-specific experts. To apply the model on a\nspecific task, we propose a modality aware pruning technique, which only\nretains experts specialized for the task modalities. This yields efficient\nmodality-specific models while simplifying fine-tuning and deployment for the\nmodalities of interest. We experimentally validate MAPEX on diverse remote\nsensing datasets and show strong performance compared to fully supervised\ntraining and state-of-the-art remote sensing foundation models. Code is\navailable at https://github.com/HSG-AIML/MAPEX.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07887", "categories": ["cs.CL", "cs.CE", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.07887", "abs": "https://arxiv.org/abs/2507.07887", "authors": ["Achuth Chandrasekhar", "Amir Barati Farimani"], "title": "Automating MD simulations for Proteins using Large language Models: NAMD-Agent", "comment": "34 pages", "summary": "Molecular dynamics simulations are an essential tool in understanding protein\nstructure, dynamics, and function at the atomic level. However, preparing high\nquality input files for MD simulations can be a time consuming and error prone\nprocess. In this work, we introduce an automated pipeline that leverages Large\nLanguage Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with\npython scripting and Selenium based web automation to streamline the generation\nof MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based\ninterface for preparing simulation-ready inputs for NAMD. By integrating\nGemini's code generation and iterative refinement capabilities, simulation\nscripts are automatically written, executed, and revised to navigate CHARMM\nGUI, extract appropriate parameters, and produce the required NAMD input files.\nPost processing is performed using additional software to further refine the\nsimulation outputs, thereby enabling a complete and largely hands free\nworkflow. Our results demonstrate that this approach reduces setup time,\nminimizes manual errors, and offers a scalable solution for handling multiple\nprotein systems in parallel. This automated framework paves the way for broader\napplication of LLMs in computational structural biology, offering a robust and\nadaptable platform for future developments in simulation automation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["code generation"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07585", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.07585", "abs": "https://arxiv.org/abs/2507.07585", "authors": ["Wenfeng Jia", "Bin Liang", "Yuxi Lu", "Attavit Wilaiwongsakul", "Muhammad Arif Khan", "Lihong Zheng"], "title": "HOTA: Hierarchical Overlap-Tiling Aggregation for Large-Area 3D Flood Mapping", "comment": null, "summary": "Floods are among the most frequent natural hazards and cause significant\nsocial and economic damage. Timely, large-scale information on flood extent and\ndepth is essential for disaster response; however, existing products often\ntrade spatial detail for coverage or ignore flood depth altogether. To bridge\nthis gap, this work presents HOTA: Hierarchical Overlap-Tiling Aggregation, a\nplug-and-play, multi-scale inference strategy. When combined with SegFormer and\na dual-constraint depth estimation module, this approach forms a complete 3D\nflood-mapping pipeline. HOTA applies overlapping tiles of different sizes to\nmultispectral Sentinel-2 images only during inference, enabling the SegFormer\nmodel to capture both local features and kilometre-scale inundation without\nchanging the network weights or retraining. The subsequent depth module is\nbased on a digital elevation model (DEM) differencing method, which refines the\n2D mask and estimates flood depth by enforcing (i) zero depth along the flood\nboundary and (ii) near-constant flood volume with respect to the DEM. A case\nstudy on the March 2021 Kempsey (Australia) flood shows that HOTA, when coupled\nwith SegFormer, improves IoU from 73\\% (U-Net baseline) to 84\\%. The resulting\n3D surface achieves a mean absolute boundary error of less than 0.5 m. These\nresults demonstrate that HOTA can produce accurate, large-area 3D flood maps\nsuitable for rapid disaster response.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07484", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07484", "abs": "https://arxiv.org/abs/2507.07484", "authors": ["Kaiqu Liang", "Haimin Hu", "Xuandong Zhao", "Dawn Song", "Thomas L. Griffiths", "Jaime Fernández Fisac"], "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models", "comment": "Project page, code & data: https://machine-bullshit.github.io", "summary": "Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to\nstatements made without regard to their truth value. While previous work has\nexplored large language model (LLM) hallucination and sycophancy, we propose\nmachine bullshit as an overarching conceptual framework that can allow\nresearchers to characterize the broader phenomenon of emergent loss of\ntruthfulness in LLMs and shed light on its underlying mechanisms. We introduce\nthe Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and\npropose a complementary taxonomy analyzing four qualitative forms of bullshit:\nempty rhetoric, paltering, weasel words, and unverified claims. We conduct\nempirical evaluations on the Marketplace dataset, the Political Neutrality\ndataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI\nassistants) explicitly designed to evaluate machine bullshit. Our results\ndemonstrate that model fine-tuning with reinforcement learning from human\nfeedback (RLHF) significantly exacerbates bullshit and inference-time\nchain-of-thought (CoT) prompting notably amplify specific bullshit forms,\nparticularly empty rhetoric and paltering. We also observe prevalent machine\nbullshit in political contexts, with weasel words as the dominant strategy. Our\nfindings highlight systematic challenges in AI alignment and provide new\ninsights toward more truthful LLM behavior.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "truthfulness"], "score": 3}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07495", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07495", "abs": "https://arxiv.org/abs/2507.07495", "authors": ["Mihir Parmar", "Palash Goyal", "Xin Liu", "Yiwen Song", "Mingyang Ling", "Chitta Baral", "Hamid Palangi", "Tomas Pfister"], "title": "PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving", "comment": "15 Pages", "summary": "Recently, decomposing complex problems into simple subtasks--a crucial part\nof human-like natural planning--to solve the given problem has significantly\nboosted the performance of large language models (LLMs). However, leveraging\nsuch planning structures during post-training to boost the performance of\nsmaller open-source LLMs remains underexplored. Motivated by this, we introduce\nPLAN-TUNING, a unified post-training framework that (i) distills synthetic task\ndecompositions (termed \"planning trajectories\") from large-scale LLMs and (ii)\nfine-tunes smaller models via supervised and reinforcement-learning objectives\ndesigned to mimic these planning processes to improve complex reasoning. On\nGSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by\nan average $\\sim7\\%$. Furthermore, plan-tuned models show better generalization\ncapabilities on out-of-domain datasets, with average $\\sim10\\%$ and $\\sim12\\%$\nperformance improvements on OlympiadBench and AIME 2024, respectively. Our\ndetailed analysis demonstrates how planning trajectories improves complex\nreasoning capabilities, showing that PLAN-TUNING is an effective strategy for\nimproving task-specific performance of smaller LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07957", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07957", "abs": "https://arxiv.org/abs/2507.07957", "authors": ["Yu Wang", "Xi Chen"], "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents", "comment": null, "summary": "Although memory capabilities of AI agents are gaining increasing attention,\nexisting solutions remain fundamentally limited. Most rely on flat, narrowly\nscoped memory components, constraining their ability to personalize, abstract,\nand reliably recall user-specific information over time. To this end, we\nintroduce MIRIX, a modular, multi-agent memory system that redefines the future\nof AI memory by solving the field's most critical challenge: enabling language\nmodels to truly remember. Unlike prior approaches, MIRIX transcends text to\nembrace rich visual and multimodal experiences, making memory genuinely useful\nin real-world scenarios. MIRIX consists of six distinct, carefully structured\nmemory types: Core, Episodic, Semantic, Procedural, Resource Memory, and\nKnowledge Vault, coupled with a multi-agent framework that dynamically controls\nand coordinates updates and retrieval. This design enables agents to persist,\nreason over, and accurately retrieve diverse, long-term user data at scale. We\nvalidate MIRIX in two demanding settings. First, on ScreenshotVQA, a\nchallenging multimodal benchmark comprising nearly 20,000 high-resolution\ncomputer screenshots per sequence, requiring deep contextual understanding and\nwhere no existing memory systems can be applied, MIRIX achieves 35% higher\naccuracy than the RAG baseline while reducing storage requirements by 99.9%.\nSecond, on LOCOMO, a long-form conversation benchmark with single-modal textual\ninput, MIRIX attains state-of-the-art performance of 85.4%, far surpassing\nexisting baselines. These results show that MIRIX sets a new performance\nstandard for memory-augmented LLM agents. To allow users to experience our\nmemory system, we provide a packaged application powered by MIRIX. It monitors\nthe screen in real time, builds a personalized memory base, and offers\nintuitive visualization and secure local storage to ensure privacy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07509", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.07509", "abs": "https://arxiv.org/abs/2507.07509", "authors": ["Yuanchen Shi", "Longyin Zhang", "Fang Kong"], "title": "Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System", "comment": "10pages,8 figures", "summary": "The growing need for psychological support due to increasing pressures has\nexposed the scarcity of relevant datasets, particularly in non-English\nlanguages. To address this, we propose a framework that leverages limited\nreal-world data and expert knowledge to fine-tune two large language models:\nDialog Generator and Dialog Modifier. The Generator creates large-scale\npsychological counseling dialogues based on predefined paths, which guide\nsystem response strategies and user interactions, forming the basis for\neffective support. The Modifier refines these dialogues to align with\nreal-world data quality. Through both automated and manual review, we construct\nthe Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K\ndialogues across 13 groups, 16 psychological problems, 13 causes, and 12\nsupport focuses. Additionally, we introduce the Comprehensive Agent Dialogue\nSupport System (CADSS), where a Profiler analyzes user characteristics, a\nSummarizer condenses dialogue history, a Planner selects strategies, and a\nSupporter generates empathetic responses. The experimental results of the\nStrategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate\nthat CADSS achieves state-of-the-art performance on both CPsDD and ESConv\ndatasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07620", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07620", "abs": "https://arxiv.org/abs/2507.07620", "authors": ["Marc Lafon", "Yannis Karmim", "Julio Silva-Rodriguez", "Paul Couairon", "Clément Rambour", "Raphaël Fournier-Sniehotta", "Ismail Ben Ayed", "Jose Dolz", "Nicolas Thome"], "title": "ViLU: Learning Vision-Language Uncertainties for Failure Prediction", "comment": null, "summary": "Reliable Uncertainty Quantification (UQ) and failure prediction remain open\nchallenges for Vision-Language Models (VLMs). We introduce ViLU, a new\nVision-Language Uncertainty quantification framework that contextualizes\nuncertainty estimates by leveraging all task-relevant textual representations.\nViLU constructs an uncertainty-aware multi-modal representation by integrating\nthe visual embedding, the predicted textual embedding, and an image-conditioned\ntextual representation via cross-attention. Unlike traditional UQ methods based\non loss prediction, ViLU trains an uncertainty predictor as a binary classifier\nto distinguish correct from incorrect predictions using a weighted binary\ncross-entropy loss, making it loss-agnostic. In particular, our proposed\napproach is well-suited for post-hoc settings, where only vision and text\nembeddings are available without direct access to the model itself. Extensive\nexperiments on diverse datasets show the significant gains of our method\ncompared to state-of-the-art failure prediction methods. We apply our method to\nstandard classification datasets, such as ImageNet-1k, as well as large-scale\nimage-caption datasets like CC12M and LAION-400M. Ablation studies highlight\nthe critical role of our architecture and training in achieving effective\nuncertainty quantification. Our code is publicly available and can be found\nhere: https://github.com/ykrmm/ViLU.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07572", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07572", "abs": "https://arxiv.org/abs/2507.07572", "authors": ["Yupu Liang", "Yaping Zhang", "Zhiyang Zhang", "Yang Zhao", "Lu Xiang", "Chengqing Zong", "Yu Zhou"], "title": "Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation", "comment": "Accepted by ACL 2025 Main", "summary": "Document Image Machine Translation (DIMT) aims to translate text within\ndocument images, facing generalization challenges due to limited training data\nand the complex interplay between visual and textual information. To address\nthese challenges, we introduce M4Doc, a novel single-to-mix modality alignment\nframework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an\nimage-only encoder with the multimodal representations of an MLLM, pre-trained\non large-scale document image datasets. This alignment enables a lightweight\nDIMT model to learn crucial visual-textual correlations during training. During\ninference, M4Doc bypasses the MLLM, maintaining computational efficiency while\nbenefiting from its multimodal knowledge. Comprehensive experiments demonstrate\nsubstantial improvements in translation quality, especially in cross-domain\ngeneralization and challenging document image scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07638", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07638", "abs": "https://arxiv.org/abs/2507.07638", "authors": ["F. Xavier Gaya-Morey", "Julia Sanchez-Perez", "Cristina Manresa-Yee", "Jose M. Buades-Rubio"], "title": "Bridging the gap in FER: addressing age bias in deep learning", "comment": null, "summary": "Facial Expression Recognition (FER) systems based on deep learning have\nachieved impressive performance in recent years. However, these models often\nexhibit demographic biases, particularly with respect to age, which can\ncompromise their fairness and reliability. In this work, we present a\ncomprehensive study of age-related bias in deep FER models, with a particular\nfocus on the elderly population. We first investigate whether recognition\nperformance varies across age groups, which expressions are most affected, and\nwhether model attention differs depending on age. Using Explainable AI (XAI)\ntechniques, we identify systematic disparities in expression recognition and\nattention patterns, especially for \"neutral\", \"sadness\", and \"anger\" in elderly\nindividuals. Based on these findings, we propose and evaluate three bias\nmitigation strategies: Multi-task Learning, Multi-modal Input, and Age-weighted\nLoss. Our models are trained on a large-scale dataset, AffectNet, with\nautomatically estimated age labels and validated on balanced benchmark datasets\nthat include underrepresented age groups. Results show consistent improvements\nin recognition accuracy for elderly individuals, particularly for the most\nerror-prone expressions. Saliency heatmap analysis reveals that models trained\nwith age-aware strategies attend to more relevant facial regions for each age\ngroup, helping to explain the observed improvements. These findings suggest\nthat age-related bias in FER can be effectively mitigated using simple training\nmodifications, and that even approximate demographic labels can be valuable for\npromoting fairness in large-scale affective computing systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07586", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07586", "abs": "https://arxiv.org/abs/2507.07586", "authors": ["Cooper Doyle"], "title": "Bayesian Discrete Diffusion Beats Autoregressive Perplexity", "comment": "12 pages, 2 figures, 2 tables", "summary": "We reveal a hidden Bayesian core of discrete-diffusion language models by\nshowing that the expected denoiser output under the forward masking\ndistribution recovers the exact posterior over clean tokens. Under minimal\nassumptions, Monte Carlo marginalization over K independent corruptions\nconverges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of\nconsistency and finite-sample error bounds. Building on this insight, we\nintroduce a lightweight inference-time ensemble that averages K\nmask-and-denoise passes to obtain posterior-aware token probabilities and\nuncertainty estimates at no extra training cost. On WikiText-2, our method\nachieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite\nusing a model of comparable size. Code is available at\nhttps://github.com/mercury0100/bayesradd.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07685", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07685", "abs": "https://arxiv.org/abs/2507.07685", "authors": ["Shin'ya Yamaguchi", "Kosuke Nishida", "Daiki Chijiwa"], "title": "Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought", "comment": "17 pages, 4 figures", "summary": "Large vision-language models (LVLMs) have demonstrated remarkable\ncapabilities by integrating pre-trained vision encoders with large language\nmodels (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting\nhas been adapted for LVLMs to enhance multi-modal reasoning by generating\nintermediate rationales based on visual and textual inputs. While CoT is\nassumed to improve grounding and accuracy in LVLMs, our experiments reveal a\nkey challenge: existing LVLMs often ignore the contents of generated rationales\nin CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as\na KL-constrained reward maximization focused on rationale-conditional\nlog-likelihood. As the optimal solution, we propose rationale-enhanced decoding\n(RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes\nvisual and rationale information by multiplying distinct image-conditional and\nrationale-conditional next token distributions. Extensive experiments show that\nRED consistently and significantly improves reasoning over standard CoT and\nother decoding methods across multiple benchmarks and LVLMs. Our work offers a\npractical and effective approach to improve both the faithfulness and accuracy\nof CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded\nmulti-modal systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07685", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07685", "abs": "https://arxiv.org/abs/2507.07685", "authors": ["Shin'ya Yamaguchi", "Kosuke Nishida", "Daiki Chijiwa"], "title": "Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought", "comment": "17 pages, 4 figures", "summary": "Large vision-language models (LVLMs) have demonstrated remarkable\ncapabilities by integrating pre-trained vision encoders with large language\nmodels (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting\nhas been adapted for LVLMs to enhance multi-modal reasoning by generating\nintermediate rationales based on visual and textual inputs. While CoT is\nassumed to improve grounding and accuracy in LVLMs, our experiments reveal a\nkey challenge: existing LVLMs often ignore the contents of generated rationales\nin CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as\na KL-constrained reward maximization focused on rationale-conditional\nlog-likelihood. As the optimal solution, we propose rationale-enhanced decoding\n(RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes\nvisual and rationale information by multiplying distinct image-conditional and\nrationale-conditional next token distributions. Extensive experiments show that\nRED consistently and significantly improves reasoning over standard CoT and\nother decoding methods across multiple benchmarks and LVLMs. Our work offers a\npractical and effective approach to improve both the faithfulness and accuracy\nof CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded\nmulti-modal systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07687", "abs": "https://arxiv.org/abs/2507.07687", "authors": ["Peixian Zhuang", "Yijian Wang", "Zhenqi Fu", "Hongliang Zhang", "Sam Kwong", "Chongyi Li"], "title": "Tree-Mamba: A Tree-Aware Mamba for Underwater Monocular Depth Estimation", "comment": null, "summary": "Underwater Monocular Depth Estimation (UMDE) is a critical task that aims to\nestimate high-precision depth maps from underwater degraded images caused by\nlight absorption and scattering effects in marine environments. Recently,\nMamba-based methods have achieved promising performance across various vision\ntasks; however, they struggle with the UMDE task because their inflexible state\nscanning strategies fail to model the structural features of underwater images\neffectively. Meanwhile, existing UMDE datasets usually contain unreliable depth\nlabels, leading to incorrect object-depth relationships between underwater\nimages and their corresponding depth maps. To overcome these limitations, we\ndevelop a novel tree-aware Mamba method, dubbed Tree-Mamba, for estimating\naccurate monocular depth maps from underwater degraded images. Specifically, we\npropose a tree-aware scanning strategy that adaptively constructs a minimum\nspanning tree based on feature similarity. The spatial topological features\namong the tree nodes are then flexibly aggregated through bottom-up and\ntop-down traversals, enabling stronger multi-scale feature representation\ncapabilities. Moreover, we construct an underwater depth estimation benchmark\n(called BlueDepth), which consists of 38,162 underwater image pairs with\nreliable depth labels. This benchmark serves as a foundational dataset for\ntraining existing deep learning-based UMDE methods to learn accurate\nobject-depth relationships. Extensive experiments demonstrate the superiority\nof the proposed Tree-Mamba over several leading methods in both qualitative\nresults and quantitative evaluations with competitive computational efficiency.\nCode and dataset will be available at https://wyjgr.github.io/Tree-Mamba.html.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07708", "categories": ["cs.CV", "I.4.3"], "pdf": "https://arxiv.org/pdf/2507.07708", "abs": "https://arxiv.org/abs/2507.07708", "authors": ["Wei Shang", "Dongwei Ren", "Wanying Zhang", "Pengfei Zhu", "Qinghua Hu", "Wangmeng Zuo"], "title": "Motion-Aware Adaptive Pixel Pruning for Efficient Local Motion Deblurring", "comment": "Accepted by ACMMM 2025", "summary": "Local motion blur in digital images originates from the relative motion\nbetween dynamic objects and static imaging systems during exposure. Existing\ndeblurring methods face significant challenges in addressing this problem due\nto their inefficient allocation of computational resources and inadequate\nhandling of spatially varying blur patterns. To overcome these limitations, we\nfirst propose a trainable mask predictor that identifies blurred regions in the\nimage. During training, we employ blur masks to exclude sharp regions. For\ninference optimization, we implement structural reparameterization by\nconverting $3\\times 3$ convolutions to computationally efficient $1\\times 1$\nconvolutions, enabling pixel-level pruning of sharp areas to reduce\ncomputation. Second, we develop an intra-frame motion analyzer that translates\nrelative pixel displacements into motion trajectories, establishing adaptive\nguidance for region-specific blur restoration. Our method is trained end-to-end\nusing a combination of reconstruction loss, reblur loss, and mask loss guided\nby annotated blur masks. Extensive experiments demonstrate superior performance\nover state-of-the-art methods on both local and global blur datasets while\nreducing FLOPs by 49\\% compared to SOTA models (e.g., LMD-ViT). The source code\nis available at https://github.com/shangwei5/M2AENet.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference optimization"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07817", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07817", "abs": "https://arxiv.org/abs/2507.07817", "authors": ["Anwoy Chatterjee", "H S V N S Kowndinya Renduchintala", "Sumit Bhatia", "Tanmoy Chakraborty"], "title": "On the Effect of Instruction Tuning Loss on Generalization", "comment": "Transactions of the Association for Computational Linguistics (TACL)", "summary": "Instruction Tuning has emerged as a pivotal post-training paradigm that\nenables pre-trained language models to better follow user instructions. Despite\nits significance, little attention has been given to optimizing the loss\nfunction used. A fundamental, yet often overlooked, question is whether the\nconventional auto-regressive objective - where loss is computed only on\nresponse tokens, excluding prompt tokens - is truly optimal for instruction\ntuning. In this work, we systematically investigate the impact of\ndifferentially weighting prompt and response tokens in instruction tuning loss,\nand propose Weighted Instruction Tuning (WIT) as a better alternative to\nconventional instruction tuning. Through extensive experiments on five language\nmodels of different families and scale, three finetuning datasets of different\nsizes, and five diverse evaluation benchmarks, we show that the standard\ninstruction tuning loss often yields suboptimal performance and limited\nrobustness to input prompt variations. We find that a low-to-moderate weight\nfor prompt tokens coupled with a moderate-to-high weight for response tokens\nyields the best-performing models across settings and also serve as better\nstarting points for the subsequent preference alignment training. These\nfindings highlight the need to reconsider instruction tuning loss and offer\nactionable insights for developing more robust and generalizable models. Our\ncode is open-sourced at https://github.com/kowndinya-renduchintala/WIT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07730", "abs": "https://arxiv.org/abs/2507.07730", "authors": ["Théo Danielou", "Daniel Tordjman", "Pierre Manceron", "Corentin Dancette"], "title": "RAPS-3D: Efficient interactive segmentation for 3D radiological imaging", "comment": "Abstract accepted at MIUA 2025", "summary": "Promptable segmentation, introduced by the Segment Anything Model (SAM), is a\npromising approach for medical imaging, as it enables clinicians to guide and\nrefine model predictions interactively. However, SAM's architecture is designed\nfor 2D images and does not extend naturally to 3D volumetric data such as CT or\nMRI scans. Adapting 2D models to 3D typically involves autoregressive\nstrategies, where predictions are propagated slice by slice, resulting in\nincreased inference complexity. Processing large 3D volumes also requires\nsignificant computational resources, often leading existing 3D methods to also\nadopt complex strategies like sliding-window inference to manage memory usage,\nat the cost of longer inference times and greater implementation complexity. In\nthis paper, we present a simplified 3D promptable segmentation method, inspired\nby SegVol, designed to reduce inference time and eliminate prompt management\ncomplexities associated with sliding windows while achieving state-of-the-art\nperformance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07734", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.07734", "abs": "https://arxiv.org/abs/2507.07734", "authors": ["Michael Neumeier", "Jules Lecomte", "Nils Kazinski", "Soubarna Banik", "Bing Li", "Axel von Arnim"], "title": "EEvAct: Early Event-Based Action Recognition with High-Rate Two-Stream Spiking Neural Networks", "comment": "International Conference on Neuromorphic Systems (ICONS) 2025", "summary": "Recognizing human activities early is crucial for the safety and\nresponsiveness of human-robot and human-machine interfaces. Due to their high\ntemporal resolution and low latency, event-based vision sensors are a perfect\nmatch for this early recognition demand. However, most existing processing\napproaches accumulate events to low-rate frames or space-time voxels which\nlimits the early prediction capabilities. In contrast, spiking neural networks\n(SNNs) can process the events at a high-rate for early predictions, but most\nworks still fall short on final accuracy. In this work, we introduce a\nhigh-rate two-stream SNN which closes this gap by outperforming previous work\nby 2% in final accuracy on the large-scale THU EACT-50 dataset. We benchmark\nthe SNNs within a novel early event-based recognition framework by reporting\nTop-1 and Top-5 recognition scores for growing observation time. Finally, we\nexemplify the impact of these methods on a real-world task of early action\ntriggering for human motion capture in sports.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety", "accuracy"], "score": 4}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07957", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07957", "abs": "https://arxiv.org/abs/2507.07957", "authors": ["Yu Wang", "Xi Chen"], "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents", "comment": null, "summary": "Although memory capabilities of AI agents are gaining increasing attention,\nexisting solutions remain fundamentally limited. Most rely on flat, narrowly\nscoped memory components, constraining their ability to personalize, abstract,\nand reliably recall user-specific information over time. To this end, we\nintroduce MIRIX, a modular, multi-agent memory system that redefines the future\nof AI memory by solving the field's most critical challenge: enabling language\nmodels to truly remember. Unlike prior approaches, MIRIX transcends text to\nembrace rich visual and multimodal experiences, making memory genuinely useful\nin real-world scenarios. MIRIX consists of six distinct, carefully structured\nmemory types: Core, Episodic, Semantic, Procedural, Resource Memory, and\nKnowledge Vault, coupled with a multi-agent framework that dynamically controls\nand coordinates updates and retrieval. This design enables agents to persist,\nreason over, and accurately retrieve diverse, long-term user data at scale. We\nvalidate MIRIX in two demanding settings. First, on ScreenshotVQA, a\nchallenging multimodal benchmark comprising nearly 20,000 high-resolution\ncomputer screenshots per sequence, requiring deep contextual understanding and\nwhere no existing memory systems can be applied, MIRIX achieves 35% higher\naccuracy than the RAG baseline while reducing storage requirements by 99.9%.\nSecond, on LOCOMO, a long-form conversation benchmark with single-modal textual\ninput, MIRIX attains state-of-the-art performance of 85.4%, far surpassing\nexisting baselines. These results show that MIRIX sets a new performance\nstandard for memory-augmented LLM agents. To allow users to experience our\nmemory system, we provide a packaged application powered by MIRIX. It monitors\nthe screen in real time, builds a personalized memory base, and offers\nintuitive visualization and secure local storage to ensure privacy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07776", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07776", "abs": "https://arxiv.org/abs/2507.07776", "authors": ["Dren Fazlija", "Monty-Maximilian Zühlke", "Johanna Schrader", "Arkadij Orlov", "Clara Stein", "Iyiola E. Olatunji", "Daniel Kudenko"], "title": "SCOOTER: A Human Evaluation Framework for Unrestricted Adversarial Examples", "comment": "42 pages, 16 figures, 11 tables, Under Review, Code:\n  https://github.com/DrenFazlija/Scooter, Data:\n  https://doi.org/10.5281/zenodo.15771501", "summary": "Unrestricted adversarial attacks aim to fool computer vision models without\nbeing constrained by $\\ell_p$-norm bounds to remain imperceptible to humans,\nfor example, by changing an object's color. This allows attackers to circumvent\ntraditional, norm-bounded defense strategies such as adversarial training or\ncertified defense strategies. However, due to their unrestricted nature, there\nare also no guarantees of norm-based imperceptibility, necessitating human\nevaluations to verify just how authentic these adversarial examples look. While\nsome related work assesses this vital quality of adversarial attacks, none\nprovide statistically significant insights. This issue necessitates a unified\nframework that supports and streamlines such an assessment for evaluating and\ncomparing unrestricted attacks. To close this gap, we introduce SCOOTER - an\nopen-source, statistically powered framework for evaluating unrestricted\nadversarial examples. Our contributions are: $(i)$ best-practice guidelines for\ncrowd-study power, compensation, and Likert equivalence bounds to measure\nimperceptibility; $(ii)$ the first large-scale human vs. model comparison\nacross 346 human participants showing that three color-space attacks and three\ndiffusion-based attacks fail to produce imperceptible images. Furthermore, we\nfound that GPT-4o can serve as a preliminary test for imperceptibility, but it\nonly consistently detects adversarial examples for four out of six tested\nattacks; $(iii)$ open-source software tools, including a browser-based task\ntemplate to collect annotations and analysis scripts in Python and R; $(iv)$ an\nImageNet-derived benchmark dataset containing 3K real images, 7K adversarial\nexamples, and over 34K human ratings. Our findings demonstrate that automated\nvision systems do not align with human perception, reinforcing the need for a\nground-truth SCOOTER benchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07982", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07982", "abs": "https://arxiv.org/abs/2507.07982", "authors": ["Haoyu Wu", "Diankun Wu", "Tianyu He", "Junliang Guo", "Yang Ye", "Yueqi Duan", "Jiang Bian"], "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling", "comment": "18 pages, project page: https://GeometryForcing.github.io", "summary": "Videos inherently represent 2D projections of a dynamic 3D world. However,\nour analysis suggests that video diffusion models trained solely on raw video\ndata often fail to capture meaningful geometric-aware structure in their\nlearned representations. To bridge this gap between video diffusion models and\nthe underlying 3D nature of the physical world, we propose Geometry Forcing, a\nsimple yet effective method that encourages video diffusion models to\ninternalize latent 3D representations. Our key insight is to guide the model's\nintermediate representations toward geometry-aware structure by aligning them\nwith features from a pretrained geometric foundation model. To this end, we\nintroduce two complementary alignment objectives: Angular Alignment, which\nenforces directional consistency via cosine similarity, and Scale Alignment,\nwhich preserves scale-related information by regressing unnormalized geometric\nfeatures from normalized diffusion representation. We evaluate Geometry Forcing\non both camera view-conditioned and action-conditioned video generation tasks.\nExperimental results demonstrate that our method substantially improves visual\nquality and 3D consistency over the baseline methods. Project page:\nhttps://GeometryForcing.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07990", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07990", "abs": "https://arxiv.org/abs/2507.07990", "authors": ["Jeongseok Hyun", "Sukjun Hwang", "Su Ho Han", "Taeoh Kim", "Inwoong Lee", "Dongyoon Wee", "Joon-Young Lee", "Seon Joo Kim", "Minho Shim"], "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs", "comment": "Accepted at ICCV2025; Project page:\n  https://www.jshyun.me/projects/sttm", "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "dimension"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07838", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07838", "abs": "https://arxiv.org/abs/2507.07838", "authors": ["Paul McHard", "Florent P. Audonnet", "Oliver Summerell", "Sebastian Andraos", "Paul Henderson", "Gerardo Aragon-Camarasa"], "title": "3D-ADAM: A Dataset for 3D Anomaly Detection in Advanced Manufacturing", "comment": null, "summary": "Surface defects are one of the largest contributors to low yield in the\nmanufacturing sector. Accurate and reliable detection of defects during the\nmanufacturing process is therefore of great value across the sector.\nState-of-the-art approaches to automated defect detection yield impressive\nperformance on current datasets, yet still fall short in real-world\nmanufacturing settings and developing improved methods relies on large datasets\nrepresentative of real-world scenarios. Unfortunately, high-quality,\nhigh-precision RGB+3D industrial anomaly detection datasets are scarce, and\ntypically do not reflect real-world industrial deployment scenarios. To address\nthis, we introduce 3D-ADAM, the first large-scale industry-relevant dataset for\nhigh-precision 3D Anomaly Detection. 3D-ADAM comprises 14,120 high-resolution\nscans across 217 unique parts, captured using 4 industrial depth imaging\nsensors. It includes 27,346 annotated defect instances from 12 categories,\ncovering the breadth of industrial surface defects. 3D-ADAM uniquely captures\nan additional 8,110 annotations of machine element features, spanning the range\nof relevant mechanical design form factors. Unlike existing datasets, 3D-ADAM\nis captured in a real industrial environment with variations in part position\nand orientation, camera positioning, ambient lighting conditions, as well as\npartial occlusions. Our evaluation of SOTA models across various RGB+3D anomaly\ndetection tasks demonstrates the significant challenge this dataset presents to\ncurrent approaches. We further validated the industrial relevance and quality\nof the dataset through an expert labelling survey conducted by industry\npartners. By providing this challenging benchmark, 3D-ADAM aims to accelerate\nthe development of robust 3D Anomaly Detection models capable of meeting the\ndemands of modern manufacturing environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07978", "abs": "https://arxiv.org/abs/2507.07978", "authors": ["Longfei Li", "Zhiwen Fan", "Wenyan Cong", "Xinhang Liu", "Yuyang Yin", "Matt Foutter", "Panwang Pan", "Chenyu You", "Yue Wang", "Zhangyang Wang", "Yao Zhao", "Marco Pavone", "Yunchao Wei"], "title": "Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions", "comment": "Project Page: https://marsgenai.github.io", "summary": "Synthesizing realistic Martian landscape videos is crucial for mission\nrehearsal and robotic simulation. However, this task poses unique challenges\ndue to the scarcity of high-quality Martian data and the significant domain gap\nbetween Martian and terrestrial imagery. To address these challenges, we\npropose a holistic solution composed of two key components: 1) A data curation\npipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian\nenvironments from real stereo navigation images, sourced from NASA's Planetary\nData System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A\nMartian terrain video generator, MarsGen, which synthesizes novel videos\nvisually realistic and geometrically consistent with the 3D structure encoded\nin the data. Our M3arsSynth engine spans a wide range of Martian terrains and\nacquisition dates, enabling the generation of physically accurate 3D surface\nmodels at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data,\nsynthesizes videos conditioned on an initial image frame and, optionally,\ncamera trajectories or textual prompts, allowing for video generation in novel\nenvironments. Experimental results show that our approach outperforms video\nsynthesis models trained on terrestrial datasets, achieving superior visual\nfidelity and 3D structural consistency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07982", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07982", "abs": "https://arxiv.org/abs/2507.07982", "authors": ["Haoyu Wu", "Diankun Wu", "Tianyu He", "Junliang Guo", "Yang Ye", "Yueqi Duan", "Jiang Bian"], "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling", "comment": "18 pages, project page: https://GeometryForcing.github.io", "summary": "Videos inherently represent 2D projections of a dynamic 3D world. However,\nour analysis suggests that video diffusion models trained solely on raw video\ndata often fail to capture meaningful geometric-aware structure in their\nlearned representations. To bridge this gap between video diffusion models and\nthe underlying 3D nature of the physical world, we propose Geometry Forcing, a\nsimple yet effective method that encourages video diffusion models to\ninternalize latent 3D representations. Our key insight is to guide the model's\nintermediate representations toward geometry-aware structure by aligning them\nwith features from a pretrained geometric foundation model. To this end, we\nintroduce two complementary alignment objectives: Angular Alignment, which\nenforces directional consistency via cosine similarity, and Scale Alignment,\nwhich preserves scale-related information by regressing unnormalized geometric\nfeatures from normalized diffusion representation. We evaluate Geometry Forcing\non both camera view-conditioned and action-conditioned video generation tasks.\nExperimental results demonstrate that our method substantially improves visual\nquality and 3D consistency over the baseline methods. Project page:\nhttps://GeometryForcing.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07985", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07985", "abs": "https://arxiv.org/abs/2507.07985", "authors": ["Bijay Gurung", "David T. Hoffmann", "Thomas Brox"], "title": "CLIP Won't Learn Object-Attribute Binding from Natural Data and Here is Why", "comment": null, "summary": "Contrastive vision-language models like CLIP are used for a large variety of\napplications, such as zero-shot classification or as vision encoder for\nmulti-modal models. Despite their popularity, their representations show major\nlimitations. For instance, CLIP models learn bag-of-words representations and,\nas a consequence, fail to distinguish whether an image is of \"a yellow\nsubmarine and a blue bus\" or \"a blue submarine and a yellow bus\". Previous\nattempts to fix this issue added hard negatives during training or modified the\narchitecture, but failed to resolve the problem in its entirety. We suspect\nthat the missing insights to solve the binding problem for CLIP are hidden in\nthe arguably most important part of learning algorithms: the data. In this\nwork, we fill this gap by rigorously identifying the influence of data\nproperties on CLIP's ability to learn binding using a synthetic dataset. We\nfind that common properties of natural data such as low attribute density,\nincomplete captions, and the saliency bias, a tendency of human captioners to\ndescribe the object that is \"most salient\" to them have a detrimental effect on\nbinding performance. In contrast to common belief, we find that neither scaling\nthe batch size, i.e., implicitly adding more hard negatives, nor explicitly\ncreating hard negatives enables CLIP to learn reliable binding. Only when the\ndata expresses our identified data properties CLIP learns almost perfect\nbinding.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07990", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07990", "abs": "https://arxiv.org/abs/2507.07990", "authors": ["Jeongseok Hyun", "Sukjun Hwang", "Su Ho Han", "Taeoh Kim", "Inwoong Lee", "Dongyoon Wee", "Joon-Young Lee", "Seon Joo Kim", "Minho Shim"], "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs", "comment": "Accepted at ICCV2025; Project page:\n  https://www.jshyun.me/projects/sttm", "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "dimension"], "score": 2}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.08000", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08000", "abs": "https://arxiv.org/abs/2507.08000", "authors": ["Helen Qu", "Sang Michael Xie"], "title": "Impact of Pretraining Word Co-occurrence on Compositional Generalization in Multimodal Models", "comment": null, "summary": "CLIP and large multimodal models (LMMs) have better accuracy on examples\ninvolving concepts that are highly represented in the training data. However,\nthe role of concept combinations in the training data on compositional\ngeneralization is largely unclear -- for instance, how does accuracy vary when\na common object appears in an uncommon pairing with another object? In this\npaper, we investigate how word co-occurrence statistics in the pretraining\ndataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\nperformance. To disentangle the effects of word co-occurrence frequencies from\nsingle-word frequencies, we measure co-occurrence with pointwise mutual\ninformation (PMI), which normalizes the joint probability of two words\nco-occurring by the probability of co-occurring independently. Using\nsynthetically generated images with a variety of concept pairs, we show a\nstrong correlation between PMI in the CLIP pretraining data and zero-shot\naccuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\nbetween images in the top and bottom 5% of PMI values), demonstrating that even\naccuracy on common concepts is affected by the combination of concepts in the\nimage. Leveraging this finding, we reproduce this effect in natural images by\nediting them to contain pairs with varying PMI, resulting in a correlation of\nr=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\nbuilt on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\nhighlight the need for algorithms and architectures that improve compositional\ngeneralization in multimodal models without scaling the training data\ncombinatorially. Our code is available at\nhttps://github.com/helenqu/multimodal-pretraining-pmi.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-07-11.jsonl"}
{"id": "2507.07572", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07572", "abs": "https://arxiv.org/abs/2507.07572", "authors": ["Yupu Liang", "Yaping Zhang", "Zhiyang Zhang", "Yang Zhao", "Lu Xiang", "Chengqing Zong", "Yu Zhou"], "title": "Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation", "comment": "Accepted by ACL 2025 Main", "summary": "Document Image Machine Translation (DIMT) aims to translate text within\ndocument images, facing generalization challenges due to limited training data\nand the complex interplay between visual and textual information. To address\nthese challenges, we introduce M4Doc, a novel single-to-mix modality alignment\nframework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an\nimage-only encoder with the multimodal representations of an MLLM, pre-trained\non large-scale document image datasets. This alignment enables a lightweight\nDIMT model to learn crucial visual-textual correlations during training. During\ninference, M4Doc bypasses the MLLM, maintaining computational efficiency while\nbenefiting from its multimodal knowledge. Comprehensive experiments demonstrate\nsubstantial improvements in translation quality, especially in cross-domain\ngeneralization and challenging document image scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-11.jsonl"}
