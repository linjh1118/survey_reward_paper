{"id": "2507.12508", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12508", "abs": "https://arxiv.org/abs/2507.12508", "authors": ["Yuncong Yang", "Jiageng Liu", "Zheyuan Zhang", "Siyuan Zhou", "Reuben Tan", "Jianwei Yang", "Yilun Du", "Chuang Gan"], "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning", "comment": "Project Page: https://umass-embodied-agi.github.io/MindJourney", "summary": "Spatial reasoning in 3D space is central to human cognition and indispensable\nfor embodied tasks such as navigation and manipulation. However,\nstate-of-the-art vision-language models (VLMs) struggle frequently with tasks\nas simple as anticipating how a scene will look after an egocentric motion:\nthey perceive 2D images but lack an internal model of 3D dynamics. We therefore\npropose MindJourney, a test-time scaling framework that grants a VLM with this\nmissing capability by coupling it to a controllable world model based on video\ndiffusion. The VLM iteratively sketches a concise camera trajectory, while the\nworld model synthesizes the corresponding view at each step. The VLM then\nreasons over this multi-view evidence gathered during the interactive\nexploration. Without any fine-tuning, our MindJourney achieves over an average\n8% performance boost on the representative spatial reasoning benchmark SAT,\nshowing that pairing VLMs with world models for test-time scaling offers a\nsimple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also\nimproves upon the test-time inference VLMs trained through reinforcement\nlearning, which demonstrates the potential of our method that utilizes world\nmodels for test-time scaling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12508", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12508", "abs": "https://arxiv.org/abs/2507.12508", "authors": ["Yuncong Yang", "Jiageng Liu", "Zheyuan Zhang", "Siyuan Zhou", "Reuben Tan", "Jianwei Yang", "Yilun Du", "Chuang Gan"], "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning", "comment": "Project Page: https://umass-embodied-agi.github.io/MindJourney", "summary": "Spatial reasoning in 3D space is central to human cognition and indispensable\nfor embodied tasks such as navigation and manipulation. However,\nstate-of-the-art vision-language models (VLMs) struggle frequently with tasks\nas simple as anticipating how a scene will look after an egocentric motion:\nthey perceive 2D images but lack an internal model of 3D dynamics. We therefore\npropose MindJourney, a test-time scaling framework that grants a VLM with this\nmissing capability by coupling it to a controllable world model based on video\ndiffusion. The VLM iteratively sketches a concise camera trajectory, while the\nworld model synthesizes the corresponding view at each step. The VLM then\nreasons over this multi-view evidence gathered during the interactive\nexploration. Without any fine-tuning, our MindJourney achieves over an average\n8% performance boost on the representative spatial reasoning benchmark SAT,\nshowing that pairing VLMs with world models for test-time scaling offers a\nsimple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also\nimproves upon the test-time inference VLMs trained through reinforcement\nlearning, which demonstrates the potential of our method that utilizes world\nmodels for test-time scaling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12952", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12952", "abs": "https://arxiv.org/abs/2507.12952", "authors": ["Jiaxiu Jiang", "Wenbo Li", "Jingjing Ren", "Yuping Qiu", "Yong Guo", "Xiaogang Xu", "Han Wu", "Wangmeng Zuo"], "title": "LoViC: Efficient Long Video Generation with Context Compression", "comment": "Project page: https://jiangjiaxiu.github.io/lovic/", "summary": "Despite recent advances in diffusion transformers (DiTs) for text-to-video\ngeneration, scaling to long-duration content remains challenging due to the\nquadratic complexity of self-attention. While prior efforts -- such as sparse\nattention and temporally autoregressive models -- offer partial relief, they\noften compromise temporal coherence or scalability. We introduce LoViC, a\nDiT-based framework trained on million-scale open-domain videos, designed to\nproduce long, coherent videos through a segment-wise generation process. At the\ncore of our approach is FlexFormer, an expressive autoencoder that jointly\ncompresses video and text into unified latent representations. It supports\nvariable-length inputs with linearly adjustable compression rates, enabled by a\nsingle query token design based on the Q-Former architecture. Additionally, by\nencoding temporal context through position-aware mechanisms, our model\nseamlessly supports prediction, retradiction, interpolation, and multi-shot\ngeneration within a unified paradigm. Extensive experiments across diverse\ntasks validate the effectiveness and versatility of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12667", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.12667", "abs": "https://arxiv.org/abs/2507.12667", "authors": ["Siyuan Yao", "Chaoli Wang"], "title": "VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians", "comment": null, "summary": "Visualization of large-scale time-dependent simulation data is crucial for\ndomain scientists to analyze complex phenomena, but it demands significant I/O\nbandwidth, storage, and computational resources. To enable effective\nvisualization on local, low-end machines, recent advances in view synthesis\ntechniques, such as neural radiance fields, utilize neural networks to generate\nnovel visualizations for volumetric scenes. However, these methods focus on\nreconstruction quality rather than facilitating interactive visualization\nexploration, such as feature extraction and tracking. We introduce VolSegGS, a\nnovel Gaussian splatting framework that supports interactive segmentation and\ntracking in dynamic volumetric scenes for exploratory visualization and\nanalysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic\nvolumetric scene, allowing for real-time novel view synthesis. For accurate\nsegmentation, we leverage the view-independent colors of Gaussians for\ncoarse-level segmentation and refine the results with an affinity field network\nfor fine-level segmentation. Additionally, by embedding segmentation results\nwithin the Gaussians, we ensure that their deformation enables continuous\ntracking of segmented regions over time. We demonstrate the effectiveness of\nVolSegGS with several time-varying datasets and compare our solutions against\nstate-of-the-art methods. With the ability to interact with a dynamic scene in\nreal time and provide flexible segmentation and tracking capabilities, VolSegGS\noffers a powerful solution under low computational demands. This framework\nunlocks exciting new possibilities for time-varying volumetric data analysis\nand visualization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12494", "categories": ["cs.AI", "cs.GT", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12494", "abs": "https://arxiv.org/abs/2507.12494", "authors": ["Dustin Holley", "Jovin D'sa", "Hossein Nourkhiz Mahjoub", "Gibran Ali"], "title": "MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents", "comment": "8 pages", "summary": "Enhancing simulation environments to replicate real-world driver behavior,\ni.e., more humanlike sim agents, is essential for developing autonomous vehicle\ntechnology. In the context of highway merging, previous works have studied the\noperational-level yielding dynamics of lag vehicles in response to a merging\ncar at highway on-ramps. Other works focusing on tactical decision modeling\ngenerally consider limited action sets or utilize payoff functions with large\nparameter sets and limited payoff bounds. In this work, we aim to improve the\nsimulation of the highway merge scenario by targeting a game theoretic model\nfor tactical decision-making with improved payoff functions and lag actions. We\ncouple this with an underlying dynamics model to have a unified decision and\ndynamics model that can capture merging interactions and simulate more\nrealistic interactions in an explainable and interpretable fashion. The\nproposed model demonstrated good reproducibility of complex interactions when\nvalidated on a real-world dataset. The model was finally integrated into a high\nfidelity simulation environment and confirmed to have adequate computation time\nefficiency for use in large-scale simulations to support autonomous vehicle\ndevelopment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12590", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12590", "abs": "https://arxiv.org/abs/2507.12590", "authors": ["Judy Long", "Tao Liu", "Sean Alexander Woznicki", "Miljana MarkoviÄ‡", "Oskar Marko", "Molly Sears"], "title": "Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer Learning Workflows", "comment": "A review article. 41 pages, 22 figures. Preprint", "summary": "Crop mapping involves identifying and classifying crop types using spatial\ndata, primarily derived from remote sensing imagery. This study presents the\nfirst comprehensive review of large-scale, pixel-wise crop mapping workflows,\nencompassing both conventional supervised methods and emerging transfer\nlearning approaches. To identify the optimal supervised crop mapping workflows,\nwe conducted systematic experiments, comparing six widely adopted satellite\nimage-based preprocessing methods, alongside eleven supervised pixel-wise\nclassification models. Additionally, we assessed the synergistic impact of\nvaried training sample sizes and variable combinations. Moreover, we identified\noptimal transfer learning techniques for different magnitudes of domain shift.\nThe evaluation of best methods was conducted across five diverse agricultural\nsites. Landsat 8 served as the primary satellite data source. Labels come from\nCDL trusted pixels and field surveys.\n  Our findings reveal three key insights. First, fine-scale interval\npreprocessing paired with Transformer models consistently delivered optimal\nperformance for both supervised and transferable workflows. RF offered rapid\ntraining and competitive performance in conventional supervised learning and\ndirect transfer to similar domains. Second, transfer learning techniques\nenhanced workflow adaptability, with UDA being effective for homogeneous crop\nclasses while fine-tuning remains robust across diverse scenarios. Finally,\nworkflow choice depends heavily on the availability of labeled samples. With a\nsufficient sample size, supervised training typically delivers more accurate\nand generalizable results. Below a certain threshold, transfer learning that\nmatches the level of domain shift is a viable alternative to achieve crop\nmapping. Repository:\nBest-Practices-for-Large-Scale-Pixel-Wise-Crop-Mapping-and-Transfer-Learning-Workflows", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12602", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12602", "abs": "https://arxiv.org/abs/2507.12602", "authors": ["Said Ohamouddou", "Abdellatif El Afia", "Hanaa El Afia", "Raddouane Chiheb"], "title": "MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification", "comment": null, "summary": "Tree species classification from terrestrial LiDAR point clouds is\nchallenging because of the complex multi-scale geometric structures in forest\nenvironments. Existing approaches using multi-scale dynamic graph convolutional\nneural networks (MS-DGCNN) employ parallel multi-scale processing, which fails\nto capture the semantic relationships between the hierarchical levels of the\ntree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion\ndynamic graph convolutional network that uses semantically meaningful feature\nextraction at local, branch, and canopy scales with cross-scale information\npropagation. Our method employs scale-specific feature engineering, including\nstandard geometric features for the local scale, normalized relative vectors\nfor the branch scale, and distance information for the canopy scale. This\nhierarchical approach replaces uniform parallel processing with semantically\ndifferentiated representations that are aligned with the natural tree\nstructure. Under the same proposed tree species data augmentation strategy for\nall experiments, MS-DGCNN++ achieved an accuracy of 94.96 \\% on STPCTLS,\noutperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On\nFOR-species20K, it achieves 67.25\\% accuracy (6.1\\% improvement compared to\nMS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN\nand MS-DGCNN with overall accuracies of 93.15\\% on ModelNet40 and 94.05\\% on\nModelNet10. With lower parameters and reduced complexity compared to\nstate-of-the-art transformer approaches, our method is suitable for\nresource-constrained applications while maintaining a competitive accuracy.\nBeyond tree classification, the method generalizes to standard 3D object\nrecognition, establishing it as a versatile solution for diverse point cloud\nprocessing applications. The implementation code is publicly available at\nhttps://github.com/said-ohamouddou/MS-DGCNN2.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12820", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12820", "abs": "https://arxiv.org/abs/2507.12820", "authors": ["Shiquan Wang", "Ruiyu Fang", "Zhongjiang He", "Shuangyong Song", "Yongxiang Li"], "title": "Emotional Support with LLM-based Empathetic Dialogue Generation", "comment": null, "summary": "Emotional Support Conversation (ESC) aims to provide empathetic and effective\nemotional assistance through dialogue, addressing the growing demand for mental\nhealth support. This paper presents our solution for the NLPCC 2025 Task 8 ESC\nevaluation, where we leverage large-scale language models enhanced by prompt\nengineering and finetuning techniques. We explore both parameter-efficient\nLow-Rank Adaptation and full-parameter fine-tuning strategies to improve the\nmodel's ability to generate supportive and contextually appropriate responses.\nOur best model ranked second in the competition, highlighting the potential of\ncombining LLMs with effective adaptation methods for ESC tasks. Future work\nwill focus on further enhancing emotional understanding and response\npersonalization to build more practical and reliable emotional support systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dialogue"], "score": 2}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12646", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12646", "abs": "https://arxiv.org/abs/2507.12646", "authors": ["Kaihua Chen", "Tarasha Khurana", "Deva Ramanan"], "title": "Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos", "comment": "Project page: https://cog-nvs.github.io/", "summary": "We explore novel-view synthesis for dynamic scenes from monocular videos.\nPrior approaches rely on costly test-time optimization of 4D representations or\ndo not preserve scene geometry when trained in a feed-forward manner. Our\napproach is based on three key insights: (1) covisible pixels (that are visible\nin both the input and target views) can be rendered by first reconstructing the\ndynamic 3D scene and rendering the reconstruction from the novel-views and (2)\nhidden pixels in novel views can be \"inpainted\" with feed-forward 2D video\ndiffusion models. Notably, our video inpainting diffusion model (CogNVS) can be\nself-supervised from 2D videos, allowing us to train it on a large corpus of\nin-the-wild videos. This in turn allows for (3) CogNVS to be applied zero-shot\nto novel test videos via test-time finetuning. We empirically verify that\nCogNVS outperforms almost all prior art for novel-view synthesis of dynamic\nscenes from monocular videos.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12759", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12759", "abs": "https://arxiv.org/abs/2507.12759", "authors": ["Yunxiang Zhang", "Muhammad Khalifa", "Lechen Zhang", "Xin Liu", "Ayoung Lee", "Xinliang Frederick Zhang", "Farima Fatahi Bayat", "Lu Wang"], "title": "Logit Arithmetic Elicits Long Reasoning Capabilities Without Training", "comment": null, "summary": "Large reasoning models (LRMs) can do complex reasoning via long\nchain-of-thought (CoT) involving cognitive strategies such as backtracking and\nself-correction. Recent studies suggest that some models inherently possess\nthese long reasoning abilities, which may be unlocked via extra training. Our\nwork first investigates whether we can elicit such behavior without any\ntraining. To this end, we propose a decoding-time approach, ThinkLogit, which\nutilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for\nlong reasoning using a substantially smaller model as guider. We then show that\nwe can further boost performance by training the guider model with preference\noptimization over correct/incorrect reasoning pairs sampled from both the\ntarget and guider model -- a setup we refer to as ThinkLogit-DPO. Our\nexperiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative\nimprovement in pass@1 by 26% and 29%, respectively, over four mathematical\ndatasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model\n21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills\nacquired through reinforcement learning, improving pass@1 by 13% relative\ncompared to the Qwen2.5-32B base model. Our work presents a\ncomputationally-efficient method to elicit long reasoning in large models with\nminimal or no additional training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "DPO"], "score": 3}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12663", "abs": "https://arxiv.org/abs/2507.12663", "authors": ["Inamullah", "Ernesto Elias Vidal Rosas", "Imran Razzak", "Shoaib Jameel"], "title": "Integrated Oculomics and Lipidomics Reveal Microvascular Metabolic Signatures Associated with Cardiovascular Health in a Healthy Cohort", "comment": null, "summary": "Cardiovascular disease (CVD) remains the leading global cause of mortality,\nyet current risk stratification methods often fail to detect early, subclinical\nchanges. Previous studies have generally not integrated retinal\nmicrovasculature characteristics with comprehensive serum lipidomic profiles as\npotential indicators of CVD risk. In this study, an innovative imaging omics\nframework was introduced, combining retinal microvascular traits derived\nthrough deep learning based image processing with serum lipidomic data to\nhighlight asymptomatic biomarkers of cardiovascular risk beyond the\nconventional lipid panel. This represents the first large scale, covariate\nadjusted and stratified correlation analysis conducted in a healthy population,\nwhich is essential for identifying early indicators of disease. Retinal\nphenotypes were quantified using automated image analysis tools, while serum\nlipid profiling was performed by Ultra High Performance Liquid Chromatography\nElectrospray ionization High resolution mass spectrometry (UHPLC ESI HRMS).\nStrong, age- and sex-independent correlations were established, particularly\nbetween average artery width, vessel density, and lipid subclasses such as\ntriacylglycerols (TAGs), diacylglycerols (DAGs), and ceramides (Cers). These\nassociations suggest a converging mechanism of microvascular remodeling under\nmetabolic stress. By linking detailed\n  vascular structural phenotypes to specific lipid species, this study fills a\ncritical gap in the understanding of early CVD pathogenesis. This integration\nnot only offers a novel perspective on microvascular metabolic associations but\nalso presents a significant opportunity for the identification of robust,\nnon-invasive biomarkers. Ultimately, these findings may support improved early\ndetection, targeted prevention, and personalized approaches in cardiovascular\nhealthcare.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12769", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.12769", "abs": "https://arxiv.org/abs/2507.12769", "authors": ["Keli Zheng", "Zerong Xie"], "title": "Synergy: End-to-end Concept Model", "comment": null, "summary": "In this paper, we present Synergy, a language model that bridges different\nlevels of abstraction in an end-to-end fashion through a learned routing\nmechanism. Focusing on low-level linguistic abstraction, we trained our model\nas a byte-level language model. Our model spontaneously learns to tokenize\nbytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)\ntokenizers while keeping comparable performance. By comparing with Llama3, we\nobserved an advantage of Synergy under the same model scale and training\ndataset size. Further studies show that the middle part (the higher abstraction\npart) of our model performs better when positional encodings are removed,\nsuggesting the emergence of position-independent concepts. These findings\ndemonstrate the feasibility of tokenizer-free architectures, paving the way for\nmore robust and flexible pipelines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12675", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.12675", "abs": "https://arxiv.org/abs/2507.12675", "authors": ["Christina Thrainer", "Md Meftahul Ferdaus", "Mahdi Abdelguerfi", "Christian Guetl", "Steven Sloan", "Kendall N. Niles", "Ken Pathak"], "title": "FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks", "comment": null, "summary": "Automated structural defect segmentation in civil infrastructure faces a\ncritical challenge: achieving high accuracy while maintaining computational\nefficiency for real-time deployment. This paper presents FORTRESS\n(Function-composition Optimized Real-Time Resilient Structural Segmentation), a\nnew architecture that balances accuracy and speed by using a special method\nthat combines depthwise separable convolutions with adaptive Kolmogorov-Arnold\nNetwork integration. FORTRESS incorporates three key innovations: a systematic\ndepthwise separable convolution framework achieving a 3.6x parameter reduction\nper layer, adaptive TiKAN integration that selectively applies function\ncomposition transformations only when computationally beneficial, and\nmulti-scale attention fusion combining spatial, channel, and KAN-enhanced\nfeatures across decoder levels. The architecture achieves remarkable efficiency\ngains with 91% parameter reduction (31M to 2.9M), 91% computational complexity\nreduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while\ndelivering superior segmentation performance. Evaluation on benchmark\ninfrastructure datasets demonstrates state-of-the-art results with an F1- score\nof 0.771 and a mean IoU of 0.677, significantly outperforming existing methods\nincluding U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves\nessential for optimal performance, establishing FORTRESS as a robust solution\nfor practical structural defect segmentation in resource-constrained\nenvironments where both accuracy and computational efficiency are paramount.\nComprehensive architectural specifications are provided in the Supplemental\nMaterial. Source code is available at URL:\nhttps://github.com/faeyelab/fortress-paper-code.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12727", "categories": ["cs.CV", "I.4"], "pdf": "https://arxiv.org/pdf/2507.12727", "abs": "https://arxiv.org/abs/2507.12727", "authors": ["Peijun Wang", "Jinhua Zhao"], "title": "SOD-YOLO: Enhancing YOLO-Based Detection of Small Objects in UAV Imagery", "comment": null, "summary": "Small object detection remains a challenging problem in the field of object\ndetection. To address this challenge, we propose an enhanced YOLOv8-based\nmodel, SOD-YOLO. This model integrates an ASF mechanism in the neck to enhance\nmulti-scale feature fusion, adds a Small Object Detection Layer (named P2) to\nprovide higher-resolution feature maps for better small object detection, and\nemploys Soft-NMS to refine confidence scores and retain true positives.\nExperimental results demonstrate that SOD-YOLO significantly improves detection\nperformance, achieving a 36.1% increase in mAP$_{50:95}$ and 20.6% increase in\nmAP$_{50}$ on the VisDrone2019-DET dataset compared to the baseline model.\nThese enhancements make SOD-YOLO a practical and efficient solution for small\nobject detection in UAV imagery. Our source code, hyper-parameters, and model\nweights are available at https://github.com/iamwangxiaobai/SOD-YOLO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12758", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12758", "abs": "https://arxiv.org/abs/2507.12758", "authors": ["Wangzheng Shi", "Yinglin Zheng", "Yuxin Lin", "Jianmin Bao", "Ming Zeng", "Dong Chen"], "title": "HairShifter: Consistent and High-Fidelity Video Hair Transfer via Anchor-Guided Animation", "comment": null, "summary": "Hair transfer is increasingly valuable across domains such as social media,\ngaming, advertising, and entertainment. While significant progress has been\nmade in single-image hair transfer, video-based hair transfer remains\nchallenging due to the need for temporal consistency, spatial fidelity, and\ndynamic adaptability. In this work, we propose HairShifter, a novel \"Anchor\nFrame + Animation\" framework that unifies high-quality image hair transfer with\nsmooth and coherent video animation. At its core, HairShifter integrates a\nImage Hair Transfer (IHT) module for precise per-frame transformation and a\nMulti-Scale Gated SPADE Decoder to ensure seamless spatial blending and\ntemporal coherence. Our method maintains hairstyle fidelity across frames while\npreserving non-hair regions. Extensive experiments demonstrate that HairShifter\nachieves state-of-the-art performance in video hairstyle transfer, combining\nsuperior visual quality, temporal consistency, and scalability. The code will\nbe publicly available. We believe this work will open new avenues for\nvideo-based hairstyle transfer and establish a robust baseline in this field.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12760", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12760", "abs": "https://arxiv.org/abs/2507.12760", "authors": ["Ruicheng Zhang", "Haowei Guo", "Kanghui Tian", "Jun Zhou", "Mingliang Yan", "Zeyu Zhang", "Shen Zhao"], "title": "Unified Medical Image Segmentation with State Space Modeling Snake", "comment": "This paper has been accepted by ACM MM 2025", "summary": "Unified Medical Image Segmentation (UMIS) is critical for comprehensive\nanatomical assessment but faces challenges due to multi-scale structural\nheterogeneity. Conventional pixel-based approaches, lacking object-level\nanatomical insight and inter-organ relational modeling, struggle with\nmorphological complexity and feature conflicts, limiting their efficacy in\nUMIS. We propose Mamba Snake, a novel deep snake framework enhanced by state\nspace modeling for UMIS. Mamba Snake frames multi-contour evolution as a\nhierarchical state space atlas, effectively modeling macroscopic inter-organ\ntopological relationships and microscopic contour refinements. We introduce a\nsnake-specific vision state space module, the Mamba Evolution Block (MEB),\nwhich leverages effective spatiotemporal information aggregation for adaptive\nrefinement of complex morphologies. Energy map shape priors further ensure\nrobust long-range contour evolution in heterogeneous data. Additionally, a\ndual-classification synergy mechanism is incorporated to concurrently optimize\ndetection and segmentation, mitigating under-segmentation of microstructures in\nUMIS. Extensive evaluations across five clinical datasets reveal Mamba Snake's\nsuperior performance, with an average Dice improvement of 3\\% over\nstate-of-the-art methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.13337", "categories": ["cs.AI", "cs.CC", "math.LO"], "pdf": "https://arxiv.org/pdf/2507.13337", "abs": "https://arxiv.org/abs/2507.13337", "authors": ["Gal Beniamini", "Yuval Dor", "Alon Vinnikov", "Shir Granot Peled", "Or Weinstein", "Or Sharir", "Noam Wies", "Tomer Nussbaum", "Ido Ben Shaul", "Tomer Zekharya", "Yoav Levine", "Shai Shalev-Shwartz", "Amnon Shashua"], "title": "FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming", "comment": null, "summary": "Frontier AI models demonstrate formidable breadth of knowledge. But how close\nare they to true human -- or superhuman -- expertise? Genuine experts can\ntackle the hardest problems and push the boundaries of scientific\nunderstanding. To illuminate the limits of frontier model capabilities, we turn\naway from contrived competitive programming puzzles, and instead focus on\nreal-life research problems.\n  We construct FormulaOne, a benchmark that lies at the intersection of graph\ntheory, logic, and algorithms, all well within the training distribution of\nfrontier models. Our problems are incredibly demanding, requiring an array of\nreasoning steps. The dataset has three key properties. First, it is of\ncommercial interest and relates to practical large-scale optimisation problems,\nsuch as those arising in routing, scheduling, and network design. Second, it is\ngenerated from the highly expressive framework of Monadic Second-Order (MSO)\nlogic on graphs, paving the way toward automatic problem generation at scale;\nideal for building RL environments. Third, many of our problems are intimately\nrelated to the frontier of theoretical computer science, and to central\nconjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As\nsuch, any significant algorithmic progress on our dataset, beyond known\nresults, could carry profound theoretical implications.\n  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on\nFormulaOne, solving less than 1% of the questions, even when given 10 attempts\nand explanatory fewshot examples -- highlighting how far they remain from\nexpert-level understanding in some domains. To support further research, we\nadditionally curate FormulaOne-Warmup, offering a set of simpler tasks, from\nthe same distribution. We release the full corpus along with a comprehensive\nevaluation framework.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12771", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12771", "abs": "https://arxiv.org/abs/2507.12771", "authors": ["Min-Jeong Lee", "Hee-Dong Kim", "Seong-Whan Lee"], "title": "Local Representative Token Guided Merging for Text-to-Image Generation", "comment": "6 pages", "summary": "Stable diffusion is an outstanding image generation model for text-to-image,\nbut its time-consuming generation process remains a challenge due to the\nquadratic complexity of attention operations. Recent token merging methods\nimprove efficiency by reducing the number of tokens during attention\noperations, but often overlook the characteristics of attention-based image\ngeneration models, limiting their effectiveness. In this paper, we propose\nlocal representative token guided merging (ReToM), a novel token merging\nstrategy applicable to any attention mechanism in image generation. To merge\ntokens based on various contextual information, ReToM defines local boundaries\nas windows within attention inputs and adjusts window sizes. Furthermore, we\nintroduce a representative token, which represents the most representative\ntoken per window by computing similarity at a specific timestep and selecting\nthe token with the highest average similarity. This approach preserves the most\nsalient local features while minimizing computational overhead. Experimental\nresults show that ReToM achieves a 6.2% improvement in FID and higher CLIP\nscores compared to the baseline, while maintaining comparable inference time.\nWe empirically demonstrate that ReToM is effective in balancing visual quality\nand computational efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.13238", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13238", "abs": "https://arxiv.org/abs/2507.13238", "authors": ["Ashray Gupta", "Rohan Joseph", "Sunny Rai"], "title": "HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language Models", "comment": null, "summary": "Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12795", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12795", "abs": "https://arxiv.org/abs/2507.12795", "authors": ["Penglei Sun", "Yaoxian Song", "Xiangru Zhu", "Xiang Liu", "Qiang Wang", "Yue Liu", "Changqun Xia", "Tiefeng Li", "Yang Yang", "Xiaowen Chu"], "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning", "comment": null, "summary": "Scene understanding enables intelligent agents to interpret and comprehend\ntheir environment. While existing large vision-language models (LVLMs) for\nscene understanding have primarily focused on indoor household tasks, they face\ntwo significant limitations when applied to outdoor large-scale scene\nunderstanding. First, outdoor scenarios typically encompass larger-scale\nenvironments observed through various sensors from multiple viewpoints (e.g.,\nbird view and terrestrial view), while existing indoor LVLMs mainly analyze\nsingle visual modalities within building-scale contexts from humanoid\nviewpoints. Second, existing LVLMs suffer from missing multidomain perception\noutdoor data and struggle to effectively integrate 2D and 3D visual\ninformation. To address the aforementioned limitations, we build the first\nmultidomain perception outdoor scene understanding dataset, named\n\\textbf{\\underline{SVM-City}}, deriving from multi\\textbf{\\underline{S}}cale\nscenarios with multi\\textbf{\\underline{V}}iew and\nmulti\\textbf{\\underline{M}}odal instruction tuning data. It contains $420$k\nimages and $4, 811$M point clouds with $567$k question-answering pairs from\nvehicles, low-altitude drones, high-altitude aerial planes, and satellite. To\neffectively fuse the multimodal data in the absence of one modality, we\nintroduce incomplete multimodal learning to model outdoor scene understanding\nand design the LVLM named \\textbf{\\underline{City-VLM}}. Multimodal fusion is\nrealized by constructing a joint probabilistic distribution space rather than\nimplementing directly explicit fusion operations (e.g., concatenation).\nExperimental results on three typical outdoor scene understanding tasks show\nCity-VLM achieves $18.14 \\%$ performance surpassing existing LVLMs in\nquestion-answering tasks averagely. Our method demonstrates pragmatic and\ngeneralization performance across multiple outdoor scenes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12602", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12602", "abs": "https://arxiv.org/abs/2507.12602", "authors": ["Said Ohamouddou", "Abdellatif El Afia", "Hanaa El Afia", "Raddouane Chiheb"], "title": "MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification", "comment": null, "summary": "Tree species classification from terrestrial LiDAR point clouds is\nchallenging because of the complex multi-scale geometric structures in forest\nenvironments. Existing approaches using multi-scale dynamic graph convolutional\nneural networks (MS-DGCNN) employ parallel multi-scale processing, which fails\nto capture the semantic relationships between the hierarchical levels of the\ntree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion\ndynamic graph convolutional network that uses semantically meaningful feature\nextraction at local, branch, and canopy scales with cross-scale information\npropagation. Our method employs scale-specific feature engineering, including\nstandard geometric features for the local scale, normalized relative vectors\nfor the branch scale, and distance information for the canopy scale. This\nhierarchical approach replaces uniform parallel processing with semantically\ndifferentiated representations that are aligned with the natural tree\nstructure. Under the same proposed tree species data augmentation strategy for\nall experiments, MS-DGCNN++ achieved an accuracy of 94.96 \\% on STPCTLS,\noutperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On\nFOR-species20K, it achieves 67.25\\% accuracy (6.1\\% improvement compared to\nMS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN\nand MS-DGCNN with overall accuracies of 93.15\\% on ModelNet40 and 94.05\\% on\nModelNet10. With lower parameters and reduced complexity compared to\nstate-of-the-art transformer approaches, our method is suitable for\nresource-constrained applications while maintaining a competitive accuracy.\nBeyond tree classification, the method generalizes to standard 3D object\nrecognition, establishing it as a versatile solution for diverse point cloud\nprocessing applications. The implementation code is publicly available at\nhttps://github.com/said-ohamouddou/MS-DGCNN2.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.13255", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13255", "abs": "https://arxiv.org/abs/2507.13255", "authors": ["Lyucheng Wu", "Mengru Wang", "Ziwen Xu", "Tri Cao", "Nay Oo", "Bryan Hooi", "Shumin Deng"], "title": "Automating Steering for Safe Multimodal Large Language Models", "comment": "Working in progress. 22 pages (8+ for main); 25 figures; 1 table", "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12675", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.12675", "abs": "https://arxiv.org/abs/2507.12675", "authors": ["Christina Thrainer", "Md Meftahul Ferdaus", "Mahdi Abdelguerfi", "Christian Guetl", "Steven Sloan", "Kendall N. Niles", "Ken Pathak"], "title": "FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks", "comment": null, "summary": "Automated structural defect segmentation in civil infrastructure faces a\ncritical challenge: achieving high accuracy while maintaining computational\nefficiency for real-time deployment. This paper presents FORTRESS\n(Function-composition Optimized Real-Time Resilient Structural Segmentation), a\nnew architecture that balances accuracy and speed by using a special method\nthat combines depthwise separable convolutions with adaptive Kolmogorov-Arnold\nNetwork integration. FORTRESS incorporates three key innovations: a systematic\ndepthwise separable convolution framework achieving a 3.6x parameter reduction\nper layer, adaptive TiKAN integration that selectively applies function\ncomposition transformations only when computationally beneficial, and\nmulti-scale attention fusion combining spatial, channel, and KAN-enhanced\nfeatures across decoder levels. The architecture achieves remarkable efficiency\ngains with 91% parameter reduction (31M to 2.9M), 91% computational complexity\nreduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while\ndelivering superior segmentation performance. Evaluation on benchmark\ninfrastructure datasets demonstrates state-of-the-art results with an F1- score\nof 0.771 and a mean IoU of 0.677, significantly outperforming existing methods\nincluding U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves\nessential for optimal performance, establishing FORTRESS as a robust solution\nfor practical structural defect segmentation in resource-constrained\nenvironments where both accuracy and computational efficiency are paramount.\nComprehensive architectural specifications are provided in the Supplemental\nMaterial. Source code is available at URL:\nhttps://github.com/faeyelab/fortress-paper-code.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.13266", "categories": ["cs.CL", "cs.AI", "68T50"], "pdf": "https://arxiv.org/pdf/2507.13266", "abs": "https://arxiv.org/abs/2507.13266", "authors": ["Jiazheng Li", "Hong Lu", "Kaiyue Wen", "Zaiwen Yang", "Jiaxuan Gao", "Hongzhou Lin", "Yi Wu", "Jingzhao Zhang"], "title": "QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation", "comment": "19 pages, 8 figures", "summary": "Reinforcement learning (RL) has become a key component in training large\nlanguage reasoning models (LLMs). However, recent studies questions its\neffectiveness in improving multi-step reasoning-particularly on hard problems.\nTo address this challenge, we propose a simple yet effective strategy via\nQuestion Augmentation: introduce partial solutions during training to reduce\nproblem difficulty and provide more informative learning signals. Our method,\nQuestA, when applied during RL training on math reasoning tasks, not only\nimproves pass@1 but also pass@k-particularly on problems where standard RL\nstruggles to make progress. This enables continual improvement over strong\nopen-source models such as DeepScaleR and OpenMath Nemotron, further enhancing\ntheir reasoning capabilities. We achieve new state-of-the-art results on math\nbenchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)\non AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical\nexplanations that QuestA improves sample efficiency, offering a practical and\ngeneralizable pathway for expanding reasoning capability through RL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12759", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12759", "abs": "https://arxiv.org/abs/2507.12759", "authors": ["Yunxiang Zhang", "Muhammad Khalifa", "Lechen Zhang", "Xin Liu", "Ayoung Lee", "Xinliang Frederick Zhang", "Farima Fatahi Bayat", "Lu Wang"], "title": "Logit Arithmetic Elicits Long Reasoning Capabilities Without Training", "comment": null, "summary": "Large reasoning models (LRMs) can do complex reasoning via long\nchain-of-thought (CoT) involving cognitive strategies such as backtracking and\nself-correction. Recent studies suggest that some models inherently possess\nthese long reasoning abilities, which may be unlocked via extra training. Our\nwork first investigates whether we can elicit such behavior without any\ntraining. To this end, we propose a decoding-time approach, ThinkLogit, which\nutilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for\nlong reasoning using a substantially smaller model as guider. We then show that\nwe can further boost performance by training the guider model with preference\noptimization over correct/incorrect reasoning pairs sampled from both the\ntarget and guider model -- a setup we refer to as ThinkLogit-DPO. Our\nexperiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative\nimprovement in pass@1 by 26% and 29%, respectively, over four mathematical\ndatasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model\n21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills\nacquired through reinforcement learning, improving pass@1 by 13% relative\ncompared to the Qwen2.5-32B base model. Our work presents a\ncomputationally-efficient method to elicit long reasoning in large models with\nminimal or no additional training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "DPO"], "score": 3}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12760", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12760", "abs": "https://arxiv.org/abs/2507.12760", "authors": ["Ruicheng Zhang", "Haowei Guo", "Kanghui Tian", "Jun Zhou", "Mingliang Yan", "Zeyu Zhang", "Shen Zhao"], "title": "Unified Medical Image Segmentation with State Space Modeling Snake", "comment": "This paper has been accepted by ACM MM 2025", "summary": "Unified Medical Image Segmentation (UMIS) is critical for comprehensive\nanatomical assessment but faces challenges due to multi-scale structural\nheterogeneity. Conventional pixel-based approaches, lacking object-level\nanatomical insight and inter-organ relational modeling, struggle with\nmorphological complexity and feature conflicts, limiting their efficacy in\nUMIS. We propose Mamba Snake, a novel deep snake framework enhanced by state\nspace modeling for UMIS. Mamba Snake frames multi-contour evolution as a\nhierarchical state space atlas, effectively modeling macroscopic inter-organ\ntopological relationships and microscopic contour refinements. We introduce a\nsnake-specific vision state space module, the Mamba Evolution Block (MEB),\nwhich leverages effective spatiotemporal information aggregation for adaptive\nrefinement of complex morphologies. Energy map shape priors further ensure\nrobust long-range contour evolution in heterogeneous data. Additionally, a\ndual-classification synergy mechanism is incorporated to concurrently optimize\ndetection and segmentation, mitigating under-segmentation of microstructures in\nUMIS. Extensive evaluations across five clinical datasets reveal Mamba Snake's\nsuperior performance, with an average Dice improvement of 3\\% over\nstate-of-the-art methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12769", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.12769", "abs": "https://arxiv.org/abs/2507.12769", "authors": ["Keli Zheng", "Zerong Xie"], "title": "Synergy: End-to-end Concept Model", "comment": null, "summary": "In this paper, we present Synergy, a language model that bridges different\nlevels of abstraction in an end-to-end fashion through a learned routing\nmechanism. Focusing on low-level linguistic abstraction, we trained our model\nas a byte-level language model. Our model spontaneously learns to tokenize\nbytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)\ntokenizers while keeping comparable performance. By comparing with Llama3, we\nobserved an advantage of Synergy under the same model scale and training\ndataset size. Further studies show that the middle part (the higher abstraction\npart) of our model performs better when positional encodings are removed,\nsuggesting the emergence of position-independent concepts. These findings\ndemonstrate the feasibility of tokenizer-free architectures, paving the way for\nmore robust and flexible pipelines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12771", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12771", "abs": "https://arxiv.org/abs/2507.12771", "authors": ["Min-Jeong Lee", "Hee-Dong Kim", "Seong-Whan Lee"], "title": "Local Representative Token Guided Merging for Text-to-Image Generation", "comment": "6 pages", "summary": "Stable diffusion is an outstanding image generation model for text-to-image,\nbut its time-consuming generation process remains a challenge due to the\nquadratic complexity of attention operations. Recent token merging methods\nimprove efficiency by reducing the number of tokens during attention\noperations, but often overlook the characteristics of attention-based image\ngeneration models, limiting their effectiveness. In this paper, we propose\nlocal representative token guided merging (ReToM), a novel token merging\nstrategy applicable to any attention mechanism in image generation. To merge\ntokens based on various contextual information, ReToM defines local boundaries\nas windows within attention inputs and adjusts window sizes. Furthermore, we\nintroduce a representative token, which represents the most representative\ntoken per window by computing similarity at a specific timestep and selecting\nthe token with the highest average similarity. This approach preserves the most\nsalient local features while minimizing computational overhead. Experimental\nresults show that ReToM achieves a 6.2% improvement in FID and higher CLIP\nscores compared to the baseline, while maintaining comparable inference time.\nWe empirically demonstrate that ReToM is effective in balancing visual quality\nand computational efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12795", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12795", "abs": "https://arxiv.org/abs/2507.12795", "authors": ["Penglei Sun", "Yaoxian Song", "Xiangru Zhu", "Xiang Liu", "Qiang Wang", "Yue Liu", "Changqun Xia", "Tiefeng Li", "Yang Yang", "Xiaowen Chu"], "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning", "comment": null, "summary": "Scene understanding enables intelligent agents to interpret and comprehend\ntheir environment. While existing large vision-language models (LVLMs) for\nscene understanding have primarily focused on indoor household tasks, they face\ntwo significant limitations when applied to outdoor large-scale scene\nunderstanding. First, outdoor scenarios typically encompass larger-scale\nenvironments observed through various sensors from multiple viewpoints (e.g.,\nbird view and terrestrial view), while existing indoor LVLMs mainly analyze\nsingle visual modalities within building-scale contexts from humanoid\nviewpoints. Second, existing LVLMs suffer from missing multidomain perception\noutdoor data and struggle to effectively integrate 2D and 3D visual\ninformation. To address the aforementioned limitations, we build the first\nmultidomain perception outdoor scene understanding dataset, named\n\\textbf{\\underline{SVM-City}}, deriving from multi\\textbf{\\underline{S}}cale\nscenarios with multi\\textbf{\\underline{V}}iew and\nmulti\\textbf{\\underline{M}}odal instruction tuning data. It contains $420$k\nimages and $4, 811$M point clouds with $567$k question-answering pairs from\nvehicles, low-altitude drones, high-altitude aerial planes, and satellite. To\neffectively fuse the multimodal data in the absence of one modality, we\nintroduce incomplete multimodal learning to model outdoor scene understanding\nand design the LVLM named \\textbf{\\underline{City-VLM}}. Multimodal fusion is\nrealized by constructing a joint probabilistic distribution space rather than\nimplementing directly explicit fusion operations (e.g., concatenation).\nExperimental results on three typical outdoor scene understanding tasks show\nCity-VLM achieves $18.14 \\%$ performance surpassing existing LVLMs in\nquestion-answering tasks averagely. Our method demonstrates pragmatic and\ngeneralization performance across multiple outdoor scenes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12857", "abs": "https://arxiv.org/abs/2507.12857", "authors": ["Shiqi Huang", "Shuting He", "Huaiyuan Qin", "Bihan Wen"], "title": "SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation", "comment": "ICCV 2025", "summary": "Most existing remote sensing instance segmentation approaches are designed\nfor close-vocabulary prediction, limiting their ability to recognize novel\ncategories or generalize across datasets. This restricts their applicability in\ndiverse Earth observation scenarios. To address this, we introduce\nopen-vocabulary (OV) learning for remote sensing instance segmentation. While\ncurrent OV segmentation models perform well on natural image datasets, their\ndirect application to remote sensing faces challenges such as diverse\nlandscapes, seasonal variations, and the presence of small or ambiguous objects\nin aerial imagery. To overcome these challenges, we propose $\\textbf{SCORE}$\n($\\textbf{S}$cene $\\textbf{C}$ontext matters in $\\textbf{O}$pen-vocabulary\n$\\textbf{RE}$mote sensing instance segmentation), a framework that integrates\nmulti-granularity scene context, i.e., regional context and global context, to\nenhance both visual and textual representations. Specifically, we introduce\nRegion-Aware Integration, which refines class embeddings with regional context\nto improve object distinguishability. Additionally, we propose Global Context\nAdaptation, which enriches naive text embeddings with remote sensing global\ncontext, creating a more adaptable and expressive linguistic latent space for\nthe classifier. We establish new benchmarks for OV remote sensing instance\nsegmentation across diverse datasets. Experimental results demonstrate that,\nour proposed method achieves SOTA performance, which provides a robust solution\nfor large-scale, real-world geospatial analysis. Our code is available at\nhttps://github.com/HuangShiqi128/SCORE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12820", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12820", "abs": "https://arxiv.org/abs/2507.12820", "authors": ["Shiquan Wang", "Ruiyu Fang", "Zhongjiang He", "Shuangyong Song", "Yongxiang Li"], "title": "Emotional Support with LLM-based Empathetic Dialogue Generation", "comment": null, "summary": "Emotional Support Conversation (ESC) aims to provide empathetic and effective\nemotional assistance through dialogue, addressing the growing demand for mental\nhealth support. This paper presents our solution for the NLPCC 2025 Task 8 ESC\nevaluation, where we leverage large-scale language models enhanced by prompt\nengineering and finetuning techniques. We explore both parameter-efficient\nLow-Rank Adaptation and full-parameter fine-tuning strategies to improve the\nmodel's ability to generate supportive and contextually appropriate responses.\nOur best model ranked second in the competition, highlighting the potential of\ncombining LLMs with effective adaptation methods for ESC tasks. Future work\nwill focus on further enhancing emotional understanding and response\npersonalization to build more practical and reliable emotional support systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dialogue"], "score": 2}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12933", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12933", "abs": "https://arxiv.org/abs/2507.12933", "authors": ["Dongyeun Lee", "Jiwan Hur", "Hyounguk Shon", "Jae Young Lee", "Junmo Kim"], "title": "DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization", "comment": "Accepted by ICCV 2025", "summary": "Diffusion models have achieved remarkable success in image generation but\ncome with significant computational costs, posing challenges for deployment in\nresource-constrained environments. Recent post-training quantization (PTQ)\nmethods have attempted to mitigate this issue by focusing on the iterative\nnature of diffusion models. However, these approaches often overlook outliers,\nleading to degraded performance at low bit-widths. In this paper, we propose a\nDMQ which combines Learned Equivalent Scaling (LES) and channel-wise\nPower-of-Two Scaling (PTS) to effectively address these challenges. Learned\nEquivalent Scaling optimizes channel-wise scaling factors to redistribute\nquantization difficulty between weights and activations, reducing overall\nquantization error. Recognizing that early denoising steps, despite having\nsmall quantization errors, crucially impact the final output due to error\naccumulation, we incorporate an adaptive timestep weighting scheme to\nprioritize these critical steps during learning. Furthermore, identifying that\nlayers such as skip connections exhibit high inter-channel variance, we\nintroduce channel-wise Power-of-Two Scaling for activations. To ensure robust\nselection of PTS factors even with small calibration set, we introduce a voting\nalgorithm that enhances reliability. Extensive experiments demonstrate that our\nmethod significantly outperforms existing works, especially at low bit-widths\nsuch as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image\ngeneration quality and model stability. The code is available at\nhttps://github.com/LeeDongYeun/dmq.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12905", "abs": "https://arxiv.org/abs/2507.12905", "authors": ["Tomohiro Suzuki", "Ryota Tanaka", "Calvin Yeung", "Keisuke Fujii"], "title": "AthleticsPose: Authentic Sports Motion Dataset on Athletic Field and Evaluation of Monocular 3D Pose Estimation Ability", "comment": "9 pages, 5 figures, 5 tables", "summary": "Monocular 3D pose estimation is a promising, flexible alternative to costly\nmotion capture systems for sports analysis. However, its practical application\nis hindered by two factors: a lack of realistic sports datasets and unclear\nreliability for sports tasks. To address these challenges, we introduce the\nAthleticsPose dataset, a new public dataset featuring ``real'' motions captured\nfrom 23 athletes performing various athletics events on an athletic field.\nUsing this dataset, we trained a representative 3D pose estimation model and\nperformed a comprehensive evaluation. Our results show that the model trained\non AthleticsPose significantly outperforms a baseline model trained on an\nimitated sports motion dataset, reducing MPJPE by approximately 75 %. These\nresults show the importance of training on authentic sports motion data, as\nmodels based on imitated motions do not effectively transfer to real-world\nmotions. Further analysis reveals that estimation accuracy is sensitive to\ncamera view and subject scale. In case studies of kinematic indicators, the\nmodel demonstrated the potential to capture individual differences in knee\nangles but struggled with higher-speed metrics, such as knee-drive velocity,\ndue to prediction biases. This work provides the research community with a\nvaluable dataset and clarifies the potential and practical limitations of using\nmonocular 3D pose estimation for sports motion analysis. Our dataset, code, and\ncheckpoints are available at https://github.com/SZucchini/AthleticsPose.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12933", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12933", "abs": "https://arxiv.org/abs/2507.12933", "authors": ["Dongyeun Lee", "Jiwan Hur", "Hyounguk Shon", "Jae Young Lee", "Junmo Kim"], "title": "DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization", "comment": "Accepted by ICCV 2025", "summary": "Diffusion models have achieved remarkable success in image generation but\ncome with significant computational costs, posing challenges for deployment in\nresource-constrained environments. Recent post-training quantization (PTQ)\nmethods have attempted to mitigate this issue by focusing on the iterative\nnature of diffusion models. However, these approaches often overlook outliers,\nleading to degraded performance at low bit-widths. In this paper, we propose a\nDMQ which combines Learned Equivalent Scaling (LES) and channel-wise\nPower-of-Two Scaling (PTS) to effectively address these challenges. Learned\nEquivalent Scaling optimizes channel-wise scaling factors to redistribute\nquantization difficulty between weights and activations, reducing overall\nquantization error. Recognizing that early denoising steps, despite having\nsmall quantization errors, crucially impact the final output due to error\naccumulation, we incorporate an adaptive timestep weighting scheme to\nprioritize these critical steps during learning. Furthermore, identifying that\nlayers such as skip connections exhibit high inter-channel variance, we\nintroduce channel-wise Power-of-Two Scaling for activations. To ensure robust\nselection of PTS factors even with small calibration set, we introduce a voting\nalgorithm that enhances reliability. Extensive experiments demonstrate that our\nmethod significantly outperforms existing works, especially at low bit-widths\nsuch as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image\ngeneration quality and model stability. The code is available at\nhttps://github.com/LeeDongYeun/dmq.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.12945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12945", "abs": "https://arxiv.org/abs/2507.12945", "authors": ["Yucheng Tang", "Yunguan Fu", "Weixi Yi", "Yipei Wang", "Daniel C. Alexander", "Rhodri Davies", "Yipeng Hu"], "title": "Analysis of Image-and-Text Uncertainty Propagation in Multimodal Large Language Models with Cardiac MR-Based Applications", "comment": "It is accepted by 28th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI) 2025", "summary": "Multimodal large language models (MLLMs) can process and integrate\ninformation from multimodality sources, such as text and images. However,\ninterrelationship among input modalities, uncertainties due to individual\nuni-modal data and potential clinical applications following such an\nuncertainty decomposition are yet fully understood in the context of\nlarge-scale MLLMs. In this work, we propose a multimodal uncertainty\npropagation model (MUPM) based on uncertainty propagation, to characterise the\nrelationship among the uncertainties arising from image-only, text-only, and\njoint image-text variations in MLLM inputs. Using real clinical data consisting\nof cardiac MR scans and digital health records, we describe that MUPMs can be\noptimised robustly with a few samples. We then show that the fitted MUPMs are\ngeneralisable across different input data distributions and, perhaps\nsurprisingly, across different downstream tasks. Such a transferability may be\nexplained by the shared pretraining, comparatively light MLLM fine-tuning,\nalong with the low-dimensional nature of the MUPMs. More importantly, this\nlearned transferability, quantifying the relationship between these\nuncertainties, led to direct clinical applications in which uncertainties may\nbe estimated and thus analysed robustly for varying data or even a novel set of\ncardiac disease prediction tasks. In addition, we show experimentally the\nefficiency in multimodal data required for estimating the overall uncertainty\nand its ability to identify redundant factors, both of which are considered\npractical yet clinically useful applications with the proposed MUPMs. Codes are\navailable at https://github.com/yucheng722/MUPM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.13238", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13238", "abs": "https://arxiv.org/abs/2507.13238", "authors": ["Ashray Gupta", "Rohan Joseph", "Sunny Rai"], "title": "HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language Models", "comment": null, "summary": "Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.13255", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13255", "abs": "https://arxiv.org/abs/2507.13255", "authors": ["Lyucheng Wu", "Mengru Wang", "Ziwen Xu", "Tri Cao", "Nay Oo", "Bryan Hooi", "Shumin Deng"], "title": "Automating Steering for Safe Multimodal Large Language Models", "comment": "Working in progress. 22 pages (8+ for main); 25 figures; 1 table", "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.13266", "categories": ["cs.CL", "cs.AI", "68T50"], "pdf": "https://arxiv.org/pdf/2507.13266", "abs": "https://arxiv.org/abs/2507.13266", "authors": ["Jiazheng Li", "Hong Lu", "Kaiyue Wen", "Zaiwen Yang", "Jiaxuan Gao", "Hongzhou Lin", "Yi Wu", "Jingzhao Zhang"], "title": "QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation", "comment": "19 pages, 8 figures", "summary": "Reinforcement learning (RL) has become a key component in training large\nlanguage reasoning models (LLMs). However, recent studies questions its\neffectiveness in improving multi-step reasoning-particularly on hard problems.\nTo address this challenge, we propose a simple yet effective strategy via\nQuestion Augmentation: introduce partial solutions during training to reduce\nproblem difficulty and provide more informative learning signals. Our method,\nQuestA, when applied during RL training on math reasoning tasks, not only\nimproves pass@1 but also pass@k-particularly on problems where standard RL\nstruggles to make progress. This enables continual improvement over strong\nopen-source models such as DeepScaleR and OpenMath Nemotron, further enhancing\ntheir reasoning capabilities. We achieve new state-of-the-art results on math\nbenchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)\non AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical\nexplanations that QuestA improves sample efficiency, offering a practical and\ngeneralizable pathway for expanding reasoning capability through RL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.13018", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13018", "abs": "https://arxiv.org/abs/2507.13018", "authors": ["Songlin Li", "Guofeng Yu", "Zhiqing Guo", "Yunfeng Diao", "Dan Ma", "Gaobo Yang", "Liejun Wang"], "title": "Beyond Fully Supervised Pixel Annotations: Scribble-Driven Weakly-Supervised Framework for Image Manipulation Localization", "comment": null, "summary": "Deep learning-based image manipulation localization (IML) methods have\nachieved remarkable performance in recent years, but typically rely on\nlarge-scale pixel-level annotated datasets. To address the challenge of\nacquiring high-quality annotations, some recent weakly supervised methods\nutilize image-level labels to segment manipulated regions. However, the\nperformance is still limited due to insufficient supervision signals. In this\nstudy, we explore a form of weak supervision that improves the annotation\nefficiency and detection performance, namely scribble annotation supervision.\nWe re-annotated mainstream IML datasets with scribble labels and propose the\nfirst scribble-based IML (Sc-IML) dataset. Additionally, we propose the first\nscribble-based weakly supervised IML framework. Specifically, we employ\nself-supervised training with a structural consistency loss to encourage the\nmodel to produce consistent predictions under multi-scale and augmented inputs.\nIn addition, we propose a prior-aware feature modulation module (PFMM) that\nadaptively integrates prior information from both manipulated and authentic\nregions for dynamic feature adjustment, further enhancing feature\ndiscriminability and prediction consistency in complex scenes. We also propose\na gated adaptive fusion module (GAFM) that utilizes gating mechanisms to\nregulate information flow during feature fusion, guiding the model toward\nemphasizing potential tampered regions. Finally, we propose a confidence-aware\nentropy minimization loss (${\\mathcal{L}}_{ {CEM }}$). This loss dynamically\nregularizes predictions in weakly annotated or unlabeled regions based on model\nuncertainty, effectively suppressing unreliable predictions. Experimental\nresults show that our method outperforms existing fully supervised approaches\nin terms of average performance both in-distribution and out-of-distribution.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "consistency"], "score": 3}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.13061", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13061", "abs": "https://arxiv.org/abs/2507.13061", "authors": ["Jingyao Wang", "Yiming Chen", "Lingyu Si", "Changwen Zheng"], "title": "Advancing Complex Wide-Area Scene Understanding with Hierarchical Coresets Selection", "comment": null, "summary": "Scene understanding is one of the core tasks in computer vision, aiming to\nextract semantic information from images to identify objects, scene categories,\nand their interrelationships. Although advancements in Vision-Language Models\n(VLMs) have driven progress in this field, existing VLMs still face challenges\nin adaptation to unseen complex wide-area scenes. To address the challenges,\nthis paper proposes a Hierarchical Coresets Selection (HCS) mechanism to\nadvance the adaptation of VLMs in complex wide-area scene understanding. It\nprogressively refines the selected regions based on the proposed theoretically\nguaranteed importance function, which considers utility, representativeness,\nrobustness, and synergy. Without requiring additional fine-tuning, HCS enables\nVLMs to achieve rapid understandings of unseen scenes at any scale using\nminimal interpretable regions while mitigating insufficient feature density.\nHCS is a plug-and-play method that is compatible with any VLM. Experiments\ndemonstrate that HCS achieves superior performance and universality in various\ntasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.13107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13107", "abs": "https://arxiv.org/abs/2507.13107", "authors": ["Xiaohan Guo", "Yusong Cai", "Zejia Liu", "Zhengning Wang", "Lili Pan", "Hongliang Li"], "title": "R^2MoE: Redundancy-Removal Mixture of Experts for Lifelong Concept Learning", "comment": null, "summary": "Enabling large-scale generative models to continuously learn new visual\nconcepts is essential for personalizing pre-trained models to meet individual\nuser preferences. Existing approaches for continual visual concept learning are\nconstrained by two fundamental challenges: catastrophic forgetting and\nparameter expansion. In this paper, we propose Redundancy-Removal Mixture of\nExperts (R^2MoE), a parameter-efficient framework for lifelong visual concept\nlearning that effectively learns new concepts while incurring minimal parameter\noverhead. Our framework includes three key innovative contributions: First, we\npropose a mixture-of-experts framework with a routing distillation mechanism\nthat enables experts to acquire concept-specific knowledge while preserving the\ngating network's routing capability, thereby effectively mitigating\ncatastrophic forgetting. Second, we propose a strategy for eliminating\nredundant layer-wise experts that reduces the number of expert parameters by\nfully utilizing previously learned experts. Third, we employ a hierarchical\nlocal attention-guided inference approach to mitigate interference between\ngenerated visual concepts. Extensive experiments have demonstrated that our\nmethod generates images with superior conceptual fidelity compared to the\nstate-of-the-art (SOTA) method, achieving an impressive 87.8\\% reduction in\nforgetting rates and 63.3\\% fewer parameters on the CustomConcept 101 dataset.\nOur code is available at {https://github.com/learninginvision/R2MoE}", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
{"id": "2507.13347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13347", "abs": "https://arxiv.org/abs/2507.13347", "authors": ["Yifan Wang", "Jianjun Zhou", "Haoyi Zhu", "Wenzheng Chang", "Yang Zhou", "Zizun Li", "Junyi Chen", "Jiangmiao Pang", "Chunhua Shen", "Tong He"], "title": "$Ï€^3$: Scalable Permutation-Equivariant Visual Geometry Learning", "comment": "Project page: https://yyfz.github.io/pi3/", "summary": "We introduce $\\pi^3$, a feed-forward neural network that offers a novel\napproach to visual geometry reconstruction, breaking the reliance on a\nconventional fixed reference view. Previous methods often anchor their\nreconstructions to a designated viewpoint, an inductive bias that can lead to\ninstability and failures if the reference is suboptimal. In contrast, $\\pi^3$\nemploys a fully permutation-equivariant architecture to predict\naffine-invariant camera poses and scale-invariant local point maps without any\nreference frames. This design makes our model inherently robust to input\nordering and highly scalable. These advantages enable our simple and bias-free\napproach to achieve state-of-the-art performance on a wide range of tasks,\nincluding camera pose estimation, monocular/video depth estimation, and dense\npoint map reconstruction. Code and models are publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-18.jsonl"}
