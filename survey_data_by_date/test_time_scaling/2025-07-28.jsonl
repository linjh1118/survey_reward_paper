{"id": "2507.18742", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18742", "abs": "https://arxiv.org/abs/2507.18742", "authors": ["Víctor Gallego"], "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement", "comment": "Accepted to SCALR Workshop @ COLM 2025", "summary": "Language models (LMs) are susceptible to in-context reward hacking, where\nthey exploit flaws in tainted or faulty written specifications or rubrics to\nachieve high scores without fulfilling the user's true intent. We introduce\nSpecification Self-Correction (SSC), a novel, test-time framework that enables\nan LM to identify and correct flaws within its own guiding specification. SSC\nemploys a multi-step inference process where the model first generates a\nresponse based on a potentially tainted specification, critiques its output,\nand then revises the specification itself to remove the exploitable loophole. A\nfinal, more robust response is then generated using this self-corrected\nspecification. Across experiments spanning creative writing and agentic coding\ntasks with several LMs, we demonstrate that while models initially game tainted\nspecifications in 50-70\\% of cases, the SSC process reduces this vulnerability\nby over 90\\%. This dynamic repair occurs at inference time, requires no weight\nmodification, and leads to more robustly aligned model behavior. Code at\nhttps://github.com/vicgalle/specification-self-correction .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference time", "self-correction"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward hacking"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18742", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18742", "abs": "https://arxiv.org/abs/2507.18742", "authors": ["Víctor Gallego"], "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement", "comment": "Accepted to SCALR Workshop @ COLM 2025", "summary": "Language models (LMs) are susceptible to in-context reward hacking, where\nthey exploit flaws in tainted or faulty written specifications or rubrics to\nachieve high scores without fulfilling the user's true intent. We introduce\nSpecification Self-Correction (SSC), a novel, test-time framework that enables\nan LM to identify and correct flaws within its own guiding specification. SSC\nemploys a multi-step inference process where the model first generates a\nresponse based on a potentially tainted specification, critiques its output,\nand then revises the specification itself to remove the exploitable loophole. A\nfinal, more robust response is then generated using this self-corrected\nspecification. Across experiments spanning creative writing and agentic coding\ntasks with several LMs, we demonstrate that while models initially game tainted\nspecifications in 50-70\\% of cases, the SSC process reduces this vulnerability\nby over 90\\%. This dynamic repair occurs at inference time, requires no weight\nmodification, and leads to more robustly aligned model behavior. Code at\nhttps://github.com/vicgalle/specification-self-correction .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference time", "self-correction"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward hacking"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19064", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19064", "abs": "https://arxiv.org/abs/2507.19064", "authors": ["Haochen Han", "Alex Jinpeng Wang", "Fangming Liu"], "title": "Negation-Aware Test-Time Adaptation for Vision-Language Models", "comment": "This paper will be submitted to the IEEE for possible publication", "summary": "In this paper, we study a practical but less-touched problem in\nVision-Language Models (VLMs), \\ie, negation understanding. Specifically, many\nreal-world applications require models to explicitly identify what is false or\nnon-existent, \\eg, radiologists may search for images that exclude specific\nconditions. Despite the impressive transferability of VLMs through large-scale\ntraining, they suffer from a critical limitation that fails to handle negation.\nTo address this challenge, existing methods attribute its root cause to the\nscarcity of negation training data and propose to fine-tune VLMs on massive\ndata containing explicit negation. Undoubtedly, such data-centric solutions\ndemand substantial data and computational resources, limiting their sustainable\nwidespread adoption. To tackle negation in a low-carbon manner, we empirically\nobserve that the key obstacle lies in the dual-concept shifts between the\naffirmation and negation distributions. Therefore, we propose a Negation-Aware\nTest-Time Adaptation (NEAT) method to efficiently adjust distribution-related\nparameters during inference. In brief, NEAT can reduce distribution shift in\nconsistent semantics while eliminating false distributional consistency in\nunrelated semantics. Extensive experiments on the various negation\nunderstanding tasks verify the effectiveness of the proposed method. The code\nis available at https://github.com/hhc1997/NEAT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scale", "test-time adaptation"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18677", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2507.18677", "abs": "https://arxiv.org/abs/2507.18677", "authors": ["Siyu Mu", "Wei Xuan Chan", "Choon Hwai Yap"], "title": "HeartUnloadNet: A Weakly-Supervised Cycle-Consistent Graph Network for Predicting Unloaded Cardiac Geometry from Diastolic States", "comment": "Codes are available at https://github.com/SiyuMU/Loaded2UnNet", "summary": "The unloaded cardiac geometry (i.e., the state of the heart devoid of luminal\npressure) serves as a valuable zero-stress and zero-strain reference and is\ncritical for personalized biomechanical modeling of cardiac function, to\nunderstand both healthy and diseased physiology and to predict the effects of\ncardiac interventions. However, estimating the unloaded geometry from clinical\nimages remains a challenging task. Traditional approaches rely on inverse\nfinite element (FE) solvers that require iterative optimization and are\ncomputationally expensive. In this work, we introduce HeartUnloadNet, a deep\nlearning framework that predicts the unloaded left ventricular (LV) shape\ndirectly from the end diastolic (ED) mesh while explicitly incorporating\nbiophysical priors. The network accepts a mesh of arbitrary size along with\nphysiological parameters such as ED pressure, myocardial stiffness scale, and\nfiber helix orientation, and outputs the corresponding unloaded mesh. It adopts\na graph attention architecture and employs a cycle-consistency strategy to\nenable bidirectional (loading and unloading) prediction, allowing for partial\nself-supervision that improves accuracy and reduces the need for large training\ndatasets. Trained and tested on 20,700 FE simulations across diverse LV\ngeometries and physiological conditions, HeartUnloadNet achieves sub-millimeter\naccuracy, with an average DSC of 0.986 and HD of 0.083 cm, while reducing\ninference time to just 0.02 seconds per case, over 10^5 times faster and\nsignificantly more accurate than traditional inverse FE solvers. Ablation\nstudies confirm the effectiveness of the architecture. Notably, the\ncycle-consistent design enables the model to maintain a DSC of 97% even with as\nfew as 200 training samples. This work thus presents a scalable and accurate\nsurrogate for inverse FE solvers, supporting real-time clinical applications in\nthe future.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19372", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19372", "abs": "https://arxiv.org/abs/2507.19372", "authors": ["Flavio Petruzzellis", "Alberto Testolin", "Alessandro Sperduti"], "title": "Learning neuro-symbolic convergent term rewriting systems", "comment": "48 pages, 31 figures. Submitted for review by Artificial Intelligence\n  Journal", "summary": "Building neural systems that can learn to execute symbolic algorithms is a\nchallenging open problem in artificial intelligence, especially when aiming for\nstrong generalization and out-of-distribution performance. In this work, we\nintroduce a general framework for learning convergent term rewriting systems\nusing a neuro-symbolic architecture inspired by the rewriting algorithm itself.\nWe present two modular implementations of such architecture: the Neural\nRewriting System (NRS) and the Fast Neural Rewriting System (FastNRS). As a\nresult of algorithmic-inspired design and key architectural elements, both\nmodels can generalize to out-of-distribution instances, with FastNRS offering\nsignificant improvements in terms of memory efficiency, training speed, and\ninference time. We evaluate both architectures on four tasks involving the\nsimplification of mathematical formulas and further demonstrate their\nversatility in a multi-domain learning scenario, where a single model is\ntrained to solve multiple types of problems simultaneously. The proposed system\nsignificantly outperforms two strong neural baselines: the Neural Data Router,\na recent transformer variant specifically designed to solve algorithmic\nproblems, and GPT-4o, one of the most powerful general-purpose large-language\nmodels. Moreover, our system matches or outperforms the latest o1-preview model\nfrom OpenAI that excels in reasoning benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "o1"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19451", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19451", "abs": "https://arxiv.org/abs/2507.19451", "authors": ["Baijun Ye", "Minghui Qin", "Saining Zhang", "Moonjun Gong", "Shaoting Zhu", "Zebang Shen", "Luan Zhang", "Lu Zhang", "Hao Zhao", "Hang Zhao"], "title": "GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous Driving with Gaussian Splatting", "comment": "ICCV 2025. Project Page: https://gs-occ3d.github.io/", "summary": "Occupancy is crucial for autonomous driving, providing essential geometric\npriors for perception and planning. However, existing methods predominantly\nrely on LiDAR-based occupancy annotations, which limits scalability and\nprevents leveraging vast amounts of potential crowdsourced data for\nauto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only\nframework that directly reconstructs occupancy. Vision-only occupancy\nreconstruction poses significant challenges due to sparse viewpoints, dynamic\nscene elements, severe occlusions, and long-horizon motion. Existing\nvision-based methods primarily rely on mesh representation, which suffer from\nincomplete geometry and additional post-processing, limiting scalability. To\novercome these issues, GS-Occ3D optimizes an explicit occupancy representation\nusing an Octree-based Gaussian Surfel formulation, ensuring efficiency and\nscalability. Additionally, we decompose scenes into static background, ground,\nand dynamic objects, enabling tailored modeling strategies: (1) Ground is\nexplicitly reconstructed as a dominant structural element, significantly\nimproving large-area consistency; (2) Dynamic vehicles are separately modeled\nto better capture motion-related occupancy patterns. Extensive experiments on\nthe Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry\nreconstruction results. By curating vision-only binary occupancy labels from\ndiverse urban scenes, we show their effectiveness for downstream occupancy\nmodels on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes.\nIt highlights the potential of large-scale vision-based occupancy\nreconstruction as a new paradigm for autonomous driving perception. Project\nPage: https://gs-occ3d.github.io/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18791", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18791", "abs": "https://arxiv.org/abs/2507.18791", "authors": ["Yilun Yang", "Yekun Chai"], "title": "Evaluating Code-Mixing in LLMs Across 18 Languages", "comment": null, "summary": "Code-mixing, the practice of switching between languages within a\nconversation, presents unique challenges for traditional natural language\nprocessing. Existing benchmarks, such as LinCE and GLUECoS, are limited by\nnarrow language pairings and tasks, failing to adequately evaluate the\ncode-mixing capabilities of large language models (LLMs). Despite the\nsignificance of code-mixing for multilingual users, research on LLMs in this\ncontext remains limited. Additionally, current methods for generating\ncode-mixed data are underdeveloped. In this paper, we conduct a comprehensive\nevaluation of LLMs' performance on code-mixed data across 18 languages from\nseven language families. We also propose a novel approach for generating\nsynthetic code-mixed texts by combining word substitution with GPT-4 prompting.\nOur analysis reveals consistent underperformance of LLMs on code-mixed datasets\ninvolving multiple language families. We suggest that improvements in training\ndata size, model scale, and few-shot learning could enhance their performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18664", "categories": ["cs.GR", "cs.CV", "I.3.2; I.4.10"], "pdf": "https://arxiv.org/pdf/2507.18664", "abs": "https://arxiv.org/abs/2507.18664", "authors": ["Aidan Murray", "Eddie Waite", "Caleb Ross", "Scarlet Mitchell", "Alexander Bradley", "Joanna Jamrozy", "Kenny Mitchell"], "title": "Generating real-time detailed ground visualisations from sparse aerial point clouds", "comment": "CVMP Short Paper. 1 page, 3 figures, CVMP 2022: The 19th ACM SIGGRAPH\n  European Conference on Visual Media Production, London. This work was\n  supported by the European Union's Horizon 2020 research and innovation\n  programme under Grant 101017779", "summary": "Building realistic wide scale outdoor 3D content with sufficient visual\nquality to observe at walking eye level or from driven vehicles is often\ncarried out by large teams of artists skilled in modelling, texturing, material\nshading and lighting, which typically leads to both prohibitive costs and\nreduced accuracy honoring the variety of real world ground truth landscapes. In\nour proposed method, we define a process to automatically amplify real-world\nscanned data and render real-time in animated 3D to explore at close range with\nhigh quality for training, simulation, video game and visualisation\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18827", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18827", "abs": "https://arxiv.org/abs/2507.18827", "authors": ["Pranav Gupta"], "title": "CueBuddy: helping non-native English speakers navigate English-centric STEM education", "comment": null, "summary": "Students across the world in STEM classes, especially in the Global South,\nfall behind their peers who are more fluent in English, despite being at par\nwith them in terms of scientific prerequisites. While many of them are able to\nfollow everyday English at ease, key terms in English stay challenging. In most\ncases, such students have had most of their course prerequisites in a lower\nresource language. Live speech translation to lower resource languages is a\npromising area of research, however, models for speech translation can be too\nexpensive on a large scale and often struggle with technical content. In this\npaper, we describe CueBuddy, which aims to remediate these issues by providing\nreal-time \"lexical cues\" through technical keyword spotting along real-time\nmultilingual glossary lookup to help students stay up to speed with complex\nEnglish jargon without disrupting their concentration on the lecture. We also\ndescribe the limitations and future extensions of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18795", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18795", "abs": "https://arxiv.org/abs/2507.18795", "authors": ["Fatima Al-Ani", "Molly Wang", "Jevon Charles", "Aaron Ong", "Joshua Forday", "Vinayak Modi"], "title": "Simulation-Driven Reinforcement Learning in Queuing Network Routing Optimization", "comment": null, "summary": "This study focuses on the development of a simulation-driven reinforcement\nlearning (RL) framework for optimizing routing decisions in complex queueing\nnetwork systems, with a particular emphasis on manufacturing and communication\napplications. Recognizing the limitations of traditional queueing methods,\nwhich often struggle with dynamic, uncertain environments, we propose a robust\nRL approach leveraging Deep Deterministic Policy Gradient (DDPG) combined with\nDyna-style planning (Dyna-DDPG). The framework includes a flexible and\nconfigurable simulation environment capable of modeling diverse queueing\nscenarios, disruptions, and unpredictable conditions. Our enhanced Dyna-DDPG\nimplementation incorporates separate predictive models for next-state\ntransitions and rewards, significantly improving stability and sample\nefficiency. Comprehensive experiments and rigorous evaluations demonstrate the\nframework's capability to rapidly learn effective routing policies that\nmaintain robust performance under disruptions and scale effectively to larger\nnetwork sizes. Additionally, we highlight strong software engineering practices\nemployed to ensure reproducibility and maintainability of the framework,\nenabling practical deployment in real-world scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy gradient", "reinforcement learning"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18972", "categories": ["cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18972", "abs": "https://arxiv.org/abs/2507.18972", "authors": ["Gromit Yeuk-Yin Chan", "Luis Gustavo Nonato", "Themis Palpanas", "Cláudio T. Silva", "Juliana Freire"], "title": "TiVy: Time Series Visual Summary for Scalable Visualization", "comment": "to be published in TVCG (IEEE VIS 2025)", "summary": "Visualizing multiple time series presents fundamental tradeoffs between\nscalability and visual clarity. Time series capture the behavior of many\nlarge-scale real-world processes, from stock market trends to urban activities.\nUsers often gain insights by visualizing them as line charts, juxtaposing or\nsuperposing multiple time series to compare them and identify trends and\npatterns. However, existing representations struggle with scalability: when\ncovering long time spans, leading to visual clutter from too many small\nmultiples or overlapping lines. We propose TiVy, a new algorithm that\nsummarizes time series using sequential patterns. It transforms the series into\na set of symbolic sequences based on subsequence visual similarity using\nDynamic Time Warping (DTW), then constructs a disjoint grouping of similar\nsubsequences based on the frequent sequential patterns. The grouping result, a\nvisual summary of time series, provides uncluttered superposition with fewer\nsmall multiples. Unlike common clustering techniques, TiVy extracts similar\nsubsequences (of varying lengths) aligned in time. We also present an\ninteractive time series visualization that renders large-scale time series in\nreal-time. Our experimental evaluation shows that our algorithm (1) extracts\nclear and accurate patterns when visualizing time series data, (2) achieves a\nsignificant speed-up (1000X) compared to a straightforward DTW clustering. We\nalso demonstrate the efficiency of our approach to explore hidden structures in\nmassive time series data in two usage scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18868", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.18868", "abs": "https://arxiv.org/abs/2507.18868", "authors": ["Alex Noviello", "Claas Beger", "Jacob Groner", "Kevin Ellis", "Weinan Sun"], "title": "A Neuroscience-Inspired Dual-Process Model of Compositional Generalization", "comment": null, "summary": "Systematic compositional generalization - constructing and understanding\nnovel combinations of known building blocks - remains a core challenge for AI\nsystems. Human cognition achieves this flexibility via the interplay of the\nhippocampus (HPC) and prefrontal cortex (PFC): the hippocampus rapidly encodes\nepisodes, and the prefrontal cortex consolidates them into reusable schemas for\nreasoning. Drawing on these insights, we present MIRAGE (Meta-Inference with\nRules and Abstractions from Generalized Experience), a framework that achieves\nsystematic generalization on compositional tasks. MIRAGE has two interacting\nmodules mirroring the brain's deliberative HPC-PFC loop and intuitive\nneocortical pattern recognition. (1) The meta-trained Transformer Neural\nDecomposer, paralleling neocortical \"System 1\" computation, is trained on a\ntask-agnostic stream of randomly sampled compositional grammars and applies one\ndecomposition step per pass, with successive passes iteratively refining the\nsequence representation. (2) The Schema Engine, analogous to the HPC-PFC\n\"System 2\" loop, dynamically extracts, ranks, and applies reusable schemas,\nstoring variable bindings in episodic memory and expanding them when needed. By\nexplicitly equipping the Transformer component of MIRAGE with actively managed\nschematic structures, our model performs systematic compositional operations\nthrough explicit schema application and transformation, relying solely on\nfrozen weights when solving entirely novel tasks. This approach demonstrates\nsystematic compositional generalization on the SCAN benchmark, achieving > 99%\naccuracy on all task splits with only 1.19M parameters in the transformer\nmodule. Ablation studies confirm that MIRAGE's systematicity critically depends\non the quality of extracted schemas and the model's iterative refinement\nprocess.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18655", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.18655", "abs": "https://arxiv.org/abs/2507.18655", "authors": ["James Dickens", "Kamyar Hamad"], "title": "Part Segmentation of Human Meshes via Multi-View Human Parsing", "comment": null, "summary": "Recent advances in point cloud deep learning have led to models that achieve\nhigh per-part labeling accuracy on large-scale point clouds, using only the raw\ngeometry of unordered point sets. In parallel, the field of human parsing\nfocuses on predicting body part and clothing/accessory labels from images. This\nwork aims to bridge these two domains by enabling per-vertex semantic\nsegmentation of large-scale human meshes. To achieve this, a pseudo-ground\ntruth labeling pipeline is developed for the Thuman2.1 dataset: meshes are\nfirst aligned to a canonical pose, segmented from multiple viewpoints, and the\nresulting point-level labels are then backprojected onto the original mesh to\nproduce per-point pseudo ground truth annotations. Subsequently, a novel,\nmemory-efficient sampling strategy is introduced, a windowed iterative farthest\npoint sampling (FPS) with space-filling curve-based serialization to\neffectively downsample the point clouds. This is followed by a purely geometric\nsegmentation using PointTransformer, enabling semantic parsing of human meshes\nwithout relying on texture information. Experimental results confirm the\neffectiveness and accuracy of the proposed approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19172", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19172", "abs": "https://arxiv.org/abs/2507.19172", "authors": ["Jiyao Wang", "Xiao Yang", "Qingyong Hu", "Jiankai Tang", "Can Liu", "Dengbo He", "Yuntao Wang", "Yingcong Chen", "Kaishun Wu"], "title": "PhysDrive: A Multimodal Remote Physiological Measurement Dataset for In-vehicle Driver Monitoring", "comment": "It is the initial version, not the final version", "summary": "Robust and unobtrusive in-vehicle physiological monitoring is crucial for\nensuring driving safety and user experience. While remote physiological\nmeasurement (RPM) offers a promising non-invasive solution, its translation to\nreal-world driving scenarios is critically constrained by the scarcity of\ncomprehensive datasets. Existing resources are often limited in scale, modality\ndiversity, the breadth of biometric annotations, and the range of captured\nconditions, thereby omitting inherent real-world challenges in driving. Here,\nwe present PhysDrive, the first large-scale multimodal dataset for contactless\nin-vehicle physiological sensing with dedicated consideration on various\nmodality settings and driving factors. PhysDrive collects data from 48 drivers,\nincluding synchronized RGB, near-infrared camera, and raw mmWave radar data,\naccompanied with six synchronized ground truths (ECG, BVP, Respiration, HR, RR,\nand SpO2). It covers a wide spectrum of naturalistic driving conditions,\nincluding driver motions, dynamic natural light, vehicle types, and road\nconditions. We extensively evaluate both signal-processing and deep-learning\nmethods on PhysDrive, establishing a comprehensive benchmark across all\nmodalities, and release full open-source code with compatibility for mainstream\npublic toolboxes. We envision PhysDrive will serve as a foundational resource\nand accelerate research on multimodal driver monitoring and smart-cockpit\nsystems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18915", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18915", "abs": "https://arxiv.org/abs/2507.18915", "authors": ["Ananya Sahu", "Amith Ananthram", "Kathleen McKeown"], "title": "Mining Contextualized Visual Associations from Images for Creativity Understanding", "comment": null, "summary": "Understanding another person's creative output requires a shared language of\nassociation. However, when training vision-language models such as CLIP, we\nrely on web-scraped datasets containing short, predominantly literal, alt-text.\nIn this work, we introduce a method for mining contextualized associations for\nsalient visual elements in an image that can scale to any unlabeled dataset.\nGiven an image, we can use these mined associations to generate high quality\ncreative captions at increasing degrees of abstraction. With our method, we\nproduce a new dataset of visual associations and 1.7m creative captions for the\nimages in MSCOCO. Human evaluation confirms that these captions remain visually\ngrounded while exhibiting recognizably increasing abstraction. Moreover,\nfine-tuning a visual encoder on this dataset yields meaningful improvements in\nzero-shot image-text retrieval in two creative domains: poetry and metaphor\nvisualization. We release our dataset, our generation code and our models for\nuse by the broader community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18667", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18667", "abs": "https://arxiv.org/abs/2507.18667", "authors": ["Nicholas Fidalgo", "Aaron Contreras", "Katherine Harvey", "Johnny Ni"], "title": "Gen-AI Police Sketches with Stable Diffusion", "comment": null, "summary": "This project investigates the use of multimodal AI-driven approaches to\nautomate and enhance suspect sketching. Three pipelines were developed and\nevaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model\nintegrated with a pre-trained CLIP model for text-image alignment, and (3)\nnovel approach incorporating LoRA fine-tuning of the CLIP model, applied to\nself-attention and cross-attention layers, and integrated with Stable\nDiffusion. An ablation study confirmed that fine-tuning both self- and\ncross-attention layers yielded the best alignment between text descriptions and\nsketches. Performance testing revealed that Model 1 achieved the highest\nstructural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of\n25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced\nperceptual similarity (LPIPS), with Model 3 showing improvement over Model 2\nbut still trailing Model 1. Qualitatively, sketches generated by Model 1\ndemonstrated the clearest facial features, highlighting its robustness as a\nbaseline despite its simplicity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19364", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.19364", "abs": "https://arxiv.org/abs/2507.19364", "authors": ["Patrick Taillandier", "Jean Daniel Zucker", "Arnaud Grignard", "Benoit Gaudou", "Nghi Quang Huynh", "Alexis Drogoul"], "title": "Integrating LLM in Agent-Based Social Simulation: Opportunities and Challenges", "comment": null, "summary": "This position paper examines the use of Large Language Models (LLMs) in\nsocial simulation, analyzing both their potential and their limitations from a\ncomputational social science perspective. The first part reviews recent\nfindings on the ability of LLMs to replicate key aspects of human cognition,\nincluding Theory of Mind reasoning and social inference, while also\nhighlighting significant limitations such as cognitive biases, lack of true\nunderstanding, and inconsistencies in behavior. The second part surveys\nemerging applications of LLMs in multi-agent simulation frameworks, focusing on\nsystem architectures, scale, and validation strategies. Notable projects such\nas Generative Agents (Smallville) and AgentSociety are discussed in terms of\ntheir design choices, empirical grounding, and methodological innovations.\nParticular attention is given to the challenges of behavioral fidelity,\ncalibration, and reproducibility in large-scale LLM-driven simulations. The\nfinal section distinguishes between contexts where LLMs, like other black-box\nsystems, offer direct value-such as interactive simulations and serious\ngames-and those where their use is more problematic, notably in explanatory or\npredictive modeling. The paper concludes by advocating for hybrid approaches\nthat integrate LLMs into traditional agent-based modeling platforms (GAMA,\nNetlogo, etc), enabling modelers to combine the expressive flexibility of\nlanguage-based reasoning with the transparency and analytical rigor of\nclassical rule-based systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18678", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18678", "abs": "https://arxiv.org/abs/2507.18678", "authors": ["Xingyu Miao", "Haoran Duan", "Quanhao Qian", "Jiuniu Wang", "Yang Long", "Ling Shao", "Deli Zhao", "Ran Xu", "Gongjie Zhang"], "title": "Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting", "comment": "ICCV 2025 (Highlight)", "summary": "Spatial intelligence is emerging as a transformative frontier in AI, yet it\nremains constrained by the scarcity of large-scale 3D datasets. Unlike the\nabundant 2D imagery, acquiring 3D data typically requires specialized sensors\nand laborious annotation. In this work, we present a scalable pipeline that\nconverts single-view images into comprehensive, scale- and appearance-realistic\n3D representations - including point clouds, camera poses, depth maps, and\npseudo-RGBD - via integrated depth estimation, camera calibration, and scale\ncalibration. Our method bridges the gap between the vast repository of imagery\nand the increasing demand for spatial scene understanding. By automatically\ngenerating authentic, scale-aware 3D data from images, we significantly reduce\ndata collection costs and open new avenues for advancing spatial intelligence.\nWe release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D,\nand demonstrate through extensive experiments that our generated data can\nbenefit various 3D tasks, ranging from fundamental perception to MLLM-based\nreasoning. These results validate our pipeline as an effective solution for\ndeveloping AI systems capable of perceiving, understanding, and interacting\nwith physical environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18973", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18973", "abs": "https://arxiv.org/abs/2507.18973", "authors": ["Bohan Yao", "Vikas Yadav"], "title": "A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation", "comment": "21 pages, 3 figures", "summary": "Augmenting large language models (LLMs) with external tools is a promising\navenue for developing high-performance mathematical reasoning systems. Prior\ntool-augmented approaches typically finetune an LLM to select and invoke a\nsingle tool at each reasoning step and show promising results on simpler math\nreasoning benchmarks such as GSM8K. However, these approaches struggle with\nmore complex math problems that require precise reasoning over multiple steps.\nTo address this limitation, in this work, we propose Multi-TAG, a Multi-Tool\nAGgregation-based framework. Instead of relying on a single tool, Multi-TAG\nguides an LLM to concurrently invoke multiple tools at each reasoning step. It\nthen aggregates their diverse outputs to verify and refine the reasoning\nprocess, enhancing solution robustness and accuracy. Notably, Multi-TAG is a\nfinetuning-free, inference-only framework, making it readily applicable to any\nLLM backbone, including large open-weight models which are computationally\nexpensive to finetune and proprietary frontier models which cannot be finetuned\nwith custom recipes. We evaluate Multi-TAG on four challenging benchmarks:\nMATH500, AIME, AMC, and OlympiadBench. Across both open-weight and\nclosed-source LLM backbones, Multi-TAG consistently and substantially\noutperforms state-of-the-art baselines, achieving average improvements of 6.0%\nto 7.5% over state-of-the-art baselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18713", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18713", "abs": "https://arxiv.org/abs/2507.18713", "authors": ["Yun Chen", "Matthew Haines", "Jingkang Wang", "Krzysztof Baron-Lis", "Sivabalan Manivasagam", "Ze Yang", "Raquel Urtasun"], "title": "SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time", "comment": null, "summary": "High-fidelity sensor simulation of light-based sensors such as cameras and\nLiDARs is critical for safe and accurate autonomy testing. Neural radiance\nfield (NeRF)-based methods that reconstruct sensor observations via ray-casting\nof implicit representations have demonstrated accurate simulation of driving\nscenes, but are slow to train and render, hampering scale. 3D Gaussian\nSplatting (3DGS) has demonstrated faster training and rendering times through\nrasterization, but is primarily restricted to pinhole camera sensors,\npreventing usage for realistic multi-sensor autonomy evaluation. Moreover, both\nNeRF and 3DGS couple the representation with the rendering procedure (implicit\nnetworks for ray-based evaluation, particles for rasterization), preventing\ninteroperability, which is key for general usage. In this work, we present\nSparse Local Fields (SaLF), a novel volumetric representation that supports\nrasterization and raytracing. SaLF represents volumes as a sparse set of 3D\nvoxel primitives, where each voxel is a local implicit field. SaLF has fast\ntraining (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS\nLiDAR), has adaptive pruning and densification to easily handle large scenes,\nand can support non-pinhole cameras and spinning LiDARs. We demonstrate that\nSaLF has similar realism as existing self-driving sensor simulation methods\nwhile improving efficiency and enhancing capabilities, enabling more scalable\nsimulation. https://waabi.ai/salf/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18741", "categories": ["cs.CV", "cs.DL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18741", "abs": "https://arxiv.org/abs/2507.18741", "authors": ["Tristan Repolusk", "Eduardo Veas"], "title": "KuiSCIMA v2.0: Improved Baselines, Calibration, and Cross-Notation Generalization for Historical Chinese Music Notations in Jiang Kui's Baishidaoren Gequ", "comment": "International Conference on Document Analysis and Recognition. This\n  preprint has not undergone any post-submission improvements or corrections.\n  The Version of Record of this contribution is published in \"19th\n  International Conference on Document Analysis and Recognition (ICDAR 2025),\n  Wuhan, China, September 16-21, 2025, Proceedings\", and is available online at\n  the External DOI field below", "summary": "Optical Music Recognition (OMR) for historical Chinese musical notations,\nsuch as suzipu and l\\\"ul\\\"upu, presents unique challenges due to high class\nimbalance and limited training data. This paper introduces significant\nadvancements in OMR for Jiang Kui's influential collection Baishidaoren Gequ\nfrom 1202. In this work, we develop and evaluate a character recognition model\nfor scarce imbalanced data. We improve upon previous baselines by reducing the\nCharacter Error Rate (CER) from 10.4% to 7.1% for suzipu, despite working with\n77 highly imbalanced classes, and achieve a remarkable CER of 0.9% for\nl\\\"ul\\\"upu. Our models outperform human transcribers, with an average human CER\nof 15.9% and a best-case CER of 7.6%. We employ temperature scaling to achieve\na well-calibrated model with an Expected Calibration Error (ECE) below 0.0162.\nUsing a leave-one-edition-out cross-validation approach, we ensure robust\nperformance across five historical editions. Additionally, we extend the\nKuiSCIMA dataset to include all 109 pieces from Baishidaoren Gequ, encompassing\nsuzipu, l\\\"ul\\\"upu, and jianzipu notations. Our findings advance the\ndigitization and accessibility of historical Chinese music, promoting cultural\ndiversity in OMR and expanding its applicability to underrepresented music\ntraditions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18667", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18667", "abs": "https://arxiv.org/abs/2507.18667", "authors": ["Nicholas Fidalgo", "Aaron Contreras", "Katherine Harvey", "Johnny Ni"], "title": "Gen-AI Police Sketches with Stable Diffusion", "comment": null, "summary": "This project investigates the use of multimodal AI-driven approaches to\nautomate and enhance suspect sketching. Three pipelines were developed and\nevaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model\nintegrated with a pre-trained CLIP model for text-image alignment, and (3)\nnovel approach incorporating LoRA fine-tuning of the CLIP model, applied to\nself-attention and cross-attention layers, and integrated with Stable\nDiffusion. An ablation study confirmed that fine-tuning both self- and\ncross-attention layers yielded the best alignment between text descriptions and\nsketches. Performance testing revealed that Model 1 achieved the highest\nstructural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of\n25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced\nperceptual similarity (LPIPS), with Model 3 showing improvement over Model 2\nbut still trailing Model 1. Qualitatively, sketches generated by Model 1\ndemonstrated the clearest facial features, highlighting its robustness as a\nbaseline despite its simplicity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18743", "abs": "https://arxiv.org/abs/2507.18743", "authors": ["Xinjun Cheng", "Yiguo He", "Junjie Zhu", "Chunping Qiu", "Jun Wang", "Qiangjuan Huang", "Ke Yang"], "title": "SAR-TEXT: A Large-Scale SAR Image-Text Dataset Built with SAR-Narrator and Progressive Transfer Learning", "comment": "IEEE Submission", "summary": "Vision Language Models (VLMs) have achieved remarkable breakthroughs in the\nfield of remote sensing in recent years. Synthetic Aperture Radar (SAR)\nimagery, with its all-weather capability, is essential in remote sensing, yet\nthe lack of large-scale, high-quality SAR image-text datasets hinders its\nsemantic understanding. In this paper, we construct SAR-Text, a large-scale and\nhigh-quality dataset consisting of over 130,000 SAR image-text pairs. To\nconstruct the SAR-Text dataset, we design the SAR-Narrator framework, which\ngenerates textual descriptions for SAR images through a multi-stage progressive\ntransfer learning strategy. To verify the effectiveness of the SAR-TEXT\ndataset, we conduct experiments on three typical vision-language tasks:\nimage-text retrieval, image captioning, and visual question answering (VQA).\nSpecifically, we construct three representative models on SAR-TEXT:\nSAR-RS-CLIP, SAR-RS-CoCa, and SAR-GPT. SAR-RS-CLIP achieves notable\nimprovements in retrieval performance, boosting average recall by 16.43% and\n10.54% on the OSdataset-512 and HRSID test sets, respectively. In the\ncaptioning task, SAR-RS-CoCa achieves BLEU-4, SPICE, and CIDEr scores exceeding\nthose of the original CoCa model by more than 8x, 4x, and 10x, respectively. In\nthe VQA task, SAR-GPT outperforms baseline and single-stage models on multiple\nSAR-VQA datasets, demonstrating stronger semantic understanding and reasoning\nability, as further confirmed by qualitative results. It is worth noting that,\nas a flexible captioning tool, SAR-Narrator can be readily adopted by the\ncommunity to construct larger-scale SAR image-text datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18678", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18678", "abs": "https://arxiv.org/abs/2507.18678", "authors": ["Xingyu Miao", "Haoran Duan", "Quanhao Qian", "Jiuniu Wang", "Yang Long", "Ling Shao", "Deli Zhao", "Ran Xu", "Gongjie Zhang"], "title": "Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting", "comment": "ICCV 2025 (Highlight)", "summary": "Spatial intelligence is emerging as a transformative frontier in AI, yet it\nremains constrained by the scarcity of large-scale 3D datasets. Unlike the\nabundant 2D imagery, acquiring 3D data typically requires specialized sensors\nand laborious annotation. In this work, we present a scalable pipeline that\nconverts single-view images into comprehensive, scale- and appearance-realistic\n3D representations - including point clouds, camera poses, depth maps, and\npseudo-RGBD - via integrated depth estimation, camera calibration, and scale\ncalibration. Our method bridges the gap between the vast repository of imagery\nand the increasing demand for spatial scene understanding. By automatically\ngenerating authentic, scale-aware 3D data from images, we significantly reduce\ndata collection costs and open new avenues for advancing spatial intelligence.\nWe release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D,\nand demonstrate through extensive experiments that our generated data can\nbenefit various 3D tasks, ranging from fundamental perception to MLLM-based\nreasoning. These results validate our pipeline as an effective solution for\ndeveloping AI systems capable of perceiving, understanding, and interacting\nwith physical environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19195", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19195", "abs": "https://arxiv.org/abs/2507.19195", "authors": ["Chaymaa Abbas", "Mariette Awad", "Razane Tajeddine"], "title": "Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?", "comment": null, "summary": "Despite the ongoing improvements in the design of large language models\n(LLMs) to foster inclusion and balanced responses, these systems remain\nsusceptible to encoding and amplifying social biases. This study examines how\ndialectal variation, specifically African American Vernacular English (AAVE)\nversus Standard American English (SAE), interacts with data poisoning to\ninfluence toxicity in outputs. Using both small- and medium-scale LLaMA models,\nwe show that even minimal exposure to poisoned data significantly increases\ntoxicity for AAVE inputs, while it remains comparatively unaffected for SAE.\nLarger models exhibit a more significant amplification effect which suggests\nheightened susceptibility with scale. To further assess these disparities, we\nemployed GPT-4o as a fairness auditor, which identified harmful stereotypical\npatterns disproportionately tied to AAVE inputs, including portrayals of\naggression, criminality, and intellectual inferiority. These findings\nunderscore the compounding impact of data poisoning and dialectal bias and\nemphasize the need for dialect-aware evaluation, targeted debiasing\ninterventions, and socially responsible training protocols during development.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18788", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18788", "abs": "https://arxiv.org/abs/2507.18788", "authors": ["Hitesh Kumar Gupta"], "title": "Tell Me What You See: An Iterative Deep Learning Framework for Image Captioning", "comment": "16 pages, 12 total figures (including a 7-figure appendix), 4 tables", "summary": "Image captioning, a task at the confluence of computer vision and natural\nlanguage processing, requires a sophisticated understanding of both visual\nscenes and linguistic structure. While modern approaches are dominated by\nlarge-scale Transformer architectures, this paper documents a systematic,\niterative development of foundational image captioning models, progressing from\na simple CNN-LSTM encoder-decoder to a competitive attention-based system. We\npresent a series of five models, beginning with Genesis and concluding with\nNexus, an advanced model featuring an EfficientNetV2B3 backbone and a dynamic\nattention mechanism. Our experiments chart the impact of architectural\nenhancements and demonstrate a key finding within the classic CNN-LSTM\nparadigm: merely upgrading the visual backbone without a corresponding\nattention mechanism can degrade performance, as the single-vector bottleneck\ncannot transmit the richer visual detail. This insight validates the\narchitectural shift to attention. Trained on the MS COCO 2017 dataset, our\nfinal model, Nexus, achieves a BLEU-4 score of 31.4, surpassing several\nfoundational benchmarks and validating our iterative design process. This work\nprovides a clear, replicable blueprint for understanding the core architectural\nprinciples that underpin modern vision-language tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18788", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18788", "abs": "https://arxiv.org/abs/2507.18788", "authors": ["Hitesh Kumar Gupta"], "title": "Tell Me What You See: An Iterative Deep Learning Framework for Image Captioning", "comment": "16 pages, 12 total figures (including a 7-figure appendix), 4 tables", "summary": "Image captioning, a task at the confluence of computer vision and natural\nlanguage processing, requires a sophisticated understanding of both visual\nscenes and linguistic structure. While modern approaches are dominated by\nlarge-scale Transformer architectures, this paper documents a systematic,\niterative development of foundational image captioning models, progressing from\na simple CNN-LSTM encoder-decoder to a competitive attention-based system. We\npresent a series of five models, beginning with Genesis and concluding with\nNexus, an advanced model featuring an EfficientNetV2B3 backbone and a dynamic\nattention mechanism. Our experiments chart the impact of architectural\nenhancements and demonstrate a key finding within the classic CNN-LSTM\nparadigm: merely upgrading the visual backbone without a corresponding\nattention mechanism can degrade performance, as the single-vector bottleneck\ncannot transmit the richer visual detail. This insight validates the\narchitectural shift to attention. Trained on the MS COCO 2017 dataset, our\nfinal model, Nexus, achieves a BLEU-4 score of 31.4, surpassing several\nfoundational benchmarks and validating our iterative design process. This work\nprovides a clear, replicable blueprint for understanding the core architectural\nprinciples that underpin modern vision-language tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18925", "abs": "https://arxiv.org/abs/2507.18925", "authors": ["Heitor R. Medeiros", "Atif Belal", "Masih Aminbeidokhti", "Eric Granger", "Marco Pedersoli"], "title": "WiSE-OD: Benchmarking Robustness in Infrared Object Detection", "comment": "8 pages, conference", "summary": "Object detection (OD) in infrared (IR) imagery is critical for low-light and\nnighttime applications. However, the scarcity of large-scale IR datasets forces\nmodels to rely on weights pre-trained on RGB images. While fine-tuning on IR\nimproves accuracy, it often compromises robustness under distribution shifts\ndue to the inherent modality gap between RGB and IR. To address this, we\nintroduce LLVIP-C and FLIR-C, two cross-modality out-of-distribution (OOD)\nbenchmarks built by applying corruption to standard IR datasets. Additionally,\nto fully leverage the complementary knowledge from RGB and infrared trained\nmodels, we propose WiSE-OD, a weight-space ensembling method with two variants:\nWiSE-OD$_{ZS}$, which combines RGB zero-shot and IR fine-tuned weights, and\nWiSE-OD$_{LP}$, which blends zero-shot and linear probing. Evaluated across\nthree RGB-pretrained detectors and two robust baselines, WiSE-OD improves both\ncross-modality and corruption robustness without any additional training or\ninference cost.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18973", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18973", "abs": "https://arxiv.org/abs/2507.18973", "authors": ["Bohan Yao", "Vikas Yadav"], "title": "A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation", "comment": "21 pages, 3 figures", "summary": "Augmenting large language models (LLMs) with external tools is a promising\navenue for developing high-performance mathematical reasoning systems. Prior\ntool-augmented approaches typically finetune an LLM to select and invoke a\nsingle tool at each reasoning step and show promising results on simpler math\nreasoning benchmarks such as GSM8K. However, these approaches struggle with\nmore complex math problems that require precise reasoning over multiple steps.\nTo address this limitation, in this work, we propose Multi-TAG, a Multi-Tool\nAGgregation-based framework. Instead of relying on a single tool, Multi-TAG\nguides an LLM to concurrently invoke multiple tools at each reasoning step. It\nthen aggregates their diverse outputs to verify and refine the reasoning\nprocess, enhancing solution robustness and accuracy. Notably, Multi-TAG is a\nfinetuning-free, inference-only framework, making it readily applicable to any\nLLM backbone, including large open-weight models which are computationally\nexpensive to finetune and proprietary frontier models which cannot be finetuned\nwith custom recipes. We evaluate Multi-TAG on four challenging benchmarks:\nMATH500, AIME, AMC, and OlympiadBench. Across both open-weight and\nclosed-source LLM backbones, Multi-TAG consistently and substantially\noutperforms state-of-the-art baselines, achieving average improvements of 6.0%\nto 7.5% over state-of-the-art baselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19419", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19419", "abs": "https://arxiv.org/abs/2507.19419", "authors": ["Mohammad Aflah Khan", "Ameya Godbole", "Johnny Tian-Zheng Wei", "Ryan Wang", "James Flemings", "Krishna Gummadi", "Willie Neiswanger", "Robin Jia"], "title": "TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability", "comment": null, "summary": "Understanding the relationship between training data and model behavior\nduring pretraining is crucial, but existing workflows make this process\ncumbersome, fragmented, and often inaccessible to researchers. We present\nTokenSmith, an open-source library for interactive editing, inspection, and\nanalysis of datasets used in Megatron-style pretraining frameworks such as\nGPT-NeoX, Megatron, and NVIDIA NeMo. TokenSmith supports a wide range of\noperations including searching, viewing, ingesting, exporting, inspecting, and\nsampling data, all accessible through a simple user interface and a modular\nbackend. It also enables structured editing of pretraining data without\nrequiring changes to training code, simplifying dataset debugging, validation,\nand experimentation.\n  TokenSmith is designed as a plug and play addition to existing large language\nmodel pretraining workflows, thereby democratizing access to production-grade\ndataset tooling. TokenSmith is hosted on GitHub1, with accompanying\ndocumentation and tutorials. A demonstration video is also available on\nYouTube.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19004", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19004", "abs": "https://arxiv.org/abs/2507.19004", "authors": ["Siyi Xun", "Yue Sun", "Jingkun Chen", "Zitong Yu", "Tong Tong", "Xiaohong Liu", "Mingxiang Wu", "Tao Tan"], "title": "MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment", "comment": "We note that the version after peer review of this paper has been\n  provisionally accepted by The 28th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI 2025)", "summary": "Rapid advances in medical imaging technology underscore the critical need for\nprecise and automated image quality assessment (IQA) to ensure diagnostic\naccuracy. Existing medical IQA methods, however, struggle to generalize across\ndiverse modalities and clinical scenarios. In response, we introduce MedIQA,\nthe first comprehensive foundation model for medical IQA, designed to handle\nvariability in image dimensions, modalities, anatomical regions, and types. We\ndeveloped a large-scale multi-modality dataset with plentiful manually\nannotated quality scores to support this. Our model integrates a salient slice\nassessment module to focus on diagnostically relevant regions feature retrieval\nand employs an automatic prompt strategy that aligns upstream physical\nparameter pre-training with downstream expert annotation fine-tuning. Extensive\nexperiments demonstrate that MedIQA significantly outperforms baselines in\nmultiple downstream tasks, establishing a scalable framework for medical IQA\nand advancing diagnostic workflows and clinical decision-making.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19457", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE", "I.2.7; I.2.6; I.2.4; I.2.8"], "pdf": "https://arxiv.org/pdf/2507.19457", "abs": "https://arxiv.org/abs/2507.19457", "authors": ["Lakshya A Agrawal", "Shangyin Tan", "Dilara Soylu", "Noah Ziems", "Rishi Khare", "Krista Opsahl-Ong", "Arnav Singhvi", "Herumb Shandilya", "Michael J Ryan", "Meng Jiang", "Christopher Potts", "Koushik Sen", "Alexandros G. Dimakis", "Ion Stoica", "Dan Klein", "Matei Zaharia", "Omar Khattab"], "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization\n(GRPO), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt\noptimizer that thoroughly incorporates natural language reflection to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the Pareto frontier of its own attempts. As a result of GEPA's design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for code optimization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18925", "abs": "https://arxiv.org/abs/2507.18925", "authors": ["Heitor R. Medeiros", "Atif Belal", "Masih Aminbeidokhti", "Eric Granger", "Marco Pedersoli"], "title": "WiSE-OD: Benchmarking Robustness in Infrared Object Detection", "comment": "8 pages, conference", "summary": "Object detection (OD) in infrared (IR) imagery is critical for low-light and\nnighttime applications. However, the scarcity of large-scale IR datasets forces\nmodels to rely on weights pre-trained on RGB images. While fine-tuning on IR\nimproves accuracy, it often compromises robustness under distribution shifts\ndue to the inherent modality gap between RGB and IR. To address this, we\nintroduce LLVIP-C and FLIR-C, two cross-modality out-of-distribution (OOD)\nbenchmarks built by applying corruption to standard IR datasets. Additionally,\nto fully leverage the complementary knowledge from RGB and infrared trained\nmodels, we propose WiSE-OD, a weight-space ensembling method with two variants:\nWiSE-OD$_{ZS}$, which combines RGB zero-shot and IR fine-tuned weights, and\nWiSE-OD$_{LP}$, which blends zero-shot and linear probing. Evaluated across\nthree RGB-pretrained detectors and two robust baselines, WiSE-OD improves both\ncross-modality and corruption robustness without any additional training or\ninference cost.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19119", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19119", "abs": "https://arxiv.org/abs/2507.19119", "authors": ["Yanghong Liu", "Xingping Dong", "Ming Li", "Weixing Zhang", "Yidong Lou"], "title": "PatchTraj: Dynamic Patch Representation Learning for Time-Frequency Trajectory Prediction", "comment": null, "summary": "Pedestrian trajectory prediction is crucial for autonomous driving and\nrobotics. While existing point-based and grid-based methods expose two key\nlimitations: insufficiently modeling human motion dynamics, as they fail to\nbalance local motion details with long-range spatiotemporal dependencies, and\nthe time representation lacks interaction with the frequency domain in modeling\ntrajectory sequences. To address these challenges, we propose PatchTraj, a\ndynamic patch-based trajectory prediction framework that unifies time-domain\nand frequency-domain representations. Specifically, we decompose the trajectory\ninto raw time sequences and frequency components, employing dynamic patch\npartitioning for multi-scale trajectory segmentation to capture hierarchical\nmotion patterns. Each patch is processed by an adaptive embedding layer with\nscale-aware feature extraction, followed by hierarchical feature aggregation to\nmodel both fine-grained and long-range dependencies. The outputs of two\nbranches interact via cross-modal attention, enabling complementary fusion of\ntemporal and spectral cues. Finally, a Transformer encoder-decoder integrates\nboth modalities to autoregressively predict future trajectories. Extensive\nexperiments on ETH-UCY, SDD, NBA, and JRDB datasets demonstrate that our method\nachieves state-of-the-art performance with high efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18958", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18958", "abs": "https://arxiv.org/abs/2507.18958", "authors": ["Xiaocheng Fang", "Jieyi Cai", "Huanyu Liu", "Chengju Zhou", "Minhua Lu", "Bingzhi Chen"], "title": "PerioDet: Large-Scale Panoramic Radiograph Benchmark for Clinical-Oriented Apical Periodontitis Detection", "comment": "MICCAI 2025(Early Accept)", "summary": "Apical periodontitis is a prevalent oral pathology that presents significant\npublic health challenges. Despite advances in automated diagnostic systems\nacross various medical fields, the development of Computer-Aided Diagnosis\n(CAD) applications for apical periodontitis is still constrained by the lack of\na large-scale, high-quality annotated dataset. To address this issue, we\nrelease a large-scale panoramic radiograph benchmark called \"PerioXrays\",\ncomprising 3,673 images and 5,662 meticulously annotated instances of apical\nperiodontitis. To the best of our knowledge, this is the first benchmark\ndataset for automated apical periodontitis diagnosis. This paper further\nproposes a clinical-oriented apical periodontitis detection (PerioDet)\nparadigm, which jointly incorporates Background-Denoising Attention (BDA) and\nIoU-Dynamic Calibration (IDC) mechanisms to address the challenges posed by\nbackground noise and small targets in automated detection. Extensive\nexperiments on the PerioXrays dataset demonstrate the superiority of PerioDet\nin advancing automated apical periodontitis detection. Additionally, a\nwell-designed human-computer collaborative experiment underscores the clinical\napplicability of our method as an auxiliary diagnostic tool for professional\ndentists.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19195", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19195", "abs": "https://arxiv.org/abs/2507.19195", "authors": ["Chaymaa Abbas", "Mariette Awad", "Razane Tajeddine"], "title": "Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?", "comment": null, "summary": "Despite the ongoing improvements in the design of large language models\n(LLMs) to foster inclusion and balanced responses, these systems remain\nsusceptible to encoding and amplifying social biases. This study examines how\ndialectal variation, specifically African American Vernacular English (AAVE)\nversus Standard American English (SAE), interacts with data poisoning to\ninfluence toxicity in outputs. Using both small- and medium-scale LLaMA models,\nwe show that even minimal exposure to poisoned data significantly increases\ntoxicity for AAVE inputs, while it remains comparatively unaffected for SAE.\nLarger models exhibit a more significant amplification effect which suggests\nheightened susceptibility with scale. To further assess these disparities, we\nemployed GPT-4o as a fairness auditor, which identified harmful stereotypical\npatterns disproportionately tied to AAVE inputs, including portrayals of\naggression, criminality, and intellectual inferiority. These findings\nunderscore the compounding impact of data poisoning and dialectal bias and\nemphasize the need for dialect-aware evaluation, targeted debiasing\ninterventions, and socially responsible training protocols during development.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19201", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19201", "abs": "https://arxiv.org/abs/2507.19201", "authors": ["Xin Li", "Kaixiang Yang", "Qiang Li", "Zhiwei Wang"], "title": "Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model", "comment": "Accepted, ACM Multimedia 2025, 10 pages, 5 figures", "summary": "Mammography is the most commonly used imaging modality for breast cancer\nscreening, driving an increasing demand for deep-learning techniques to support\nlarge-scale analysis. However, the development of accurate and robust methods\nis often limited by insufficient data availability and a lack of diversity in\nlesion characteristics. While generative models offer a promising solution for\ndata synthesis, current approaches often fail to adequately emphasize\nlesion-specific features and their relationships with surrounding tissues. In\nthis paper, we propose Gated Conditional Diffusion Model (GCDM), a novel\nframework designed to jointly synthesize holistic mammogram images and\nlocalized lesions. GCDM is built upon a latent denoising diffusion framework,\nwhere the noised latent image is concatenated with a soft mask embedding that\nrepresents breast, lesion, and their transitional regions, ensuring anatomical\ncoherence between them during the denoising process. To further emphasize\nlesion-specific features, GCDM incorporates a gated conditioning branch that\nguides the denoising process by dynamically selecting and fusing the most\nrelevant radiomic and geometric properties of lesions, effectively capturing\ntheir interplay. Experimental results demonstrate that GCDM achieves precise\ncontrol over small lesion areas while enhancing the realism and diversity of\nsynthesized mammograms. These advancements position GCDM as a promising tool\nfor clinical applications in mammogram synthesis. Our code is available at\nhttps://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19321", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19321", "abs": "https://arxiv.org/abs/2507.19321", "authors": ["Viktar Dubovik", "Łukasz Struski", "Jacek Tabor", "Dawid Rymarczyk"], "title": "SIDE: Sparse Information Disentanglement for Explainable Artificial Intelligence", "comment": null, "summary": "Understanding the decisions made by deep neural networks is essential in\nhigh-stakes domains such as medical imaging and autonomous driving. Yet, these\nmodels often lack transparency, particularly in computer vision.\nPrototypical-parts-based neural networks have emerged as a promising solution\nby offering concept-level explanations. However, most are limited to\nfine-grained classification tasks, with few exceptions such as InfoDisent.\nInfoDisent extends prototypical models to large-scale datasets like ImageNet,\nbut produces complex explanations.\n  We introduce Sparse Information Disentanglement for Explainability (SIDE), a\nnovel method that improves the interpretability of prototypical parts through a\ndedicated training and pruning scheme that enforces sparsity. Combined with\nsigmoid activations in place of softmax, this approach allows SIDE to associate\neach class with only a small set of relevant prototypes. Extensive experiments\nshow that SIDE matches the accuracy of existing methods while reducing\nexplanation size by over $90\\%$, substantially enhancing the understandability\nof prototype-based explanations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19004", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19004", "abs": "https://arxiv.org/abs/2507.19004", "authors": ["Siyi Xun", "Yue Sun", "Jingkun Chen", "Zitong Yu", "Tong Tong", "Xiaohong Liu", "Mingxiang Wu", "Tao Tan"], "title": "MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment", "comment": "We note that the version after peer review of this paper has been\n  provisionally accepted by The 28th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI 2025)", "summary": "Rapid advances in medical imaging technology underscore the critical need for\nprecise and automated image quality assessment (IQA) to ensure diagnostic\naccuracy. Existing medical IQA methods, however, struggle to generalize across\ndiverse modalities and clinical scenarios. In response, we introduce MedIQA,\nthe first comprehensive foundation model for medical IQA, designed to handle\nvariability in image dimensions, modalities, anatomical regions, and types. We\ndeveloped a large-scale multi-modality dataset with plentiful manually\nannotated quality scores to support this. Our model integrates a salient slice\nassessment module to focus on diagnostically relevant regions feature retrieval\nand employs an automatic prompt strategy that aligns upstream physical\nparameter pre-training with downstream expert annotation fine-tuning. Extensive\nexperiments demonstrate that MedIQA significantly outperforms baselines in\nmultiple downstream tasks, establishing a scalable framework for medical IQA\nand advancing diagnostic workflows and clinical decision-making.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19457", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE", "I.2.7; I.2.6; I.2.4; I.2.8"], "pdf": "https://arxiv.org/pdf/2507.19457", "abs": "https://arxiv.org/abs/2507.19457", "authors": ["Lakshya A Agrawal", "Shangyin Tan", "Dilara Soylu", "Noah Ziems", "Rishi Khare", "Krista Opsahl-Ong", "Arnav Singhvi", "Herumb Shandilya", "Michael J Ryan", "Meng Jiang", "Christopher Potts", "Koushik Sen", "Alexandros G. Dimakis", "Ion Stoica", "Dan Klein", "Matei Zaharia", "Omar Khattab"], "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization\n(GRPO), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt\noptimizer that thoroughly incorporates natural language reflection to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the Pareto frontier of its own attempts. As a result of GEPA's design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for code optimization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19118", "abs": "https://arxiv.org/abs/2507.19118", "authors": ["Abu Sadat Mohammad Salehin Amit", "Xiaoli Zhang", "Md Masum Billa Shagar", "Zhaojun Liu", "Xiongfei Li", "Fanlong Meng"], "title": "Cross Spatial Temporal Fusion Attention for Remote Sensing Object Detection via Image Feature Matching", "comment": null, "summary": "Effectively describing features for cross-modal remote sensing image matching\nremains a challenging task due to the significant geometric and radiometric\ndifferences between multimodal images. Existing methods primarily extract\nfeatures at the fully connected layer but often fail to capture cross-modal\nsimilarities effectively. We propose a Cross Spatial Temporal Fusion (CSTF)\nmechanism that enhances feature representation by integrating scale-invariant\nkeypoints detected independently in both reference and query images. Our\napproach improves feature matching in two ways: First, by creating\ncorrespondence maps that leverage information from multiple image regions\nsimultaneously, and second, by reformulating the similarity matching process as\na classification task using SoftMax and Fully Convolutional Network (FCN)\nlayers. This dual approach enables CSTF to maintain sensitivity to distinctive\nlocal features while incorporating broader contextual information, resulting in\nrobust matching across diverse remote sensing modalities. To demonstrate the\npractical utility of improved feature matching, we evaluate CSTF on object\ndetection tasks using the HRSC2016 and DOTA benchmark datasets. Our method\nachieves state-of-theart performance with an average mAP of 90.99% on HRSC2016\nand 90.86% on DOTA, outperforming existing models. The CSTF model maintains\ncomputational efficiency with an inference speed of 12.5 FPS. These results\nvalidate that our approach to crossmodal feature matching directly enhances\ndownstream remote sensing applications such as object detection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19119", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19119", "abs": "https://arxiv.org/abs/2507.19119", "authors": ["Yanghong Liu", "Xingping Dong", "Ming Li", "Weixing Zhang", "Yidong Lou"], "title": "PatchTraj: Dynamic Patch Representation Learning for Time-Frequency Trajectory Prediction", "comment": null, "summary": "Pedestrian trajectory prediction is crucial for autonomous driving and\nrobotics. While existing point-based and grid-based methods expose two key\nlimitations: insufficiently modeling human motion dynamics, as they fail to\nbalance local motion details with long-range spatiotemporal dependencies, and\nthe time representation lacks interaction with the frequency domain in modeling\ntrajectory sequences. To address these challenges, we propose PatchTraj, a\ndynamic patch-based trajectory prediction framework that unifies time-domain\nand frequency-domain representations. Specifically, we decompose the trajectory\ninto raw time sequences and frequency components, employing dynamic patch\npartitioning for multi-scale trajectory segmentation to capture hierarchical\nmotion patterns. Each patch is processed by an adaptive embedding layer with\nscale-aware feature extraction, followed by hierarchical feature aggregation to\nmodel both fine-grained and long-range dependencies. The outputs of two\nbranches interact via cross-modal attention, enabling complementary fusion of\ntemporal and spectral cues. Finally, a Transformer encoder-decoder integrates\nboth modalities to autoregressively predict future trajectories. Extensive\nexperiments on ETH-UCY, SDD, NBA, and JRDB datasets demonstrate that our method\nachieves state-of-the-art performance with high efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19186", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19186", "abs": "https://arxiv.org/abs/2507.19186", "authors": ["Niklas Bubeck", "Yundi Zhang", "Suprosanna Shit", "Daniel Rueckert", "Jiazhen Pan"], "title": "Reconstruct or Generate: Exploring the Spectrum of Generative Modeling for Cardiac MRI", "comment": null, "summary": "In medical imaging, generative models are increasingly relied upon for two\ndistinct but equally critical tasks: reconstruction, where the goal is to\nrestore medical imaging (usually inverse problems like inpainting or\nsuperresolution), and generation, where synthetic data is created to augment\ndatasets or carry out counterfactual analysis. Despite shared architecture and\nlearning frameworks, they prioritize different goals: generation seeks high\nperceptual quality and diversity, while reconstruction focuses on data fidelity\nand faithfulness. In this work, we introduce a \"generative model zoo\" and\nsystematically analyze how modern latent diffusion models and autoregressive\nmodels navigate the reconstruction-generation spectrum. We benchmark a suite of\ngenerative models across representative cardiac medical imaging tasks, focusing\non image inpainting with varying masking ratios and sampling strategies, as\nwell as unconditional image generation. Our findings show that diffusion models\noffer superior perceptual quality for unconditional generation but tend to\nhallucinate as masking ratios increase, whereas autoregressive models maintain\nstable perceptual performance across masking levels, albeit with generally\nlower fidelity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19201", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19201", "abs": "https://arxiv.org/abs/2507.19201", "authors": ["Xin Li", "Kaixiang Yang", "Qiang Li", "Zhiwei Wang"], "title": "Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model", "comment": "Accepted, ACM Multimedia 2025, 10 pages, 5 figures", "summary": "Mammography is the most commonly used imaging modality for breast cancer\nscreening, driving an increasing demand for deep-learning techniques to support\nlarge-scale analysis. However, the development of accurate and robust methods\nis often limited by insufficient data availability and a lack of diversity in\nlesion characteristics. While generative models offer a promising solution for\ndata synthesis, current approaches often fail to adequately emphasize\nlesion-specific features and their relationships with surrounding tissues. In\nthis paper, we propose Gated Conditional Diffusion Model (GCDM), a novel\nframework designed to jointly synthesize holistic mammogram images and\nlocalized lesions. GCDM is built upon a latent denoising diffusion framework,\nwhere the noised latent image is concatenated with a soft mask embedding that\nrepresents breast, lesion, and their transitional regions, ensuring anatomical\ncoherence between them during the denoising process. To further emphasize\nlesion-specific features, GCDM incorporates a gated conditioning branch that\nguides the denoising process by dynamically selecting and fusing the most\nrelevant radiomic and geometric properties of lesions, effectively capturing\ntheir interplay. Experimental results demonstrate that GCDM achieves precise\ncontrol over small lesion areas while enhancing the realism and diversity of\nsynthesized mammograms. These advancements position GCDM as a promising tool\nfor clinical applications in mammogram synthesis. Our code is available at\nhttps://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19209", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.19209", "abs": "https://arxiv.org/abs/2507.19209", "authors": ["Xiaoyu Zhang", "Zhifeng Bao", "Hai Dong", "Ziwei Wang", "Jiajun Liu"], "title": "Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting with CounterNet", "comment": null, "summary": "Autonomous vehicles generate massive volumes of point cloud data, yet only a\nsubset is relevant for specific tasks such as collision detection, traffic\nanalysis, or congestion monitoring. Effectively querying this data is essential\nto enable targeted analytics. In this work, we formalize point cloud querying\nby defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, each\naligned with distinct analytical scenarios. All these queries rely heavily on\naccurate object counts to produce meaningful results, making precise object\ncounting a critical component of query execution. Prior work has focused on\nindexing techniques for 2D video data, assuming detection models provide\naccurate counting information. However, when applied to 3D point cloud data,\nstate-of-the-art detection models often fail to generate reliable object\ncounts, leading to substantial errors in query results. To address this\nlimitation, we propose CounterNet, a heatmap-based network designed for\naccurate object counting in large-scale point cloud data. Rather than focusing\non accurate object localization, CounterNet detects object presence by finding\nobject centers to improve counting accuracy. We further enhance its performance\nwith a feature map partitioning strategy using overlapping regions, enabling\nbetter handling of both small and large objects in complex traffic scenes. To\nadapt to varying frame characteristics, we introduce a per-frame dynamic model\nselection strategy that selects the most effective configuration for each\ninput. Evaluations on three real-world autonomous vehicle datasets show that\nCounterNet improves counting accuracy by 5% to 20% across object categories,\nresulting in more reliable query outcomes across all supported query types.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19213", "abs": "https://arxiv.org/abs/2507.19213", "authors": ["Hanbing Wu", "Ping Jiang", "Anyang Su", "Chenxu Zhao", "Tianyu Fu", "Minghui Wu", "Beiping Tan", "Huiying Li"], "title": "PRE-MAP: Personalized Reinforced Eye-tracking Multimodal LLM for High-Resolution Multi-Attribute Point Prediction", "comment": null, "summary": "Visual selective attention, driven by individual preferences, regulates human\nprioritization of visual stimuli by bridging subjective cognitive mechanisms\nwith objective visual elements, thereby steering the semantic interpretation\nand hierarchical processing of dynamic visual scenes. However, existing models\nand datasets predominantly neglect the influence of subjective cognitive\ndiversity on fixation behavior. Conventional saliency prediction models,\ntypically employing segmentation approaches, rely on low-resolution imagery to\ngenerate saliency heatmaps, subsequently upscaled to native resolutions, which\nlimiting their capacity to capture personalized attention patterns.\nFurthermore, MLLMs are constrained by factors such as hallucinations, making it\nvery costly to strictly adhere to the expected format in tasks involving\nmultiple point predictions, and achieving precise point positioning is\nchallenging. To address these limitations, we present Subjective Personalized\nAttention for Advertisement Videos, namely SPA-ADV, a large-scale multimodal\ndataset capturing gaze behaviors from over 4,500 participants varying in age\nand gender with 486 videos. Furthermore, we propose PRE-MAP, a novel\neye-tracking saliency model that characterizes Personalized visual disparities\nthrough Reinforcement learning-optimized Eye-tracking, built upon MLLMs and\nguided by Multi-Attribute user profiles to predict Points. To ensure MLLMs\nproduce prediction points that are both format-correct and spatially accurate,\nwe introduce Consistency Group Relative Policy Optimization (C-GRPO), inspired\nby the variability in eye movement points and Multi-Attribute profiles.\nExtensive experiments on SPA-ADV and other benchmarks demonstrate the\neffectiveness of our approach. The code and dataset are available at\n\\href{https://github.com/mininglamp-MLLM/PRE-MAP}{this URL}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19232", "abs": "https://arxiv.org/abs/2507.19232", "authors": ["Donggeun Lim", "Jinseok Bae", "Inwoo Hwang", "Seungmin Lee", "Hwanhee Lee", "Young Min Kim"], "title": "Event-Driven Storytelling with Multiple Lifelike Humans in a 3D Scene", "comment": "16 pages, project page:\n  https://rms0329.github.io/Event-Driven-Storytelling/", "summary": "In this work, we propose a framework that creates a lively virtual dynamic\nscene with contextual motions of multiple humans. Generating multi-human\ncontextual motion requires holistic reasoning over dynamic relationships among\nhuman-human and human-scene interactions. We adapt the power of a large\nlanguage model (LLM) to digest the contextual complexity within textual input\nand convert the task into tangible subproblems such that we can generate\nmulti-agent behavior beyond the scale that was not considered before.\nSpecifically, our event generator formulates the temporal progression of a\ndynamic scene into a sequence of small events. Each event calls for a\nwell-defined motion involving relevant characters and objects. Next, we\nsynthesize the motions of characters at positions sampled based on spatial\nguidance. We employ a high-level module to deliver scalable yet comprehensive\ncontext, translating events into relative descriptions that enable the\nretrieval of precise coordinates. As the first to address this problem at scale\nand with diversity, we offer a benchmark to assess diverse aspects of\ncontextual reasoning. Benchmark results and user studies show that our\nframework effectively captures scene context with high scalability. The code\nand benchmark, along with result videos, are available at our project page:\nhttps://rms0329.github.io/Event-Driven-Storytelling/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19253", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19253", "abs": "https://arxiv.org/abs/2507.19253", "authors": ["An Xiang", "Zixuan Huang", "Xitong Gao", "Kejiang Ye", "Cheng-zhong Xu"], "title": "BridgeNet: A Unified Multimodal Framework for Bridging 2D and 3D Industrial Anomaly Detection", "comment": null, "summary": "Industrial anomaly detection for 2D objects has gained significant attention\nand achieved progress in anomaly detection (AD) methods. However, identifying\n3D depth anomalies using only 2D information is insufficient. Despite\nexplicitly fusing depth information into RGB images or using point cloud\nbackbone networks to extract depth features, both approaches struggle to\nadequately represent 3D information in multimodal scenarios due to the\ndisparities among different modal information. Additionally, due to the\nscarcity of abnormal samples in industrial data, especially in multimodal\nscenarios, it is necessary to perform anomaly generation to simulate real-world\nabnormal samples. Therefore, we propose a novel unified multimodal anomaly\ndetection framework to address these issues. Our contributions consist of 3 key\naspects. (1) We extract visible depth information from 3D point cloud data\nsimply and use 2D RGB images to represent appearance, which disentangles depth\nand appearance to support unified anomaly generation. (2) Benefiting from the\nflexible input representation, the proposed Multi-Scale Gaussian Anomaly\nGenerator and Unified Texture Anomaly Generator can generate richer anomalies\nin RGB and depth. (3) All modules share parameters for both RGB and depth data,\neffectively bridging 2D and 3D anomaly detection. Subsequent modules can\ndirectly leverage features from both modalities without complex fusion.\nExperiments show our method outperforms state-of-the-art (SOTA) on MVTec-3D AD\nand Eyecandies datasets. Code available at:\nhttps://github.com/Xantastic/BridgeNet", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19262", "abs": "https://arxiv.org/abs/2507.19262", "authors": ["Monika Wysoczańska", "Shyamal Buch", "Anurag Arnab", "Cordelia Schmid"], "title": "OVFact: Measuring and Improving Open-Vocabulary Factuality for Long Caption Models", "comment": null, "summary": "Large vision-language models (VLMs) often struggle to generate long and\nfactual captions. However, traditional measures for hallucination and\nfactuality are not well suited for evaluating longer, more diverse captions and\nin settings where ground-truth human-annotated captions are unavailable. We\nintroduce OV-Fact, a novel method for measuring caption factuality of long\ncaptions that leverages open-vocabulary visual grounding and tool-based\nverification without depending on human annotations. Our method improves\nagreement with human judgments and captures both caption descriptiveness\n(recall) and factual precision in the same metric. Furthermore, unlike previous\nmetrics, our reference-free method design enables new applications towards\nfactuality-based data filtering. We observe models trained on an\nOVFact-filtered (2.5-5x less) subset of a large-scale, noisy (VLM-generated)\npretraining set meaningfully improve factuality precision without sacrificing\ncaption descriptiveness across a range of downstream long caption benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "agreement"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19264", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19264", "abs": "https://arxiv.org/abs/2507.19264", "authors": ["Sijie Li", "Chen Chen", "Jungong Han"], "title": "SimMLM: A Simple Framework for Multi-modal Learning with Missing Modality", "comment": null, "summary": "In this paper, we propose SimMLM, a simple yet powerful framework for\nmultimodal learning with missing modalities. Unlike existing approaches that\nrely on sophisticated network architectures or complex data imputation\ntechniques, SimMLM provides a generic and effective solution that can adapt to\nvarious missing modality scenarios with improved accuracy and robustness.\nSpecifically, SimMLM consists of a generic Dynamic Mixture of Modality Experts\n(DMoME) architecture, featuring a dynamic, learnable gating mechanism that\nautomatically adjusts each modality's contribution in both full and partial\nmodality settings. A key innovation of SimMLM is the proposed More vs. Fewer\n(MoFe) ranking loss, which ensures that task accuracy improves or remains\nstable as more modalities are made available. This aligns the model with an\nintuitive principle: removing one or more modalities should not increase\naccuracy. We validate SimMLM on multimodal medical image segmentation (BraTS\n2018) and multimodal classification (UPMC Food-101, avMNIST) tasks, where it\nconsistently surpasses competitive methods, demonstrating superior accuracy,\ninterpretability, robustness, and reliability across both complete and missing\nmodality scenarios at test time.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19280", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19280", "abs": "https://arxiv.org/abs/2507.19280", "authors": ["Liang Yao", "Fan Liu", "Hongbo Lu", "Chuanyi Zhang", "Rui Min", "Shengxiang Xu", "Shimin Di", "Pai Peng"], "title": "RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow", "comment": null, "summary": "Remote sensing imagery presents vast, inherently unstructured spatial data,\ndemanding sophisticated reasoning to interpret complex user intents and\ncontextual relationships beyond simple recognition tasks. In this paper, we aim\nto construct an Earth observation workflow to handle complex queries by\nreasoning about spatial context and user intent. As a reasoning workflow, it\nshould be somewhat autonomous, where predefined ground-truth reasoning paths do\nnot constrain the learning process. Furthermore, its architecture ought to be\nunified yet flexible, enabling the model to perform diverse reasoning tasks\nwith distinct output formats through a single forward pass. Existing remote\nsensing approaches fail to address these requirements, as they rely on\nsupervised fine-tuning paradigms that constrain the autonomy of reasoning. To\nthis end, we propose RemoteReasoner, a flexible and robust workflow for remote\nsensing reasoning tasks. The design of RemoteReasoner integrates a multi-modal\nlarge language model (MLLM) for interpreting user instructions and localizing\ntargets, together with task adaptation strategies that enable multi-granularity\noutput generation. In contrast to existing methods, our framework is trained\nwith reinforcement learning (RL) to endow the MLLM sufficient autonomy for\nprecise reasoning. At the inference stage, our adaptation strategies enable\ndiverse output formats at inference time without requiring task-specific\ndecoders or further fine-tuning. Preliminary experiments demonstrated that\nRemoteReasoner achieves remarkable performance across multi-granularity\nreasoning tasks, including region-level and pixel-level. Additionally, our\nframework enables novel capabilities such as the contour extraction task beyond\nthe reach of existing reasoning pipelines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19321", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19321", "abs": "https://arxiv.org/abs/2507.19321", "authors": ["Viktar Dubovik", "Łukasz Struski", "Jacek Tabor", "Dawid Rymarczyk"], "title": "SIDE: Sparse Information Disentanglement for Explainable Artificial Intelligence", "comment": null, "summary": "Understanding the decisions made by deep neural networks is essential in\nhigh-stakes domains such as medical imaging and autonomous driving. Yet, these\nmodels often lack transparency, particularly in computer vision.\nPrototypical-parts-based neural networks have emerged as a promising solution\nby offering concept-level explanations. However, most are limited to\nfine-grained classification tasks, with few exceptions such as InfoDisent.\nInfoDisent extends prototypical models to large-scale datasets like ImageNet,\nbut produces complex explanations.\n  We introduce Sparse Information Disentanglement for Explainability (SIDE), a\nnovel method that improves the interpretability of prototypical parts through a\ndedicated training and pruning scheme that enforces sparsity. Combined with\nsigmoid activations in place of softmax, this approach allows SIDE to associate\neach class with only a small set of relevant prototypes. Extensive experiments\nshow that SIDE matches the accuracy of existing methods while reducing\nexplanation size by over $90\\%$, substantially enhancing the understandability\nof prototype-based explanations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19409", "abs": "https://arxiv.org/abs/2507.19409", "authors": ["Toufiq Parag", "Ahmed Elgammal"], "title": "Modality Agnostic Efficient Long Range Encoder", "comment": null, "summary": "The long-context capability of recent large transformer models can be\nsurmised to rely on techniques such as attention/model parallelism, as well as\nhardware-level optimizations. While these strategies allow input lengths to\nscale to millions of tokens, they do not fundamentally mitigate the quadratic\ncomputational and memory complexity of the core attention mechanism. In this\npaper, we address the challenge of long-context processing on a single device\nusing generic implementations by reducing the quadratic memory footprint and\ninference cost. Existing approaches to extend the context length for generic\nsingle device implementations -- such as token merging and modified attentions\n-- are often modality specific and attain a suboptimal tradeoff between\naccuracy and efficiency. To overcome these limitations, we propose MAELRE\n(Modality Agnostic Efficient Long Range Encoder), a unified and efficient\ntransformer architecture designed for long-range encoding across diverse\nmodalities. MAELRE integrates token merging with attention approximation,\nprogressively merging tokens at different stages of internal computational\nblocks. It employs a lightweight attention approximation when the number of\ntokens is large, and switches to standard dot-product attention as the sequence\nbecomes shorter through successive aggregation. We demonstrate that MAELRE\nachieves superior accuracy while reducing computational cost compared to\nexisting long-context models on classification tasks spanning multiple\nmodalities, including text, time series, audio, and vision.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19468", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19468", "abs": "https://arxiv.org/abs/2507.19468", "authors": ["Federico Baldassarre", "Marc Szafraniec", "Basile Terver", "Vasil Khalidov", "Francisco Massa", "Yann LeCun", "Patrick Labatut", "Maximilian Seitzer", "Piotr Bojanowski"], "title": "Back to the Features: DINO as a Foundation for Video World Models", "comment": null, "summary": "We present DINO-world, a powerful generalist video world model trained to\npredict future frames in the latent space of DINOv2. By leveraging a\npre-trained image encoder and training a future predictor on a large-scale\nuncurated video dataset, DINO-world learns the temporal dynamics of diverse\nscenes, from driving and indoor scenes to simulated environments. We show that\nDINO-world outperforms previous models on a variety of video prediction\nbenchmarks, e.g. segmentation and depth forecasting, and demonstrates strong\nunderstanding of intuitive physics. Furthermore, we show that it is possible to\nfine-tune the predictor on observation-action trajectories. The resulting\naction-conditioned world model can be used for planning by simulating candidate\ntrajectories in latent space.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18664", "categories": ["cs.GR", "cs.CV", "I.3.2; I.4.10"], "pdf": "https://arxiv.org/pdf/2507.18664", "abs": "https://arxiv.org/abs/2507.18664", "authors": ["Aidan Murray", "Eddie Waite", "Caleb Ross", "Scarlet Mitchell", "Alexander Bradley", "Joanna Jamrozy", "Kenny Mitchell"], "title": "Generating real-time detailed ground visualisations from sparse aerial point clouds", "comment": "CVMP Short Paper. 1 page, 3 figures, CVMP 2022: The 19th ACM SIGGRAPH\n  European Conference on Visual Media Production, London. This work was\n  supported by the European Union's Horizon 2020 research and innovation\n  programme under Grant 101017779", "summary": "Building realistic wide scale outdoor 3D content with sufficient visual\nquality to observe at walking eye level or from driven vehicles is often\ncarried out by large teams of artists skilled in modelling, texturing, material\nshading and lighting, which typically leads to both prohibitive costs and\nreduced accuracy honoring the variety of real world ground truth landscapes. In\nour proposed method, we define a process to automatically amplify real-world\nscanned data and render real-time in animated 3D to explore at close range with\nhigh quality for training, simulation, video game and visualisation\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.18915", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18915", "abs": "https://arxiv.org/abs/2507.18915", "authors": ["Ananya Sahu", "Amith Ananthram", "Kathleen McKeown"], "title": "Mining Contextualized Visual Associations from Images for Creativity Understanding", "comment": null, "summary": "Understanding another person's creative output requires a shared language of\nassociation. However, when training vision-language models such as CLIP, we\nrely on web-scraped datasets containing short, predominantly literal, alt-text.\nIn this work, we introduce a method for mining contextualized associations for\nsalient visual elements in an image that can scale to any unlabeled dataset.\nGiven an image, we can use these mined associations to generate high quality\ncreative captions at increasing degrees of abstraction. With our method, we\nproduce a new dataset of visual associations and 1.7m creative captions for the\nimages in MSCOCO. Human evaluation confirms that these captions remain visually\ngrounded while exhibiting recognizably increasing abstraction. Moreover,\nfine-tuning a visual encoder on this dataset yields meaningful improvements in\nzero-shot image-text retrieval in two creative domains: poetry and metaphor\nvisualization. We release our dataset, our generation code and our models for\nuse by the broader community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-07-28.jsonl"}
{"id": "2507.19172", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19172", "abs": "https://arxiv.org/abs/2507.19172", "authors": ["Jiyao Wang", "Xiao Yang", "Qingyong Hu", "Jiankai Tang", "Can Liu", "Dengbo He", "Yuntao Wang", "Yingcong Chen", "Kaishun Wu"], "title": "PhysDrive: A Multimodal Remote Physiological Measurement Dataset for In-vehicle Driver Monitoring", "comment": "It is the initial version, not the final version", "summary": "Robust and unobtrusive in-vehicle physiological monitoring is crucial for\nensuring driving safety and user experience. While remote physiological\nmeasurement (RPM) offers a promising non-invasive solution, its translation to\nreal-world driving scenarios is critically constrained by the scarcity of\ncomprehensive datasets. Existing resources are often limited in scale, modality\ndiversity, the breadth of biometric annotations, and the range of captured\nconditions, thereby omitting inherent real-world challenges in driving. Here,\nwe present PhysDrive, the first large-scale multimodal dataset for contactless\nin-vehicle physiological sensing with dedicated consideration on various\nmodality settings and driving factors. PhysDrive collects data from 48 drivers,\nincluding synchronized RGB, near-infrared camera, and raw mmWave radar data,\naccompanied with six synchronized ground truths (ECG, BVP, Respiration, HR, RR,\nand SpO2). It covers a wide spectrum of naturalistic driving conditions,\nincluding driver motions, dynamic natural light, vehicle types, and road\nconditions. We extensively evaluate both signal-processing and deep-learning\nmethods on PhysDrive, establishing a comprehensive benchmark across all\nmodalities, and release full open-source code with compatibility for mainstream\npublic toolboxes. We envision PhysDrive will serve as a foundational resource\nand accelerate research on multimodal driver monitoring and smart-cockpit\nsystems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-07-28.jsonl"}
