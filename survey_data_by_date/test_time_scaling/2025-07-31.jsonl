{"id": "2507.22545", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22545", "abs": "https://arxiv.org/abs/2507.22545", "authors": ["Sung-Min Lee", "Siyoon Lee", "Juyeon Kim", "Kyungmin Roh"], "title": "ControlMed: Adding Reasoning Control to Medical Language Model", "comment": "13 pages", "summary": "Reasoning Large Language Models (LLMs) with enhanced accuracy and\nexplainability are increasingly being adopted in the medical domain, as the\nlife-critical nature of clinical decision-making demands reliable support.\nDespite these advancements, existing reasoning LLMs often generate\nunnecessarily lengthy reasoning processes, leading to significant computational\noverhead and response latency. These limitations hinder their practical\ndeployment in real-world clinical environments. To address these challenges, we\nintroduce \\textbf{ControlMed}, a medical language model that enables users to\nactively control the length of the reasoning process at inference time through\nfine-grained control markers. ControlMed is trained through a three-stage\npipeline: 1) pre-training on a large-scale synthetic medical instruction\ndataset covering both \\textit{direct} and \\textit{reasoning responses}; 2)\nsupervised fine-tuning with multi-length reasoning data and explicit\nlength-control markers; and 3) reinforcement learning with model-based reward\nsignals to enhance factual accuracy and response quality. Experimental results\non a variety of English and Korean medical benchmarks demonstrate that our\nmodel achieves similar or better performance compared to state-of-the-art\nmodels. Furthermore, users can flexibly balance reasoning accuracy and\ncomputational efficiency by controlling the reasoning length as needed. These\nfindings demonstrate that ControlMed is a practical and adaptable solution for\nclinical question answering and medical information analysis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "question answering", "fine-grained"], "score": 4}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22187", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22187", "abs": "https://arxiv.org/abs/2507.22187", "authors": ["Adam M. Morgan", "Adeen Flinker"], "title": "A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models", "comment": null, "summary": "We present an automated pipeline for estimating Verb Frame Frequencies\n(VFFs), the frequency with which a verb appears in particular syntactic frames.\nVFFs provide a powerful window into syntax in both human and machine language\nsystems, but existing tools for calculating them are limited in scale,\naccuracy, or accessibility. We use large language models (LLMs) to generate a\ncorpus of sentences containing 476 English verbs. Next, by instructing an LLM\nto behave like an expert linguist, we had it analyze the syntactic structure of\nthe sentences in this corpus. This pipeline outperforms two widely used\nsyntactic parsers across multiple evaluation datasets. Furthermore, it requires\nfar fewer resources than manual parsing (the gold-standard), thereby enabling\nrapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF\ndatabase with broader verb coverage, finer-grained syntactic distinctions, and\nexplicit estimates of the relative frequencies of structural alternates\ncommonly studied in psycholinguistics. The pipeline is easily customizable and\nextensible to new verbs, syntactic frames, and even other languages. We present\nthis work as a proof of concept for automated frame frequency estimation, and\nrelease all code and data to support future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22099", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22099", "abs": "https://arxiv.org/abs/2507.22099", "authors": ["Shuqing Li", "Qiang Chen", "Xiaoxue Ren", "Michael R. Lyu"], "title": "Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?", "comment": null, "summary": "Physics Engines (PEs) are fundamental software frameworks that simulate\nphysical interactions in applications ranging from entertainment to\nsafety-critical systems. Despite their importance, PEs suffer from physics\nfailures, deviations from expected physical behaviors that can compromise\nsoftware reliability, degrade user experience, and potentially cause critical\nfailures in autonomous vehicles or medical robotics. Current testing approaches\nfor PE-based software are inadequate, typically requiring white-box access and\nfocusing on crash detection rather than semantically complex physics failures.\nThis paper presents the first large-scale empirical study characterizing\nphysics failures in PE-based software. We investigate three research questions\naddressing the manifestations of physics failures, the effectiveness of\ndetection techniques, and developer perceptions of current detection practices.\nOur contributions include: (1) a taxonomy of physics failure manifestations;\n(2) a comprehensive evaluation of detection methods including deep learning,\nprompt-based techniques, and large multimodal models; and (3) actionable\ninsights from developer experiences for improving detection approaches. To\nsupport future research, we release PhysiXFails, code, and other materials at\nhttps://sites.google.com/view/physics-failure-detection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "reliability"], "score": 3}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22448", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22448", "abs": "https://arxiv.org/abs/2507.22448", "authors": ["Jingwei Zuo", "Maksim Velikanov", "Ilyas Chahed", "Younes Belkada", "Dhia Eddine Rhayem", "Guillaume Kunsch", "Hakim Hacid", "Hamza Yous", "Brahim Farhat", "Ibrahim Khadraoui", "Mugariya Farooq", "Giulia Campesan", "Ruxandra Cojocaru", "Yasser Djilali", "Shi Hu", "Iheb Chaabane", "Puneesh Khanna", "Mohamed El Amine Seddik", "Ngoc Dung Huynh", "Phuc Le Khac", "Leen AlQadi", "Billel Mokeddem", "Mohamed Chami", "Abdalgader Abubaker", "Mikhail Lubinets", "Kacper Piskorski", "Slim Frikha"], "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance", "comment": "Technical report of Falcon-H1 model series", "summary": "In this report, we introduce Falcon-H1, a new series of large language models\n(LLMs) featuring hybrid architecture designs optimized for both high\nperformance and efficiency across diverse use cases. Unlike earlier Falcon\nmodels built solely on Transformer or Mamba architectures, Falcon-H1 adopts a\nparallel hybrid approach that combines Transformer-based attention with State\nSpace Models (SSMs), known for superior long-context memory and computational\nefficiency. We systematically revisited model design, data strategy, and\ntraining dynamics, challenging conventional practices in the field. Falcon-H1\nis released in multiple configurations, including base and instruction-tuned\nvariants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized\ninstruction-tuned models are also available, totaling over 30 checkpoints on\nHugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and\nexceptional parameter and training efficiency. The flagship Falcon-H1-34B\nmatches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,\nand Llama3.3-70B, while using fewer parameters and less data. Smaller models\nshow similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B\nmodels, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.\nThese models excel across reasoning, mathematics, multilingual tasks,\ninstruction following, and scientific knowledge. With support for up to 256K\ncontext tokens and 18 languages, Falcon-H1 is suitable for a wide range of\napplications. All models are released under a permissive open-source license,\nunderscoring our commitment to accessible and impactful AI research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22346", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.22346", "abs": "https://arxiv.org/abs/2507.22346", "authors": ["Pei Deng", "Wenqian Zhou", "Hanlin Wu"], "title": "DeltaVLM: Interactive Remote Sensing Image Change Analysis via Instruction-guided Difference Perception", "comment": "12 pages, 5 figures. Submitted to IEEE Transactions on Geoscience and\n  Remote Sensing (TGRS). Code and dataset are available at\n  https://github.com/hanlinwu/DeltaVLM", "summary": "Accurate interpretation of land-cover changes in multi-temporal satellite\nimagery is critical for real-world scenarios. However, existing methods\ntypically provide only one-shot change masks or static captions, limiting their\nability to support interactive, query-driven analysis. In this work, we\nintroduce remote sensing image change analysis (RSICA) as a new paradigm that\ncombines the strengths of change detection and visual question answering to\nenable multi-turn, instruction-guided exploration of changes in bi-temporal\nremote sensing images. To support this task, we construct ChangeChat-105k, a\nlarge-scale instruction-following dataset, generated through a hybrid\nrule-based and GPT-assisted process, covering six interaction types: change\ncaptioning, classification, quantification, localization, open-ended question\nanswering, and multi-turn dialogues. Building on this dataset, we propose\nDeltaVLM, an end-to-end architecture tailored for interactive RSICA. DeltaVLM\nfeatures three innovations: (1) a fine-tuned bi-temporal vision encoder to\ncapture temporal differences; (2) a visual difference perception module with a\ncross-semantic relation measuring (CSRM) mechanism to interpret changes; and\n(3) an instruction-guided Q-former to effectively extract query-relevant\ndifference information from visual changes, aligning them with textual\ninstructions. We train DeltaVLM on ChangeChat-105k using a frozen large\nlanguage model, adapting only the vision and alignment modules to optimize\nefficiency. Extensive experiments and ablation studies demonstrate that\nDeltaVLM achieves state-of-the-art performance on both single-turn captioning\nand multi-turn interactive change analysis, outperforming existing multimodal\nlarge language models and remote sensing vision-language models. Code, dataset\nand pre-trained weights are available at https://github.com/hanlinwu/DeltaVLM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22533", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22533", "abs": "https://arxiv.org/abs/2507.22533", "authors": ["Dongchen Li", "Jitao Liang", "Wei Li", "Xiaoyu Wang", "Longbing Cao", "Kun Yu"], "title": "CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records", "comment": null, "summary": "Large Language Models (LLMs) hold significant promise for improving clinical\ndecision support and reducing physician burnout by synthesizing complex,\nlongitudinal cancer Electronic Health Records (EHRs). However, their\nimplementation in this critical field faces three primary challenges: the\ninability to effectively process the extensive length and multilingual nature\nof patient records for accurate temporal analysis; a heightened risk of\nclinical hallucination, as conventional grounding techniques such as\nRetrieval-Augmented Generation (RAG) do not adequately incorporate\nprocess-oriented clinical guidelines; and unreliable evaluation metrics that\nhinder the validation of AI systems in oncology. To address these issues, we\npropose CliCARE, a framework for Grounding Large Language Models in Clinical\nGuidelines for Decision Support over Longitudinal Cancer Electronic Health\nRecords. The framework operates by transforming unstructured, longitudinal EHRs\ninto patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range\ndependencies, and then grounding the decision support process by aligning these\nreal-world patient trajectories with a normative guideline knowledge graph.\nThis approach provides oncologists with evidence-grounded decision support by\ngenerating a high-fidelity clinical summary and an actionable recommendation.\nWe validated our framework using large-scale, longitudinal data from a private\nChinese cancer dataset and the public English MIMIC-IV dataset. In these\ndiverse settings, CliCARE significantly outperforms strong baselines, including\nleading long-context LLMs and Knowledge Graph-enhanced RAG methods. The\nclinical validity of our results is supported by a robust evaluation protocol,\nwhich demonstrates a high correlation with assessments made by expert\noncologists.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation"], "score": 3}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22099", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22099", "abs": "https://arxiv.org/abs/2507.22099", "authors": ["Shuqing Li", "Qiang Chen", "Xiaoxue Ren", "Michael R. Lyu"], "title": "Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?", "comment": null, "summary": "Physics Engines (PEs) are fundamental software frameworks that simulate\nphysical interactions in applications ranging from entertainment to\nsafety-critical systems. Despite their importance, PEs suffer from physics\nfailures, deviations from expected physical behaviors that can compromise\nsoftware reliability, degrade user experience, and potentially cause critical\nfailures in autonomous vehicles or medical robotics. Current testing approaches\nfor PE-based software are inadequate, typically requiring white-box access and\nfocusing on crash detection rather than semantically complex physics failures.\nThis paper presents the first large-scale empirical study characterizing\nphysics failures in PE-based software. We investigate three research questions\naddressing the manifestations of physics failures, the effectiveness of\ndetection techniques, and developer perceptions of current detection practices.\nOur contributions include: (1) a taxonomy of physics failure manifestations;\n(2) a comprehensive evaluation of detection methods including deep learning,\nprompt-based techniques, and large multimodal models; and (3) actionable\ninsights from developer experiences for improving detection approaches. To\nsupport future research, we release PhysiXFails, code, and other materials at\nhttps://sites.google.com/view/physics-failure-detection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "reliability"], "score": 3}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22407", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.22407", "abs": "https://arxiv.org/abs/2507.22407", "authors": ["Seungryong Lee", "Woojeong Baek", "Younghyun Kim", "Eunwoo Kim", "Haru Moon", "Donggon Yoo", "Eunbyung Park"], "title": "Moiré Zero: An Efficient and High-Performance Neural Architecture for Moiré Removal", "comment": "Project page: https://sngryonglee.github.io/MoireZero", "summary": "Moir\\'e patterns, caused by frequency aliasing between fine repetitive\nstructures and a camera sensor's sampling process, have been a significant\nobstacle in various real-world applications, such as consumer photography and\nindustrial defect inspection. With the advancements in deep learning\nalgorithms, numerous studies-predominantly based on convolutional neural\nnetworks-have suggested various solutions to address this issue. Despite these\nefforts, existing approaches still struggle to effectively eliminate artifacts\ndue to the diverse scales, orientations, and color shifts of moir\\'e patterns,\nprimarily because the constrained receptive field of CNN-based architectures\nlimits their ability to capture the complex characteristics of moir\\'e\npatterns. In this paper, we propose MZNet, a U-shaped network designed to bring\nimages closer to a 'Moire-Zero' state by effectively removing moir\\'e patterns.\nIt integrates three specialized components: Multi-Scale Dual Attention Block\n(MSDAB) for extracting and refining multi-scale features, Multi-Shape Large\nKernel Convolution Block (MSLKB) for capturing diverse moir\\'e structures, and\nFeature Fusion-Based Skip Connection for enhancing information flow. Together,\nthese components enhance local texture restoration and large-scale artifact\nsuppression. Experiments on benchmark datasets demonstrate that MZNet achieves\nstate-of-the-art performance on high-resolution datasets and delivers\ncompetitive results on lower-resolution dataset, while maintaining a low\ncomputational cost, suggesting that it is an efficient and practical solution\nfor real-world applications. Project page:\nhttps://sngryonglee.github.io/MoireZero", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22187", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22187", "abs": "https://arxiv.org/abs/2507.22187", "authors": ["Adam M. Morgan", "Adeen Flinker"], "title": "A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models", "comment": null, "summary": "We present an automated pipeline for estimating Verb Frame Frequencies\n(VFFs), the frequency with which a verb appears in particular syntactic frames.\nVFFs provide a powerful window into syntax in both human and machine language\nsystems, but existing tools for calculating them are limited in scale,\naccuracy, or accessibility. We use large language models (LLMs) to generate a\ncorpus of sentences containing 476 English verbs. Next, by instructing an LLM\nto behave like an expert linguist, we had it analyze the syntactic structure of\nthe sentences in this corpus. This pipeline outperforms two widely used\nsyntactic parsers across multiple evaluation datasets. Furthermore, it requires\nfar fewer resources than manual parsing (the gold-standard), thereby enabling\nrapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF\ndatabase with broader verb coverage, finer-grained syntactic distinctions, and\nexplicit estimates of the relative frequencies of structural alternates\ncommonly studied in psycholinguistics. The pipeline is easily customizable and\nextensible to new verbs, syntactic frames, and even other languages. We present\nthis work as a proof of concept for automated frame frequency estimation, and\nrelease all code and data to support future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22412", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22412", "abs": "https://arxiv.org/abs/2507.22412", "authors": ["Sijie Wang", "Siqi Li", "Yawei Zhang", "Shangshu Yu", "Shenghai Yuan", "Rui She", "Quanjiang Guo", "JinXuan Zheng", "Ong Kang Howe", "Leonrich Chandra", "Shrivarshann Srijeyan", "Aditya Sivadas", "Toshan Aggarwal", "Heyuan Liu", "Hongming Zhang", "Chujie Chen", "Junyu Jiang", "Lihua Xie", "Wee Peng Tay"], "title": "UAVScenes: A Multi-Modal Dataset for UAVs", "comment": "Accepted by ICCV 2025", "summary": "Multi-modal perception is essential for unmanned aerial vehicle (UAV)\noperations, as it enables a comprehensive understanding of the UAVs'\nsurrounding environment. However, most existing multi-modal UAV datasets are\nprimarily biased toward localization and 3D reconstruction tasks, or only\nsupport map-level semantic segmentation due to the lack of frame-wise\nannotations for both camera images and LiDAR point clouds. This limitation\nprevents them from being used for high-level scene understanding tasks. To\naddress this gap and advance multi-modal UAV perception, we introduce\nUAVScenes, a large-scale dataset designed to benchmark various tasks across\nboth 2D and 3D modalities. Our benchmark dataset is built upon the\nwell-calibrated multi-modal UAV dataset MARS-LVIG, originally developed only\nfor simultaneous localization and mapping (SLAM). We enhance this dataset by\nproviding manually labeled semantic annotations for both frame-wise images and\nLiDAR point clouds, along with accurate 6-degree-of-freedom (6-DoF) poses.\nThese additions enable a wide range of UAV perception tasks, including\nsegmentation, depth estimation, 6-DoF localization, place recognition, and\nnovel view synthesis (NVS). Our dataset is available at\nhttps://github.com/sijieaaa/UAVScenes", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22623", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22623", "abs": "https://arxiv.org/abs/2507.22623", "authors": ["Daniil Gurgurov", "Katharina Trinley", "Ivan Vykopal", "Josef van Genabith", "Simon Ostermann", "Roberto Zamparelli"], "title": "Multilingual Political Views of Large Language Models: Identification and Steering", "comment": "pre-print", "summary": "Large language models (LLMs) are increasingly used in everyday tools and\napplications, raising concerns about their potential influence on political\nviews. While prior research has shown that LLMs often exhibit measurable\npolitical biases--frequently skewing toward liberal or progressive\npositions--key gaps remain. Most existing studies evaluate only a narrow set of\nmodels and languages, leaving open questions about the generalizability of\npolitical biases across architectures, scales, and multilingual settings.\nMoreover, few works examine whether these biases can be actively controlled.\n  In this work, we address these gaps through a large-scale study of political\norientation in modern open-source instruction-tuned LLMs. We evaluate seven\nmodels, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using\nthe Political Compass Test with 11 semantically equivalent paraphrases per\nstatement to ensure robust measurement. Our results reveal that larger models\nconsistently shift toward libertarian-left positions, with significant\nvariations across languages and model families. To test the manipulability of\npolitical stances, we utilize a simple center-of-mass activation intervention\ntechnique and show that it reliably steers model responses toward alternative\nideological positions across multiple languages. Our code is publicly available\nat https://github.com/d-gurgurov/Political-Ideologies-LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22421", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22421", "abs": "https://arxiv.org/abs/2507.22421", "authors": ["Shahla John"], "title": "Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A Unified Framework for Action Recognition and Object Tracking", "comment": null, "summary": "Real-time video analysis remains a challenging problem in computer vision,\nrequiring efficient processing of both spatial and temporal information while\nmaintaining computational efficiency. Existing approaches often struggle to\nbalance accuracy and speed, particularly in resource-constrained environments.\nIn this work, we present a unified framework that leverages advanced\nspatial-temporal modeling techniques for simultaneous action recognition and\nobject tracking. Our approach builds upon recent advances in parallel sequence\nmodeling and introduces a novel hierarchical attention mechanism that\nadaptively focuses on relevant spatial regions across temporal sequences. We\ndemonstrate that our method achieves state-of-the-art performance on standard\nbenchmarks while maintaining real-time inference speeds. Extensive experiments\non UCF-101, HMDB-51, and MOT17 datasets show improvements of 3.2% in action\nrecognition accuracy and 2.8% in tracking precision compared to existing\nmethods, with 40% faster inference time.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22431", "abs": "https://arxiv.org/abs/2507.22431", "authors": ["Zhixiang Wei", "Guangting Wang", "Xiaoxiao Ma", "Ke Mei", "Huaian Chen", "Yi Jin", "Fengyun Rao"], "title": "HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models", "comment": null, "summary": "Large-scale but noisy image-text pair data have paved the way for the success\nof Contrastive Language-Image Pretraining (CLIP). As the foundation vision\nencoder, CLIP in turn serves as the cornerstone for most large vision-language\nmodels (LVLMs). This interdependence naturally raises an interesting question:\nCan we reciprocally leverage LVLMs to enhance the quality of image-text pair\ndata, thereby opening the possibility of a self-reinforcing cycle for\ncontinuous improvement? In this work, we take a significant step toward this\nvision by introducing an LVLM-driven data refinement pipeline. Our framework\nleverages LVLMs to process images and their raw alt-text, generating four\ncomplementary textual formulas: long positive descriptions, long negative\ndescriptions, short positive tags, and short negative tags. Applying this\npipeline to the curated DFN-Large dataset yields VLM-150M, a refined dataset\nenriched with multi-grained annotations. Based on this dataset, we further\npropose a training paradigm that extends conventional contrastive learning by\nincorporating negative descriptions and short tags as additional supervised\nsignals. The resulting model, namely HQ-CLIP, demonstrates remarkable\nimprovements across diverse benchmarks. Within a comparable training data\nscale, our approach achieves state-of-the-art performance in zero-shot\nclassification, cross-modal retrieval, and fine-grained visual understanding\ntasks. In retrieval benchmarks, HQ-CLIP even surpasses standard CLIP models\ntrained on the DFN-2B dataset, which contains 10$\\times$ more training data\nthan ours. All code, data, and models are available at\nhttps://zxwei.site/hqclip.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22454", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.22454", "abs": "https://arxiv.org/abs/2507.22454", "authors": ["Jiuming Liu", "Zheng Huang", "Mengmeng Liu", "Tianchen Deng", "Francesco Nex", "Hao Cheng", "Hesheng Wang"], "title": "TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation", "comment": "Accepted by IROS 2025. Code:https://github.com/IRMVLab/TopoLiDM", "summary": "LiDAR scene generation is critical for mitigating real-world LiDAR data\ncollection costs and enhancing the robustness of downstream perception tasks in\nautonomous driving. However, existing methods commonly struggle to capture\ngeometric realism and global topological consistency. Recent LiDAR Diffusion\nModels (LiDMs) predominantly embed LiDAR points into the latent space for\nimproved generation efficiency, which limits their interpretable ability to\nmodel detailed geometric structures and preserve global topological\nconsistency. To address these challenges, we propose TopoLiDM, a novel\nframework that integrates graph neural networks (GNNs) with diffusion models\nunder topological regularization for high-fidelity LiDAR generation. Our\napproach first trains a topological-preserving VAE to extract latent graph\nrepresentations by graph construction and multiple graph convolutional layers.\nThen we freeze the VAE and generate novel latent topological graphs through the\nlatent diffusion models. We also introduce 0-dimensional persistent homology\n(PH) constraints, ensuring the generated LiDAR scenes adhere to real-world\nglobal topological structures. Extensive experiments on the KITTI-360 dataset\ndemonstrate TopoLiDM's superiority over state-of-the-art methods, achieving\nimprovements of 22.6% lower Frechet Range Image Distance (FRID) and 9.2% lower\nMinimum Matching Distance (MMD). Notably, our model also enables fast\ngeneration speed with an average inference time of 1.68 samples/s, showcasing\nits scalability for real-world applications. We will release the related codes\nat https://github.com/IRMVLab/TopoLiDM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22421", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22421", "abs": "https://arxiv.org/abs/2507.22421", "authors": ["Shahla John"], "title": "Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A Unified Framework for Action Recognition and Object Tracking", "comment": null, "summary": "Real-time video analysis remains a challenging problem in computer vision,\nrequiring efficient processing of both spatial and temporal information while\nmaintaining computational efficiency. Existing approaches often struggle to\nbalance accuracy and speed, particularly in resource-constrained environments.\nIn this work, we present a unified framework that leverages advanced\nspatial-temporal modeling techniques for simultaneous action recognition and\nobject tracking. Our approach builds upon recent advances in parallel sequence\nmodeling and introduces a novel hierarchical attention mechanism that\nadaptively focuses on relevant spatial regions across temporal sequences. We\ndemonstrate that our method achieves state-of-the-art performance on standard\nbenchmarks while maintaining real-time inference speeds. Extensive experiments\non UCF-101, HMDB-51, and MOT17 datasets show improvements of 3.2% in action\nrecognition accuracy and 2.8% in tracking precision compared to existing\nmethods, with 40% faster inference time.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22522", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.22522", "abs": "https://arxiv.org/abs/2507.22522", "authors": ["Ziyi Wang", "Peiming Li", "Hong Liu", "Zhichao Deng", "Can Wang", "Jun Liu", "Junsong Yuan", "Mengyuan Liu"], "title": "Recognizing Actions from Robotic View for Natural Human-Robot Interaction", "comment": "8 pages, 4 figures, Accepted to ICCV2025", "summary": "Natural Human-Robot Interaction (N-HRI) requires robots to recognize human\nactions at varying distances and states, regardless of whether the robot itself\nis in motion or stationary. This setup is more flexible and practical than\nconventional human action recognition tasks. However, existing benchmarks\ndesigned for traditional action recognition fail to address the unique\ncomplexities in N-HRI due to limited data, modalities, task categories, and\ndiversity of subjects and environments. To address these challenges, we\nintroduce ACTIVE (Action from Robotic View), a large-scale dataset tailored\nspecifically for perception-centric robotic views prevalent in mobile service\nrobots. ACTIVE comprises 30 composite action categories, 80 participants, and\n46,868 annotated video instances, covering both RGB and point cloud modalities.\nParticipants performed various human actions in diverse environments at\ndistances ranging from 3m to 50m, while the camera platform was also mobile,\nsimulating real-world scenarios of robot perception with varying camera heights\ndue to uneven ground. This comprehensive and challenging benchmark aims to\nadvance action and attribute recognition research in N-HRI. Furthermore, we\npropose ACTIVE-PC, a method that accurately perceives human actions at long\ndistances using Multilevel Neighborhood Sampling, Layered Recognizers, Elastic\nEllipse Query, and precise decoupling of kinematic interference from human\nactions. Experimental results demonstrate the effectiveness of ACTIVE-PC. Our\ncode is available at:\nhttps://github.com/wangzy01/ACTIVE-Action-from-Robotic-View.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22607", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22607", "abs": "https://arxiv.org/abs/2507.22607", "authors": ["Ruifeng Yuan", "Chenghao Xiao", "Sicong Leng", "Jianyu Wang", "Long Li", "Weiwen Xu", "Hou Pong Chan", "Deli Zhao", "Tingyang Xu", "Zhongyu Wei", "Hao Zhang", "Yu Rong"], "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning", "comment": "21 pages, 5 figures, 6 tables. Work in progress", "summary": "Reinforcement learning has proven its effectiveness in enhancing the\nreasoning capabilities of large language models. Recent research efforts have\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\ninherent complexity and diversity of multimodal tasks, especially in semantic\ncontent and problem formulations, existing models often exhibit unstable\nperformance across various domains and difficulty levels. To address these\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\ngradually increasing difficulty, substantially improving its reasoning\nabilities across diverse multimodal contexts. The framework introduces two key\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\nadjusting training difficulty across successive RL training stages; and (2) a\ndynamic length reward mechanism, which encourages the model to adaptively\nregulate its reasoning path length according to task complexity, thus balancing\nreasoning efficiency with correctness. Experimental evaluations demonstrate\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\nlogic, and general understanding, validating the effectiveness of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22576", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22576", "abs": "https://arxiv.org/abs/2507.22576", "authors": ["Galadrielle Humblot-Renaux", "Gianni Franchi", "Sergio Escalera", "Thomas B. Moeslund"], "title": "COOkeD: Ensemble-based OOD detection in the era of zero-shot CLIP", "comment": "accepted at ICCVW'25 - Systematic Trust in AI Models: Ensuring\n  Fairness, Reliability, Explainability, and Accountability in Machine Learning\n  Frameworks", "summary": "Out-of-distribution (OOD) detection is an important building block in\ntrustworthy image recognition systems as unknown classes may arise at\ntest-time. OOD detection methods typically revolve around a single classifier,\nleading to a split in the research field between the classical supervised\nsetting (e.g. ResNet18 classifier trained on CIFAR100) vs. the zero-shot\nsetting (class names fed as prompts to CLIP). In both cases, an overarching\nchallenge is that the OOD detection performance is implicitly constrained by\nthe classifier's capabilities on in-distribution (ID) data. In this work, we\nshow that given a little open-mindedness from both ends, remarkable OOD\ndetection can be achieved by instead creating a heterogeneous ensemble - COOkeD\ncombines the predictions of a closed-world classifier trained end-to-end on a\nspecific dataset, a zero-shot CLIP classifier, and a linear probe classifier\ntrained on CLIP image features. While bulky at first sight, this approach is\nmodular, post-hoc and leverages the availability of pre-trained VLMs, thus\nintroduces little overhead compared to training a single standard classifier.\nWe evaluate COOkeD on popular CIFAR100 and ImageNet benchmarks, but also\nconsider more challenging, realistic settings ranging from training-time label\nnoise, to test-time covariate shift, to zero-shot shift which has been\npreviously overlooked. Despite its simplicity, COOkeD achieves state-of-the-art\nperformance and greater robustness compared to both classical and CLIP-based\nOOD detection methods. Code is available at https://github.com/glhr/COOkeD", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22522", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.22522", "abs": "https://arxiv.org/abs/2507.22522", "authors": ["Ziyi Wang", "Peiming Li", "Hong Liu", "Zhichao Deng", "Can Wang", "Jun Liu", "Junsong Yuan", "Mengyuan Liu"], "title": "Recognizing Actions from Robotic View for Natural Human-Robot Interaction", "comment": "8 pages, 4 figures, Accepted to ICCV2025", "summary": "Natural Human-Robot Interaction (N-HRI) requires robots to recognize human\nactions at varying distances and states, regardless of whether the robot itself\nis in motion or stationary. This setup is more flexible and practical than\nconventional human action recognition tasks. However, existing benchmarks\ndesigned for traditional action recognition fail to address the unique\ncomplexities in N-HRI due to limited data, modalities, task categories, and\ndiversity of subjects and environments. To address these challenges, we\nintroduce ACTIVE (Action from Robotic View), a large-scale dataset tailored\nspecifically for perception-centric robotic views prevalent in mobile service\nrobots. ACTIVE comprises 30 composite action categories, 80 participants, and\n46,868 annotated video instances, covering both RGB and point cloud modalities.\nParticipants performed various human actions in diverse environments at\ndistances ranging from 3m to 50m, while the camera platform was also mobile,\nsimulating real-world scenarios of robot perception with varying camera heights\ndue to uneven ground. This comprehensive and challenging benchmark aims to\nadvance action and attribute recognition research in N-HRI. Furthermore, we\npropose ACTIVE-PC, a method that accurately perceives human actions at long\ndistances using Multilevel Neighborhood Sampling, Layered Recognizers, Elastic\nEllipse Query, and precise decoupling of kinematic interference from human\nactions. Experimental results demonstrate the effectiveness of ACTIVE-PC. Our\ncode is available at:\nhttps://github.com/wangzy01/ACTIVE-Action-from-Robotic-View.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22533", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22533", "abs": "https://arxiv.org/abs/2507.22533", "authors": ["Dongchen Li", "Jitao Liang", "Wei Li", "Xiaoyu Wang", "Longbing Cao", "Kun Yu"], "title": "CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records", "comment": null, "summary": "Large Language Models (LLMs) hold significant promise for improving clinical\ndecision support and reducing physician burnout by synthesizing complex,\nlongitudinal cancer Electronic Health Records (EHRs). However, their\nimplementation in this critical field faces three primary challenges: the\ninability to effectively process the extensive length and multilingual nature\nof patient records for accurate temporal analysis; a heightened risk of\nclinical hallucination, as conventional grounding techniques such as\nRetrieval-Augmented Generation (RAG) do not adequately incorporate\nprocess-oriented clinical guidelines; and unreliable evaluation metrics that\nhinder the validation of AI systems in oncology. To address these issues, we\npropose CliCARE, a framework for Grounding Large Language Models in Clinical\nGuidelines for Decision Support over Longitudinal Cancer Electronic Health\nRecords. The framework operates by transforming unstructured, longitudinal EHRs\ninto patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range\ndependencies, and then grounding the decision support process by aligning these\nreal-world patient trajectories with a normative guideline knowledge graph.\nThis approach provides oncologists with evidence-grounded decision support by\ngenerating a high-fidelity clinical summary and an actionable recommendation.\nWe validated our framework using large-scale, longitudinal data from a private\nChinese cancer dataset and the public English MIMIC-IV dataset. In these\ndiverse settings, CliCARE significantly outperforms strong baselines, including\nleading long-context LLMs and Knowledge Graph-enhanced RAG methods. The\nclinical validity of our results is supported by a robust evaluation protocol,\nwhich demonstrates a high correlation with assessments made by expert\noncologists.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation"], "score": 3}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22607", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22607", "abs": "https://arxiv.org/abs/2507.22607", "authors": ["Ruifeng Yuan", "Chenghao Xiao", "Sicong Leng", "Jianyu Wang", "Long Li", "Weiwen Xu", "Hou Pong Chan", "Deli Zhao", "Tingyang Xu", "Zhongyu Wei", "Hao Zhang", "Yu Rong"], "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning", "comment": "21 pages, 5 figures, 6 tables. Work in progress", "summary": "Reinforcement learning has proven its effectiveness in enhancing the\nreasoning capabilities of large language models. Recent research efforts have\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\ninherent complexity and diversity of multimodal tasks, especially in semantic\ncontent and problem formulations, existing models often exhibit unstable\nperformance across various domains and difficulty levels. To address these\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\ngradually increasing difficulty, substantially improving its reasoning\nabilities across diverse multimodal contexts. The framework introduces two key\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\nadjusting training difficulty across successive RL training stages; and (2) a\ndynamic length reward mechanism, which encourages the model to adaptively\nregulate its reasoning path length according to task complexity, thus balancing\nreasoning efficiency with correctness. Experimental evaluations demonstrate\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\nlogic, and general understanding, validating the effectiveness of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22626", "abs": "https://arxiv.org/abs/2507.22626", "authors": ["Shenghao Zhu", "Yifei Chen", "Weihong Chen", "Yuanhan Wang", "Chang Liu", "Shuo Jiang", "Feiwei Qin", "Changmiao Wang"], "title": "Bridging the Gap in Missing Modalities: Leveraging Knowledge Distillation and Style Matching for Brain Tumor Segmentation", "comment": "11 pages, 2 figures", "summary": "Accurate and reliable brain tumor segmentation, particularly when dealing\nwith missing modalities, remains a critical challenge in medical image\nanalysis. Previous studies have not fully resolved the challenges of tumor\nboundary segmentation insensitivity and feature transfer in the absence of key\nimaging modalities. In this study, we introduce MST-KDNet, aimed at addressing\nthese critical issues. Our model features Multi-Scale Transformer Knowledge\nDistillation to effectively capture attention weights at various resolutions,\nDual-Mode Logit Distillation to improve the transfer of knowledge, and a Global\nStyle Matching Module that integrates feature matching with adversarial\nlearning. Comprehensive experiments conducted on the BraTS and FeTS 2024\ndatasets demonstrate that MST-KDNet surpasses current leading methods in both\nDice and HD95 scores, particularly in conditions with substantial modality\nloss. Our approach shows exceptional robustness and generalization potential,\nmaking it a promising candidate for real-world clinical applications. Our\nsource code is available at https://github.com/Quanato607/MST-KDNet.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22576", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22576", "abs": "https://arxiv.org/abs/2507.22576", "authors": ["Galadrielle Humblot-Renaux", "Gianni Franchi", "Sergio Escalera", "Thomas B. Moeslund"], "title": "COOkeD: Ensemble-based OOD detection in the era of zero-shot CLIP", "comment": "accepted at ICCVW'25 - Systematic Trust in AI Models: Ensuring\n  Fairness, Reliability, Explainability, and Accountability in Machine Learning\n  Frameworks", "summary": "Out-of-distribution (OOD) detection is an important building block in\ntrustworthy image recognition systems as unknown classes may arise at\ntest-time. OOD detection methods typically revolve around a single classifier,\nleading to a split in the research field between the classical supervised\nsetting (e.g. ResNet18 classifier trained on CIFAR100) vs. the zero-shot\nsetting (class names fed as prompts to CLIP). In both cases, an overarching\nchallenge is that the OOD detection performance is implicitly constrained by\nthe classifier's capabilities on in-distribution (ID) data. In this work, we\nshow that given a little open-mindedness from both ends, remarkable OOD\ndetection can be achieved by instead creating a heterogeneous ensemble - COOkeD\ncombines the predictions of a closed-world classifier trained end-to-end on a\nspecific dataset, a zero-shot CLIP classifier, and a linear probe classifier\ntrained on CLIP image features. While bulky at first sight, this approach is\nmodular, post-hoc and leverages the availability of pre-trained VLMs, thus\nintroduces little overhead compared to training a single standard classifier.\nWe evaluate COOkeD on popular CIFAR100 and ImageNet benchmarks, but also\nconsider more challenging, realistic settings ranging from training-time label\nnoise, to test-time covariate shift, to zero-shot shift which has been\npreviously overlooked. Despite its simplicity, COOkeD achieves state-of-the-art\nperformance and greater robustness compared to both classical and CLIP-based\nOOD detection methods. Code is available at https://github.com/glhr/COOkeD", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22607", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22607", "abs": "https://arxiv.org/abs/2507.22607", "authors": ["Ruifeng Yuan", "Chenghao Xiao", "Sicong Leng", "Jianyu Wang", "Long Li", "Weiwen Xu", "Hou Pong Chan", "Deli Zhao", "Tingyang Xu", "Zhongyu Wei", "Hao Zhang", "Yu Rong"], "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning", "comment": "21 pages, 5 figures, 6 tables. Work in progress", "summary": "Reinforcement learning has proven its effectiveness in enhancing the\nreasoning capabilities of large language models. Recent research efforts have\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\ninherent complexity and diversity of multimodal tasks, especially in semantic\ncontent and problem formulations, existing models often exhibit unstable\nperformance across various domains and difficulty levels. To address these\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\ngradually increasing difficulty, substantially improving its reasoning\nabilities across diverse multimodal contexts. The framework introduces two key\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\nadjusting training difficulty across successive RL training stages; and (2) a\ndynamic length reward mechanism, which encourages the model to adaptively\nregulate its reasoning path length according to task complexity, thus balancing\nreasoning efficiency with correctness. Experimental evaluations demonstrate\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\nlogic, and general understanding, validating the effectiveness of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22668", "abs": "https://arxiv.org/abs/2507.22668", "authors": ["Hongbin Lin", "Yifan Jiang", "Juangui Xu", "Jesse Jiaxi Xu", "Yi Lu", "Zhengyu Hu", "Ying-Cong Chen", "Hao Wang"], "title": "Graph-Guided Dual-Level Augmentation for 3D Scene Segmentation", "comment": "15 pages, 11 figures, to be published in ACMMM 2025 Conference", "summary": "3D point cloud segmentation aims to assign semantic labels to individual\npoints in a scene for fine-grained spatial understanding. Existing methods\ntypically adopt data augmentation to alleviate the burden of large-scale\nannotation. However, most augmentation strategies only focus on local\ntransformations or semantic recomposition, lacking the consideration of global\nstructural dependencies within scenes. To address this limitation, we propose a\ngraph-guided data augmentation framework with dual-level constraints for\nrealistic 3D scene synthesis. Our method learns object relationship statistics\nfrom real-world data to construct guiding graphs for scene generation.\nLocal-level constraints enforce geometric plausibility and semantic consistency\nbetween objects, while global-level constraints maintain the topological\nstructure of the scene by aligning the generated layout with the guiding graph.\nExtensive experiments on indoor and outdoor datasets demonstrate that our\nframework generates diverse and high-quality augmented scenes, leading to\nconsistent improvements in point cloud segmentation performance across various\nmodels.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22805", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22805", "abs": "https://arxiv.org/abs/2507.22805", "authors": ["Yuqi Pang", "Bowen Yang", "Yun Cao", "Fan Rong", "Xiaoyu Li", "Chen He"], "title": "MoCHA: Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention", "comment": null, "summary": "Vision large language models (VLLMs) are focusing primarily on handling\ncomplex and fine-grained visual information by incorporating advanced vision\nencoders and scaling up visual models. However, these approaches face high\ntraining and inference costs, as well as challenges in extracting visual\ndetails, effectively bridging across modalities. In this work, we propose a\nnovel visual framework, MoCHA, to address these issues. Our framework\nintegrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to\nextract complementary visual features and is equipped with a sparse Mixture of\nExperts Connectors (MoECs) module to dynamically select experts tailored to\ndifferent visual dimensions. To mitigate redundant or insufficient use of the\nvisual information encoded by the MoECs module, we further design a\nHierarchical Group Attention (HGA) with intra- and inter-group operations and\nan adaptive gating strategy for encoded visual features. We train MoCHA on two\nmainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance\nacross various benchmarks. Notably, MoCHA outperforms state-of-the-art\nopen-weight models on various tasks. For example, compared to CuMo\n(Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate\nhallucination by showing improvements of 3.25% in POPE and to follow visual\ninstructions by raising 153 points on MME. Finally, ablation studies further\nconfirm the effectiveness and robustness of the proposed MoECs and HGA in\nimproving the overall performance of MoCHA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["Vicuna", "fine-grained"], "score": 2}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22781", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22781", "abs": "https://arxiv.org/abs/2507.22781", "authors": ["Xuecheng Wu", "Danlei Huang", "Heli Sun", "Xinyi Yin", "Yifan Wang", "Hao Wang", "Jia Zhang", "Fei Wang", "Peihao Guo", "Suyu Xing", "Junxiao Xue", "Liang He"], "title": "HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training", "comment": null, "summary": "Advances in Generative AI have made video-level deepfake detection\nincreasingly challenging, exposing the limitations of current detection\ntechniques. In this paper, we present HOLA, our solution to the Video-Level\nDeepfake Detection track of 2025 1M-Deepfakes Detection Challenge. Inspired by\nthe success of large-scale pre-training in the general domain, we first scale\naudio-visual self-supervised pre-training in the multimodal video-level\ndeepfake detection, which leverages our self-built dataset of 1.81M samples,\nthereby leading to a unified two-stage framework. To be specific, HOLA features\nan iterative-aware cross-modal learning module for selective audio-visual\ninteractions, hierarchical contextual modeling with gated aggregations under\nthe local-global perspective, and a pyramid-like refiner for scale-aware\ncross-grained semantic enhancements. Moreover, we propose the pseudo supervised\nsingal injection strategy to further boost model performance. Extensive\nexperiments across expert models and MLLMs impressivly demonstrate the\neffectiveness of our proposed HOLA. We also conduct a series of ablation\nstudies to explore the crucial design factors of our introduced components.\nRemarkably, our HOLA ranks 1st, outperforming the second by 0.0476 AUC on the\nTestA set.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22805", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22805", "abs": "https://arxiv.org/abs/2507.22805", "authors": ["Yuqi Pang", "Bowen Yang", "Yun Cao", "Fan Rong", "Xiaoyu Li", "Chen He"], "title": "MoCHA: Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention", "comment": null, "summary": "Vision large language models (VLLMs) are focusing primarily on handling\ncomplex and fine-grained visual information by incorporating advanced vision\nencoders and scaling up visual models. However, these approaches face high\ntraining and inference costs, as well as challenges in extracting visual\ndetails, effectively bridging across modalities. In this work, we propose a\nnovel visual framework, MoCHA, to address these issues. Our framework\nintegrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to\nextract complementary visual features and is equipped with a sparse Mixture of\nExperts Connectors (MoECs) module to dynamically select experts tailored to\ndifferent visual dimensions. To mitigate redundant or insufficient use of the\nvisual information encoded by the MoECs module, we further design a\nHierarchical Group Attention (HGA) with intra- and inter-group operations and\nan adaptive gating strategy for encoded visual features. We train MoCHA on two\nmainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance\nacross various benchmarks. Notably, MoCHA outperforms state-of-the-art\nopen-weight models on various tasks. For example, compared to CuMo\n(Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate\nhallucination by showing improvements of 3.25% in POPE and to follow visual\ninstructions by raising 153 points on MME. Finally, ablation studies further\nconfirm the effectiveness and robustness of the proposed MoECs and HGA in\nimproving the overall performance of MoCHA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["Vicuna", "fine-grained"], "score": 2}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22827", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22827", "abs": "https://arxiv.org/abs/2507.22827", "authors": ["Yilei Jiang", "Yaozhi Zheng", "Yuxuan Wan", "Jiaming Han", "Qunzhong Wang", "Michael R. Lyu", "Xiangyu Yue"], "title": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents", "comment": null, "summary": "Automating the transformation of user interface (UI) designs into front-end\ncode holds significant promise for accelerating software development and\ndemocratizing design workflows. While recent large language models (LLMs) have\ndemonstrated progress in text-to-code generation, many existing approaches rely\nsolely on natural language prompts, limiting their effectiveness in capturing\nspatial layout and visual design intent. In contrast, UI development in\npractice is inherently multimodal, often starting from visual sketches or\nmockups. To address this gap, we introduce a modular multi-agent framework that\nperforms UI-to-code generation in three interpretable stages: grounding,\nplanning, and generation. The grounding agent uses a vision-language model to\ndetect and label UI components, the planning agent constructs a hierarchical\nlayout using front-end engineering priors, and the generation agent produces\nHTML/CSS code via adaptive prompt-based synthesis. This design improves\nrobustness, interpretability, and fidelity over end-to-end black-box methods.\nFurthermore, we extend the framework into a scalable data engine that\nautomatically produces large-scale image-code pairs. Using these synthetic\nexamples, we fine-tune and reinforce an open-source VLM, yielding notable gains\nin UI understanding and code quality. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in layout accuracy,\nstructural coherence, and code correctness. Our code is made publicly available\nat https://github.com/leigest519/ScreenCoder.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "code generation"], "score": 2}}, "source_file": "2025-07-31.jsonl"}
{"id": "2507.22873", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22873", "abs": "https://arxiv.org/abs/2507.22873", "authors": ["Simon Pochinda", "Momen K. Tageldeen", "Mark Thompson", "Tony Rinaldi", "Troy Giorshev", "Keith Lee", "Jie Zhou", "Frederick Walls"], "title": "LCS: An AI-based Low-Complexity Scaler for Power-Efficient Super-Resolution of Game Content", "comment": "8 pages, 3 figures", "summary": "The increasing complexity of content rendering in modern games has led to a\nproblematic growth in the workload of the GPU. In this paper, we propose an\nAI-based low-complexity scaler (LCS) inspired by state-of-the-art efficient\nsuper-resolution (ESR) models which could offload the workload on the GPU to a\nlow-power device such as a neural processing unit (NPU). The LCS is trained on\nGameIR image pairs natively rendered at low and high resolution. We utilize\nadversarial training to encourage reconstruction of perceptually important\ndetails, and apply reparameterization and quantization techniques to reduce\nmodel complexity and size. In our comparative analysis we evaluate the LCS\nalongside the publicly available AMD hardware-based Edge Adaptive Scaling\nFunction (EASF) and AMD FidelityFX Super Resolution 1 (FSR1) on five different\nmetrics, and find that the LCS achieves better perceptual quality,\ndemonstrating the potential of ESR models for upscaling on resource-constrained\ndevices.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}}, "source_file": "2025-07-31.jsonl"}
