{"id": "2508.10024", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.0"], "pdf": "https://arxiv.org/pdf/2508.10024", "abs": "https://arxiv.org/abs/2508.10024", "authors": ["J. Pablo Muñoz", "Jinjie Yuan"], "title": "RTTC: Reward-Guided Collaborative Test-Time Compute", "comment": null, "summary": "Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the\nperformance of Large Language Models (LLMs) at inference, leveraging strategies\nsuch as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).\nHowever, the optimal adaptation strategy varies across queries, and\nindiscriminate application of TTC strategy incurs substantial computational\noverhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a\nnovel framework that adaptively selects the most effective TTC strategy for\neach query via a pretrained reward model, maximizing downstream accuracy across\ndiverse domains and tasks. RTTC operates in a distributed server-client\narchitecture, retrieving relevant samples from a remote knowledge base and\napplying RAG or lightweight fine-tuning on client devices only when necessary.\nTo further mitigate redundant computation, we propose Query-State Caching,\nwhich enables the efficient reuse of historical query states at both retrieval\nand adaptation levels. Extensive experiments across multiple LLMs and\nbenchmarks demonstrate that RTTC consistently achieves superior accuracy\ncompared to vanilla RAG or TTT, validating the necessity of adaptive,\nreward-guided TTC selection and the potential of RTTC for scalable,\nhigh-performance language model adaptation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time training", "test-time compute"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10024", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.0"], "pdf": "https://arxiv.org/pdf/2508.10024", "abs": "https://arxiv.org/abs/2508.10024", "authors": ["J. Pablo Muñoz", "Jinjie Yuan"], "title": "RTTC: Reward-Guided Collaborative Test-Time Compute", "comment": null, "summary": "Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the\nperformance of Large Language Models (LLMs) at inference, leveraging strategies\nsuch as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).\nHowever, the optimal adaptation strategy varies across queries, and\nindiscriminate application of TTC strategy incurs substantial computational\noverhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a\nnovel framework that adaptively selects the most effective TTC strategy for\neach query via a pretrained reward model, maximizing downstream accuracy across\ndiverse domains and tasks. RTTC operates in a distributed server-client\narchitecture, retrieving relevant samples from a remote knowledge base and\napplying RAG or lightweight fine-tuning on client devices only when necessary.\nTo further mitigate redundant computation, we propose Query-State Caching,\nwhich enables the efficient reuse of historical query states at both retrieval\nand adaptation levels. Extensive experiments across multiple LLMs and\nbenchmarks demonstrate that RTTC consistently achieves superior accuracy\ncompared to vanilla RAG or TTT, validating the necessity of adaptive,\nreward-guided TTC selection and the potential of RTTC for scalable,\nhigh-performance language model adaptation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time training", "test-time compute"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10104", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10104", "abs": "https://arxiv.org/abs/2508.10104", "authors": ["Oriane Siméoni", "Huy V. Vo", "Maximilian Seitzer", "Federico Baldassarre", "Maxime Oquab", "Cijo Jose", "Vasil Khalidov", "Marc Szafraniec", "Seungeun Yi", "Michaël Ramamonjisoa", "Francisco Massa", "Daniel Haziza", "Luca Wehrstedt", "Jianyuan Wang", "Timothée Darcet", "Théo Moutakanni", "Leonel Sentana", "Claire Roberts", "Andrea Vedaldi", "Jamie Tolan", "John Brandt", "Camille Couprie", "Julien Mairal", "Hervé Jégou", "Patrick Labatut", "Piotr Bojanowski"], "title": "DINOv3", "comment": null, "summary": "Self-supervised learning holds the promise of eliminating the need for manual\ndata annotation, enabling models to scale effortlessly to massive datasets and\nlarger architectures. By not being tailored to specific tasks or domains, this\ntraining paradigm has the potential to learn visual representations from\ndiverse sources, ranging from natural to aerial images -- using a single\nalgorithm. This technical report introduces DINOv3, a major milestone toward\nrealizing this vision by leveraging simple yet effective strategies. First, we\nleverage the benefit of scaling both dataset and model size by careful data\npreparation, design, and optimization. Second, we introduce a new method called\nGram anchoring, which effectively addresses the known yet unsolved issue of\ndense feature maps degrading during long training schedules. Finally, we apply\npost-hoc strategies that further enhance our models' flexibility with respect\nto resolution, model size, and alignment with text. As a result, we present a\nversatile vision foundation model that outperforms the specialized state of the\nart across a broad range of settings, without fine-tuning. DINOv3 produces\nhigh-quality dense features that achieve outstanding performance on various\nvision tasks, significantly surpassing previous self- and weakly-supervised\nfoundation models. We also share the DINOv3 suite of vision models, designed to\nadvance the state of the art on a wide spectrum of tasks and data by providing\nscalable solutions for diverse resource constraints and deployment scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10177", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10177", "abs": "https://arxiv.org/abs/2508.10177", "authors": ["Stepan Kulibaba", "Artem Dzhalilov", "Roman Pakhomov", "Oleg Svidchenko", "Alexander Gasnikov", "Aleksei Shpilman"], "title": "KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems", "comment": null, "summary": "Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive\ncapabilities but face significant limitations such as constrained exploration\nstrategies and a severe execution bottleneck. Exploration is hindered by\none-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)\napproaches that fail to recombine strong partial solutions. The execution\nbottleneck arises from lengthy code validation cycles that stifle iterative\nrefinement. To overcome these challenges, we introduce KompeteAI, a novel\nAutoML framework with dynamic solution space exploration. Unlike previous MCTS\nmethods that treat ideas in isolation, KompeteAI introduces a merging stage\nthat composes top candidates. We further expand the hypothesis space by\nintegrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle\nnotebooks and arXiv papers to incorporate real-world strategies. KompeteAI also\naddresses the execution bottleneck via a predictive scoring model and an\naccelerated debugging method, assessing solution potential using early stage\nmetrics to avoid costly full-code execution. This approach accelerates pipeline\nevaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,\nAIDE, and Ml-Master) by an average of 3\\% on the primary AutoML benchmark,\nMLE-Bench. Additionally, we propose Kompete-bench to address limitations in\nMLE-Bench, where KompeteAI also achieves state-of-the-art results", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search", "MCTS"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10014", "abs": "https://arxiv.org/abs/2508.10014", "authors": ["Lingfeng Zhou", "Jialing Zhang", "Jin Gao", "Mohan Jiang", "Dequan Wang"], "title": "PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?", "comment": "Accepted by COLM 2025", "summary": "Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,\nwhich may fail to reflect how humans perceive role fidelity. A key prerequisite\nfor human-aligned evaluation is role identification, the ability to recognize\nwho is speaking based on dialogue context. We argue that any meaningful\njudgment of role-playing quality (how well a character is played) fundamentally\ndepends on first correctly attributing words and actions to the correct persona\n(who is speaking). We present PersonaEval, the first benchmark designed to test\nwhether LLM evaluators can reliably identify human roles. PersonaEval uses\nhuman-authored dialogues from novels, scripts, and video transcripts,\nchallenging models to determine the correct persona according to the\nconversation context. Our experiments, including a human study, show that even\nthe best-performing LLMs reach only around 69% accuracy, well below the level\nneeded for reliable evaluation. In contrast, human participants perform near\nceiling with 90.8% accuracy, highlighting that current LLM evaluators are still\nnot human enough to effectively judge role-play scenarios. To better understand\nthis gap, we examine training-time adaptation and test-time compute, suggesting\nthat reliable evaluation requires more than task-specific tuning, but depends\non strong, human-like reasoning abilities in LLM evaluators. We release our\nbenchmark at https://github.com/maple-zhou/PersonaEval.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time compute"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "dialogue"], "score": 4}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10747", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10747", "abs": "https://arxiv.org/abs/2508.10747", "authors": ["Sangwoo Jeon", "Juchul Shin", "Gyeong-Tae Kim", "YeonJe Cho", "Seongwoo Kim"], "title": "Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning", "comment": "16 pages, 10 figures", "summary": "Generalized planning using deep reinforcement learning (RL) combined with\ngraph neural networks (GNNs) has shown promising results in various symbolic\nplanning domains described by PDDL. However, existing approaches typically\nrepresent planning states as fully connected graphs, leading to a combinatorial\nexplosion in edge information and substantial sparsity as problem scales grow,\nespecially evident in large grid-based environments. This dense representation\nresults in diluted node-level information, exponentially increases memory\nrequirements, and ultimately makes learning infeasible for larger-scale\nproblems. To address these challenges, we propose a sparse, goal-aware GNN\nrepresentation that selectively encodes relevant local relationships and\nexplicitly integrates spatial features related to the goal. We validate our\napproach by designing novel drone mission scenarios based on PDDL within a grid\nworld, effectively simulating realistic mission execution environments. Our\nexperimental results demonstrate that our method scales effectively to larger\ngrid sizes previously infeasible with dense graph representations and\nsubstantially improves policy generalization and success rates. Our findings\nprovide a practical foundation for addressing realistic, large-scale\ngeneralized planning tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10030", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10030", "abs": "https://arxiv.org/abs/2508.10030", "authors": ["Saaduddin Mahmud", "Mason Nakamura", "Kyle H. Wray", "Shlomo Zilberstein"], "title": "Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models", "comment": "17 pages", "summary": "Prompt optimization methods have demonstrated significant effectiveness in\naligning black-box large language models (LLMs). In parallel, inference scaling\nstrategies such as Best-of-N Sampling and Majority Voting have also proven to\nenhance alignment and performance by trading off computation. However, existing\nprompt optimization approaches are inference strategy agnostic; that is, they\noptimize prompts without regard to the inference strategy employed during\ndeployment. This constitutes a significant methodological gap, as our empirical\nand theoretical analysis reveals a strong interdependence between these two\nparadigms. Moreover, we find that user preferences regarding trade-offs among\nmultiple objectives and inference budgets substantially influence the choice of\nprompt and inference configuration. To address this gap, we introduce a unified\nnovel framework named IAPO (Inference-Aware Prompt Optimization) that jointly\noptimizes the prompt and inference scale, while being aware of the inference\nbudget and different task objectives. We then develop a fixed-budget training\nalgorithm for IAPO, which we call PSST (Prompt Scaling via Sequential\nTrimming), and analyze finite-budget guarantees on error probability. Finally,\nwe evaluate the effectiveness of PSST on six different tasks, including\nmulti-objective text generation and reasoning, and demonstrate the critical\nrole of incorporating inference-awareness when aligning black-box LLMs through\nprompt optimization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10777", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10777", "abs": "https://arxiv.org/abs/2508.10777", "authors": ["Maël Jullien", "Marco Valentino", "André Freitas"], "title": "The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference", "comment": "19 pages", "summary": "Large language models are often assumed to acquire increasingly structured,\ngeneralizable internal representations simply by scaling data and parameters.\nWe interrogate this assumption by introducing a Clinical Trial Natural Language\nInference benchmark comprising four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction.\nEach item is paired with a targeted Ground Knowledge and Meta-Level Reasoning\nVerification (GKMRV) probe, allowing us to dissociate failures of factual\naccess from failures of inference. We evaluate six contemporary LLMs under both\ndirect and chain of thought prompting.\n  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform\npoorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,\noutput inferences are highly consistent across samples (mean 0.87), indicating\na systematic application of underlying heuristics and shortcuts.\n  These results reveal fundamental structural and representational limitations:\ncurrent LLMs often possess the relevant clinical knowledge but lack the\nstructured, composable internal representations needed to deploy it reliably\n(e.g., integrating constraints, weighing evidence, or simulating\ncounterfactuals). Decoupling knowledge from reasoning with GKMRV makes this\ndissociation explicit and measurable, providing an effective framework for\nprobing the reliability of LLMs in high-stakes domains.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "chain of thought"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10137", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10137", "abs": "https://arxiv.org/abs/2508.10137", "authors": ["Nghia Trung Ngo", "Franck Dernoncourt", "Thien Huu Nguyen"], "title": "mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning", "comment": null, "summary": "Recent advancements in reasoning-reinforced Large Language Models (LLMs) have\nshown remarkable capabilities in complex reasoning tasks. However, the\nmechanism underlying their utilization of different human reasoning skills\nremains poorly investigated, especially for multilingual commonsense reasoning\nthat involves everyday knowledge across different languages and cultures. To\naddress this gap, we propose a \\textbf{M}ultilingual and Scalable Benchmark for\n\\textbf{S}kill-based \\textbf{Co}mmonsense \\textbf{Re}asoning (\\textbf{mSCoRe}).\nOur benchmark incorporates three key components that are designed to\nsystematically evaluate LLM's reasoning capabilities, including: (1) a novel\ntaxonomy of reasoning skills that enables fine-grained analysis of models'\nreasoning processes, (2) a robust data synthesis pipeline tailored specifically\nfor commonsense reasoning evaluation, and (3) a complexity scaling framework\nallowing task difficulty to scale dynamically alongside future improvements in\nLLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying\nsizes and training approaches demonstrate that \\textbf{mSCoRe} remains\nsignificantly challenging for current models, particularly at higher complexity\nlevels. Our results reveal the limitations of such reasoning-reinforced models\nwhen confronted with nuanced multilingual general and cultural commonsense. We\nfurther provide detailed analysis on the models' reasoning processes,\nsuggesting future directions for improving multilingual commonsense reasoning\ncapabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10030", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10030", "abs": "https://arxiv.org/abs/2508.10030", "authors": ["Saaduddin Mahmud", "Mason Nakamura", "Kyle H. Wray", "Shlomo Zilberstein"], "title": "Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models", "comment": "17 pages", "summary": "Prompt optimization methods have demonstrated significant effectiveness in\naligning black-box large language models (LLMs). In parallel, inference scaling\nstrategies such as Best-of-N Sampling and Majority Voting have also proven to\nenhance alignment and performance by trading off computation. However, existing\nprompt optimization approaches are inference strategy agnostic; that is, they\noptimize prompts without regard to the inference strategy employed during\ndeployment. This constitutes a significant methodological gap, as our empirical\nand theoretical analysis reveals a strong interdependence between these two\nparadigms. Moreover, we find that user preferences regarding trade-offs among\nmultiple objectives and inference budgets substantially influence the choice of\nprompt and inference configuration. To address this gap, we introduce a unified\nnovel framework named IAPO (Inference-Aware Prompt Optimization) that jointly\noptimizes the prompt and inference scale, while being aware of the inference\nbudget and different task objectives. We then develop a fixed-budget training\nalgorithm for IAPO, which we call PSST (Prompt Scaling via Sequential\nTrimming), and analyze finite-budget guarantees on error probability. Finally,\nwe evaluate the effectiveness of PSST on six different tasks, including\nmulti-objective text generation and reasoning, and demonstrate the critical\nrole of incorporating inference-awareness when aligning black-box LLMs through\nprompt optimization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10137", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10137", "abs": "https://arxiv.org/abs/2508.10137", "authors": ["Nghia Trung Ngo", "Franck Dernoncourt", "Thien Huu Nguyen"], "title": "mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning", "comment": null, "summary": "Recent advancements in reasoning-reinforced Large Language Models (LLMs) have\nshown remarkable capabilities in complex reasoning tasks. However, the\nmechanism underlying their utilization of different human reasoning skills\nremains poorly investigated, especially for multilingual commonsense reasoning\nthat involves everyday knowledge across different languages and cultures. To\naddress this gap, we propose a \\textbf{M}ultilingual and Scalable Benchmark for\n\\textbf{S}kill-based \\textbf{Co}mmonsense \\textbf{Re}asoning (\\textbf{mSCoRe}).\nOur benchmark incorporates three key components that are designed to\nsystematically evaluate LLM's reasoning capabilities, including: (1) a novel\ntaxonomy of reasoning skills that enables fine-grained analysis of models'\nreasoning processes, (2) a robust data synthesis pipeline tailored specifically\nfor commonsense reasoning evaluation, and (3) a complexity scaling framework\nallowing task difficulty to scale dynamically alongside future improvements in\nLLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying\nsizes and training approaches demonstrate that \\textbf{mSCoRe} remains\nsignificantly challenging for current models, particularly at higher complexity\nlevels. Our results reveal the limitations of such reasoning-reinforced models\nwhen confronted with nuanced multilingual general and cultural commonsense. We\nfurther provide detailed analysis on the models' reasoning processes,\nsuggesting future directions for improving multilingual commonsense reasoning\ncapabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10710", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10710", "abs": "https://arxiv.org/abs/2508.10710", "authors": ["Joohyeon Lee", "Jin-Seop Lee", "Jee-Hyong Lee"], "title": "CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation", "comment": "Under review", "summary": "Diffusion-based text-to-image generation models have demonstrated strong\nperformance in terms of image quality and diversity. However, they still\nstruggle to generate images that accurately reflect the number of objects\nspecified in the input prompt. Several approaches have been proposed that rely\non either external counting modules for iterative refinement or quantity\nrepresentations derived from learned tokens or latent features. However, they\nstill have limitations in accurately reflecting the specified number of objects\nand overlook an important structural characteristic--The number of object\ninstances in the generated image is largely determined in the early timesteps\nof the denoising process. To correctly reflect the object quantity for image\ngeneration, the highly activated regions in the object cross-attention map at\nthe early timesteps should match the input object quantity, while each region\nshould be clearly separated. To address this issue, we propose\n\\textit{CountCluster}, a method that guides the object cross-attention map to\nbe clustered according to the specified object count in the input, without\nrelying on any external tools or additional training. The proposed method\npartitions the object cross-attention map into $k$ clusters at inference time\nbased on attention scores, defines an ideal distribution in which each cluster\nis spatially well-separated, and optimizes the latent to align with this target\ndistribution. Our method achieves an average improvement of 18.5\\%p in object\ncount accuracy compared to existing methods, and demonstrates superior quantity\ncontrol performance across a variety of prompts. Code will be released at:\nhttps://github.com/JoohyeonL22/CountCluster .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "iterative refinement"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10132", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10132", "abs": "https://arxiv.org/abs/2508.10132", "authors": ["Arianna Bunnell", "Devon Cataldi", "Yannik Glaser", "Thomas K. Wolfgruber", "Steven Heymsfield", "Alan B. Zonderman", "Thomas L. Kelly", "Peter Sadowski", "John A. Shepherd"], "title": "Deep Learning Enables Large-Scale Shape and Appearance Modeling in Total-Body DXA Imaging", "comment": "Preprint of manuscript accepted to the ShapeMI workshop at MICCAI\n  2025", "summary": "Total-body dual X-ray absorptiometry (TBDXA) imaging is a relatively low-cost\nwhole-body imaging modality, widely used for body composition assessment. We\ndevelop and validate a deep learning method for automatic fiducial point\nplacement on TBDXA scans using 1,683 manually-annotated TBDXA scans. The method\nachieves 99.5% percentage correct keypoints in an external testing dataset. To\ndemonstrate the value for shape and appearance modeling (SAM), our method is\nused to place keypoints on 35,928 scans for five different TBDXA imaging modes,\nthen associations with health markers are tested in two cohorts not used for\nSAM model generation using two-sample Kolmogorov-Smirnov tests. SAM feature\ndistributions associated with health biomarkers are shown to corroborate\nexisting evidence and generate new hypotheses on body composition and shape's\nrelationship to various frailty, metabolic, inflammation, and cardiometabolic\nhealth markers. Evaluation scripts, model weights, automatic point file\ngeneration code, and triangulation files are available at\nhttps://github.com/hawaii-ai/dxa-pointplacement.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10164", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10164", "abs": "https://arxiv.org/abs/2508.10164", "authors": ["Bin Hong", "Jiayu Liu", "Zhenya Huang", "Kai Zhang", "Mengdi Zhang"], "title": "Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization", "comment": "19 pages, 5 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated strong\nperformance on complex tasks through long Chain-of-Thought (CoT) reasoning.\nHowever, their lengthy outputs increase computational costs and may lead to\noverthinking, raising challenges in balancing reasoning effectiveness and\nefficiency. Current methods for efficient reasoning often compromise reasoning\nquality or require extensive resources. This paper investigates efficient\nmethods to reduce the generation length of LRMs. We analyze generation path\ndistributions and filter generated trajectories through difficulty estimation.\nSubsequently, we analyze the convergence behaviors of the objectives of various\npreference optimization methods under a Bradley-Terry loss based framework.\nBased on the analysis, we propose Length Controlled Preference Optimization\n(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can\neffectively learn length preference with limited data and training. Extensive\nexperiments demonstrate that our approach significantly reduces the average\noutput length by over 50\\% across multiple benchmarks while maintaining the\nreasoning performance. Our work highlights the potential for computationally\nefficient approaches in guiding LRMs toward efficient reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10171", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.10171", "abs": "https://arxiv.org/abs/2508.10171", "authors": ["Aaditya Baranwal", "Abdul Mueez", "Jason Voelker", "Guneet Bhatia", "Shruti Vyas"], "title": "SynSpill: Improved Industrial Spill Detection With Synthetic Data", "comment": "Accepted at ICCV (VISION'25 Workshop) 2025", "summary": "Large-scale Vision-Language Models (VLMs) have transformed general-purpose\nvisual recognition through strong zero-shot capabilities. However, their\nperformance degrades significantly in niche, safety-critical domains such as\nindustrial spill detection, where hazardous events are rare, sensitive, and\ndifficult to annotate. This scarcity -- driven by privacy concerns, data\nsensitivity, and the infrequency of real incidents -- renders conventional\nfine-tuning of detectors infeasible for most industrial settings.\n  We address this challenge by introducing a scalable framework centered on a\nhigh-quality synthetic data generation pipeline. We demonstrate that this\nsynthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of\nVLMs and substantially boosts the performance of state-of-the-art object\ndetectors such as YOLO and DETR. Notably, in the absence of synthetic data\n(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than\nthese detectors. When SynSpill is used, both VLMs and detectors achieve marked\nimprovements, with their performance becoming comparable.\n  Our results underscore that high-fidelity synthetic data is a powerful means\nto bridge the domain gap in safety-critical applications. The combination of\nsynthetic generation and lightweight adaptation offers a cost-effective,\nscalable pathway for deploying vision systems in industrial environments where\nreal data is scarce/impractical to obtain.\n  Project Page: https://synspill.vercel.app", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10227", "abs": "https://arxiv.org/abs/2508.10227", "authors": ["Yuning Huang", "Jiahao Pang", "Fengqing Zhu", "Dong Tian"], "title": "EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting", "comment": null, "summary": "As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS)\ndemonstrates fast training/rendering with superior visual quality. The two\ntasks of 3DGS, Gaussian creation and view rendering, are typically separated\nover time or devices, and thus storage/transmission and finally compression of\n3DGS Gaussians become necessary. We begin with a correlation and statistical\nanalysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals\nthat spherical harmonic AC attributes precisely follow Laplace distributions,\nwhile mixtures of Gaussian distributions can approximate rotation, scaling, and\nopacity. Additionally, harmonic AC attributes manifest weak correlations with\nother attributes except for inherited correlations from a color space. A\nfactorized and parameterized entropy coding method, EntropyGS, is hereinafter\nproposed. During encoding, distribution parameters of each Gaussian attribute\nare estimated to assist their entropy coding. The quantization for entropy\ncoding is adaptively performed according to Gaussian attribute types. EntropyGS\ndemonstrates about 30x rate reduction on benchmark datasets while maintaining\nsimilar rendering quality compared to input 3DGS data, with a fast encoding and\ndecoding time.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation"], "score": 2}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10358", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10358", "abs": "https://arxiv.org/abs/2508.10358", "authors": ["Mengtao Zhou", "Sifan Wu", "Huan Zhang", "Qi Sima", "Bang Liu"], "title": "What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles", "comment": null, "summary": "We investigate the capacity of Large Language Models (LLMs) for imaginative\nreasoning--the proactive construction, testing, and revision of hypotheses in\ninformation-sparse environments. Existing benchmarks, often static or focused\non social deduction, fail to capture the dynamic, exploratory nature of this\nreasoning process. To address this gap, we introduce a comprehensive research\nframework based on the classic \"Turtle Soup\" game, integrating a benchmark, an\nagent, and an evaluation protocol. We present TurtleSoup-Bench, the first\nlarge-scale, bilingual, interactive benchmark for imaginative reasoning,\ncomprising 800 turtle soup puzzles sourced from both the Internet and expert\nauthors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'\nperformance in this setting. To evaluate reasoning quality, we develop a\nmulti-dimensional protocol measuring logical consistency, detail completion,\nand conclusion alignment. Experiments with leading LLMs reveal clear capability\nlimits, common failure patterns, and a significant performance gap compared to\nhumans. Our work offers new insights into LLMs' imaginative reasoning and\nestablishes a foundation for future research on exploratory agent behavior.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "multi-dimensional"], "score": 4}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10019", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10019", "abs": "https://arxiv.org/abs/2508.10019", "authors": ["Li Wang", "Changhao Zhang", "Zengqi Xiu", "Kai Lu", "Xin Yu", "Kui Zhang", "Wenjun Wu"], "title": "Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning", "comment": null, "summary": "Despite recent advances in the reasoning capabilities of Large Language\nModels (LLMs), improving the reasoning ability of Small Language Models (SLMs,\ne.g., $\\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity\nand variability of natural language: essentially equivalent problems often\nappear in diverse surface forms, often obscured by redundant or distracting\ndetails. This imposes a dual burden on SLMs: they must first extract the core\nproblem from complex linguistic input, and then perform reasoning based on that\nunderstanding. The resulting vast and noisy problem space hinders optimization,\nparticularly for models with limited capacity. To address this, we propose a\nnew framework that decouples understanding from reasoning by mapping natural\nlanguage problems into a canonical problem space-a semantically simplified yet\nexpressive domain. This enables SLMs to focus on reasoning over standardized\ninputs, free from linguistic variability. Within this framework, we introduce\nDURIT (Decoupled Understanding from Reasoning via Iterative Training), a\nthree-step algorithm that iteratively: (1) mapping natural language problems\nvia reinforcement learning, (2) aligns reasoning trajectories through\nself-distillation, and (3) trains reasoning policies in the problem space. The\nmapper and reasoner are co-trained in an alternating loop throughout this\nprocess. Experiments show that DURIT substantially improves SLMs' performance\non both in-domain and out-of-domain mathematical and logical reasoning tasks.\nBeyond improving reasoning capabilities, DURIT also improves the robustness of\nreasoning, validating decoupling understanding from reasoning as an effective\nstrategy for strengthening SLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10309", "abs": "https://arxiv.org/abs/2508.10309", "authors": ["Wenjie Zhao", "Jia Li", "Yunhui Guo"], "title": "From Pixel to Mask: A Survey of Out-of-Distribution Segmentation", "comment": null, "summary": "Out-of-distribution (OoD) detection and segmentation have attracted growing\nattention as concerns about AI security rise. Conventional OoD detection\nmethods identify the existence of OoD objects but lack spatial localization,\nlimiting their usefulness in downstream tasks. OoD segmentation addresses this\nlimitation by localizing anomalous objects at pixel-level granularity. This\ncapability is crucial for safety-critical applications such as autonomous\ndriving, where perception modules must not only detect but also precisely\nsegment OoD objects, enabling targeted control actions and enhancing overall\nsystem robustness. In this survey, we group current OoD segmentation approaches\ninto four categories: (i) test-time OoD segmentation, (ii) outlier exposure for\nsupervised training, (iii) reconstruction-based methods, (iv) and approaches\nthat leverage powerful models. We systematically review recent advances in OoD\nsegmentation for autonomous-driving scenarios, identify emerging challenges,\nand discuss promising future research directions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10026", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10026", "abs": "https://arxiv.org/abs/2508.10026", "authors": ["Kai Zhao", "Yanjun Zhao", "Jiaming Song", "Shien He", "Lusheng Zhang", "Qiang Zhang", "Tianjiao Li"], "title": "SABER: Switchable and Balanced Training for Efficient LLM Reasoning", "comment": null, "summary": "Large language models (LLMs) empowered by chain-of-thought reasoning have\nachieved impressive accuracy on complex tasks but suffer from excessive\ninference costs and latency when applied uniformly to all problems. We propose\nSABER (Switchable and Balanced Training for Efficient LLM Reasoning), a\nreinforcement learning framework that endows LLMs with user-controllable,\ntoken-budgeted reasoning. SABER first profiles each training example's\nbase-model thinking token usage and assigns it to one of the predefined budget\ntiers. During fine-tuning, the model is guided by system prompts and\nlength-aware rewards to respect its assigned budget. In parallel, we\nincorporate no-think examples to ensure the model remains reliable even when\nexplicit reasoning is turned off. SABER further supports four discrete\ninference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling\nflexible trade-offs between latency and reasoning depth. Extensive evaluations\non math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning\n(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight\nbudgets, graceful degradation, and effective cross-scale and cross-domain\ngeneralization. In particular, SABER-FastThink cuts reasoning length by 65.4%\nand yields a 3.6% accuracy gain compared with the base model on the MATH\nbenchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "code generation"], "score": 3}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10382", "abs": "https://arxiv.org/abs/2508.10382", "authors": ["Hyundo Lee", "Suhyung Choi", "Byoung-Tak Zhang", "Inwoo Hwang"], "title": "Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models", "comment": null, "summary": "Image generation models trained on large datasets can synthesize high-quality\nimages but often produce spatially inconsistent and distorted images due to\nlimited information about the underlying structures and spatial layouts. In\nthis work, we leverage intrinsic scene properties (e.g., depth, segmentation\nmaps) that provide rich information about the underlying scene, unlike prior\napproaches that solely rely on image-text pairs or use intrinsics as\nconditional inputs. Our approach aims to co-generate both images and their\ncorresponding intrinsics, enabling the model to implicitly capture the\nunderlying scene structure and generate more spatially consistent and realistic\nimages. Specifically, we first extract rich intrinsic scene properties from a\nlarge image dataset with pre-trained estimators, eliminating the need for\nadditional scene information or explicit 3D representations. We then aggregate\nvarious intrinsic scene properties into a single latent variable using an\nautoencoder. Building upon pre-trained large-scale Latent Diffusion Models\n(LDMs), our method simultaneously denoises the image and intrinsic domains by\ncarefully sharing mutual information so that the image and intrinsic reflect\neach other without degrading image quality. Experimental results demonstrate\nthat our method corrects spatial inconsistencies and produces a more natural\nlayout of scenes while maintaining the fidelity and textual alignment of the\nbase model (e.g., Stable Diffusion).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10769", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.10769", "abs": "https://arxiv.org/abs/2508.10769", "authors": ["Zhiqi Shen", "Shaojing Fan", "Danni Xu", "Terence Sim", "Mohan Kankanhalli"], "title": "Modeling Human Responses to Multimodal AI Content", "comment": null, "summary": "As AI-generated content becomes widespread, so does the risk of\nmisinformation. While prior research has primarily focused on identifying\nwhether content is authentic, much less is known about how such content\ninfluences human perception and behavior. In domains like trading or the stock\nmarket, predicting how people react (e.g., whether a news post will go viral),\ncan be more critical than verifying its factual accuracy. To address this, we\ntake a human-centered approach and introduce the MhAIM Dataset, which contains\n154,552 online posts (111,153 of them AI-generated), enabling large-scale\nanalysis of how people respond to AI-generated content. Our human study reveals\nthat people are better at identifying AI content when posts include both text\nand visuals, particularly when inconsistencies exist between the two. We\npropose three new metrics: trustworthiness, impact, and openness, to quantify\nhow users judge and engage with online content. We present T-Lens, an LLM-based\nagent system designed to answer user queries by incorporating predicted human\nresponses to multimodal information. At its core is HR-MCP (Human Response\nModel Context Protocol), built on the standardized Model Context Protocol\n(MCP), enabling seamless integration with any LLM. This integration allows\nT-Lens to better align with human reactions, enhancing both interpretability\nand interaction capabilities. Our work provides empirical insights and\npractical tools to equip LLMs with human-awareness capabilities. By\nhighlighting the complex interplay among AI, human cognition, and information\nreception, our findings suggest actionable strategies for mitigating the risks\nof AI-driven misinformation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10427", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10427", "abs": "https://arxiv.org/abs/2508.10427", "authors": ["Keishi Ishihara", "Kento Sasaki", "Tsubasa Takahashi", "Daiki Shiono", "Yu Yamaguchi"], "title": "STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes", "comment": "Project Page: https://turingmotors.github.io/stride-qa/", "summary": "Vision-Language Models (VLMs) have been applied to autonomous driving to\nsupport decision-making in complex real-world scenarios. However, their\ntraining on static, web-sourced image-text pairs fundamentally limits the\nprecise spatiotemporal reasoning required to understand and predict dynamic\ntraffic scenes. We address this critical gap with STRIDE-QA, a large-scale\nvisual question answering (VQA) dataset for physically grounded reasoning from\nan ego-centric perspective. Constructed from 100 hours of multi-sensor driving\ndata in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the\nlargest VQA dataset for spatiotemporal reasoning in urban driving, offering 16\nmillion QA pairs over 285K frames. Grounded by dense, automatically generated\nannotations including 3D bounding boxes, segmentation masks, and multi-object\ntracks, the dataset uniquely supports both object-centric and ego-centric\nreasoning through three novel QA tasks that require spatial localization and\ntemporal prediction. Our benchmarks demonstrate that existing VLMs struggle\nsignificantly, achieving near-zero scores on prediction consistency. In\ncontrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains,\nachieving 55% success in spatial localization and 28% consistency in future\nmotion prediction, compared to near-zero scores from general-purpose VLMs.\nTherefore, STRIDE-QA establishes a comprehensive foundation for developing more\nreliable VLMs for safety-critical autonomous systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "consistency", "question answering"], "score": 4}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10449", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10449", "abs": "https://arxiv.org/abs/2508.10449", "authors": ["Dhruv Dosi", "Rohit Meena", "Param Rajpura", "Yogesh Kumar Meena"], "title": "SkeySpot: Automating Service Key Detection for Digital Electrical Layout Plans in the Construction Industry", "comment": "6 pages, preprint accepted in IEEE SMC 2025", "summary": "Legacy floor plans, often preserved only as scanned documents, remain\nessential resources for architecture, urban planning, and facility management\nin the construction industry. However, the lack of machine-readable floor plans\nrender large-scale interpretation both time-consuming and error-prone.\nAutomated symbol spotting offers a scalable solution by enabling the\nidentification of service key symbols directly from floor plans, supporting\nworkflows such as cost estimation, infrastructure maintenance, and regulatory\ncompliance. This work introduces a labelled Digitised Electrical Layout Plans\n(DELP) dataset comprising 45 scanned electrical layout plans annotated with\n2,450 instances across 34 distinct service key classes. A systematic evaluation\nframework is proposed using pretrained object detection models for DELP\ndataset. Among the models benchmarked, YOLOv8 achieves the highest performance\nwith a mean Average Precision (mAP) of 82.5\\%. Using YOLOv8, we develop\nSkeySpot, a lightweight, open-source toolkit for real-time detection,\nclassification, and quantification of electrical symbols. SkeySpot produces\nstructured, standardised outputs that can be scaled up for interoperable\nbuilding information workflows, ultimately enabling compatibility across\ndownstream applications and regulatory platforms. By lowering dependency on\nproprietary CAD systems and reducing manual annotation effort, this approach\nmakes the digitisation of electrical layouts more accessible to small and\nmedium-sized enterprises (SMEs) in the construction industry, while supporting\nbroader goals of standardisation, interoperability, and sustainability in the\nbuilt environment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation"], "score": 3}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10457", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10457", "abs": "https://arxiv.org/abs/2508.10457", "authors": ["Hanna Herasimchyk", "Robin Labryga", "Tomislav Prusina"], "title": "Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers", "comment": "Accepted for publication at: LifeCLEF Lab at CLEF 2025 Working Notes,\n  2025, Madrid, Spain", "summary": "We present a multi-head vision transformer approach for multi-label plant\nspecies prediction in vegetation plot images, addressing the PlantCLEF 2025\nchallenge. The task involves training models on single-species plant images\nwhile testing on multi-species quadrat images, creating a drastic domain shift.\nOur methodology leverages a pre-trained DINOv2 Vision Transformer Base\n(ViT-B/14) backbone with multiple classification heads for species, genus, and\nfamily prediction, utilizing taxonomic hierarchies. Key contributions include\nmulti-scale tiling to capture plants at different scales, dynamic threshold\noptimization based on mean prediction length, and ensemble strategies through\nbagging and Hydra model architectures. The approach incorporates various\ninference techniques including image cropping to remove non-plant artifacts,\ntop-n filtering for prediction constraints, and logit thresholding strategies.\nExperiments were conducted on approximately 1.4 million training images\ncovering 7,806 plant species. Results demonstrate strong performance, making\nour submission 3rd best on the private leaderboard. Our code is available at\nhttps://github.com/geranium12/plant-clef-2025/tree/v1.0.0.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10464", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10464", "abs": "https://arxiv.org/abs/2508.10464", "authors": ["Bella Specktor-Fadida", "Malte Hoffmann"], "title": "SingleStrip: learning skull-stripping from a single labeled example", "comment": "Accepted as an oral presentation to the MICCAI 2025 Data Engineering\n  in Medical Imaging (DEMI) workshop", "summary": "Deep learning segmentation relies heavily on labeled data, but manual\nlabeling is laborious and time-consuming, especially for volumetric images such\nas brain magnetic resonance imaging (MRI). While recent domain-randomization\ntechniques alleviate the dependency on labeled data by synthesizing diverse\ntraining images from label maps, they offer limited anatomical variability when\nvery few label maps are available. Semi-supervised self-training addresses\nlabel scarcity by iteratively incorporating model predictions into the training\nset, enabling networks to learn from unlabeled data. In this work, we combine\ndomain randomization with self-training to train three-dimensional\nskull-stripping networks using as little as a single labeled example. First, we\nautomatically bin voxel intensities, yielding labels we use to synthesize\nimages for training an initial skull-stripping model. Second, we train a\nconvolutional autoencoder (AE) on the labeled example and use its\nreconstruction error to assess the quality of brain masks predicted for\nunlabeled data. Third, we select the top-ranking pseudo-labels to fine-tune the\nnetwork, achieving skull-stripping performance on out-of-distribution data that\napproaches models trained with more labeled images. We compare AE-based ranking\nto consistency-based ranking under test-time augmentation, finding that the AE\napproach yields a stronger correlation with segmentation accuracy. Our results\nhighlight the potential of combining domain randomization and AE-based quality\ncontrol to enable effective semi-supervised segmentation from extremely limited\nlabeled data. This strategy may ease the labeling burden that slows progress in\nstudies involving new anatomical structures or emerging imaging techniques.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10226", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10226", "abs": "https://arxiv.org/abs/2508.10226", "authors": ["Andrew X. Chen", "Guillermo Horga", "Sean Escola"], "title": "Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia", "comment": null, "summary": "Patients who are at clinical high risk (CHR) for schizophrenia need close\nmonitoring of their symptoms to inform appropriate treatments. The Brief\nPsychiatric Rating Scale (BPRS) is a validated, commonly used research tool for\nmeasuring symptoms in patients with schizophrenia and other psychotic\ndisorders; however, it is not commonly used in clinical practice as it requires\na lengthy structured interview. Here, we utilize large language models (LLMs)\nto predict BPRS scores from clinical interview transcripts in 409 CHR patients\nfrom the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.\nDespite the interviews not being specifically structured to measure the BPRS,\nthe zero-shot performance of the LLM predictions compared to the true\nassessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and\nintra-rater reliability. We further demonstrate that LLMs have substantial\npotential to improve and standardize the assessment of CHR patients via their\naccuracy in assessing the BPRS in foreign languages (median concordance: 0.88,\nICC: 0.70), and integrating longitudinal information in a one-shot or few-shot\nlearning approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10019", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10019", "abs": "https://arxiv.org/abs/2508.10019", "authors": ["Li Wang", "Changhao Zhang", "Zengqi Xiu", "Kai Lu", "Xin Yu", "Kui Zhang", "Wenjun Wu"], "title": "Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning", "comment": null, "summary": "Despite recent advances in the reasoning capabilities of Large Language\nModels (LLMs), improving the reasoning ability of Small Language Models (SLMs,\ne.g., $\\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity\nand variability of natural language: essentially equivalent problems often\nappear in diverse surface forms, often obscured by redundant or distracting\ndetails. This imposes a dual burden on SLMs: they must first extract the core\nproblem from complex linguistic input, and then perform reasoning based on that\nunderstanding. The resulting vast and noisy problem space hinders optimization,\nparticularly for models with limited capacity. To address this, we propose a\nnew framework that decouples understanding from reasoning by mapping natural\nlanguage problems into a canonical problem space-a semantically simplified yet\nexpressive domain. This enables SLMs to focus on reasoning over standardized\ninputs, free from linguistic variability. Within this framework, we introduce\nDURIT (Decoupled Understanding from Reasoning via Iterative Training), a\nthree-step algorithm that iteratively: (1) mapping natural language problems\nvia reinforcement learning, (2) aligns reasoning trajectories through\nself-distillation, and (3) trains reasoning policies in the problem space. The\nmapper and reasoner are co-trained in an alternating loop throughout this\nprocess. Experiments show that DURIT substantially improves SLMs' performance\non both in-domain and out-of-domain mathematical and logical reasoning tasks.\nBeyond improving reasoning capabilities, DURIT also improves the robustness of\nreasoning, validating decoupling understanding from reasoning as an effective\nstrategy for strengthening SLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10473", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.10473", "abs": "https://arxiv.org/abs/2508.10473", "authors": ["Liangrui Pan", "xiaoyu Li", "Guang Zhu", "Guanting Li", "Ruixin Wang", "Jiadi Luo", "Yaning Yang", "Liang qingchun", "Shaoliang Peng"], "title": "STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images", "comment": "Submit to AAAI2026", "summary": "Spread through air spaces (STAS) constitutes a novel invasive pattern in lung\nadenocarcinoma (LUAD), associated with tumor recurrence and diminished survival\nrates. However, large-scale STAS diagnosis in LUAD remains a labor-intensive\nendeavor, compounded by the propensity for oversight and misdiagnosis due to\nits distinctive pathological characteristics and morphological features.\nConsequently, there is a pressing clinical imperative to leverage deep learning\nmodels for STAS diagnosis. This study initially assembled histopathological\nimages from STAS patients at the Second Xiangya Hospital and the Third Xiangya\nHospital of Central South University, alongside the TCGA-LUAD cohort. Three\nsenior pathologists conducted cross-verification annotations to construct the\nSTAS-SXY, STAS-TXY, and STAS-TCGA datasets. We then propose a multi-pattern\nattention-aware multiple instance learning framework, named STAMP, to analyze\nand diagnose the presence of STAS across multi-center histopathology images.\nSpecifically, the dual-branch architecture guides the model to learn\nSTAS-associated pathological features from distinct semantic spaces.\nTransformer-based instance encoding and a multi-pattern attention aggregation\nmodules dynamically selects regions closely associated with STAS pathology,\nsuppressing irrelevant noise and enhancing the discriminative power of global\nrepresentations. Moreover, a similarity regularization constraint prevents\nfeature redundancy across branches, thereby improving overall diagnostic\naccuracy. Extensive experiments demonstrated that STAMP achieved competitive\ndiagnostic results on STAS-SXY, STAS-TXY and STAS-TCGA, with AUCs of 0.8058,\n0.8017, and 0.7928, respectively, surpassing the clinical level.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10498", "abs": "https://arxiv.org/abs/2508.10498", "authors": ["Jianda Mao", "Kaibo Wang", "Yang Xiang", "Kani Chen"], "title": "TweezeEdit: Consistent and Efficient Image Editing with Path Regularization", "comment": null, "summary": "Large-scale pre-trained diffusion models empower users to edit images through\ntext guidance. However, existing methods often over-align with target prompts\nwhile inadequately preserving source image semantics. Such approaches generate\ntarget images explicitly or implicitly from the inversion noise of the source\nimages, termed the inversion anchors. We identify this strategy as suboptimal\nfor semantic preservation and inefficient due to elongated editing paths. We\npropose TweezeEdit, a tuning- and inversion-free framework for consistent and\nefficient image editing. Our method addresses these limitations by regularizing\nthe entire denoising path rather than relying solely on the inversion anchors,\nensuring source semantic retention and shortening editing paths. Guided by\ngradient-driven regularization, we efficiently inject target prompt semantics\nalong a direct path using a consistency model. Extensive experiments\ndemonstrate TweezeEdit's superior performance in semantic preservation and\ntarget alignment, outperforming existing methods. Remarkably, it requires only\n12 steps (1.6 seconds per edit), underscoring its potential for real-time\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10522", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10522", "abs": "https://arxiv.org/abs/2508.10522", "authors": ["Quang Nguyen", "Nhat Le", "Baoru Huang", "Minh Nhat Vu", "Chengcheng Tang", "Van Nguyen", "Ngan Le", "Thieu Vo", "Anh Nguyen"], "title": "EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba", "comment": "Accepted at The 2025 IEEE/CVF International Conference on Computer\n  Vision (ICCV 2025)", "summary": "Estimating human dance motion is a challenging task with various industrial\napplications. Recently, many efforts have focused on predicting human dance\nmotion using either egocentric video or music as input. However, the task of\njointly estimating human motion from both egocentric video and music remains\nlargely unexplored. In this paper, we aim to develop a new method that predicts\nhuman dance motion from both egocentric video and music. In practice, the\negocentric view often obscures much of the body, making accurate full-pose\nestimation challenging. Additionally, incorporating music requires the\ngenerated head and body movements to align well with both visual and musical\ninputs. We first introduce EgoAIST++, a new large-scale dataset that combines\nboth egocentric views and music with more than 36 hours of dancing motion.\nDrawing on the success of diffusion models and Mamba on modeling sequences, we\ndevelop an EgoMusic Motion Network with a core Skeleton Mamba that explicitly\ncaptures the skeleton structure of the human body. We illustrate that our\napproach is theoretically supportive. Intensive experiments show that our\nmethod clearly outperforms state-of-the-art approaches and generalizes\neffectively to real-world data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10026", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10026", "abs": "https://arxiv.org/abs/2508.10026", "authors": ["Kai Zhao", "Yanjun Zhao", "Jiaming Song", "Shien He", "Lusheng Zhang", "Qiang Zhang", "Tianjiao Li"], "title": "SABER: Switchable and Balanced Training for Efficient LLM Reasoning", "comment": null, "summary": "Large language models (LLMs) empowered by chain-of-thought reasoning have\nachieved impressive accuracy on complex tasks but suffer from excessive\ninference costs and latency when applied uniformly to all problems. We propose\nSABER (Switchable and Balanced Training for Efficient LLM Reasoning), a\nreinforcement learning framework that endows LLMs with user-controllable,\ntoken-budgeted reasoning. SABER first profiles each training example's\nbase-model thinking token usage and assigns it to one of the predefined budget\ntiers. During fine-tuning, the model is guided by system prompts and\nlength-aware rewards to respect its assigned budget. In parallel, we\nincorporate no-think examples to ensure the model remains reliable even when\nexplicit reasoning is turned off. SABER further supports four discrete\ninference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling\nflexible trade-offs between latency and reasoning depth. Extensive evaluations\non math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning\n(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight\nbudgets, graceful degradation, and effective cross-scale and cross-domain\ngeneralization. In particular, SABER-FastThink cuts reasoning length by 65.4%\nand yields a 3.6% accuracy gain compared with the base model on the MATH\nbenchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "code generation"], "score": 3}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10528", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10528", "abs": "https://arxiv.org/abs/2508.10528", "authors": ["Ziye Deng", "Ruihan He", "Jiaxiang Liu", "Yuan Wang", "Zijie Meng", "Songtao Jiang", "Yong Xie", "Zuozhu Liu"], "title": "Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset", "comment": null, "summary": "Medical image grounding aims to align natural language phrases with specific\nregions in medical images, serving as a foundational task for intelligent\ndiagnosis, visual question answering (VQA), and automated report generation\n(MRG). However, existing research is constrained by limited modality coverage,\ncoarse-grained annotations, and the absence of a unified, generalizable\ngrounding framework. To address these challenges, we construct a large-scale\nmedical grounding dataset Med-GLIP-5M comprising over 5.3 million region-level\nannotations across seven imaging modalities, covering diverse anatomical\nstructures and pathological findings. The dataset supports both segmentation\nand grounding tasks with hierarchical region labels, ranging from organ-level\nboundaries to fine-grained lesions. Based on this foundation, we propose\nMed-GLIP, a modality-aware grounding framework trained on Med-GLIP-5M. Rather\nthan relying on explicitly designed expert modules, Med-GLIP implicitly\nacquires hierarchical semantic understanding from diverse training data --\nenabling it to recognize multi-granularity structures, such as distinguishing\nlungs from pneumonia lesions. Extensive experiments demonstrate that Med-GLIP\nconsistently outperforms state-of-the-art baselines across multiple grounding\nbenchmarks. Furthermore, integrating its spatial outputs into downstream tasks,\nincluding medical VQA and report generation, leads to substantial performance\ngains. Our dataset will be released soon.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering", "fine-grained"], "score": 3}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10542", "abs": "https://arxiv.org/abs/2508.10542", "authors": ["Mengyu Ren", "Yutong Li", "Hua Li", "Runmin Cong", "Sam Kwong"], "title": "GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images", "comment": null, "summary": "Salient object detection (SOD) in optical remote sensing images (ORSIs) faces\nnumerous challenges, including significant variations in target scales and low\ncontrast between targets and the background. Existing methods based on vision\ntransformers (ViTs) and convolutional neural networks (CNNs) architectures aim\nto leverage both global and local features, but the difficulty in effectively\nintegrating these heterogeneous features limits their overall performance. To\novercome these limitations, we propose a graph-enhanced contextual and regional\nperception network (GCRPNet), which builds upon the Mamba architecture to\nsimultaneously capture long-range dependencies and enhance regional feature\nrepresentation. Specifically, we employ the visual state space (VSS) encoder to\nextract multi-scale features. To further achieve deep guidance and enhancement\nof these features, we first design a difference-similarity guided hierarchical\ngraph attention module (DS-HGAM). This module strengthens cross-layer\ninteraction capabilities between features of different scales while enhancing\nthe model's structural perception,allowing it to distinguish between foreground\nand background more effectively. Then, we design the LEVSS block as the decoder\nof GCRPNet. This module integrates our proposed adaptive scanning strategy and\nmulti-granularity collaborative attention enhancement module (MCAEM). It\nperforms adaptive patch scanning on feature maps processed via multi-scale\nconvolutions, thereby capturing rich local region information and enhancing\nMamba's local modeling capability. Extensive experimental results demonstrate\nthat the proposed model achieves state-of-the-art performance, validating its\neffectiveness and superiority.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10554", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10554", "abs": "https://arxiv.org/abs/2508.10554", "authors": ["Marc J. Fischer", "Jeffrey Potts", "Gabriel Urreola", "Dax Jones", "Paolo Palmisciano", "E. Bradley Strong", "Branden Cord", "Andrew D. Hernandez", "Julia D. Sharma", "E. Brandon Strong"], "title": "AR Surgical Navigation With Surface Tracing: Comparing In-SitVisualization with Tool-Tracking Guidance for Neurosurgical Applications", "comment": "10pages, 3 figures, will be published at ISMAR 2025 (accepted)", "summary": "Augmented Reality (AR) surgical navigation systems are emerging as the next\ngeneration of intraoperative surgical guidance, promising to overcome\nlimitations of traditional navigation systems. However, known issues with AR\ndepth perception due to vergence-accommodation conflict and occlusion handling\nlimitations of the currently commercially available display technology present\nacute challenges in surgical settings where precision is paramount. This study\npresents a novel methodology for utilizing AR guidance to register anatomical\ntargets and provide real-time instrument navigation using placement of\nsimulated external ventricular drain catheters on a phantom model as the\nclinical scenario. The system registers target positions to the patient through\na novel surface tracing method and uses real-time infrared tool tracking to aid\nin catheter placement, relying only on the onboard sensors of the Microsoft\nHoloLens 2. A group of intended users performed the procedure of simulated\ninsertions under two AR guidance conditions: static in-situ visualization,\nwhere planned trajectories are overlaid directly onto the patient anatomy, and\nreal-time tool-tracking guidance, where live feedback of the catheter's pose is\nprovided relative to the plan. Following the insertion tests, computed\ntomography scans of the phantom models were acquired, allowing for evaluation\nof insertion accuracy, target deviation, angular error, and depth precision.\nSystem Usability Scale surveys assessed user experience and cognitive workload.\nTool-tracking guidance improved performance metrics across all accuracy\nmeasures and was preferred by users in subjective evaluations. A free copy of\nthis paper and all supplemental materials are available at\nhttps://bit.ly/45l89Hq.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10556", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10556", "abs": "https://arxiv.org/abs/2508.10556", "authors": ["Ruisong Han", "Zongbo Han", "Jiahao Zhang", "Mingyue Cheng", "Changqing Zhang"], "title": "Retrieval-Augmented Prompt for OOD Detection", "comment": null, "summary": "Out-of-Distribution (OOD) detection is crucial for the reliable deployment of\nmachine learning models in-the-wild, enabling accurate identification of test\nsamples that differ from the training data distribution. Existing methods rely\non auxiliary outlier samples or in-distribution (ID) data to generate outlier\ninformation for training, but due to limited outliers and their mismatch with\nreal test OOD samples, they often fail to provide sufficient semantic\nsupervision, leading to suboptimal performance. To address this, we propose a\nnovel OOD detection method called Retrieval-Augmented Prompt (RAP). RAP\naugments a pre-trained vision-language model's prompts by retrieving external\nknowledge, offering enhanced semantic supervision for OOD detection. During\ntraining, RAP retrieves descriptive words for outliers based on joint\nsimilarity with external textual knowledge and uses them to augment the model's\nOOD prompts. During testing, RAP dynamically updates OOD prompts in real-time\nbased on the encountered OOD samples, enabling the model to rapidly adapt to\nthe test environment. Our extensive experiments demonstrate that RAP achieves\nstate-of-the-art performance on large-scale OOD detection benchmarks. For\nexample, in 1-shot OOD detection on the ImageNet-1k dataset, RAP reduces the\naverage FPR95 by 7.05% and improves the AUROC by 1.71% compared to previous\nmethods. Additionally, comprehensive ablation studies validate the\neffectiveness of each module and the underlying motivations of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10568", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10568", "abs": "https://arxiv.org/abs/2508.10568", "authors": ["Humza Naveed", "Xina Zeng", "Mitch Bryson", "Nagita Mehrseresht"], "title": "Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection", "comment": "work in progress", "summary": "Foundational models have achieved significant success in diverse domains of\ncomputer vision. They learn general representations that are easily\ntransferable to tasks not seen during training. One such foundational model is\nSegment anything model (SAM), which can accurately segment objects in images.\nWe propose adapting the SAM encoder via fine-tuning for remote sensing change\ndetection (RSCD) along with spatial-temporal feature enhancement (STFE) and\nmulti-scale decoder fusion (MSDF) to detect changes robustly at multiple\nscales. Additionally, we propose a novel cross-entropy masking (CEM) loss to\nhandle high class imbalance in change detection datasets. Our method\noutperforms state-of-the-art (SOTA) methods on four change detection datasets,\nLevir-CD, WHU-CD, CLCD, and S2Looking. We achieved 2.5% F1-score improvement on\na large complex S2Looking dataset. The code is available at:\nhttps://github.com/humza909/SAM-CEM-CD", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10600", "abs": "https://arxiv.org/abs/2508.10600", "authors": ["Yuxin Cao", "Yedi Zhang", "Wentao He", "Yifan Liao", "Yan Xiao", "Chang Li", "Zhiyong Huang", "Jin Song Dong"], "title": "Towards Powerful and Practical Patch Attacks for 2D Object Detection in Autonomous Driving", "comment": "13 pages, 4 figures", "summary": "Learning-based autonomous driving systems remain critically vulnerable to\nadversarial patches, posing serious safety and security risks in their\nreal-world deployment. Black-box attacks, notable for their high attack success\nrate without model knowledge, are especially concerning, with their\ntransferability extensively studied to reduce computational costs compared to\nquery-based attacks. Previous transferability-based black-box attacks typically\nadopt mean Average Precision (mAP) as the evaluation metric and design training\nloss accordingly. However, due to the presence of multiple detected bounding\nboxes and the relatively lenient Intersection over Union (IoU) thresholds, the\nattack effectiveness of these approaches is often overestimated, resulting in\nreduced success rates in practical attacking scenarios. Furthermore, patches\ntrained on low-resolution data often fail to maintain effectiveness on\nhigh-resolution images, limiting their transferability to autonomous driving\ndatasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch\nAttack framework for 2D object detection in autonomous driving, specifically\noptimized for high-resolution datasets. First, we introduce a novel metric,\nPractical Attack Success Rate (PASR), to more accurately quantify attack\neffectiveness with greater relevance for pedestrian safety. Second, we present\na tailored Localization-Confidence Suppression Loss (LCSL) to improve attack\ntransferability under PASR. Finally, to maintain the transferability for\nhigh-resolution datasets, we further incorporate the Probabilistic\nScale-Preserving Padding (PSPP) into the patch attack pipeline as a data\npreprocessing step. Extensive experiments show that P$^3$A outperforms\nstate-of-the-art attacks on unseen models and unseen high-resolution datasets,\nboth under the proposed practical IoU-based evaluation metric and the previous\nmAP-based metrics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10226", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10226", "abs": "https://arxiv.org/abs/2508.10226", "authors": ["Andrew X. Chen", "Guillermo Horga", "Sean Escola"], "title": "Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia", "comment": null, "summary": "Patients who are at clinical high risk (CHR) for schizophrenia need close\nmonitoring of their symptoms to inform appropriate treatments. The Brief\nPsychiatric Rating Scale (BPRS) is a validated, commonly used research tool for\nmeasuring symptoms in patients with schizophrenia and other psychotic\ndisorders; however, it is not commonly used in clinical practice as it requires\na lengthy structured interview. Here, we utilize large language models (LLMs)\nto predict BPRS scores from clinical interview transcripts in 409 CHR patients\nfrom the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.\nDespite the interviews not being specifically structured to measure the BPRS,\nthe zero-shot performance of the LLM predictions compared to the true\nassessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and\nintra-rater reliability. We further demonstrate that LLMs have substantial\npotential to improve and standardize the assessment of CHR patients via their\naccuracy in assessing the BPRS in foreign languages (median concordance: 0.88,\nICC: 0.70), and integrating longitudinal information in a one-shot or few-shot\nlearning approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10736", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10736", "abs": "https://arxiv.org/abs/2508.10736", "authors": ["Xiangqi Jin", "Yuxuan Wang", "Yifeng Gao", "Zichen Wen", "Biqing Qi", "Dongrui Liu", "Linfeng Zhang"], "title": "Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs", "comment": null, "summary": "Despite large language models (LLMs) have achieved remarkable success, their\nprefix-only prompting paradigm and sequential generation process offer limited\nflexibility for bidirectional information. Diffusion large language models\n(dLLMs) present new opportunities through their bidirectional attention\nmechanisms and iterative refinement processes, enabling more flexible in-place\nprompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting\nwith Early Exit), a novel framework that transforms prefix-only prompting into\nin-place prompting specifically designed for dLLMs. ICE integrates in-place\nprompts directly within masked token positions during iterative refinement and\nemploys a confidence-aware early exit mechanism to significantly reduce\ncomputational overhead. Extensive experiments demonstrate ICE's effectiveness,\nachieving up to 17.29% accuracy improvement with 4.12$\\times$ speedup on GSM8K,\nand up to 276.67$\\times$ acceleration on MMLU while maintaining competitive\nperformance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10637", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10637", "abs": "https://arxiv.org/abs/2508.10637", "authors": ["Ryan Ramos", "Vladan Stojnić", "Giorgos Kordopatis-Zilos", "Yuta Nakashima", "Giorgos Tolias", "Noa Garcia"], "title": "Processing and acquisition traces in visual encoders: What does CLIP know about your camera?", "comment": "8 main pages, supplementary attached, ICCV 2025 highlight", "summary": "Prior work has analyzed the robustness of visual encoders to image\ntransformations and corruptions, particularly in cases where such alterations\nare not seen during training. When this occurs, they introduce a form of\ndistribution shift at test time, often leading to performance degradation. The\nprimary focus has been on severe corruptions that, when applied aggressively,\ndistort useful signals necessary for accurate semantic predictions.\n  We take a different perspective by analyzing parameters of the image\nacquisition process and transformations that may be subtle or even\nimperceptible to the human eye. We find that such parameters are systematically\nencoded in the learned visual representations and can be easily recovered. More\nstrikingly, their presence can have a profound impact, either positively or\nnegatively, on semantic predictions. This effect depends on whether there is a\nstrong correlation or anti-correlation between semantic labels and these\nacquisition-based or processing-based labels. Our code and data are available\nat: https://github.com/ryan-caesar-ramos/visual-encoder-traces", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10795", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10795", "abs": "https://arxiv.org/abs/2508.10795", "authors": ["Osama Mohammed Afzal", "Preslav Nakov", "Tom Hope", "Iryna Gurevych"], "title": "Beyond \"Not Novel Enough\": Enriching Scholarly Critique with LLM-Assisted Feedback", "comment": null, "summary": "Novelty assessment is a central yet understudied aspect of peer review,\nparticularly in high volume fields like NLP where reviewer capacity is\nincreasingly strained. We present a structured approach for automated novelty\nevaluation that models expert reviewer behavior through three stages: content\nextraction from submissions, retrieval and synthesis of related work, and\nstructured comparison for evidence based assessment. Our method is informed by\na large scale analysis of human written novelty reviews and captures key\npatterns such as independent claim verification and contextual reasoning.\nEvaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty\nassessments, the approach achieves 86.5% alignment with human reasoning and\n75.3% agreement on novelty conclusions - substantially outperforming existing\nLLM based baselines. The method produces detailed, literature aware analyses\nand improves consistency over ad hoc reviewer judgments. These results\nhighlight the potential for structured LLM assisted approaches to support more\nrigorous and transparent peer review without displacing human expertise. Data\nand code are made available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement", "consistency"], "score": 3}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10839", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2508.10839", "abs": "https://arxiv.org/abs/2508.10839", "authors": ["Jim Dilkes", "Vahid Yazdanpanah", "Sebastian Stein"], "title": "Reinforced Language Models for Sequential Decision Making", "comment": null, "summary": "Large Language Models (LLMs) show potential as sequential decision-making\nagents, but their application is often limited due to a reliance on large,\ncomputationally expensive models. This creates a need to improve smaller\nmodels, yet existing post-training methods are designed for single-turn\ninteractions and cannot handle credit assignment in multi-step agentic tasks.\nTo address this, we introduce Multi-Step Group-Relative Policy Optimization\n(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal\nText-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)\nframeworks. For credit assignment, MS-GRPO attributes the entire cumulative\nepisode reward to each individual episode step. We supplement this algorithm\nwith a novel absolute-advantage-weighted episode sampling strategy that we show\nimproves training performance. We evaluate our approach by post-training a\n3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate\nthat the method is effective in improving decision-making performance: our\npost-trained 3B parameter model outperforms a 72B parameter baseline by 50% on\nthe Frozen Lake task. This work demonstrates that targeted post-training is a\npractical and efficient alternative to relying on model scale for creating\nsequential decision-making agents using LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10874", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10874", "abs": "https://arxiv.org/abs/2508.10874", "authors": ["Yuchen Fan", "Kaiyan Zhang", "Heng Zhou", "Yuxin Zuo", "Yanxu Chen", "Yu Fu", "Xinwei Long", "Xuekai Zhu", "Che Jiang", "Yuchen Zhang", "Li Kang", "Gang Chen", "Cheng Huang", "Zhizhou He", "Bingning Wang", "Lei Bai", "Ning Ding", "Bowen Zhou"], "title": "SSRL: Self-Search Reinforcement Learning", "comment": null, "summary": "We investigate the potential of large language models (LLMs) to serve as\nefficient simulators for agentic search tasks in reinforcement learning (RL),\nthereby reducing dependence on costly interactions with external search\nengines. To this end, we first quantify the intrinsic search capability of LLMs\nvia structured prompting and repeated sampling, which we term Self-Search. Our\nresults reveal that LLMs exhibit strong scaling behavior with respect to the\ninference budget, achieving high pass@k on question-answering benchmarks,\nincluding the challenging BrowseComp task. Building on these observations, we\nintroduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability\nthrough format-based and rule-based rewards. SSRL enables models to iteratively\nrefine their knowledge utilization internally, without requiring access to\nexternal tools. Empirical evaluations demonstrate that SSRL-trained policy\nmodels provide a cost-effective and stable environment for search-driven RL\ntraining, reducing reliance on external search engines and facilitating robust\nsim-to-real transfer. We draw the following conclusions: 1) LLMs possess world\nknowledge that can be effectively elicited to achieve high performance; 2) SSRL\ndemonstrates the potential of leveraging internal knowledge to reduce\nhallucination; 3) SSRL-trained models integrate seamlessly with external search\nengines without additional effort. Our findings highlight the potential of LLMs\nto support more scalable RL agent training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10678", "abs": "https://arxiv.org/abs/2508.10678", "authors": ["Zhaoyuan Qi", "Weihua Gao", "Wenlong Niu", "Jie Tang", "Yun Li", "Xiaodong Peng"], "title": "HyperTea: A Hypergraph-based Temporal Enhancement and Alignment Network for Moving Infrared Small Target Detection", "comment": null, "summary": "In practical application scenarios, moving infrared small target detection\n(MIRSTD) remains highly challenging due to the target's small size, weak\nintensity, and complex motion pattern. Existing methods typically only model\nlow-order correlations between feature nodes and perform feature extraction and\nenhancement within a single temporal scale. Although hypergraphs have been\nwidely used for high-order correlation learning, they have received limited\nattention in MIRSTD. To explore the potential of hypergraphs and enhance\nmulti-timescale feature representation, we propose HyperTea, which integrates\nglobal and local temporal perspectives to effectively model high-order\nspatiotemporal correlations of features. HyperTea consists of three modules:\nthe global temporal enhancement module (GTEM) realizes global temporal context\nenhancement through semantic aggregation and propagation; the local temporal\nenhancement module (LTEM) is designed to capture local motion patterns between\nadjacent frames and then enhance local temporal context; additionally, we\nfurther develop a temporal alignment module (TAM) to address potential\ncross-scale feature misalignment. To our best knowledge, HyperTea is the first\nwork to integrate convolutional neural networks (CNNs), recurrent neural\nnetworks (RNNs), and hypergraph neural networks (HGNNs) for MIRSTD,\nsignificantly improving detection performance. Experiments on DAUB and IRDST\ndemonstrate its state-of-the-art (SOTA) performance. Our source codes are\navailable at https://github.com/Lurenjia-LRJ/HyperTea.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10528", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10528", "abs": "https://arxiv.org/abs/2508.10528", "authors": ["Ziye Deng", "Ruihan He", "Jiaxiang Liu", "Yuan Wang", "Zijie Meng", "Songtao Jiang", "Yong Xie", "Zuozhu Liu"], "title": "Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset", "comment": null, "summary": "Medical image grounding aims to align natural language phrases with specific\nregions in medical images, serving as a foundational task for intelligent\ndiagnosis, visual question answering (VQA), and automated report generation\n(MRG). However, existing research is constrained by limited modality coverage,\ncoarse-grained annotations, and the absence of a unified, generalizable\ngrounding framework. To address these challenges, we construct a large-scale\nmedical grounding dataset Med-GLIP-5M comprising over 5.3 million region-level\nannotations across seven imaging modalities, covering diverse anatomical\nstructures and pathological findings. The dataset supports both segmentation\nand grounding tasks with hierarchical region labels, ranging from organ-level\nboundaries to fine-grained lesions. Based on this foundation, we propose\nMed-GLIP, a modality-aware grounding framework trained on Med-GLIP-5M. Rather\nthan relying on explicitly designed expert modules, Med-GLIP implicitly\nacquires hierarchical semantic understanding from diverse training data --\nenabling it to recognize multi-granularity structures, such as distinguishing\nlungs from pneumonia lesions. Extensive experiments demonstrate that Med-GLIP\nconsistently outperforms state-of-the-art baselines across multiple grounding\nbenchmarks. Furthermore, integrating its spatial outputs into downstream tasks,\nincluding medical VQA and report generation, leads to substantial performance\ngains. Our dataset will be released soon.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering", "fine-grained"], "score": 3}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10556", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10556", "abs": "https://arxiv.org/abs/2508.10556", "authors": ["Ruisong Han", "Zongbo Han", "Jiahao Zhang", "Mingyue Cheng", "Changqing Zhang"], "title": "Retrieval-Augmented Prompt for OOD Detection", "comment": null, "summary": "Out-of-Distribution (OOD) detection is crucial for the reliable deployment of\nmachine learning models in-the-wild, enabling accurate identification of test\nsamples that differ from the training data distribution. Existing methods rely\non auxiliary outlier samples or in-distribution (ID) data to generate outlier\ninformation for training, but due to limited outliers and their mismatch with\nreal test OOD samples, they often fail to provide sufficient semantic\nsupervision, leading to suboptimal performance. To address this, we propose a\nnovel OOD detection method called Retrieval-Augmented Prompt (RAP). RAP\naugments a pre-trained vision-language model's prompts by retrieving external\nknowledge, offering enhanced semantic supervision for OOD detection. During\ntraining, RAP retrieves descriptive words for outliers based on joint\nsimilarity with external textual knowledge and uses them to augment the model's\nOOD prompts. During testing, RAP dynamically updates OOD prompts in real-time\nbased on the encountered OOD samples, enabling the model to rapidly adapt to\nthe test environment. Our extensive experiments demonstrate that RAP achieves\nstate-of-the-art performance on large-scale OOD detection benchmarks. For\nexample, in 1-shot OOD detection on the ImageNet-1k dataset, RAP reduces the\naverage FPR95 by 7.05% and improves the AUROC by 1.71% compared to previous\nmethods. Additionally, comprehensive ablation studies validate the\neffectiveness of each module and the underlying motivations of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10711", "abs": "https://arxiv.org/abs/2508.10711", "authors": ["NextStep Team", "Chunrui Han", "Guopeng Li", "Jingwei Wu", "Quan Sun", "Yan Cai", "Yuang Peng", "Zheng Ge", "Deyu Zhou", "Haomiao Tang", "Hongyu Zhou", "Kenkun Liu", "Ailin Huang", "Bin Wang", "Changxin Miao", "Deshan Sun", "En Yu", "Fukun Yin", "Gang Yu", "Hao Nie", "Haoran Lv", "Hanpeng Hu", "Jia Wang", "Jian Zhou", "Jianjian Sun", "Kaijun Tan", "Kang An", "Kangheng Lin", "Liang Zhao", "Mei Chen", "Peng Xing", "Rui Wang", "Shiyu Liu", "Shutao Xia", "Tianhao You", "Wei Ji", "Xianfang Zeng", "Xin Han", "Xuelin Zhang", "Yana Wei", "Yanming Xu", "Yimin Jiang", "Yingming Wang", "Yu Zhou", "Yucheng Han", "Ziyang Meng", "Binxing Jiao", "Daxin Jiang", "Xiangyu Zhang", "Yibo Zhu"], "title": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "comment": "Code: https://github.com/stepfun-ai/NextStep-1", "summary": "Prevailing autoregressive (AR) models for text-to-image generation either\nrely on heavy, computationally-intensive diffusion models to process continuous\nimage tokens, or employ vector quantization (VQ) to obtain discrete tokens with\nquantization loss. In this paper, we push the autoregressive paradigm forward\nwith NextStep-1, a 14B autoregressive model paired with a 157M flow matching\nhead, training on discrete text tokens and continuous image tokens with\nnext-token prediction objectives. NextStep-1 achieves state-of-the-art\nperformance for autoregressive models in text-to-image generation tasks,\nexhibiting strong capabilities in high-fidelity image synthesis. Furthermore,\nour method shows strong performance in image editing, highlighting the power\nand versatility of our unified approach. To facilitate open research, we will\nrelease our code and models to the community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10712", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10712", "abs": "https://arxiv.org/abs/2508.10712", "authors": ["Fabian Kresse", "Georgios Pilikos", "Mario Azcueta", "Nicolas Floury"], "title": "Lightweight CNNs for Embedded SAR Ship Target Detection and Classification", "comment": "Accepted at Big Data from Space 2025 (BiDS'25)", "summary": "Synthetic Aperture Radar (SAR) data enables large-scale surveillance of\nmaritime vessels. However, near-real-time monitoring is currently constrained\nby the need to downlink all raw data, perform image focusing, and subsequently\nanalyze it on the ground. On-board processing to generate higher-level products\ncould reduce the data volume that needs to be downlinked, alleviating bandwidth\nconstraints and minimizing latency. However, traditional image focusing and\nprocessing algorithms face challenges due to the satellite's limited memory,\nprocessing power, and computational resources. This work proposes and evaluates\nneural networks designed for real-time inference on unfocused SAR data acquired\nin Stripmap and Interferometric Wide (IW) modes captured with Sentinel-1. Our\nresults demonstrate the feasibility of using one of our models for on-board\nprocessing and deployment on an FPGA. Additionally, by investigating a binary\nclassification task between ships and windmills, we demonstrate that target\nclassification is possible.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10771", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10771", "abs": "https://arxiv.org/abs/2508.10771", "authors": ["Jieyu Li", "Xin Zhang", "Joey Tianyi Zhou"], "title": "AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences", "comment": "Proceedings of the 33rd ACM International Conference on Multimedia", "summary": "Recent advances in AI-generated content have fueled the rise of highly\nrealistic synthetic videos, posing severe risks to societal trust and digital\nintegrity. Existing benchmarks for video authenticity detection typically\nsuffer from limited realism, insufficient scale, and inadequate complexity,\nfailing to effectively evaluate modern vision-language models against\nsophisticated forgeries. To address this critical gap, we introduce AEGIS, a\nnovel large-scale benchmark explicitly targeting the detection of\nhyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises\nover 10,000 rigorously curated real and synthetic videos generated by diverse,\nstate-of-the-art generative models, including Stable Video Diffusion,\nCogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary\narchitectures. In particular, AEGIS features specially constructed challenging\nsubsets enhanced with robustness evaluation. Furthermore, we provide multimodal\nannotations spanning Semantic-Authenticity Descriptions, Motion Features, and\nLow-level Visual Features, facilitating authenticity detection and supporting\ndownstream tasks such as multimodal fusion and forgery localization. Extensive\nexperiments using advanced vision-language models demonstrate limited detection\ncapabilities on the most challenging subsets of AEGIS, highlighting the\ndataset's unique complexity and realism beyond the current generalization\ncapabilities of existing models. In essence, AEGIS establishes an indispensable\nevaluation benchmark, fundamentally advancing research toward developing\ngenuinely robust, reliable, broadly generalizable video authenticity detection\nmethodologies capable of addressing real-world forgery threats. Our dataset is\navailable on https://huggingface.co/datasets/Clarifiedfish/AEGIS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10839", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2508.10839", "abs": "https://arxiv.org/abs/2508.10839", "authors": ["Jim Dilkes", "Vahid Yazdanpanah", "Sebastian Stein"], "title": "Reinforced Language Models for Sequential Decision Making", "comment": null, "summary": "Large Language Models (LLMs) show potential as sequential decision-making\nagents, but their application is often limited due to a reliance on large,\ncomputationally expensive models. This creates a need to improve smaller\nmodels, yet existing post-training methods are designed for single-turn\ninteractions and cannot handle credit assignment in multi-step agentic tasks.\nTo address this, we introduce Multi-Step Group-Relative Policy Optimization\n(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal\nText-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)\nframeworks. For credit assignment, MS-GRPO attributes the entire cumulative\nepisode reward to each individual episode step. We supplement this algorithm\nwith a novel absolute-advantage-weighted episode sampling strategy that we show\nimproves training performance. We evaluate our approach by post-training a\n3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate\nthat the method is effective in improving decision-making performance: our\npost-trained 3B parameter model outperforms a 72B parameter baseline by 50% on\nthe Frozen Lake task. This work demonstrates that targeted post-training is a\npractical and efficient alternative to relying on model scale for creating\nsequential decision-making agents using LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10771", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10771", "abs": "https://arxiv.org/abs/2508.10771", "authors": ["Jieyu Li", "Xin Zhang", "Joey Tianyi Zhou"], "title": "AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences", "comment": "Proceedings of the 33rd ACM International Conference on Multimedia", "summary": "Recent advances in AI-generated content have fueled the rise of highly\nrealistic synthetic videos, posing severe risks to societal trust and digital\nintegrity. Existing benchmarks for video authenticity detection typically\nsuffer from limited realism, insufficient scale, and inadequate complexity,\nfailing to effectively evaluate modern vision-language models against\nsophisticated forgeries. To address this critical gap, we introduce AEGIS, a\nnovel large-scale benchmark explicitly targeting the detection of\nhyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises\nover 10,000 rigorously curated real and synthetic videos generated by diverse,\nstate-of-the-art generative models, including Stable Video Diffusion,\nCogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary\narchitectures. In particular, AEGIS features specially constructed challenging\nsubsets enhanced with robustness evaluation. Furthermore, we provide multimodal\nannotations spanning Semantic-Authenticity Descriptions, Motion Features, and\nLow-level Visual Features, facilitating authenticity detection and supporting\ndownstream tasks such as multimodal fusion and forgery localization. Extensive\nexperiments using advanced vision-language models demonstrate limited detection\ncapabilities on the most challenging subsets of AEGIS, highlighting the\ndataset's unique complexity and realism beyond the current generalization\ncapabilities of existing models. In essence, AEGIS establishes an indispensable\nevaluation benchmark, fundamentally advancing research toward developing\ngenuinely robust, reliable, broadly generalizable video authenticity detection\nmethodologies capable of addressing real-world forgery threats. Our dataset is\navailable on https://huggingface.co/datasets/Clarifiedfish/AEGIS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10794", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10794", "abs": "https://arxiv.org/abs/2508.10794", "authors": ["De-Xing Huang", "Xiao-Hu Zhou", "Mei-Jiang Gui", "Xiao-Liang Xie", "Shi-Qi Liu", "Shuang-Yi Wang", "Tian-Yu Xiang", "Rui-Ze Ma", "Nu-Fang Xiao", "Zeng-Guang Hou"], "title": "VasoMIM: Vascular Anatomy-Aware Masked Image Modeling for Vessel Segmentation", "comment": "14 pages, 11 figures", "summary": "Accurate vessel segmentation in X-ray angiograms is crucial for numerous\nclinical applications. However, the scarcity of annotated data presents a\nsignificant challenge, which has driven the adoption of self-supervised\nlearning (SSL) methods such as masked image modeling (MIM) to leverage\nlarge-scale unlabeled data for learning transferable representations.\nUnfortunately, conventional MIM often fails to capture vascular anatomy because\nof the severe class imbalance between vessel and background pixels, leading to\nweak vascular representations. To address this, we introduce Vascular\nanatomy-aware Masked Image Modeling (VasoMIM), a novel MIM framework tailored\nfor X-ray angiograms that explicitly integrates anatomical knowledge into the\npre-training process. Specifically, it comprises two complementary components:\nanatomy-guided masking strategy and anatomical consistency loss. The former\npreferentially masks vessel-containing patches to focus the model on\nreconstructing vessel-relevant regions. The latter enforces consistency in\nvascular semantics between the original and reconstructed images, thereby\nimproving the discriminability of vascular representations. Empirically,\nVasoMIM achieves state-of-the-art performance across three datasets. These\nfindings highlight its potential to facilitate X-ray angiogram analysis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10858", "abs": "https://arxiv.org/abs/2508.10858", "authors": ["Harold Haodong Chen", "Haojian Huang", "Qifeng Chen", "Harry Yang", "Ser-Nam Lim"], "title": "Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation", "comment": "Project Page: https://haroldchen19.github.io/PhysHPO-Page/", "summary": "Recent advancements in video generation have enabled the creation of\nhigh-quality, visually compelling videos. However, generating videos that\nadhere to the laws of physics remains a critical challenge for applications\nrequiring realism and accuracy. In this work, we propose PhysHPO, a novel\nframework for Hierarchical Cross-Modal Direct Preference Optimization, to\ntackle this challenge by enabling fine-grained preference alignment for\nphysically plausible video generation. PhysHPO optimizes video alignment across\nfour hierarchical granularities: a) Instance Level, aligning the overall video\ncontent with the input prompt; b) State Level, ensuring temporal consistency\nusing boundary frames as anchors; c) Motion Level, modeling motion trajectories\nfor realistic dynamics; and d) Semantic Level, maintaining logical consistency\nbetween narrative and visuals. Recognizing that real-world videos are the best\nreflections of physical phenomena, we further introduce an automated data\nselection pipeline to efficiently identify and utilize \"good data\" from\nexisting large-scale text-video datasets, thereby eliminating the need for\ncostly and time-intensive dataset construction. Extensive experiments on both\nphysics-focused and general capability benchmarks demonstrate that PhysHPO\nsignificantly improves physical plausibility and overall video generation\nquality of advanced models. To the best of our knowledge, this is the first\nwork to explore fine-grained preference alignment and data selection for video\ngeneration, paving the way for more realistic and human-preferred video\ngeneration paradigms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10868", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10868", "abs": "https://arxiv.org/abs/2508.10868", "authors": ["Yibo Zhang", "Li Zhang", "Rui Ma", "Nan Cao"], "title": "TexVerse: A Universe of 3D Objects with High-Resolution Textures", "comment": null, "summary": "We introduce TexVerse, a large-scale 3D dataset featuring high-resolution\ntextures. While recent advances in large-scale 3D datasets have enhanced\nhigh-resolution geometry generation, creating high-resolution textures\nend-to-end remains underexplored due to the lack of suitable datasets. TexVerse\nfills this gap with a curated collection of over 858K unique high-resolution 3D\nmodels sourced from Sketchfab, including more than 158K models with physically\nbased rendering (PBR) materials. Each model encompasses all of its\nhigh-resolution variants, bringing the total to 1.6M 3D instances. TexVerse\nalso includes specialized subsets: TexVerse-Skeleton, with 69K rigged models,\nand TexVerse-Animation, with 54K animated models, both preserving original\nskeleton and animation data uploaded by the user. We also provide detailed\nmodel annotations describing overall characteristics, structural components,\nand intricate features. TexVerse offers a high-quality data resource with\nwide-ranging potential applications in texture synthesis, PBR material\ndevelopment, animation, and various 3D vision and graphics tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10893", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10893", "abs": "https://arxiv.org/abs/2508.10893", "authors": ["Yushi Lan", "Yihang Luo", "Fangzhou Hong", "Shangchen Zhou", "Honghua Chen", "Zhaoyang Lyu", "Shuai Yang", "Bo Dai", "Chen Change Loy", "Xingang Pan"], "title": "STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer", "comment": "TL;DR: Streaming 4D reconstruction using causal transformer. Project\n  page: https://nirvanalan.github.io/projects/stream3r", "summary": "We present STream3R, a novel approach to 3D reconstruction that reformulates\npointmap prediction as a decoder-only Transformer problem. Existing\nstate-of-the-art methods for multi-view reconstruction either depend on\nexpensive global optimization or rely on simplistic memory mechanisms that\nscale poorly with sequence length. In contrast, STream3R introduces an\nstreaming framework that processes image sequences efficiently using causal\nattention, inspired by advances in modern language modeling. By learning\ngeometric priors from large-scale 3D datasets, STream3R generalizes well to\ndiverse and challenging scenarios, including dynamic scenes where traditional\nmethods often fail. Extensive experiments show that our method consistently\noutperforms prior work across both static and dynamic scene benchmarks.\nMoreover, STream3R is inherently compatible with LLM-style training\ninfrastructure, enabling efficient large-scale pretraining and fine-tuning for\nvarious downstream 3D tasks. Our results underscore the potential of causal\nTransformer models for online 3D perception, paving the way for real-time 3D\nunderstanding in streaming environments. More details can be found in our\nproject page: https://nirvanalan.github.io/projects/stream3r.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
{"id": "2508.10897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10897", "abs": "https://arxiv.org/abs/2508.10897", "authors": ["Mengyuan Liu", "Xinshun Wang", "Zhongbin Fang", "Deheng Ye", "Xia Li", "Tao Tang", "Songtao Wu", "Xiangtai Li", "Ming-Hsuan Yang"], "title": "Human-in-Context: Unified Cross-Domain 3D Human Motion Modeling via In-Context Learning", "comment": null, "summary": "This paper aims to model 3D human motion across domains, where a single model\nis expected to handle multiple modalities, tasks, and datasets. Existing\ncross-domain models often rely on domain-specific components and multi-stage\ntraining, which limits their practicality and scalability. To overcome these\nchallenges, we propose a new setting to train a unified cross-domain model\nthrough a single process, eliminating the need for domain-specific components\nand multi-stage training. We first introduce Pose-in-Context (PiC), which\nleverages in-context learning to create a pose-centric cross-domain model.\nWhile PiC generalizes across multiple pose-based tasks and datasets, it\nencounters difficulties with modality diversity, prompting strategy, and\ncontextual dependency handling. We thus propose Human-in-Context (HiC), an\nextension of PiC that broadens generalization across modalities, tasks, and\ndatasets. HiC combines pose and mesh representations within a unified\nframework, expands task coverage, and incorporates larger-scale datasets.\nAdditionally, HiC introduces a max-min similarity prompt sampling strategy to\nenhance generalization across diverse domains and a network architecture with\ndual-branch context injection for improved handling of contextual dependencies.\nExtensive experimental results show that HiC performs better than PiC in terms\nof generalization, data scale, and performance across a wide range of domains.\nThese results demonstrate the potential of HiC for building a unified\ncross-domain 3D human motion model with improved flexibility and scalability.\nThe source codes and models are available at\nhttps://github.com/BradleyWang0416/Human-in-Context.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}}, "source_file": "2025-08-15.jsonl"}
